# Core Snapshot for Project: /home/zhz/zhz_agent

--- Structure for: zhz_rag_pipeline_dagster/ ---
|-- zhz_rag_pipeline/
|-- zhz_rag_pipeline_dagster_project.egg-info/
|-- pyproject.toml
|-- setup.py
|-- zhz_rag_pipeline/
    |-- __init__.py
    |-- custom_io_managers.py
    |-- definitions.py
    |-- document_parsers.py
    |-- evaluation_assets.py
    |-- ingestion_assets.py
    |-- processing_assets.py
    |-- pydantic_models_dagster.py
    |-- resources.py
    |-- parsers/
        |-- __init__.py
        |-- docx_parser.py
        |-- html_parser.py
        |-- md_parser.py
        |-- pdf_parser.py
        |-- txt_parser.py
        |-- xlsx_parser.py
|-- zhz_rag_pipeline_dagster_project.egg-info/
    |-- PKG-INFO
    |-- SOURCES.txt
    |-- dependency_links.txt
    |-- requires.txt
    |-- top_level.txt

--- Code Contents for: zhz_rag_pipeline_dagster/ ---

File: zhz_rag_pipeline_dagster/setup.py
---------------------------------------
```python
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/setup.py
from setuptools import find_packages, setup

setup(
    name="zhz_rag_pipeline_dagster_project", # 给一个包名
    version="0.0.1",
    packages=find_packages(), # 会找到 zhz_rag_pipeline 这个包
    install_requires=[
        "dagster",
        "dagster-webserver",
        # 添加其他 zhz_rag_pipeline_dagster 项目直接依赖的库
        # 例如 kuzu, dagster-pydantic (如果之后还要用) 等
        # 但核心的 zhz_rag 包的依赖不在这里列出，它应该是独立安装的
    ],
)
```

File: zhz_rag_pipeline_dagster/zhz_rag_pipeline/__init__.py
-----------------------------------------------------------
```python
# zhz_rag_pipeline/__init__.py
# This file makes Python treat the directory as a package.
# You can also define a __version__ or import key modules here if needed.
```

File: zhz_rag_pipeline_dagster/zhz_rag_pipeline/custom_io_managers.py
---------------------------------------------------------------------
```python
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/zhz_rag_pipeline/custom_io_managers.py
import json
import os
from typing import List, Type, Union, get_args, get_origin, Any, Optional 
from dagster import UPathIOManager, InputContext, OutputContext, DagsterInvariantViolationError
from pydantic import BaseModel as PydanticBaseModel
from upath import UPath

class PydanticListJsonIOManager(UPathIOManager):
    extension: str = ".jsonl"

    def __init__(self, base_dir: Optional[str] = None): # Changed base_path to base_dir for clarity
        resolved_base_dir: UPath
        if base_dir:
            resolved_base_dir = UPath(base_dir).resolve() # Resolve to absolute path
        else:
            # Default to <DAGSTER_HOME>/storage/pydantic_jsonl_io
            # DAGSTER_HOME defaults to ~/.dagster, but can be overridden by env var
            dagster_home_str = os.getenv("DAGSTER_HOME", os.path.join(os.path.expanduser("~"), ".dagster"))
            resolved_base_dir = UPath(dagster_home_str) / "storage" / "pydantic_jsonl_io"
        
        # Ensure the directory exists
        try:
            resolved_base_dir.mkdir(parents=True, exist_ok=True)
        except Exception as e:
            # Log this error appropriately, perhaps using a direct print if logger isn't set up yet
            # or re-raise as a Dagster-specific error.
            print(f"[PydanticListJsonIOManager __init__] ERROR: Could not create bafef __init__(self, base_dir: Optse directory {resolved_base_dir}: {e}")
            # Depending on Dagster's init sequence, context.log might not be available here.
            # It's safer to let UPathIOManager handle its own base_path or ensure dir exists before.
            # For now, we proceed, UPathIOManager might handle it or fail later.

        super().__init__(base_path=resolved_base_dir)
        # Log the final base path used by the UPathIOManager instance
        # self.log available after super().__init__() in ConfigurableIOManager context
        # For direct instantiation, we might need to pass a logger or use a global one.
        # print(f"[PydanticListJsonIOManager __init__] Initialized with resolved base_path: {self.base_path}")


    def dump_to_path(self, context: OutputContext, obj: List[PydanticBaseModel], path: UPath):
        context.log.info(f"[PydanticListJsonIOManager dump_to_path] Attempting to dump to resolved path: {path.resolve()}")
        
        if not isinstance(obj, list):
            msg = f"Expected a list of Pydantic models, got {type(obj)}"
            context.log.error(msg)
            raise TypeError(msg)
        
        # Optional: More robust type checking for list items if needed, using context.dagster_type
        # For now, assume obj is List[PydanticBaseModel] based on upstream asset's type hint.

        try:
            with path.open("w", encoding="utf-8") as f:
                for model_instance in obj:
                    if not isinstance(model_instance, PydanticBaseModel):
                        context.log.warning(f"Item in list is not a Pydantic model: {type(model_instance)}. Skipping.")
                        continue
                    json_str = model_instance.json() # Pydantic V1
                    f.write(json_str + "\n")
            context.log.info(f"[PydanticListJsonIOManager dump_to_path] Successfully dumped {len(obj)} items to {path.resolve()}")
        except Exception as e:
            context.log.error(f"[PydanticListJsonIOManager dump_to_path] Failed to dump object to {path.resolve()}: {e}", exc_info=True)
            raise

    def load_from_path(self, context: InputContext, path: UPath) -> List[PydanticBaseModel]:
        context.log.info(f"[PydanticListJsonIOManager load_from_path] Attempting to load from resolved path: {path.resolve()}")
        
        list_typing_type = context.dagster_type.typing_type
        origin = get_origin(list_typing_type)
        args = get_args(list_typing_type)

        if not (origin is list and args and issubclass(args[0], PydanticBaseModel)):
            msg = (
                f"PydanticListJsonIOManager can only handle inputs of type List[PydanticModel], "
                f"but got {list_typing_type} for input '{context.name}'."
            )
            context.log.error(msg)
            raise DagsterInvariantViolationError(msg) # Use Dagster specific error
        
        model_type: Type[PydanticBaseModel] = args[0]
        context.log.info(f"[PydanticListJsonIOManager load_from_path] Target model type for list items: {model_type.__name__}")

        loaded_models: List[PydanticBaseModel] = []
        if not path.exists():
            context.log.warning(f"[PydanticListJsonIOManager load_from_path] File not found at {path.resolve()}, returning empty list for input '{context.name}'.")
            return loaded_models

        try:
            with path.open("r", encoding="utf-8") as f:
                for line_number, line in enumerate(f, 1):
                    line_content = line.strip()
                    if not line_content:
                        continue
                    try:
                        model_instance = model_type.parse_raw(line_content) # Pydantic V1
                        loaded_models.append(model_instance)
                    except Exception as e_parse:
                        context.log.error(
                            f"[PydanticListJsonIOManager load_from_path] Failed to parse JSON line {line_number} "
                            f"into {model_type.__name__} from {path.resolve()}: {e_parse}. "
                            f"Line content (first 100 chars): '{line_content[:100]}...'",
                            exc_info=True
                        )
                        # Optionally re-raise or decide to skip problematic lines
                        # For now, we'll skip
            context.log.info(f"[PydanticListJsonIOManager load_from_path] Successfully loaded {len(loaded_models)} instances of {model_type.__name__} from {path.resolve()}")
        except Exception as e_read:
            context.log.error(f"[PydanticListJsonIOManager load_from_path] Failed to read or process file {path.resolve()}: {e_read}", exc_info=True)
            raise # Re-raise if file reading itself fails catastrophically
            
        return loaded_models
```

File: zhz_rag_pipeline_dagster/zhz_rag_pipeline/definitions.py
--------------------------------------------------------------
```python
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/zhz_rag_pipeline/definitions.py
import dagster as dg
import os

from dagster import (
    define_asset_job,
    Definitions,
    in_process_executor,
    AssetSelection
)

from zhz_rag_pipeline_dagster.zhz_rag_pipeline.ingestion_assets import all_ingestion_assets
from zhz_rag_pipeline_dagster.zhz_rag_pipeline.processing_assets import all_processing_assets

from zhz_rag_pipeline_dagster.zhz_rag_pipeline.resources import (
    GGUFEmbeddingResource, GGUFEmbeddingResourceConfig,
    ChromaDBResource, ChromaDBResourceConfig,
    LocalLLMAPIResource, LocalLLMAPIResourceConfig,
    DuckDBResource,
    GeminiAPIResource, GeminiAPIResourceConfig,
    SystemResource  # <--- 添加导入 SystemResource
)
from zhz_rag_pipeline_dagster.zhz_rag_pipeline.custom_io_managers import PydanticListJsonIOManager

all_defined_assets = all_ingestion_assets + all_processing_assets

# --- 修改作业名称 ---
# 将 kuzu_kg_write_job 重命名为 duckdb_kg_build_job
duckdb_kg_build_job = define_asset_job( # <--- 修改变量名
    name="duckdb_kg_build_job",         # <--- 修改作业的实际名称
    selection=AssetSelection.all(),
    executor_def=in_process_executor
)
# --- 修改结束 ---

pydantic_io_manager_instance = PydanticListJsonIOManager()

defs = Definitions(
    assets=all_defined_assets,
    jobs=[duckdb_kg_build_job], # <--- 在 jobs 列表中使用新的作业变量名
    resources={
        "embedder": GGUFEmbeddingResource(
            embedding_model_path=os.getenv("EMBEDDING_MODEL_PATH"),
            n_ctx=int(os.getenv("EMBEDDING_N_CTX", GGUFEmbeddingResourceConfig.model_fields['n_ctx'].default)),
            n_gpu_layers=int(os.getenv("EMBEDDING_N_GPU_LAYERS", GGUFEmbeddingResourceConfig.model_fields['n_gpu_layers'].default))
        ),
        "chroma_db": ChromaDBResource(collection_name=ChromaDBResourceConfig().collection_name, persist_directory=ChromaDBResourceConfig().persist_directory),
        "LocalLLM_api": LocalLLMAPIResource(api_url=LocalLLMAPIResourceConfig().api_url, default_temperature=LocalLLMAPIResourceConfig().default_temperature, default_max_new_tokens=LocalLLMAPIResourceConfig().default_max_new_tokens),
        "duckdb_kg": DuckDBResource(),
        "gemini_api": GeminiAPIResource(model_name=GeminiAPIResourceConfig().model_name, proxy_url=GeminiAPIResourceConfig().proxy_url, default_temperature=GeminiAPIResourceConfig().default_temperature, default_max_tokens=GeminiAPIResourceConfig().default_max_tokens),
        "pydantic_json_io_manager": pydantic_io_manager_instance,
        "system_info": SystemResource()  # <--- 添加 SystemResource 实例
    }
)
```

File: zhz_rag_pipeline_dagster/zhz_rag_pipeline/document_parsers.py
-------------------------------------------------------------------
```python
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/zhz_rag_pipeline/document_parsers.py
import os
from markdown_it import MarkdownIt
import logging
from typing import List, Dict, Any, Optional, Union, Literal 

# --- 添加 Unstructured 的导入 ---
try:
    from unstructured.partition.docx import partition_docx
    from unstructured.documents.elements import Element as UnstructuredElement # 用于类型提示
    # 如果需要更细致的Unstructured元素类型，也可以导入，例如：
    # from unstructured.documents.elements import Title as UnstructuredTitle, NarrativeText as UnstructuredNarrativeText, etc.
    _UNSTRUCTURED_AVAILABLE = True
    print("INFO (document_parsers.py): Successfully imported Unstructured for DOCX.")
except ImportError:
    print("WARNING (document_parsers.py): Unstructured library not found. DOCX parsing will be a placeholder.")
    _UNSTRUCTURED_AVAILABLE = False
    class UnstructuredElement: pass # Dummy
# --- 结束添加 ---

# --- 导入我们定义的Pydantic模型 ---
# 假设这个文件和 pydantic_models_dagster.py 在同一个包下或能通过PYTHONPATH找到
try:
    from .pydantic_models_dagster import ( # 使用相对导入
        ParsedDocumentOutput,
        DocumentElementType, 
        TitleElement,
        NarrativeTextElement,
        ListItemElement,
        TableElement,
        CodeBlockElement,
        PageBreakElement,
        DocumentElementMetadata 
    )
    _PYDANTIC_MODELS_AVAILABLE_PARSERS = True
except ImportError:
    print("WARNING (document_parsers.py): Could not import Pydantic models. Using fallback Any/dict.")
    _PYDANTIC_MODELS_AVAILABLE_PARSERS = False
    class BaseModel: pass
    class DocumentElementMetadata(BaseModel): page_number: Optional[int] = None
    class ParsedDocumentOutput(BaseModel): parsed_text: str; elements: list; original_metadata: dict; summary: Optional[str] = None
    class TitleElement(BaseModel): element_type:str="title"; text:str; level:int; metadata: Optional[DocumentElementMetadata] = None
    class NarrativeTextElement(BaseModel): element_type:str="narrative_text"; text:str; metadata: Optional[DocumentElementMetadata] = None
    class ListItemElement(BaseModel): element_type:str="list_item"; text:str; level:int=0; ordered:bool=False; item_number:Optional[Union[int, str]]=None; metadata: Optional[DocumentElementMetadata] = None
    class TableElement(BaseModel): element_type:str="table"; markdown_representation:Optional[str]=None; metadata: Optional[DocumentElementMetadata] = None
    class CodeBlockElement(BaseModel): element_type:str="code_block"; code:str; language:Optional[str]=None; metadata: Optional[DocumentElementMetadata] = None
    class PageBreakElement(BaseModel): element_type:str="page_break"; metadata: Optional[DocumentElementMetadata] = None
    DocumentElementType = Any


logger = logging.getLogger(__name__) # 每个模块用自己的logger

# --- Markdown 解析逻辑 (从 poc_md_markdown_it.py 迁移并封装) ---



def _get_text_from_md_inline(inline_tokens: Optional[List[Any]]) -> str:
    # (这里是 get_text_from_inline_tokens 函数的完整代码)
    text_content = ""
    if inline_tokens is None: return ""
    for token in inline_tokens:
        if token.type == 'text':
            text_content += token.content
        elif token.type == 'code_inline':
            text_content += f"`{token.content}`"
        elif token.type == 'softbreak':
            text_content += ' ' 
        elif token.type == 'hardbreak':
            text_content += '\n'
        elif token.children: 
            text_content += _get_text_from_md_inline(token.children)
    return text_content

def _convert_md_tokens_to_elements_internal(tokens: list) -> List[Any]:
    # (这里是 convert_md_tokens_to_elements 函数的完整代码，但将其重命名为内部函数)
    # (并确保它在 _PYDANTIC_MODELS_AVAILABLE_PARSERS 为True时创建Pydantic实例，否则创建字典)
    elements: List[Any] = []
    idx = 0
    list_level_stack = [] 

    while idx < len(tokens):
        token = tokens[idx]

        if token.type == 'heading_open':
            level = int(token.tag[1:])
            idx_content = idx + 1
            text = ""
            if idx_content < len(tokens) and tokens[idx_content].type == 'inline':
                text = _get_text_from_md_inline(tokens[idx_content].children).strip()
            if _PYDANTIC_MODELS_AVAILABLE_PARSERS: elements.append(TitleElement(text=text, level=level))
            else: elements.append({"element_type": "title", "text": text, "level": level})
            idx = idx_content + 2 
            continue

        elif token.type == 'paragraph_open':
            is_list_item_para = False
            if list_level_stack and token.level >= list_level_stack[-1]["level"]:
                pass 
            if not is_list_item_para or not list_level_stack: 
                idx_content = idx + 1
                text = ""
                if idx_content < len(tokens) and tokens[idx_content].type == 'inline':
                    text = _get_text_from_md_inline(tokens[idx_content].children).strip()
                if text:
                    if _PYDANTIC_MODELS_AVAILABLE_PARSERS: elements.append(NarrativeTextElement(text=text))
                    else: elements.append({"element_type": "narrative_text", "text": text})
            idx = idx + 2 
            if idx < len(tokens) and tokens[idx-1].type == 'inline': 
                idx +=1 
            continue
        
        elif token.type == 'bullet_list_open':
            list_level_stack.append({"ordered": False, "level": token.level})
            idx += 1
            continue
        elif token.type == 'ordered_list_open':
            start_num = token.attrs.get('start', 1)
            list_level_stack.append({"ordered": True, "current_num": start_num, "level": token.level})
            idx += 1
            continue
        
        elif token.type == 'list_item_open':
            item_text = ""
            li_level = token.level
            next_token_idx = idx + 1
            if next_token_idx < len(tokens):
                next_token = tokens[next_token_idx]
                if next_token.type == 'paragraph_open' and next_token.level == li_level + 1 :
                    inline_idx = next_token_idx + 1
                    if inline_idx < len(tokens) and tokens[inline_idx].type == 'inline':
                        item_text = _get_text_from_md_inline(tokens[inline_idx].children).strip()
                elif next_token.type == 'inline' and next_token.level == li_level +1 :
                    item_text = _get_text_from_md_inline(next_token.children).strip()
            
            if list_level_stack:
                list_info = list_level_stack[-1]
                item_num_val = None
                if list_info["ordered"]:
                    item_num_val = list_info["current_num"]
                    list_info["current_num"] += 1
                
                if _PYDANTIC_MODELS_AVAILABLE_PARSERS:
                    elements.append(ListItemElement(
                        text=item_text, level=token.level, ordered=list_info["ordered"],
                        item_number=str(item_num_val) if item_num_val is not None else None))
                else:
                    elements.append({"element_type": "list_item", "text": item_text, "level":token.level, 
                                     "ordered":list_info["ordered"], "item_number":str(item_num_val) if item_num_val is not None else None})

            temp_idx = idx + 1; nesting_count = 0
            while temp_idx < len(tokens):
                if tokens[temp_idx].type == 'list_item_open' and tokens[temp_idx].level == li_level:
                    if nesting_count == 0: idx = temp_idx; break
                if tokens[temp_idx].type == 'list_item_open': nesting_count +=1
                if tokens[temp_idx].type == 'list_item_close':
                    if nesting_count == 0 and tokens[temp_idx].level == li_level: idx = temp_idx + 1; break
                    nesting_count -=1
                temp_idx += 1
            else: idx = temp_idx
            continue

        elif token.type in ['bullet_list_close', 'ordered_list_close']:
            if list_level_stack: list_level_stack.pop()
            idx += 1
            continue

        elif token.type == 'table_open':
            header_content = []; body_rows_cells = []; current_row_cells = []; in_thead = False
            temp_idx = idx + 1
            while temp_idx < len(tokens) and tokens[temp_idx].type != 'table_close':
                t_token = tokens[temp_idx]
                if t_token.type == 'thead_open': in_thead = True
                elif t_token.type == 'thead_close': in_thead = False
                elif t_token.type == 'tr_open': current_row_cells = []
                elif t_token.type in ['th_open', 'td_open']:
                    content_idx = temp_idx + 1
                    if content_idx < len(tokens) and tokens[content_idx].type == 'inline':
                        current_row_cells.append(_get_text_from_md_inline(tokens[content_idx].children).strip())
                elif t_token.type == 'tr_close':
                    if current_row_cells:
                        if in_thead or (not header_content and not body_rows_cells): header_content.append(list(current_row_cells))
                        else: body_rows_cells.append(list(current_row_cells))
                temp_idx += 1
            md_table_str = ""
            if header_content:
                md_table_str += "| " + " | ".join(header_content[0]) + " |\n"
                md_table_str += "| " + " | ".join(["---"] * len(header_content[0])) + " |\n"
            for row_data_list in body_rows_cells: md_table_str += "| " + " | ".join(row_data_list) + " |\n"
            if _PYDANTIC_MODELS_AVAILABLE_PARSERS: elements.append(TableElement(markdown_representation=md_table_str.strip()))
            else: elements.append({"element_type": "table", "markdown_representation": md_table_str.strip()})
            idx = temp_idx + 1 
            continue

        elif token.type == 'fence' or token.type == 'code_block':
            code_content = token.content.strip(); lang = token.info.strip() if token.info else None
            if _PYDANTIC_MODELS_AVAILABLE_PARSERS: elements.append(CodeBlockElement(code=code_content, language=lang))
            else: elements.append({"element_type": "code_block", "code": code_content, "language": lang})
            idx += 1
            continue
        
        elif token.type == 'hr':
            if _PYDANTIC_MODELS_AVAILABLE_PARSERS: elements.append(PageBreakElement())
            else: elements.append({"element_type": "page_break"})
            idx += 1
            continue
        
        elif token.type == 'blockquote_open':
            blockquote_text_parts = []; temp_idx = idx + 1; start_level = token.level
            while temp_idx < len(tokens):
                bq_token = tokens[temp_idx]
                if bq_token.type == 'blockquote_close' and bq_token.level == start_level: idx = temp_idx; break
                if bq_token.type == 'paragraph_open':
                    para_content_idx = temp_idx + 1
                    if para_content_idx < len(tokens) and tokens[para_content_idx].type == 'inline':
                        blockquote_text_parts.append(_get_text_from_md_inline(tokens[para_content_idx].children).strip())
                    temp_idx = para_content_idx + 1 
                    if temp_idx < len(tokens) and tokens[temp_idx].type == 'paragraph_close': temp_idx +=1
                    else: temp_idx -=1
                temp_idx +=1
            else: idx = temp_idx
            if blockquote_text_parts:
                full_text = "\n".join(blockquote_text_parts)
                if _PYDANTIC_MODELS_AVAILABLE_PARSERS: elements.append(NarrativeTextElement(text=full_text)) 
                else: elements.append({"element_type": "narrative_text", "text": full_text, "_is_blockquote": True})
            idx +=1
            continue
        idx += 1 
    return elements

def _generate_parsed_text_from_elements_internal(elements: List[Any]) -> str:
    # (这里是 generate_parsed_text_from_elements 函数的完整代码)
    # (确保它在 _PYDANTIC_MODELS_AVAILABLE_PARSERS 为True时能处理Pydantic实例，否则处理字典)
    text_parts = []
    for el_data_any in elements:
        el_data = {}
        if _PYDANTIC_MODELS_AVAILABLE_PARSERS and hasattr(el_data_any, 'model_dump'):
            el_data = el_data_any.model_dump() 
        elif isinstance(el_data_any, dict):
            el_data = el_data_any
        else: continue

        el_type = el_data.get("element_type")
        if el_type == "title": text_parts.append(f"\n{'#' * el_data.get('level',1)} {el_data.get('text','')}\n")
        elif el_type == "narrative_text": text_parts.append(el_data.get('text','') + "\n")
        elif el_type == "list_item":
            prefix = f"{el_data.get('item_number','')}. " if el_data.get('ordered') and el_data.get('item_number') else "- "
            indent = "  " * el_data.get('level',0)
            text_parts.append(f"{indent}{prefix}{el_data.get('text','')}\n")
        elif el_type == "table":
            if el_data.get('markdown_representation'): text_parts.append(f"\n[Table: {el_data.get('caption','Unnamed Table')}]\n{el_data['markdown_representation']}\n")
            elif el_data.get('text_representation'): text_parts.append(f"\n[Table: {el_data.get('caption','Unnamed Table')}]\n{el_data['text_representation']}\n")
        elif el_type == "code_block":
            lang = el_data.get('language', "") or ""
            text_parts.append(f"\n```{lang}\n{el_data.get('code','')}\n```\n")
        elif el_type == "page_break": text_parts.append("\n---\n")
        text_parts.append("\n") 
    return "".join(text_parts).strip().replace("\n\n\n", "\n\n").replace("\n\n\n", "\n\n")


def parse_markdown_to_structured_output(md_content_str: str, original_metadata: Dict[str, Any]) -> Optional[ParsedDocumentOutput]:
    """
    Top-level function to parse markdown string and return ParsedDocumentOutput.
    """
    logger.info(f"Parsing Markdown content (length: {len(md_content_str)} chars)...")
    try:
        md_parser = MarkdownIt("commonmark").enable("table") # Removed "breaks":True based on last log
        tokens = md_parser.parse(md_content_str)
        
        structured_elements = _convert_md_tokens_to_elements_internal(tokens)
        linear_text = _generate_parsed_text_from_elements_internal(structured_elements)

        if _PYDANTIC_MODELS_AVAILABLE_PARSERS:
            return ParsedDocumentOutput(
                parsed_text=linear_text,
                elements=structured_elements,
                original_metadata=original_metadata
            )
        else: # Fallback if Pydantic models aren't available (e.g. PoC context)
            return {
                 "parsed_text": linear_text,
                 "elements": structured_elements,
                 "original_metadata": original_metadata
            } # type: ignore 
    except Exception as e:
        logger.error(f"Error in parse_markdown_to_structured_output: {e}", exc_info=True)
        return None

# --- Placeholder for other parsers ---
def parse_docx_to_structured_output(file_path: str, original_metadata: Dict[str, Any]) -> Optional[ParsedDocumentOutput]:
    logger.info(f"Parsing DOCX: {file_path} (Not yet fully implemented in document_parsers.py)")
    # Here you would integrate the Unstructured logic from your PoC
    # For now, returning a basic structure
    text_content = f"[Placeholder: DOCX content for {os.path.basename(file_path)}]"
    elements = []
    if _PYDANTIC_MODELS_AVAILABLE_PARSERS:
        elements.append(NarrativeTextElement(text=text_content))
        return ParsedDocumentOutput(parsed_text=text_content, elements=elements, original_metadata=original_metadata)
    else:
        elements.append({"element_type":"narrative_text", "text":text_content})
        return {"parsed_text":text_content, "elements":elements, "original_metadata":original_metadata} # type: ignore

def parse_pdf_to_structured_output(file_path: str, original_metadata: Dict[str, Any]) -> Optional[ParsedDocumentOutput]:
    logger.info(f"Parsing PDF: {file_path} (Not yet fully implemented in document_parsers.py)")
    # Here you would integrate the PyMuPDF logic from your PoC
    text_content = f"[Placeholder: PDF content for {os.path.basename(file_path)}]"
    elements = []
    if _PYDANTIC_MODELS_AVAILABLE_PARSERS:
        elements.append(NarrativeTextElement(text=text_content))
        return ParsedDocumentOutput(parsed_text=text_content, elements=elements, original_metadata=original_metadata)
    else:
        elements.append({"element_type":"narrative_text", "text":text_content})
        return {"parsed_text":text_content, "elements":elements, "original_metadata":original_metadata} # type: ignore

def parse_xlsx_to_structured_output(file_path: str, original_metadata: Dict[str, Any]) -> Optional[ParsedDocumentOutput]:
    logger.info(f"Parsing XLSX: {file_path} (Not yet fully implemented in document_parsers.py)")
    # Here you would integrate the pandas logic from your PoC
    text_content = f"[Placeholder: XLSX content for {os.path.basename(file_path)}]"
    elements = []
    if _PYDANTIC_MODELS_AVAILABLE_PARSERS:
        elements.append(NarrativeTextElement(text=text_content)) # Or TableElement
        return ParsedDocumentOutput(parsed_text=text_content, elements=elements, original_metadata=original_metadata)
    else:
        elements.append({"element_type":"narrative_text", "text":text_content})
        return {"parsed_text":text_content, "elements":elements, "original_metadata":original_metadata} # type: ignore
        
def parse_html_to_structured_output(html_content_str: str, original_metadata: Dict[str, Any]) -> Optional[ParsedDocumentOutput]:
    logger.info(f"Parsing HTML content (length: {len(html_content_str)} chars) (Not yet fully implemented in document_parsers.py)")
    # Here you would integrate the BeautifulSoup logic from your PoC
    text_content = f"[Placeholder: HTML content snippet {html_content_str[:100]}]"
    elements = []
    if _PYDANTIC_MODELS_AVAILABLE_PARSERS:
        elements.append(NarrativeTextElement(text=text_content))
        return ParsedDocumentOutput(parsed_text=text_content, elements=elements, original_metadata=original_metadata)
    else:
        elements.append({"element_type":"narrative_text", "text":text_content})
        return {"parsed_text":text_content, "elements":elements, "original_metadata":original_metadata} # type: ignore
```

File: zhz_rag_pipeline_dagster/zhz_rag_pipeline/evaluation_assets.py
--------------------------------------------------------------------
```python
import dagster as dg
import os
from typing import Dict, List, Any # Optional 可能之后会用到

# 从项目中导入我们重构的批量评估函数和相关工具/常量
from zhz_rag.evaluation.batch_eval_cypher import run_cypher_batch_evaluation
from zhz_rag.evaluation.batch_eval_answer import run_answer_batch_evaluation
from zhz_rag.evaluation.analyze_cypher import perform_cypher_evaluation_analysis
from zhz_rag.evaluation.analyze_answer import perform_answer_evaluation_analysis
from zhz_rag.utils.common_utils import (
    find_latest_rag_interaction_log,
    RAG_INTERACTION_LOGS_DIR,
    EVALUATION_RESULTS_LOGS_DIR,
    get_evaluation_result_log_filepath
)
# 导入 GeminiAPIResource 以声明资源依赖
from zhz_rag_pipeline_dagster.zhz_rag_pipeline.resources import GeminiAPIResource

# --- 资产定义 ---

@dg.asset(
    name="latest_rag_interaction_log_for_evaluation",
    description="Provides the filepath of the latest RAG interaction log to be used for evaluation.",
    group_name="evaluation_pipeline",
    compute_kind="python" # 可选，指明计算类型
)
def latest_rag_interaction_log_for_evaluation_asset(context: dg.AssetExecutionContext) -> str:
    """
    Finds and returns the path to the latest RAG interaction log file.
    """
    log_filepath = find_latest_rag_interaction_log(RAG_INTERACTION_LOGS_DIR)
    if not log_filepath or not os.path.exists(log_filepath):
        error_msg = f"No RAG interaction log file found in directory: {RAG_INTERACTION_LOGS_DIR}"
        context.log.error(error_msg)
        raise dg.Failure(description=error_msg)
    
    context.log.info(f"Using RAG interaction log for evaluation: {log_filepath}")
    context.add_output_metadata({"log_filepath": log_filepath, "filename": os.path.basename(log_filepath)})
    return log_filepath

@dg.asset(
    name="batch_cypher_evaluations_log", # 资产名称最好能反映它产出的是日志文件
    description="Runs batch evaluation of Cypher queries and produces an evaluation log file.",
    group_name="evaluation_pipeline",
    compute_kind="python",
    # deps=[latest_rag_interaction_log_for_evaluation_asset] # 通过函数参数自动推断依赖
)
async def batch_cypher_evaluation_log_asset(
    context: dg.AssetExecutionContext,
    gemini_api: GeminiAPIResource,
    latest_rag_interaction_log_for_evaluation: str # <--- 修改参数名
) -> dg.Output[str]:
    context.log.info(f"Starting batch Cypher evaluation using log file: {latest_rag_interaction_log_for_evaluation}") # <--- 使用新参数名
    
    # 从 Dagster 配置中获取参数，或使用默认/环境变量
    # 这里我们先用之前脚本中的方式，未来可以转为 Dagster run_config
    app_version = os.getenv("APP_VERSION_TAG", "dagster_cypher_eval_0.2")
    # 对于 use_simulated_api，在 Dagster 中通常会通过资源配置或 op_config 来控制，
    # 而不是直接依赖环境变量，这样更灵活。但为了保持与脚本一致，暂时保留。
    use_simulated = os.getenv("USE_SIMULATED_GEMINI_CYPHER_EVAL", "false").lower() == "true"
    api_delay = float(os.getenv("GEMINI_API_CALL_DELAY_SECONDS", "4.1"))

    if use_simulated:
        context.log.warning("Cypher evaluation asset is using SIMULATED Gemini API calls.")

    # 调用我们重构的、现在接受 gemini_resource 的批量评估函数
    eval_stats = await run_cypher_batch_evaluation(
        gemini_resource_for_evaluator=gemini_api, # 传递注入的 Dagster 资源
        rag_interaction_log_filepath=latest_rag_interaction_log_for_evaluation,
        app_version=app_version,
        use_simulated_api=use_simulated, # 这个参数现在由 run_cypher_batch_evaluation 内部处理
        api_call_delay=api_delay
    )
    context.log.info(f"Batch Cypher evaluation completed. Statistics: {eval_stats}")

    # 确定输出的评估结果日志文件名 (与 evaluator.py 中一致)
    output_log_filepath = get_evaluation_result_log_filepath(evaluation_name="cypher_gemini_flash")
    
    # 确保目录存在 (get_evaluation_result_log_filepath 内部的 log_interaction_data 会处理)
    # 但这里我们也可以提前确保，或者依赖 log_interaction_data
    os.makedirs(os.path.dirname(output_log_filepath), exist_ok=True)
            
    metadata = {"evaluation_stats": eval_stats, "output_filepath": output_log_filepath}
    if eval_stats.get("cypher_queries_evaluated", 0) == 0:
        metadata["warning"] = "No Cypher queries were evaluated. Output log might be empty."
        context.log.warning(metadata["warning"])

    return dg.Output(output_log_filepath, metadata=metadata)


@dg.asset(
    name="batch_answer_evaluations_log", # 资产名称
    description="Runs batch evaluation of generated answers from RAG logs using Gemini.",
    group_name="evaluation_pipeline",
    compute_kind="python",
    # deps=[latest_rag_interaction_log_for_evaluation_asset] # 通过函数参数自动推断依赖
)
async def batch_answer_evaluation_log_asset(
    context: dg.AssetExecutionContext,
    gemini_api: GeminiAPIResource,
    latest_rag_interaction_log_for_evaluation: str # <--- 修改参数名
) -> dg.Output[str]:
    context.log.info(f"Starting batch Answer evaluation using log file: {latest_rag_interaction_log_for_evaluation}") # <--- 使用新参数名
    app_version = os.getenv("APP_VERSION_TAG", "dagster_answer_eval_0.2")
    use_simulated = os.getenv("USE_SIMULATED_GEMINI_ANSWER_EVAL", "false").lower() == "true"
    api_delay = float(os.getenv("GEMINI_API_CALL_DELAY_SECONDS", "4.1"))

    if use_simulated:
        context.log.warning("Answer evaluation asset is using SIMULATED Gemini API calls.")

    eval_stats = await run_answer_batch_evaluation(
        gemini_resource_for_evaluator=gemini_api, # 传递注入的 Dagster 资源
        rag_interaction_log_filepath=latest_rag_interaction_log_for_evaluation,
        app_version=app_version,
        use_simulated_api=use_simulated, # 这个参数现在由 run_answer_batch_evaluation 内部处理
        api_call_delay=api_delay
    )
    context.log.info(f"Batch Answer evaluation completed. Statistics: {eval_stats}")

    output_log_filepath = get_evaluation_result_log_filepath(evaluation_name="answer_gemini_flash")
    os.makedirs(os.path.dirname(output_log_filepath), exist_ok=True)

    metadata = {"evaluation_stats": eval_stats, "output_filepath": output_log_filepath}
    if eval_stats.get("answers_evaluated", 0) == 0:
        metadata["warning"] = "No answers were evaluated. Output log might be empty."
        context.log.warning(metadata["warning"])
        
    return dg.Output(output_log_filepath, metadata=metadata)

@dg.asset(
    name="cypher_evaluation_analysis_report", # 资产名称
    description="Generates a CSV analysis report from Cypher evaluation results.",
    group_name="evaluation_pipeline",
    compute_kind="python",
    # deps=[batch_cypher_evaluation_log_asset] # 通过函数参数自动推断依赖
)
def cypher_analysis_report_asset(
    context: dg.AssetExecutionContext,
    batch_cypher_evaluations_log: str # 上游资产的输出 (即 cypher 评估日志文件的路径)
) -> dg.Output[str]: # 输出 CSV 报告文件的路径
    """
    Analyzes Cypher evaluation logs and produces a CSV report.
    """
    context.log.info(f"Starting Cypher evaluation analysis using log file: {batch_cypher_evaluations_log}")

    if not os.path.exists(batch_cypher_evaluations_log):
        error_msg = f"Input Cypher evaluation log file not found: {batch_cypher_evaluations_log}"
        context.log.error(error_msg)
        raise dg.Failure(description=error_msg)

    # 构建输出CSV文件的路径
    # 我们希望CSV文件也存储在 EVALUATION_RESULTS_LOGS_DIR 目录下
    # 文件名可以基于输入日志名或固定一个模式
    base_input_log_name = os.path.basename(batch_cypher_evaluations_log)
    # 从 "eval_results_cypher_gemini_flash_YYYYMMDD.jsonl" 生成 "analysis_cypher_gemini_flash_YYYYMMDD.csv"
    if base_input_log_name.startswith("eval_results_") and base_input_log_name.endswith(".jsonl"):
        analysis_file_name = "analysis_" + base_input_log_name[len("eval_results_"):-len(".jsonl")] + ".csv"
    else: # Fallback naming
        analysis_file_name = f"analysis_cypher_report_{context.run_id[:8]}.csv"
    
    output_csv_filepath = os.path.join(EVALUATION_RESULTS_LOGS_DIR, analysis_file_name)
    
    success = perform_cypher_evaluation_analysis(
        evaluation_log_filepath=batch_cypher_evaluations_log,
        output_csv_filepath=output_csv_filepath
    )

    if success:
        context.log.info(f"Cypher evaluation analysis report generated: {output_csv_filepath}")
        return dg.Output(output_csv_filepath, metadata={"output_csv_filepath": output_csv_filepath, "source_log": base_input_log_name})
    else:
        error_msg = f"Cypher evaluation analysis failed for log file: {batch_cypher_evaluations_log}"
        context.log.error(error_msg)
        raise dg.Failure(description=error_msg)


@dg.asset(
    name="answer_evaluation_analysis_report", # 资产名称
    description="Generates a CSV analysis report from Answer evaluation results.",
    group_name="evaluation_pipeline",
    compute_kind="python",
    # deps=[batch_answer_evaluations_log_asset] # 通过函数参数自动推断依赖
)
def answer_analysis_report_asset(
    context: dg.AssetExecutionContext,
    batch_answer_evaluations_log: str # 上游资产的输出 (即 answer 评估日志文件的路径)
) -> dg.Output[str]: # 输出 CSV 报告文件的路径
    """
    Analyzes Answer evaluation logs and produces a CSV report.
    """
    context.log.info(f"Starting Answer evaluation analysis using log file: {batch_answer_evaluations_log}")

    if not os.path.exists(batch_answer_evaluations_log):
        error_msg = f"Input Answer evaluation log file not found: {batch_answer_evaluations_log}"
        context.log.error(error_msg)
        raise dg.Failure(description=error_msg)

    base_input_log_name = os.path.basename(batch_answer_evaluations_log)
    if base_input_log_name.startswith("eval_results_") and base_input_log_name.endswith(".jsonl"):
        analysis_file_name = "analysis_" + base_input_log_name[len("eval_results_"):-len(".jsonl")] + ".csv"
    else: # Fallback naming
        analysis_file_name = f"analysis_answer_report_{context.run_id[:8]}.csv"
        
    output_csv_filepath = os.path.join(EVALUATION_RESULTS_LOGS_DIR, analysis_file_name)

    success = perform_answer_evaluation_analysis(
        evaluation_log_filepath=batch_answer_evaluations_log,
        output_csv_filepath=output_csv_filepath
    )

    if success:
        context.log.info(f"Answer evaluation analysis report generated: {output_csv_filepath}")
        return dg.Output(output_csv_filepath, metadata={"output_csv_filepath": output_csv_filepath, "source_log": base_input_log_name})
    else:
        error_msg = f"Answer evaluation analysis failed for log file: {batch_answer_evaluations_log}"
        context.log.error(error_msg)
        raise dg.Failure(description=error_msg)

# 将所有评估相关的资产收集到一个列表中，方便在 definitions.py 中引用
all_evaluation_assets = [
    latest_rag_interaction_log_for_evaluation_asset,
    batch_cypher_evaluation_log_asset,
    batch_answer_evaluation_log_asset,
    cypher_analysis_report_asset, # <--- 新增
    answer_analysis_report_asset, # <--- 新增
]
```

File: zhz_rag_pipeline_dagster/zhz_rag_pipeline/ingestion_assets.py
-------------------------------------------------------------------
```python
# zhz_rag_pipeline/ingestion_assets.py
import dagster as dg
import os
from typing import List, Dict, Any, Union, Optional

# --- 修改：导入分发器并设置Pydantic可用性标志 ---
# 尝试导入Pydantic模型，并设置一个标志，以便在模型不可用时代码可以优雅地降级。
try:
    from .pydantic_models_dagster import LoadedDocumentOutput, ParsedDocumentOutput, NarrativeTextElement
    _PYDANTIC_AVAILABLE = True
except ImportError:
    LoadedDocumentOutput = dict  # type: ignore
    ParsedDocumentOutput = dict  # type: ignore
    NarrativeTextElement = dict  # type: ignore
    _PYDANTIC_AVAILABLE = False

from .parsers import dispatch_parsing # <--- 修改导入路径
# --- 修改结束 ---

class LoadDocumentsConfig(dg.Config):
    documents_directory: str = "/home/zhz/zhz_agent/data/raw_documents/" # 更新后的原始文档目录
    allowed_extensions: List[str] = [".txt", ".md", ".docx", ".pdf", ".xlsx", ".html", ".htm"] # 扩大允许范围以测试所有解析器

@dg.asset(
    name="raw_documents",
    description="Loads raw documents from a specified directory.",
    group_name="ingestion" # 给资产分组
)
def load_documents_asset(
    context: dg.AssetExecutionContext, 
    config: LoadDocumentsConfig
) -> List[Any]: 
    
    loaded_docs: List[Any] = [] 
    target_directory = config.documents_directory
    allowed_exts = tuple(config.allowed_extensions) 

    context.log.info(f"Scanning directory: {target_directory} for files with extensions: {allowed_exts}")

    if not os.path.isdir(target_directory):
        context.log.error(f"Directory not found: {target_directory}")
        return loaded_docs

    for filename in os.listdir(target_directory):
        file_path = os.path.join(target_directory, filename)
        if os.path.isfile(file_path):
            file_name_lower = filename.lower()
            file_extension = os.path.splitext(file_name_lower)[1]

            if file_extension in allowed_exts:
                context.log.info(f"Found matching file: {file_path}")
                try:
                    # 对于需要路径的解析器（如docx, pdf），我们可能不需要立即读取内容
                    # 但为了简单起见并支持文本解析器，我们仍然读取
                    with open(file_path, 'rb') as f: # 以二进制模式读取以更好地处理不同编码
                        raw_bytes = f.read()
                    
                    # 在parse_document_asset中进行更智能的解码
                    doc_output_data = {
                        "document_path": file_path,
                        "file_type": file_extension,
                        "raw_content": raw_bytes, # 传递原始字节
                        "metadata": {
                            "filename": filename,
                            "source_directory": target_directory,
                            "size_bytes": os.path.getsize(file_path)
                        }
                    }

                    if _PYDANTIC_AVAILABLE:
                        loaded_docs.append(LoadedDocumentOutput(**doc_output_data))
                    else:
                        loaded_docs.append(doc_output_data) # type: ignore

                    context.log.info(f"Successfully loaded and created output for: {file_path}")
                except Exception as e:
                    context.log.error(f"Failed to read or process file {file_path}: {e}")
            else:
                context.log.debug(f"Skipping file with non-allowed extension: {file_path}")
        else:
            context.log.debug(f"Skipping non-file item: {file_path}")
            
    if not loaded_docs:
        context.log.warning(f"No matching documents found in {target_directory}")

    if loaded_docs:
        first_doc_path = loaded_docs[0].document_path if _PYDANTIC_AVAILABLE else loaded_docs[0]['document_path']
        context.add_output_metadata(
            metadata={
                "num_documents_loaded": len(loaded_docs),
                "first_document_path": first_doc_path if loaded_docs else "N/A"
            }
        )
    return loaded_docs





@dg.asset(
    name="parsed_documents",
    description="Parses loaded documents into text and extracts basic structure using a dispatcher.",
    group_name="ingestion",
    io_manager_key="pydantic_json_io_manager"
)
def parse_document_asset(
    context: dg.AssetExecutionContext,
    raw_documents: List[Any]
) -> List[Any]:
    parsed_docs_output_list: List[Any] = []
    context.log.info(f"Received {len(raw_documents)} documents to parse.")

    for doc_input_any in raw_documents:
        doc_path: str
        file_ext: str
        raw_content_bytes: Optional[bytes] = None # 将用于文本文件解码
        original_metadata: Dict[str, Any]

        if _PYDANTIC_AVAILABLE and isinstance(doc_input_any, LoadedDocumentOutput):
            doc_path = doc_input_any.document_path
            file_ext = doc_input_any.file_type.lower()
            if isinstance(doc_input_any.raw_content, bytes): # 确保是字节
                raw_content_bytes = doc_input_any.raw_content
            original_metadata = doc_input_any.metadata.copy()
        elif isinstance(doc_input_any, dict):
            doc_path = doc_input_any.get('document_path', '')
            file_ext = doc_input_any.get('file_type', '').lower()
            raw_content_value = doc_input_any.get('raw_content')
            if isinstance(raw_content_value, bytes):
                raw_content_bytes = raw_content_value
            original_metadata = doc_input_any.get('metadata', {}).copy()
        else:
            context.log.error(f"Skipping document with unexpected type: {type(doc_input_any)}")
            continue
        
        context.log.info(f"Attempting to parse document: {doc_path} (Type: {file_ext})")
        
        current_original_metadata = original_metadata.copy() # 使用副本以防修改影响其他迭代
        current_original_metadata["source_file_path"] = doc_path # 确保路径在元数据中

        parsed_output: Optional[Any] = None
        input_for_parser: Union[str, bytes] # 将传递给 dispatch_parsing 的内容

        # --- 修改：只对文本类型文件尝试解码 ---
        if file_ext in [".md", ".html", ".htm", ".txt"]:
            content_str = ""
            if raw_content_bytes:
                try:
                    content_str = raw_content_bytes.decode('utf-8')
                except UnicodeDecodeError:
                    try:
                        content_str = raw_content_bytes.decode('gbk', errors='replace') # 使用replace以避免失败
                        context.log.warning(f"Decoded {doc_path} with GBK (UTF-8 failed).")
                    except Exception as e_decode_gbk:
                        context.log.error(f"Could not decode content for text file {doc_path} with UTF-8 or GBK: {e_decode_gbk}")
                        content_str = f"[Unreadable Text Content: {doc_path}]"
            else: # 如果 raw_content_bytes 为 None 或空
                context.log.warning(f"No raw_content (bytes) found for text file {doc_path}. Treating as empty.")
            input_for_parser = content_str
        else: # 对于二进制文件 (docx, pdf, xlsx)，直接传递路径
            input_for_parser = doc_path
        
        try:
            parsed_output = dispatch_parsing(file_ext, input_for_parser, current_original_metadata)

            # --- 修改：回退逻辑调整 ---
            if not parsed_output:
                context.log.warning(f"Parser for '{file_ext}' returned no output for {doc_path}.")
                fallback_text = ""
                if file_ext in [".md", ".html", ".htm", ".txt"] and isinstance(input_for_parser, str):
                    # 对于文本文件，如果解析器失败，原始解码的字符串是最好的回退
                    fallback_text = input_for_parser 
                    context.log.info(f"Using raw decoded text as fallback for {doc_path}.")
                else:
                    # 对于二进制文件或其他解析失败的情况，标记为无法解析
                    fallback_text = f"[Content Not Parsed by Specific Parser: {doc_path}]"
                    context.log.warning(f"Binary or complex file {doc_path} could not be parsed by its specific parser.")

                if _PYDANTIC_AVAILABLE:
                    elements = [NarrativeTextElement(text=fallback_text)] if fallback_text else []
                    parsed_output = ParsedDocumentOutput(
                        parsed_text=fallback_text, 
                        elements=elements, 
                        original_metadata=current_original_metadata
                    )
                else:
                    elements = [{"element_type": "narrative_text", "text": fallback_text}] if fallback_text else []
                    parsed_output = {
                        "parsed_text": fallback_text, 
                        "elements": elements, 
                        "original_metadata": current_original_metadata
                    }
            
            if parsed_output:
                if _PYDANTIC_AVAILABLE and not isinstance(parsed_output, ParsedDocumentOutput) and isinstance(parsed_output, dict):
                    try:
                        parsed_output = ParsedDocumentOutput(**parsed_output)
                    except Exception as e_cast:
                        context.log.error(f"Failed to cast parsed_output dict to Pydantic model for {doc_path}: {e_cast}")
                        error_text_cast = f"[Casting Error for {doc_path}]"
                        elements_cast = [NarrativeTextElement(text=error_text_cast)] if _PYDANTIC_AVAILABLE else [{"element_type":"narrative_text", "text":error_text_cast}]
                        parsed_output = ParsedDocumentOutput(parsed_text=error_text_cast, elements=elements_cast, original_metadata=current_original_metadata) if _PYDANTIC_AVAILABLE else {"parsed_text":error_text_cast, "elements":elements_cast, "original_metadata":current_original_metadata}
                
                parsed_docs_output_list.append(parsed_output)
                context.log.info(f"Successfully processed (parsed or created fallback for): {doc_path}")

        except Exception as e_parse_asset:
            context.log.error(f"Critical error during parsing asset for {doc_path}: {e_parse_asset}", exc_info=True)
            error_text_critical = f"[Critical Parsing Exception for {doc_path}: {str(e_parse_asset)}]"
            if _PYDANTIC_AVAILABLE:
                elements_critical = [NarrativeTextElement(text=error_text_critical)]
                error_output = ParsedDocumentOutput(
                    parsed_text=error_text_critical, elements=elements_critical, original_metadata=current_original_metadata
                )
            else:
                elements_critical = [{"element_type": "narrative_text", "text": error_text_critical}]
                error_output = {
                    "parsed_text": error_text_critical, "elements": elements_critical, "original_metadata": current_original_metadata
                }
            parsed_docs_output_list.append(error_output)
            
    if parsed_docs_output_list:
        context.add_output_metadata(
            metadata={
                "num_documents_processed_for_parsing": len(raw_documents),
                "num_parsed_document_outputs_generated": len(parsed_docs_output_list),
            }
        )
    return parsed_docs_output_list



all_ingestion_assets = [load_documents_asset, parse_document_asset]
```

File: zhz_rag_pipeline_dagster/zhz_rag_pipeline/processing_assets.py
--------------------------------------------------------------------
```python
#  文件: zhz_rag_pipeline_dagster/zhz_rag_pipeline/processing_assets.py

import json
import asyncio
import re
import dagster as dg
from typing import List, Dict, Any, Optional, Union
import uuid
from langchain_text_splitters import RecursiveCharacterTextSplitter
import hashlib
import pandas as pd
from zhz_rag.utils.common_utils import normalize_text_for_id
from zhz_rag_pipeline_dagster.zhz_rag_pipeline.pydantic_models_dagster import (
    ChunkOutput,
    ParsedDocumentOutput,
    EmbeddingOutput,
    KGTripleSetOutput,
    ExtractedEntity,
    ExtractedRelation,
    # --- 添加导入我们需要的元素类型 ---
    TitleElement,
    NarrativeTextElement,
    ListItemElement,
    TableElement,
    CodeBlockElement,
    ImageElement,
    PageBreakElement,
    HeaderElement,
    FooterElement,
    DocumentElementMetadata
    # --- 结束添加 ---
)
from zhz_rag_pipeline_dagster.zhz_rag_pipeline.resources import (
    GGUFEmbeddingResource,
    ChromaDBResource,
    DuckDBResource,
    LocalLLMAPIResource,
    SystemResource  # <--- 添加这一行以导入 SystemResource
)
import jieba
import bm25s
import pickle
import numpy as np
import os
from zhz_rag.utils.common_utils import normalize_text_for_id

_PYDANTIC_AVAILABLE = False
try:
    from .pydantic_models_dagster import ( # 使用相对导入
        ChunkOutput,
        ParsedDocumentOutput,
        EmbeddingOutput,
        KGTripleSetOutput,
        ExtractedEntity,
        ExtractedRelation,
        TitleElement,
        NarrativeTextElement,
        ListItemElement,
        TableElement,
        CodeBlockElement,
        ImageElement,
        PageBreakElement,
        HeaderElement,
        FooterElement,
        DocumentElementMetadata # <--- 确保这里导入了 DocumentElementMetadata
    )
    _PYDANTIC_AVAILABLE = True
    # 如果 Pydantic 可用，我们也可以直接从模型中获取 DocumentElementType
    # from .pydantic_models_dagster import DocumentElementType # 如果需要更精确的类型提示
except ImportError:
    # 定义占位符
    class BaseModel: pass
    class ChunkOutput(BaseModel): pass
    class ParsedDocumentOutput(BaseModel): pass
    class EmbeddingOutput(BaseModel): pass
    class KGTripleSetOutput(BaseModel): pass
    class ExtractedEntity(BaseModel): pass
    class ExtractedRelation(BaseModel): pass
    class TitleElement(BaseModel): pass
    class NarrativeTextElement(BaseModel): pass
    class ListItemElement(BaseModel): pass
    class TableElement(BaseModel): pass
    class CodeBlockElement(BaseModel): pass
    class ImageElement(BaseModel): pass
    class PageBreakElement(BaseModel): pass
    class HeaderElement(BaseModel): pass
    class FooterElement(BaseModel): pass
    class DocumentElementMetadata(BaseModel): pass # <--- 定义占位符
    DocumentElementType = Any # type: ignore
# --- 结束 Pydantic 模型导入 ---

import logging
logger = logging.getLogger(__name__)
if not logger.handlers:
    handler = logging.StreamHandler()
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    logger.setLevel(logging.DEBUG) # <--- 确保是 DEBUG
    logger.info(f"Logger for {__name__} (processing_assets) configured with DEBUG level.")


class TextChunkerConfig(dg.Config):
    chunk_size: int = 1000 
    chunk_overlap: int = 100
    max_element_text_length_before_split: int = 1200 # 一个1200字符的段落如果语义连贯，可以考虑不切分。
    target_sentence_split_chunk_size: int = 600    # 略微增大子块的目标大小，使其包含更多上下文。
    sentence_split_chunk_overlap_sentences: int = 2  # 增加到2句重叠，以期在子块之间提供更好的语义连接。
    # --- 合并策略参数 ---
    min_chunk_length_to_avoid_merge: int = 250
    max_merged_chunk_size: int = 750


def split_text_into_sentences(text: str) -> List[str]:
    """
    Splits text into sentences using a regex-based approach.
    Handles common sentence terminators and aims to preserve meaningful units.
    """
    if not text:
        return []
    # 改进的句子分割正则表达式，考虑了中英文句号、问号、感叹号
    # 并尝试处理省略号和一些特殊情况。
    # (?<=[。？！\.!\?]) 会匹配这些标点符号后面的位置 (lookbehind)
    # \s* 匹配标点后的任意空格
    # (?!$) 确保不是字符串末尾 (避免在末尾标点后产生空句子)
    # 对于中文，句号、问号、感叹号通常直接结束句子。
    # 对于英文，. ! ? 后面通常有空格或换行。
    
    # 一个更简单的版本，直接按标点分割，然后清理
    sentences = re.split(r'([。？！\.!\?])', text)
    result = []
    current_sentence = ""
    for i in range(0, len(sentences), 2):
        part = sentences[i]
        terminator = sentences[i+1] if i+1 < len(sentences) else ""
        current_sentence = part + terminator
        if current_sentence.strip():
            result.append(current_sentence.strip())
    
    # 如果上面的分割不理想，可以尝试更复杂的，但这个简单版本通常够用
    # 例如：
    # sentences = re.split(r'(?<=[。？！\.!\?])\s*', text)
    # sentences = [s.strip() for s in sentences if s.strip()]
    return result if result else [text] # 如果无法分割，返回原文本作为一个句子



def split_markdown_table_by_rows(
    markdown_table_text: str,
    target_chunk_size: int,
    max_chunk_size: int,
    context: Optional[dg.AssetExecutionContext] = None
) -> List[Dict[str, Any]]: # <--- 修改返回类型
    """
    Splits a Markdown table string by its data rows.
    Returns a list of dictionaries, each containing 'text', 'start_row_index', and 'end_row_index'.
    Indices are 0-based relative to the start of data_rows.
    """
    sub_chunks_data: List[Dict[str, Any]] = [] # <--- 修改变量名和类型
    lines = markdown_table_text.strip().split('\n')
    
    if len(lines) < 2: # 至少需要表头和分隔行
        if context: context.log.warning(f"Markdown table has less than 2 lines ({len(lines)}). Cannot process for row splitting. Returning as single chunk.")
        # 对于无法按预期处理的表格，返回原始文本及无效的行索引
        return [{"text": markdown_table_text, "start_row_index": -1, "end_row_index": -1}]


    header_row = lines[0]
    separator_row = lines[1]
    data_rows = lines[2:]

    if not data_rows:
        if context: context.log.warning("Markdown table has no data rows. Returning header and separator as single chunk.")
        return [{"text": f"{header_row}\n{separator_row}", "start_row_index": -1, "end_row_index": -1}]

    if not (separator_row.strip().startswith('|') and separator_row.strip().endswith('|') and '-' in separator_row):
        if context: context.log.warning(f"Markdown table separator row looks non-standard: '{separator_row}'. Row splitting might be unreliable.")
        # 即使分隔符不标准，也尝试继续，但标记行索引为不可靠
        # 或者直接返回整个表作为一个块，标记行索引无效
        # 为简单起见，我们先尝试，但如果这个情况频繁且导致问题，应返回整个表
        # return [{"text": markdown_table_text, "start_row_index": -2, "end_row_index": -2}] # -2 表示分隔符问题

    current_sub_chunk_lines = [header_row, separator_row]
    current_sub_chunk_char_count = len(header_row) + 1 + len(separator_row) + 1
    current_sub_chunk_start_row_idx = 0 # 记录当前子块数据行的起始索引

    for i, data_row in enumerate(data_rows):
        row_len = len(data_row)
        if not data_row.strip():
            if i == current_sub_chunk_start_row_idx and len(current_sub_chunk_lines) == 2 : # 如果是新块的第一个数据行就是空的，则递增起始行索引
                current_sub_chunk_start_row_idx +=1
            continue # 跳过空数据行

        # 条件：1. 当前块只有表头；2. 加入新行不超过目标大小；3. 当前块只有表头且加入第一行数据即使超目标大小但不超最大大小
        should_add_to_current = (
            len(current_sub_chunk_lines) == 2 or
            (current_sub_chunk_char_count + row_len + 1 <= target_chunk_size) or
            (len(current_sub_chunk_lines) == 2 and (current_sub_chunk_char_count + row_len + 1 > target_chunk_size) and (current_sub_chunk_char_count + row_len + 1 <= max_chunk_size))
        )

        if should_add_to_current and (current_sub_chunk_char_count + row_len + 1 <= max_chunk_size):
            current_sub_chunk_lines.append(data_row)
            current_sub_chunk_char_count += row_len + 1
        else:
            # 完成当前子块
            if len(current_sub_chunk_lines) > 2: # 确保至少有一行数据
                sub_chunks_data.append({
                    "text": "\n".join(current_sub_chunk_lines),
                    "start_row_index": current_sub_chunk_start_row_idx,
                    "end_row_index": i - 1 # 上一行是这个块的结束行
                })
                if context: context.log.debug(f"  Table sub-chunk created: rows {current_sub_chunk_start_row_idx}-{i-1}")

            # 开始新的子块
            current_sub_chunk_lines = [header_row, separator_row, data_row] # 当前行是新块的第一行数据
            current_sub_chunk_char_count = len(header_row) + 1 + len(separator_row) + 1 + len(data_row) + 1
            current_sub_chunk_start_row_idx = i
            
            # 如果当前行（作为新块的第一行）自己就超长了（非常罕见，除非max_chunk_size很小）
            if current_sub_chunk_char_count > max_chunk_size:
                if context: context.log.warning(f"  Single data row (original index {i}) with header exceeds max_chunk_size. Creating chunk for this row anyway.")
                # 即使超长，也为这一行创建一个块
                sub_chunks_data.append({
                    "text": "\n".join(current_sub_chunk_lines), # 包含表头、分隔符和这一超长行
                    "start_row_index": i,
                    "end_row_index": i 
                })
                if context: context.log.debug(f"  Table sub-chunk created for single very long row: {i}-{i}")
                # 重置，准备下一个数据行（如果有的话）
                current_sub_chunk_lines = [header_row, separator_row]
                current_sub_chunk_char_count = len(header_row) + 1 + len(separator_row) + 1
                current_sub_chunk_start_row_idx = i + 1


    # 添加最后一个正在构建的子块
    if len(current_sub_chunk_lines) > 2:
        sub_chunks_data.append({
            "text": "\n".join(current_sub_chunk_lines),
            "start_row_index": current_sub_chunk_start_row_idx,
            "end_row_index": len(data_rows) - 1 # 最后的数据行
        })
        if context: context.log.debug(f"  Table sub-chunk created (last): rows {current_sub_chunk_start_row_idx}-{len(data_rows)-1}")

    if not sub_chunks_data and markdown_table_text:
        if context: context.log.warning("Markdown table splitting by row resulted in no sub-chunks. Returning original table as one.")
        return [{"text": markdown_table_text, "start_row_index": 0, "end_row_index": len(data_rows) -1 if data_rows else -1}]
        
    return sub_chunks_data



def split_code_block_by_blank_lines(
    code_text: str,
    target_chunk_size: int, # 复用配置，但对于代码块，这个更像是一个上限指导
    max_chunk_size: int,    # 作为硬上限
    context: Optional[dg.AssetExecutionContext] = None
) -> List[str]:
    """
    Splits a code block string by blank lines (one or more empty lines).
    Tries to keep resulting chunks from exceeding max_chunk_size.
    """
    if not code_text.strip():
        return []

    # 使用正则表达式匹配一个或多个连续的空行作为分隔符
    # \n\s*\n 匹配一个换行符，后跟零或多个空白字符，再跟一个换行符
    potential_splits = re.split(r'(\n\s*\n)', code_text) # 保留分隔符以便后续处理
    
    sub_chunks = []
    current_chunk_lines = []
    current_chunk_char_count = 0

    # 第一个块总是从头开始
    if potential_splits:
        first_part = potential_splits.pop(0).strip()
        if first_part:
            current_chunk_lines.append(first_part)
            current_chunk_char_count += len(first_part)

    while potential_splits:
        delimiter = potential_splits.pop(0) # 这是分隔符 \n\s*\n
        if not potential_splits: # 没有更多内容了
            if delimiter.strip(): # 如果分隔符本身不是纯空白，也算内容
                 current_chunk_lines.append(delimiter.rstrip()) # 保留末尾的换行
                 current_chunk_char_count += len(delimiter.rstrip())
            break 
        
        next_part = potential_splits.pop(0).strip()
        if not next_part: # 如果下一个部分是空的，只处理分隔符
            if delimiter.strip():
                current_chunk_lines.append(delimiter.rstrip())
                current_chunk_char_count += len(delimiter.rstrip())
            continue

        # 检查加入 delimiter 和 next_part 是否会超长
        # 对于代码，我们通常希望在逻辑断点（空行）处分割，即使块较小
        # 但如果单个由空行分隔的块本身就超过 max_chunk_size，则需要进一步处理（目前简单截断或接受）
        
        # 简化逻辑：如果当前块非空，并且加入下一个部分（包括分隔的空行）会超过目标大小，
        # 或者严格超过最大大小，则结束当前块。
        # 这里的分隔符（空行）本身也应该被视为块的一部分，或者作为块的自然结束。

        # 更简单的策略：每个由 re.split 分割出来的非空部分（即代码段）自成一块
        # 如果代码段本身过长，则接受它，或者未来再细分
        if current_chunk_lines: # 如果当前块有内容
            # 检查如果加上 next_part 是否会超长（这里可以简化，因为空行分割通常意味着逻辑单元）
            # 我们先假设每个由空行分割的块都是一个独立的单元
            sub_chunks.append("\n".join(current_chunk_lines))
            if context: context.log.debug(f"  Code sub-chunk created (blank line split), len: {current_chunk_char_count}")
            current_chunk_lines = []
            current_chunk_char_count = 0
        
        if next_part: # 开始新的块
            current_chunk_lines.append(next_part)
            current_chunk_char_count += len(next_part)

    # 添加最后一个正在构建的子块
    if current_chunk_lines:
        sub_chunks.append("\n".join(current_chunk_lines))
        if context: context.log.debug(f"  Code sub-chunk created (blank line split, last), len: {current_chunk_char_count}")

    if not sub_chunks and code_text: # 如果完全没分割出任何东西（例如代码没有空行）
        if context: context.log.warning("Code block splitting by blank lines resulted in no sub-chunks. Returning original code block.")
        # 对于这种情况，我们可能需要一个字符分割器作为最终回退
        # 但为了简单起见，我们先返回原始代码块
        # 如果原始代码块 > max_chunk_size，它仍然会是一个大块
        if len(code_text) > max_chunk_size:
            if context: context.log.warning(f"  Original code block (len: {len(code_text)}) exceeds max_chunk_size ({max_chunk_size}) and was not split by blank lines. Consider character splitting as fallback.")
            # 这里可以插入 RecursiveCharacterTextSplitter 逻辑
            # from langchain_text_splitters import RecursiveCharacterTextSplitter
            # char_splitter = RecursiveCharacterTextSplitter(chunk_size=max_chunk_size, chunk_overlap=0, separators=["\n", " ", ""])
            # sub_chunks = char_splitter.split_text(code_text)
            # if context: context.log.info(f"    Fallback: Code block character split into {len(sub_chunks)} parts.")
            # return sub_chunks
        return [code_text] # 暂时返回原块

    # 过滤掉完全是空字符串的块 (re.split 可能产生)
    final_sub_chunks = [chunk for chunk in sub_chunks if chunk.strip()]
    return final_sub_chunks if final_sub_chunks else [code_text]



@dg.asset(
    name="text_chunks",
    description="Cleans/chunks documents. Splits long elements, merges short ones, enriches with contextual metadata.",
    group_name="processing",
    deps=["parsed_documents"]
)
def clean_chunk_text_asset(
    context: dg.AssetExecutionContext,
    config: TextChunkerConfig,
    parsed_documents: List[ParsedDocumentOutput]
) -> List[ChunkOutput]:
    initial_chunks: List[ChunkOutput] = []
    context.log.info(f"Received {len(parsed_documents)} parsed documents for initial chunking.")
    context.log.info(f"Initial chunking config: MaxElemLen={config.max_element_text_length_before_split}, "
                     f"TargetSubChunkSize={config.target_sentence_split_chunk_size}, "
                     f"SentenceOverlap={config.sentence_split_chunk_overlap_sentences}")

    for doc_idx, parsed_doc in enumerate(parsed_documents):
        doc_id_from_meta = parsed_doc.original_metadata.get("filename", f"doc_{doc_idx}_{str(uuid.uuid4())}")
        context.log.info(f"Processing document for initial chunks: {doc_id_from_meta}")

        current_title_hierarchy: Dict[int, str] = {}

        if not parsed_doc.elements:
            context.log.warning(f"Document {doc_id_from_meta} has no structured elements.")
            if parsed_doc.parsed_text and parsed_doc.parsed_text.strip():
                context.log.info(f"  Fallback: Document {doc_id_from_meta} has no elements, but has parsed_text. "
                                 f"Applying RecursiveCharacterTextSplitter to parsed_text.")
                from langchain_text_splitters import RecursiveCharacterTextSplitter
                text_splitter_fallback = RecursiveCharacterTextSplitter(
                    chunk_size=config.chunk_size,
                    chunk_overlap=config.chunk_overlap,
                    length_function=len
                )
                try:
                    chunks_text_list = text_splitter_fallback.split_text(parsed_doc.parsed_text.strip())
                    for i, chunk_text_content in enumerate(chunks_text_list):
                        chunk_meta = parsed_doc.original_metadata.copy()
                        chunk_meta.update({
                            "chunk_number_in_doc": len(initial_chunks) + 1,
                            "sub_chunk_sequence": i + 1,
                            "total_sub_chunks": len(chunks_text_list),
                            "chunking_strategy": "fallback_recursive_char_split_on_parsed_text",
                            "original_element_text_length": len(parsed_doc.parsed_text.strip()),
                            "title_hierarchy": {},
                            "is_merged_chunk": False,
                        })
                        initial_chunks.append(ChunkOutput(
                            chunk_text=chunk_text_content,
                            source_document_id=doc_id_from_meta,
                            chunk_metadata=chunk_meta
                        ))
                except Exception as e_fallback_split:
                    context.log.error(f"  Error during fallback splitting for {doc_id_from_meta}: {e_fallback_split}")
            continue

        doc_internal_chunk_counter = 0

        for element_idx, element in enumerate(parsed_doc.elements):
            element_text_content = ""
            element_type_str = "unknown"
            source_element_metadata_from_element_obj = {}

            if isinstance(element, TitleElement) or (isinstance(element, dict) and element.get("element_type") == "title"):
                element_text_content = element.text if hasattr(element, 'text') else element.get("text", "")
                element_type_str = "title"
                title_level = element.level if hasattr(element, 'level') else element.get("level", 1)
                source_element_metadata_from_element_obj["level"] = title_level
                
                keys_to_remove = [lvl for lvl in current_title_hierarchy if lvl >= title_level]
                for key in keys_to_remove:
                    del current_title_hierarchy[key]
                current_title_hierarchy[title_level] = element_text_content.strip()
                logger.debug(f"  Updated title hierarchy for {doc_id_from_meta}: {current_title_hierarchy}")

            elif isinstance(element, NarrativeTextElement) or (isinstance(element, dict) and element.get("element_type") == "narrative_text"):
                element_text_content = element.text if hasattr(element, 'text') else element.get("text", "")
                element_type_str = "narrative_text"
            elif isinstance(element, ListItemElement) or (isinstance(element, dict) and element.get("element_type") == "list_item"):
                element_text_content = element.text if hasattr(element, 'text') else element.get("text", "")
                element_type_str = "list_item"
                source_element_metadata_from_element_obj["item_level"] = element.level if hasattr(element, 'level') else element.get("level")
                source_element_metadata_from_element_obj["ordered"] = element.ordered if hasattr(element, 'ordered') else element.get("ordered")
                source_element_metadata_from_element_obj["item_number"] = element.item_number if hasattr(element, 'item_number') else element.get("item_number")
            elif isinstance(element, TableElement) or (isinstance(element, dict) and element.get("element_type") == "table"):
                if hasattr(element, 'markdown_representation') and element.markdown_representation:
                    element_text_content = element.markdown_representation
                elif hasattr(element, 'text_representation') and element.text_representation:
                    element_text_content = element.text_representation
                elif isinstance(element, dict):
                    element_text_content = element.get("markdown_representation") or element.get("text_representation", "")
                element_type_str = "table"
                source_element_metadata_from_element_obj["caption"] = element.caption if hasattr(element, 'caption') else element.get("caption")
            elif isinstance(element, CodeBlockElement) or (isinstance(element, dict) and element.get("element_type") == "code_block"):
                element_text_content = element.code if hasattr(element, 'code') else element.get("code", "")
                element_type_str = "code_block"
                source_element_metadata_from_element_obj["language"] = element.language if hasattr(element, 'language') else element.get("language")
            elif isinstance(element, PageBreakElement) or (isinstance(element, dict) and element.get("element_type") == "page_break"):
                element_type_str = "page_break"

            element_text_content = element_text_content.strip() if element_text_content else ""

            if not element_text_content and element_type_str != "page_break":
                context.log.debug(f"  Element {element_idx} (type: {element_type_str}) in {doc_id_from_meta} has no text content, skipping chunk creation.")
                continue
            
            base_chunk_meta = parsed_doc.original_metadata.copy()
            base_chunk_meta.update({
                "source_element_index": element_idx,
                "source_element_type": element_type_str,
                "title_hierarchy": current_title_hierarchy.copy(),
                "is_merged_chunk": False,
            })
            if source_element_metadata_from_element_obj:
                base_chunk_meta.update(source_element_metadata_from_element_obj)
            
            element_specific_meta = None
            if hasattr(element, 'metadata') and element.metadata:
                if _PYDANTIC_AVAILABLE and isinstance(element.metadata, DocumentElementMetadata):
                    element_specific_meta = element.metadata.model_dump(exclude_none=True)
                elif isinstance(element.metadata, dict):
                    element_specific_meta = element.metadata
            elif isinstance(element, dict) and element.get("metadata"):
                element_specific_meta = element.get("metadata")
            if element_specific_meta:
                base_chunk_meta.update({f"el_{k}": v for k, v in element_specific_meta.items()})

            if len(element_text_content) > config.max_element_text_length_before_split:
                sub_chunks_created_count = 0
                
                if element_type_str == "table" and hasattr(element, 'markdown_representation') and element.markdown_representation:
                    context.log.info(f"  Table Element {element_idx} (markdown_len: {len(element.markdown_representation)}) in {doc_id_from_meta} is too long. Attempting row-based splitting.")
                    table_sub_chunks_data = split_markdown_table_by_rows(
                        element.markdown_representation,
                        config.target_sentence_split_chunk_size,
                        config.max_merged_chunk_size,
                        context
                    )
                    
                    if table_sub_chunks_data and \
                       (len(table_sub_chunks_data) > 1 or
                       (len(table_sub_chunks_data) == 1 and table_sub_chunks_data[0]["text"] != element.markdown_representation) or
                       (len(table_sub_chunks_data) == 1 and table_sub_chunks_data[0]["start_row_index"] != -1)):
                        
                        context.log.info(f"    Successfully split/processed table into {len(table_sub_chunks_data)} sub-chunk(s) by row.")
                        for sub_idx, chunk_data_dict in enumerate(table_sub_chunks_data):
                            doc_internal_chunk_counter += 1
                            chunk_meta = base_chunk_meta.copy()
                            chunk_meta.update({
                                "chunk_number_in_doc": doc_internal_chunk_counter,
                                "source_element_type": "table_row_chunk",
                                "chunking_strategy": "table_split_by_row_v1",
                                "is_split_from_long_element": True,
                                "original_element_text_length": len(element.markdown_representation),
                                "sub_chunk_sequence_in_element": sub_idx + 1,
                                "total_sub_chunks_from_element": len(table_sub_chunks_data),
                                "table_original_start_row": chunk_data_dict["start_row_index"],
                                "table_original_end_row": chunk_data_dict["end_row_index"],
                            })
                            initial_chunks.append(ChunkOutput(chunk_text=chunk_data_dict["text"], source_document_id=doc_id_from_meta, chunk_metadata=chunk_meta))
                            sub_chunks_created_count += 1
                        if sub_chunks_created_count > 0: continue
                    else:
                        context.log.warning(f"    Row-based splitting of table (idx {element_idx}) did not result in valid sub-chunks or was not applicable. Falling back to sentence splitting for the entire table markdown.")

                elif element_type_str == "code_block":
                    context.log.info(f"  Code Block Element {element_idx} (len: {len(element_text_content)}) in {doc_id_from_meta} is too long. Attempting blank line splitting.")
                    code_sub_chunks = split_code_block_by_blank_lines(
                        element_text_content,
                        config.target_sentence_split_chunk_size,
                        config.max_merged_chunk_size,
                        context
                    )
                    if code_sub_chunks and (len(code_sub_chunks) > 1 or code_sub_chunks[0] != element_text_content):
                        context.log.info(f"    Successfully split code block into {len(code_sub_chunks)} sub-chunks by blank lines.")
                        for sub_idx, sub_chunk_text in enumerate(code_sub_chunks):
                            if not sub_chunk_text.strip(): continue
                            doc_internal_chunk_counter += 1
                            chunk_meta = base_chunk_meta.copy()
                            chunk_meta.update({
                                "chunk_number_in_doc": doc_internal_chunk_counter,
                                "source_element_type": "code_fragment",
                                "chunking_strategy": "code_split_by_blank_line_v1",
                                "is_split_from_long_element": True,
                                "original_element_text_length": len(element_text_content),
                                "sub_chunk_sequence_in_element": sub_idx + 1,
                                "total_sub_chunks_from_element": len(code_sub_chunks),
                            })
                            initial_chunks.append(ChunkOutput(chunk_text=sub_chunk_text, source_document_id=doc_id_from_meta, chunk_metadata=chunk_meta))
                            sub_chunks_created_count += 1
                        if sub_chunks_created_count > 0: continue
                    else:
                        context.log.warning(f"    Blank line splitting of code block (idx {element_idx}) did not result in multiple chunks or failed. Falling back to sentence/char splitting for the code content.")

                context.log.info(f"  Element {element_idx} (type: {element_type_str}, len: {len(element_text_content)}) in {doc_id_from_meta} is too long. Attempting sentence splitting (or fallback).")
                sentences = split_text_into_sentences(element_text_content)
                if not sentences:
                    context.log.warning(f"    Could not split element {element_idx} into sentences. Using RecursiveCharacterTextSplitter as fallback.")
                    from langchain_text_splitters import RecursiveCharacterTextSplitter
                    char_splitter = RecursiveCharacterTextSplitter(
                        chunk_size=config.chunk_size,
                        chunk_overlap=config.chunk_overlap,
                        length_function=len
                    )
                    sub_chunks_texts = char_splitter.split_text(element_text_content)
                    for sub_idx, sub_chunk_text in enumerate(sub_chunks_texts):
                        doc_internal_chunk_counter += 1
                        chunk_meta = base_chunk_meta.copy()
                        chunk_meta.update({
                            "chunk_number_in_doc": doc_internal_chunk_counter,
                            "chunking_strategy": "element_char_split_due_to_no_sentence_split_v1",
                            "is_split_from_long_element": True,
                            "original_element_text_length": len(element_text_content),
                            "sub_chunk_sequence_in_element": sub_idx + 1,
                            "total_sub_chunks_from_element": len(sub_chunks_texts),
                        })
                        initial_chunks.append(ChunkOutput(chunk_text=sub_chunk_text, source_document_id=doc_id_from_meta, chunk_metadata=chunk_meta))
                        context.log.info(f"    Created sub-chunk {sub_idx+1}/{len(sub_chunks_texts)} (char_split) from element {element_idx} for {doc_id_from_meta}")
                else:
                    current_sub_chunk_sentences = []
                    current_sub_chunk_length = 0
                    sub_chunks_generated_for_element = []
                    for i, sentence in enumerate(sentences):
                        sentence_len = len(sentence)
                        if not current_sub_chunk_sentences or \
                           (current_sub_chunk_length + sentence_len + (1 if current_sub_chunk_sentences else 0) <= config.target_sentence_split_chunk_size):
                            current_sub_chunk_sentences.append(sentence)
                            current_sub_chunk_length += sentence_len + (1 if len(current_sub_chunk_sentences) > 1 else 0)
                        else:
                            sub_chunks_generated_for_element.append(" ".join(current_sub_chunk_sentences))
                            overlap_count = config.sentence_split_chunk_overlap_sentences
                            start_index_for_new_chunk = max(0, len(current_sub_chunk_sentences) - overlap_count)
                            current_sub_chunk_sentences = current_sub_chunk_sentences[start_index_for_new_chunk:]
                            current_sub_chunk_sentences.append(sentence)
                            current_sub_chunk_length = len(" ".join(current_sub_chunk_sentences))
                    if current_sub_chunk_sentences:
                        sub_chunks_generated_for_element.append(" ".join(current_sub_chunk_sentences))

                    for sub_idx, sub_chunk_text in enumerate(sub_chunks_generated_for_element):
                        doc_internal_chunk_counter += 1
                        chunk_meta = base_chunk_meta.copy()
                        chunk_meta.update({
                            "chunk_number_in_doc": doc_internal_chunk_counter,
                            "chunking_strategy": "element_split_by_sentence_v1",
                            "is_split_from_long_element": True,
                            "original_element_text_length": len(element_text_content),
                            "sub_chunk_sequence_in_element": sub_idx + 1,
                            "total_sub_chunks_from_element": len(sub_chunks_generated_for_element),
                        })
                        initial_chunks.append(ChunkOutput(chunk_text=sub_chunk_text, source_document_id=doc_id_from_meta, chunk_metadata=chunk_meta))
                        context.log.info(f"    Created sub-chunk {sub_idx+1}/{len(sub_chunks_generated_for_element)} (sentence_split) from element {element_idx} for {doc_id_from_meta}")
            
            else:
                doc_internal_chunk_counter += 1
                chunk_meta = base_chunk_meta.copy()
                chunk_meta.update({
                    "chunk_number_in_doc": doc_internal_chunk_counter,
                    "chunking_strategy": "element_as_chunk_v1",
                    "is_split_from_long_element": False,
                    "original_element_text_length": len(element_text_content),
                })
                initial_chunks.append(ChunkOutput(chunk_text=element_text_content, source_document_id=doc_id_from_meta, chunk_metadata=chunk_meta))
                context.log.info(f"  Created chunk from element {element_idx} (type: {element_type_str}, len: {len(element_text_content)}) in {doc_id_from_meta} (not split).")

    if not initial_chunks:
        context.log.info("No initial chunks to process for merging.")
        return []

    context.log.info(f"Starting short chunk merging process. Initial chunks: {len(initial_chunks)}")
    context.log.info(f"Merging config: MinLenToAvoidMerge={config.min_chunk_length_to_avoid_merge}, MaxMergedSize={config.max_merged_chunk_size}")
    
    merged_chunks: List[ChunkOutput] = []
    i = 0
    while i < len(initial_chunks):
        current_chunk = initial_chunks[i]
        
        merged_this_iteration = False

        if len(current_chunk.chunk_text) < config.min_chunk_length_to_avoid_merge or \
           current_chunk.chunk_metadata.get("source_element_type") == "title":

            chunks_to_merge = [current_chunk]
            current_merged_text = current_chunk.chunk_text
            current_merged_len = len(current_merged_text)
            
            j = i + 1
            if j < len(initial_chunks):
                next_chunk = initial_chunks[j]
                
                can_merge = False
                merge_reason = ""
                potential_merged_text = ""

                is_current_short_title = (
                    current_chunk.chunk_metadata.get("source_element_type") == "title" and
                    len(current_chunk.chunk_text) < config.min_chunk_length_to_avoid_merge
                )
                is_next_short_narrative = (
                    next_chunk.chunk_metadata.get("source_element_type") == "narrative_text" and
                    len(next_chunk.chunk_text) < config.min_chunk_length_to_avoid_merge
                )

                if is_current_short_title and is_next_short_narrative and \
                   current_chunk.source_document_id == next_chunk.source_document_id and \
                   current_chunk.chunk_metadata.get("source_element_index", -2) + 1 == next_chunk.chunk_metadata.get("source_element_index", -1):
                    
                    temp_merged_text = current_chunk.chunk_text + "\n\n" + next_chunk.chunk_text
                    if len(temp_merged_text) <= config.max_merged_chunk_size:
                        can_merge = True
                        merge_reason = "title_narrative"
                        potential_merged_text = temp_merged_text
                        chunks_to_merge.append(next_chunk)

                if not can_merge:
                    if current_chunk.chunk_metadata.get("is_split_from_long_element") and \
                       next_chunk.chunk_metadata.get("is_split_from_long_element") and \
                       current_chunk.chunk_metadata.get("source_document_id") == next_chunk.chunk_metadata.get("source_document_id") and \
                       current_chunk.chunk_metadata.get("source_element_index") == next_chunk.chunk_metadata.get("source_element_index"):
                        
                        separator = "\n" if current_chunk.chunk_metadata.get("source_element_type") == "list_item" else " "
                        temp_merged_text = current_merged_text + separator + next_chunk.chunk_text
                        if len(temp_merged_text) <= config.max_merged_chunk_size:
                            can_merge = True
                            merge_reason = "same_parent_split"
                            potential_merged_text = temp_merged_text
                            chunks_to_merge.append(next_chunk)
                    
                    elif not current_chunk.chunk_metadata.get("is_split_from_long_element") and \
                         not next_chunk.chunk_metadata.get("is_split_from_long_element") and \
                         current_chunk.chunk_metadata.get("source_document_id") == next_chunk.chunk_metadata.get("source_document_id") and \
                         current_chunk.chunk_metadata.get("source_element_type") == next_chunk.chunk_metadata.get("source_element_type") and \
                         current_chunk.chunk_metadata.get("source_element_index", -2) + 1 == next_chunk.chunk_metadata.get("source_element_index", -1):
                         
                        separator = "\n" if current_chunk.chunk_metadata.get("source_element_type") == "list_item" else " "
                        temp_merged_text = current_merged_text + separator + next_chunk.chunk_text
                        if len(temp_merged_text) <= config.max_merged_chunk_size:
                            can_merge = True
                            merge_reason = "adjacent_same_type_short"
                            potential_merged_text = temp_merged_text
                            chunks_to_merge.append(next_chunk)

                if can_merge:
                    first_merged_chunk = chunks_to_merge[0]
                    second_merged_chunk = chunks_to_merge[1]
                    
                    merged_meta = first_merged_chunk.chunk_metadata.copy() 
                    final_merge_reason = merge_reason if merge_reason else "unknown_merge_rule"
                    merged_meta["chunking_strategy"] = f"merged_{final_merge_reason}_v1"
                    merged_meta["is_merged_chunk"] = True
                    merged_meta["original_chunk_ids_merged"] = [c.chunk_id for c in chunks_to_merge]
                    merged_meta["original_texts_lengths_merged"] = [len(c.chunk_text) for c in chunks_to_merge]
                    
                    if merge_reason == "title_narrative":
                        merged_meta["source_element_type"] = "narrative_text"
                        merged_meta["source_element_index_range"] = [
                            first_merged_chunk.chunk_metadata.get("source_element_index"),
                            second_merged_chunk.chunk_metadata.get("source_element_index")
                        ]
                        merged_meta["title_hierarchy"] = first_merged_chunk.chunk_metadata.get("title_hierarchy", {}) 
                    elif merge_reason == "adjacent_same_type_short":
                        merged_meta["source_element_index_range"] = [
                            first_merged_chunk.chunk_metadata.get("source_element_index"),
                            second_merged_chunk.chunk_metadata.get("source_element_index")
                        ]
                    elif merge_reason == "same_parent_split":
                        merged_meta["sub_chunk_sequence_in_element_range"] = [
                            first_merged_chunk.chunk_metadata.get("sub_chunk_sequence_in_element"),
                            second_merged_chunk.chunk_metadata.get("sub_chunk_sequence_in_element")
                        ]
                    
                    merged_chunk_output = ChunkOutput(
                        chunk_text=potential_merged_text,
                        source_document_id=first_merged_chunk.source_document_id,
                        chunk_metadata=merged_meta
                    )
                    merged_chunks.append(merged_chunk_output)
                    context.log.info(f"  Merged 2 chunks (indices {i} and {i+1}) into one. New len: {len(potential_merged_text)}. Strategy: {merged_meta['chunking_strategy']}.")
                    i += 2
                    merged_this_iteration = True
            
            if not merged_this_iteration:
                merged_chunks.append(current_chunk)
                i += 1
        else:
            merged_chunks.append(current_chunk)
            i += 1
            
    context.log.info(f"Merging process finished. Total chunks after merging: {len(merged_chunks)}")

    context.add_output_metadata(metadata={"total_chunks_generated_after_merge": len(merged_chunks)})
    return merged_chunks


# 在 processing_assets.py 中

@dg.asset(
    name="text_embeddings",
    description="Generates vector embeddings for text chunks.",
    group_name="processing",
    deps=["text_chunks"]
)
def generate_embeddings_asset(
    context: dg.AssetExecutionContext,
    text_chunks: List[ChunkOutput], # <--- 这是 text_chunks 资产的输出
    embedder: GGUFEmbeddingResource
) -> List[EmbeddingOutput]:
    # +++ 新增打印语句 +++
    context.log.info(f"generate_embeddings_asset: Received {len(text_chunks)} text_chunks.")
    if text_chunks:
        context.log.info(f"generate_embeddings_asset: First chunk text (first 100 chars): '{text_chunks[0].chunk_text[:100]}'")
        context.log.info(f"generate_embeddings_asset: First chunk metadata: {text_chunks[0].chunk_metadata}")
    # +++ 结束新增打印语句 +++

    all_embeddings: List[EmbeddingOutput] = []
    if not text_chunks:
        context.log.warning("generate_embeddings_asset: No text chunks received, returning empty list.") # 添加一个明确的警告
        return all_embeddings
    
    # --- 确保 chunk_texts_to_encode 不为空才调用 embedder.encode ---
    chunk_texts_to_encode = [chunk.chunk_text for chunk in text_chunks if chunk.chunk_text and chunk.chunk_text.strip()]
    
    if not chunk_texts_to_encode:
        context.log.warning("generate_embeddings_asset: All received text chunks are empty or whitespace after filtering. Returning empty list.")
        # 即使原始 text_chunks 非空，但如果所有 chunk_text 都无效，也应该返回空 embedding 列表
        # 并且要确保下游知道期望的 EmbeddingOutput 数量可能是0
        return all_embeddings # 返回空列表是正确的

    vectors = embedder.encode(chunk_texts_to_encode)

    # --- 确保正确地将嵌入结果映射回原始的 text_chunks 列表（如果数量可能不一致）---
    # 当前的逻辑是假设 vectors 和 chunk_texts_to_encode 一一对应，并且 text_chunks 的顺序与 chunk_texts_to_encode 过滤前的顺序相关
    # 如果 chunk_texts_to_encode 进行了过滤，这里的循环需要更小心
    
    # 一个更安全的映射方式是，只为那些实际被编码的文本块创建 EmbeddingOutput
    # 但这要求下游能处理 EmbeddingOutput 列表长度可能小于 ChunkOutput 列表长度的情况，
    # 或者，我们应该为那些被过滤掉的 chunk 也创建一个带有零向量的 EmbeddingOutput。
    # 我们之前的 LocalModelHandler 修改是为了处理单个空文本，现在这里是资产层面的。

    # 保持与 LocalModelHandler 类似的健壮性：为所有传入的 text_chunks 生成 EmbeddingOutput，
    # 如果其文本为空或嵌入失败，则使用零向量。

    embedding_map = {text: vec for text, vec in zip(chunk_texts_to_encode, vectors)}

    for i, chunk_input in enumerate(text_chunks):
        model_name_for_log = embedder.embedding_model_path
        embedding_vector_for_chunk = [0.0] * embedder.get_embedding_dimension() # 默认为零向量

        if chunk_input.chunk_text and chunk_input.chunk_text.strip() and chunk_input.chunk_text in embedding_map:
            embedding_vector_for_chunk = embedding_map[chunk_input.chunk_text]
        elif chunk_input.chunk_text and chunk_input.chunk_text.strip(): 
            # 文本有效但没有在 embedding_map 中找到 (可能因为 embedder.encode 内部的某些问题)
            context.log.warning(f"generate_embeddings_asset: Valid chunk text for chunk_id {chunk_input.chunk_id} was not found in embedding_map. Using zero vector.")
        else: # 文本本身就是空的
            context.log.info(f"generate_embeddings_asset: Chunk_id {chunk_input.chunk_id} has empty text. Using zero vector.")


        all_embeddings.append(EmbeddingOutput(
            chunk_id=chunk_input.chunk_id,
            chunk_text=chunk_input.chunk_text, # 存储原始文本，即使它是空的
            embedding_vector=embedding_vector_for_chunk,
            embedding_model_name=model_name_for_log,
            original_chunk_metadata=chunk_input.chunk_metadata
        ))
    
    context.add_output_metadata(metadata={"total_embeddings_generated": len(all_embeddings)})
    return all_embeddings


@dg.asset(
    name="vector_store_embeddings",
    description="Stores text embeddings into a ChromaDB vector store.",
    group_name="indexing",
    deps=["text_embeddings"]
)
def vector_storage_asset(
    context: dg.AssetExecutionContext,
    text_embeddings: List[EmbeddingOutput], # 这应该是来自 generate_embeddings_asset 的输出
    chroma_db: ChromaDBResource # 这是注入的 ChromaDBResource 实例
) -> None:
    if not text_embeddings:
        context.log.warning("vector_storage_asset: No embeddings received, nothing to store in ChromaDB.")
        context.add_output_metadata(metadata={"num_embeddings_stored": 0})
        return

    ids_to_store = [emb.chunk_id for emb in text_embeddings]
    embeddings_to_store = [emb.embedding_vector for emb in text_embeddings]
    documents_to_store = [emb.chunk_text for emb in text_embeddings] # 获取真实的文本内容
    
    cleaned_metadatas: List[Dict[str, Any]] = []
    for i, emb_output in enumerate(text_embeddings):
        # 确保原始元数据存在且是字典
        original_meta = emb_output.original_chunk_metadata if isinstance(emb_output.original_chunk_metadata, dict) else {}
        meta = original_meta.copy() # 使用副本
        
        # 确保 chunk_text 也加入到元数据中（如果需要的话，或者仅依赖 documents 参数）
        # ChromaDB 的 documents 参数用于存储主要文本，元数据用于附加信息
        # 但为了调试和可能的回退，在元数据中也保留一份 chunk_text 是有益的
        meta["chunk_text_in_meta"] = str(emb_output.chunk_text) if emb_output.chunk_text is not None else "[TEXT IS NULL]"

        cleaned_meta_item: Dict[str, Any] = {}
        for key, value in meta.items():
            if isinstance(value, dict):
                if key == "title_hierarchy" and not value: 
                    cleaned_meta_item[key] = "None"
                    context.log.debug(f"Metadata for chunk {emb_output.chunk_id}: Replaced empty title_hierarchy dict with 'None' string.")
                else:
                    try:
                        cleaned_meta_item[key] = json.dumps(value, ensure_ascii=False)
                    except TypeError:
                        cleaned_meta_item[key] = str(value)
                        context.log.warning(f"Metadata for chunk {emb_output.chunk_id}: Could not JSON serialize dict for key '{key}', used str(). Value: {str(value)[:100]}...")
            elif isinstance(value, list):
                try:
                    cleaned_meta_item[key] = json.dumps(value, ensure_ascii=False)
                except TypeError:
                    cleaned_meta_item[key] = str(value)
                    context.log.warning(f"Metadata for chunk {emb_output.chunk_id}: Could not JSON serialize list for key '{key}', used str(). Value: {str(value)[:100]}...")
            elif value is None:
                cleaned_meta_item[key] = "" 
            else: 
                cleaned_meta_item[key] = value
        cleaned_metadatas.append(cleaned_meta_item)

    context.log.info(f"vector_storage_asset: Preparing to add/update {len(ids_to_store)} items to ChromaDB collection '{chroma_db.collection_name}'.")
    if ids_to_store:
        context.log.info(f"vector_storage_asset: Sample ID to store: {ids_to_store[0]}")
        # 确保 documents_to_store 也有对应内容，并且不是 None
        sample_doc_text = "[EMPTY DOCUMENT]"
        if documents_to_store and documents_to_store[0] is not None:
            sample_doc_text = str(documents_to_store[0])[:100] # 显示前100字符
        elif documents_to_store and documents_to_store[0] is None:
            sample_doc_text = "[DOCUMENT IS NULL]"
        context.log.info(f"vector_storage_asset: Sample document to store (from documents_to_store, first 100 chars): '{sample_doc_text}'")
        
        sample_meta_text = "[NO METADATA]"
        if cleaned_metadatas:
            sample_meta_text = str(cleaned_metadatas[0])[:200] # 显示元数据摘要
        context.log.info(f"vector_storage_asset: Sample cleaned metadata for first item: {sample_meta_text}")

    try:
        chroma_db.add_embeddings(
            ids=ids_to_store, 
            embeddings=embeddings_to_store, 
            documents=documents_to_store, # 传递真实的文本内容给ChromaDB的documents字段
            metadatas=cleaned_metadatas
        )
        # 尝试获取并记录操作后的集合计数
        # 注意: chroma_db._collection 可能是私有属性，直接访问不推荐，但为了调试可以尝试
        # 更好的方式是 ChromaDBResource 提供一个 get_collection_count() 方法
        collection_count_after_add = -1 # 默认值
        try:
            if chroma_db._collection: # 确保 _collection 不是 None
                 collection_count_after_add = chroma_db._collection.count()
        except Exception as e_count:
            context.log.warning(f"vector_storage_asset: Could not get collection count after add: {e_count}")

        context.add_output_metadata(metadata={"num_embeddings_stored": len(ids_to_store), "collection_count_after_add": collection_count_after_add})
        context.log.info(f"vector_storage_asset: Successfully called add_embeddings. Stored {len(ids_to_store)} items. Collection count now: {collection_count_after_add}")
    except Exception as e_chroma_add:
        context.log.error(f"vector_storage_asset: Failed to add embeddings to ChromaDB: {e_chroma_add}", exc_info=True)
        raise

class BM25IndexConfig(dg.Config):
    index_file_path: str = "/home/zhz/zhz_agent/zhz_rag/stored_data/bm25_index/"


@dg.asset(
    name="keyword_index",
    description="Builds and persists a BM25 keyword index from text chunks.",
    group_name="indexing",
    deps=["text_chunks"]
)
def keyword_index_asset(
    context: dg.AssetExecutionContext,
    config: BM25IndexConfig, # 确保 BM25IndexConfig 在文件某处已定义
    text_chunks: List[ChunkOutput]
) -> None:
    if not text_chunks:
        context.log.warning("keyword_index_asset: No text chunks received, skipping BM25 index building.")
        context.add_output_metadata(metadata={"num_documents_indexed": 0, "index_directory_path": config.index_file_path})
        return

    # --- 新增：检查并记录空文本块 ---
    valid_chunks_for_indexing: List[ChunkOutput] = []
    for idx, chunk in enumerate(text_chunks):
        if chunk.chunk_text and chunk.chunk_text.strip():
            valid_chunks_for_indexing.append(chunk)
        else:
            context.log.warning(f"keyword_index_asset: Chunk {idx} (ID: {chunk.chunk_id}) has empty or whitespace-only text. Skipping for BM25 indexing.")
    
    if not valid_chunks_for_indexing:
        context.log.warning("keyword_index_asset: All received text chunks have empty or whitespace-only text after filtering. Skipping BM25 index building.")
        context.add_output_metadata(metadata={"num_documents_indexed": 0, "index_directory_path": config.index_file_path})
        return
    # --- 结束新增 ---

    # 使用过滤后的有效块
    corpus_texts = [chunk.chunk_text for chunk in valid_chunks_for_indexing]
    document_ids = [chunk.chunk_id for chunk in valid_chunks_for_indexing] # 确保ID与有效文本对应

    context.log.info(f"keyword_index_asset: Preparing to index {len(corpus_texts)} valid text chunks for BM25.")
    if corpus_texts: # 仅在有数据时打印样本
        context.log.info(f"keyword_index_asset: Sample document ID for BM25: {document_ids[0]}")
        context.log.info(f"keyword_index_asset: Sample document text for BM25 (first 50 chars): '{str(corpus_texts[0])[:50]}'")

    try:
        corpus_tokenized_jieba = [list(jieba.cut_for_search(text)) for text in corpus_texts]
        context.log.info(f"keyword_index_asset: Tokenized {len(corpus_tokenized_jieba)} texts for BM25.")
        
        bm25_model = bm25s.BM25() # 使用默认参数初始化
        context.log.info("keyword_index_asset: BM25 model initialized.")
        
        bm25_model.index(corpus_tokenized_jieba)
        indexed_doc_count = len(bm25_model.doc_freqs) if hasattr(bm25_model, 'doc_freqs') and bm25_model.doc_freqs is not None else len(corpus_tokenized_jieba)
        context.log.info(f"keyword_index_asset: BM25 model indexing complete for {indexed_doc_count} documents.")
        
        index_directory = config.index_file_path
        context.log.info(f"keyword_index_asset: BM25 index will be saved to directory: {index_directory}")
        os.makedirs(index_directory, exist_ok=True)
        
        bm25_model.save(index_directory) 
        context.log.info(f"keyword_index_asset: bm25_model.save('{index_directory}') called.")
        
        doc_ids_path = os.path.join(index_directory, "doc_ids.pkl")
        with open(doc_ids_path, 'wb') as f_out:
            pickle.dump(document_ids, f_out)
        context.log.info(f"keyword_index_asset: doc_ids.pkl saved to {doc_ids_path} with {len(document_ids)} IDs.")
        
        # 验证文件是否真的创建了
        expected_params_file = os.path.join(index_directory, "params.index.json") # bm25s 保存时会创建这个
        if os.path.exists(expected_params_file) and os.path.exists(doc_ids_path):
            context.log.info(f"keyword_index_asset: Verified that BM25 index files (e.g., params.index.json, doc_ids.pkl) exist in {index_directory}.")
        else:
            context.log.error(f"keyword_index_asset: BM25 index files (e.g., params.index.json or doc_ids.pkl) NOT FOUND in {index_directory} after save operations!")
            context.log.error(f"keyword_index_asset: Check - params.index.json exists: {os.path.exists(expected_params_file)}")
            context.log.error(f"keyword_index_asset: Check - doc_ids.pkl exists: {os.path.exists(doc_ids_path)}")
            # 如果文件未找到，可能需要抛出异常以使资产失败
            # raise FileNotFoundError(f"BM25 index files not found in {index_directory} after save.")

        context.add_output_metadata(
            metadata={
                "num_documents_indexed": len(corpus_texts), 
                "index_directory_path": index_directory,
                "bm25_corpus_size_actual": indexed_doc_count
            }
        )
        context.log.info("keyword_index_asset: BM25 indexing and saving completed successfully.")
    except Exception as e_bm25:
        context.log.error(f"keyword_index_asset: Error during BM25 indexing or saving: {e_bm25}", exc_info=True)
        raise

# --- KG Extraction 相关的配置和资产 ---

from zhz_rag.llm.rag_prompts import KG_EXTRACTION_SINGLE_CHUNK_PROMPT_TEMPLATE_V1, KG_EXTRACTION_BATCH_PROMPT_TEMPLATE_V1 # 导入常量

class KGExtractionConfig(dg.Config):
    extraction_prompt_template: str = KG_EXTRACTION_SINGLE_CHUNK_PROMPT_TEMPLATE_V1
    local_llm_model_name: str = "Qwen3-1.7B-GGUF_via_llama.cpp"

DEFAULT_KG_EXTRACTION_SCHEMA = {
    "type": "object",
    "properties": {
        "entities": {
            "type": "array",
            "items": {
                "type": "object",
                "properties": {
                    "text": {"type": "string", "description": "提取到的实体原文"},
                    "label": {"type": "string", "description": "实体类型 (例如: PERSON, ORGANIZATION, TASK)"}
                },
                "required": ["text", "label"]
            },
            "description": "从文本中提取出的实体列表。"
        },
        "relations": {
            "type": "array",
            "items": {
                "type": "object",
                "properties": {
                    "head_entity_text": {"type": "string", "description": "头实体的文本"},
                    "head_entity_label": {"type": "string", "description": "头实体的类型 (例如: PERSON, TASK)"},
                    "relation_type": {"type": "string", "description": "关系类型 (例如: WORKS_AT, ASSIGNED_TO)"},
                    "tail_entity_text": {"type": "string", "description": "尾实体的文本"},
                    "tail_entity_label": {"type": "string", "description": "尾实体的类型 (例如: ORGANIZATION, PERSON)"}
                },
                "required": ["head_entity_text", "head_entity_label", "relation_type", "tail_entity_text", "tail_entity_label"]
            },
            "description": "从文本中提取出的关系三元组列表。"
        }
    },
    "required": ["entities", "relations"]
}


@dg.asset(
    name="kg_extractions",
    description="Extracts entities and relations from text chunks for knowledge graph construction.",
    group_name="kg_building",
    io_manager_key="pydantic_json_io_manager",
    deps=["text_chunks"]
)
async def kg_extraction_asset(
    context: dg.AssetExecutionContext, # Pylance 提示 dg.AssetExecutionContext 未定义 "SystemResource"
    text_chunks: List[ChunkOutput],
    config: KGExtractionConfig,
    LocalLLM_api: LocalLLMAPIResource,
    system_info: SystemResource  # <--- 我们添加了 system_info
) -> List[KGTripleSetOutput]:
    all_kg_outputs: List[KGTripleSetOutput] = []
    if not text_chunks:
        context.log.info("No text chunks received for KG extraction, skipping.")
        return all_kg_outputs

    total_input_chunks = len(text_chunks)
    total_entities_extracted_overall = 0
    total_relations_extracted_overall = 0
    successfully_processed_chunks_count = 0
    
    # 并发控制参数
    recommended_concurrency = system_info.get_recommended_concurrent_tasks(task_type="kg_extraction_llm")
    CONCURRENT_REQUESTS_LIMIT = max(1, recommended_concurrency) # 直接使用HAL推荐，但至少为1
    context.log.info(f"HAL recommended concurrency for 'kg_extraction_llm': {recommended_concurrency}. Effective limit set to: {CONCURRENT_REQUESTS_LIMIT}")
    semaphore = asyncio.Semaphore(CONCURRENT_REQUESTS_LIMIT)


    async def extract_kg_for_chunk(chunk: ChunkOutput) -> Optional[KGTripleSetOutput]:
        async with semaphore:
            # 使用单个chunk的prompt模板
            prompt = config.extraction_prompt_template.format(text_to_extract=chunk.chunk_text)
            try:
                context.log.debug(f"Starting KG extraction for chunk_id: {chunk.chunk_id}, Text (start): {chunk.chunk_text[:100]}...")
                structured_response = await LocalLLM_api.generate_structured_output(
                    prompt=prompt, 
                    json_schema=DEFAULT_KG_EXTRACTION_SCHEMA # 使用单个对象的schema
                )
                
                # 确保 structured_response 是字典类型
                if not isinstance(structured_response, dict):
                    context.log.error(f"Failed KG extraction for chunk {chunk.chunk_id}: LLM response was not a dict. Got: {type(structured_response)}. Response: {str(structured_response)[:200]}")
                    return None

                entities_data = structured_response.get("entities", [])
                extracted_entities_list = [
                    ExtractedEntity(text=normalize_text_for_id(e.get("text","")), label=e.get("label","UNKNOWN").upper())
                    for e in entities_data if isinstance(e, dict)
                ]
                
                relations_data = structured_response.get("relations", [])
                extracted_relations_list = [
                    ExtractedRelation(
                        head_entity_text=r.get('head_entity_text',""), 
                        head_entity_label=r.get('head_entity_label',"UNKNOWN").upper(), 
                        relation_type=r.get('relation_type',"UNKNOWN").upper(), 
                        tail_entity_text=r.get('tail_entity_text',""), 
                        tail_entity_label=r.get('tail_entity_label',"UNKNOWN").upper()
                    ) 
                    for r in relations_data if isinstance(r, dict) and 
                                               r.get('head_entity_text') and r.get('head_entity_label') and
                                               r.get('relation_type') and r.get('tail_entity_text') and
                                               r.get('tail_entity_label')
                ]
                
                context.log.debug(f"Finished KG extraction for chunk_id: {chunk.chunk_id}. Entities: {len(extracted_entities_list)}, Relations: {len(extracted_relations_list)}")
                return KGTripleSetOutput(
                    chunk_id=chunk.chunk_id,
                    extracted_entities=extracted_entities_list,
                    extracted_relations=extracted_relations_list,
                    extraction_model_name=config.local_llm_model_name,
                    original_chunk_metadata=chunk.chunk_metadata
                )
            except Exception as e:
                context.log.error(f"Failed KG extraction for chunk {chunk.chunk_id}: {e}", exc_info=True)
                return None 

    context.log.info(f"Starting KG extraction for {total_input_chunks} chunks with concurrency limit: {CONCURRENT_REQUESTS_LIMIT}.")
    
    tasks = [extract_kg_for_chunk(chunk) for chunk in text_chunks]
    
    results = await asyncio.gather(*tasks)
    
    context.log.info(f"Finished all KG extraction tasks. Received {len(results)} results (including potential None for failures).")

    for result_item in results:
        if result_item and isinstance(result_item, KGTripleSetOutput):
            all_kg_outputs.append(result_item)
            total_entities_extracted_overall += len(result_item.extracted_entities)
            total_relations_extracted_overall += len(result_item.extracted_relations)
            successfully_processed_chunks_count +=1
        elif result_item is None:
            context.log.warning("A KG extraction task failed and returned None.")
            
    context.log.info(f"KG extraction complete. Successfully processed {successfully_processed_chunks_count} out of {total_input_chunks} chunks.")
    context.add_output_metadata(
        metadata={
            "total_chunks_input_to_kg": total_input_chunks, # 恢复为 total_input_chunks
            "chunks_successfully_extracted_kg": successfully_processed_chunks_count,
            "total_entities_extracted": total_entities_extracted_overall, 
            "total_relations_extracted": total_relations_extracted_overall
            # 移除了批处理相关的元数据 "total_batches_processed", "batch_size_configured"
        }
    )
    return all_kg_outputs


# --- KuzuDB 构建资产链 ---

@dg.asset(
    name="duckdb_schema", # <--- 修改资产名称
    description="Creates the base schema (node and relation tables) in DuckDB.",
    group_name="kg_building",
    # deps=[kg_extraction_asset] # 保持依赖，确保在提取之后创建schema (逻辑上)
                                 # 虽然schema创建本身不直接使用提取结果，但流水线顺序上合理
)
def duckdb_schema_asset(context: dg.AssetExecutionContext, duckdb_kg: DuckDBResource, embedder: GGUFEmbeddingResource): # <--- 修改函数名和资源参数
    context.log.info("--- Starting DuckDB Schema Creation Asset ---")
    
    # 获取嵌入维度，与KuzuDB时类似
    EMBEDDING_DIM = embedder.get_embedding_dimension()
    if not EMBEDDING_DIM:
        raise ValueError("Could not determine embedding dimension from GGUFEmbeddingResource.")

    node_table_ddl = f"""
    CREATE TABLE IF NOT EXISTS ExtractedEntity (
        id_prop VARCHAR PRIMARY KEY,
        text VARCHAR,
        label VARCHAR,
        embedding FLOAT[{EMBEDDING_DIM}]
    );
    """

    relation_table_ddl = f"""
    CREATE TABLE IF NOT EXISTS KGExtractionRelation (
        relation_id VARCHAR PRIMARY KEY,
        source_node_id_prop VARCHAR,
        target_node_id_prop VARCHAR,
        relation_type VARCHAR
        -- Optional: FOREIGN KEY (source_node_id_prop) REFERENCES ExtractedEntity(id_prop),
        -- Optional: FOREIGN KEY (target_node_id_prop) REFERENCES ExtractedEntity(id_prop)
    );
    """
    # 也可以为关系表的 (source, target, type) 创建复合唯一索引或普通索引以加速查询
    relation_index_ddl = """
    CREATE INDEX IF NOT EXISTS idx_relation_source_target_type 
    ON KGExtractionRelation (source_node_id_prop, target_node_id_prop, relation_type);
    """
    
    ddl_commands = [node_table_ddl, relation_table_ddl, relation_index_ddl]

    try:
        with duckdb_kg.get_connection() as conn:
            context.log.info("Executing DuckDB DDL commands...")
            for command_idx, command in enumerate(ddl_commands):
                context.log.debug(f"Executing DDL {command_idx+1}:\n{command.strip()}")
                conn.execute(command)
            context.log.info("DuckDB Schema DDL commands executed successfully.")
    except Exception as e_ddl:
        context.log.error(f"Error during DuckDB schema creation: {e_ddl}", exc_info=True)
        raise
    context.log.info("--- DuckDB Schema Creation Asset Finished ---")


@dg.asset(
    name="duckdb_nodes", # <--- 修改资产名称
    description="Loads all unique extracted entities as nodes into DuckDB.",
    group_name="kg_building",
    deps=[duckdb_schema_asset, kg_extraction_asset] # <--- 修改依赖
)
def duckdb_nodes_asset(
    context: dg.AssetExecutionContext,
    kg_extractions: List[KGTripleSetOutput], # 来自 kg_extraction_asset 的输出
    duckdb_kg: DuckDBResource,               # <--- 修改资源参数
    embedder: GGUFEmbeddingResource          # 保持对 embedder 的依赖，用于生成嵌入
):
    context.log.info("--- Starting DuckDB Node Loading Asset (Using INSERT ON CONFLICT) ---")
    if not kg_extractions:
        context.log.warning("No KG extractions received. Skipping node loading.")
        return

    unique_nodes_data_for_insert: List[Dict[str, Any]] = []
    unique_nodes_keys = set() # 用于在Python层面去重，避免多次尝试插入相同实体

    for kg_set in kg_extractions:
        for entity in kg_set.extracted_entities:
            # 规范化文本和标签，用于生成唯一键和存储
            normalized_text = normalize_text_for_id(entity.text)
            normalized_label = entity.label.upper() # 确保标签大写
            
            # 为实体生成唯一ID (基于规范化文本和标签的哈希值)
            # 注意：如果同一个实体（相同文本和标签）在不同chunk中被提取，它们的id_prop会一样
            node_id_prop = hashlib.md5(f"{normalized_text}_{normalized_label}".encode('utf-8')).hexdigest()
            
            node_unique_key_for_py_dedup = (node_id_prop) # 使用id_prop进行Python层面的去重

            if node_unique_key_for_py_dedup not in unique_nodes_keys:
                unique_nodes_keys.add(node_unique_key_for_py_dedup)
                
                # 生成嵌入向量 (与KuzuDB时逻辑相同)
                embedding_vector_list = embedder.encode([normalized_text]) # embedder.encode期望一个列表
                final_embedding_for_db: List[float]

                if embedding_vector_list and embedding_vector_list[0] and \
                   isinstance(embedding_vector_list[0], list) and \
                   len(embedding_vector_list[0]) == embedder.get_embedding_dimension():
                    final_embedding_for_db = embedding_vector_list[0]
                else:
                    context.log.warning(f"Failed to generate valid embedding for node: {normalized_text} ({normalized_label}). Using zero vector. Embedding result: {embedding_vector_list}")
                    final_embedding_for_db = [0.0] * embedder.get_embedding_dimension()
                    
                unique_nodes_data_for_insert.append({
                    "id_prop": node_id_prop,
                    "text": normalized_text,
                    "label": normalized_label,
                    "embedding": final_embedding_for_db # DuckDB的FLOAT[]可以直接接受Python的List[float]
                })

    if not unique_nodes_data_for_insert:
        context.log.warning("No unique nodes found in extractions to load into DuckDB.")
        return

    nodes_processed_count = 0
    nodes_inserted_count = 0
    nodes_updated_count = 0

    upsert_sql = f"""
    INSERT INTO ExtractedEntity (id_prop, text, label, embedding)
    VALUES (?, ?, ?, ?)
    ON CONFLICT (id_prop) DO UPDATE SET
        text = excluded.text,
        label = excluded.label,
        embedding = excluded.embedding;
    """
    # excluded.column_name 用于引用试图插入但导致冲突的值

    try:
        with duckdb_kg.get_connection() as conn:
            context.log.info(f"Attempting to UPSERT {len(unique_nodes_data_for_insert)} unique nodes into DuckDB ExtractedEntity table...")
            
            # DuckDB 支持 executemany 用于批量操作，但对于 ON CONFLICT，逐条执行或构造大型 VALUES 列表可能更直接
            # 或者使用 pandas DataFrame + duckdb.register + CREATE TABLE AS / INSERT INTO SELECT
            # 这里为了清晰，我们先用循环执行，对于几千到几万个节点，性能尚可接受
            # 如果节点数量非常大 (几十万以上)，应考虑更优化的批量upsert策略

            for node_data_dict in unique_nodes_data_for_insert:
                params = (
                    node_data_dict["id_prop"],
                    node_data_dict["text"],
                    node_data_dict["label"],
                    node_data_dict["embedding"]
                )
                try:
                    # conn.execute() 对于 DML (如 INSERT, UPDATE) 不直接返回受影响的行数
                    # 但我们可以假设它成功了，除非抛出异常
                    conn.execute(upsert_sql, params)
                    # 无法直接判断是insert还是update，除非查询前后对比，这里简化处理
                    nodes_processed_count += 1 
                except Exception as e_upsert_item:
                    context.log.error(f"Error UPSERTING node with id_prop {node_data_dict.get('id_prop')} into DuckDB: {e_upsert_item}", exc_info=True)
            
            # 我们可以查一下表中的总行数来间接了解情况
            total_rows_after = conn.execute("SELECT COUNT(*) FROM ExtractedEntity").fetchone()[0]
            context.log.info(f"Successfully processed {nodes_processed_count} node upsert operations into DuckDB.")
            context.log.info(f"Total rows in ExtractedEntity table after upsert: {total_rows_after}")
            # 注意：这里的 nodes_processed_count 不直接等于插入或更新数，而是尝试操作的次数
            # 如果需要精确计数插入/更新，需要更复杂的逻辑或DuckDB特定功能

    except Exception as e_db_nodes:
        context.log.error(f"Error during DuckDB node loading: {e_db_nodes}", exc_info=True)
        raise
    
    context.add_output_metadata({
        "nodes_prepared_for_upsert": len(unique_nodes_data_for_insert),
        "nodes_processed_by_upsert_statement": nodes_processed_count,
    })
    context.log.info("--- DuckDB Node Loading Asset Finished ---")



@dg.asset(
    name="duckdb_relations", # <--- 修改资产名称
    description="Loads all extracted relationships into DuckDB.",
    group_name="kg_building",
    deps=[duckdb_nodes_asset] # <--- 修改依赖
)
def duckdb_relations_asset(
    context: dg.AssetExecutionContext, 
    kg_extractions: List[KGTripleSetOutput], # 来自 kg_extraction_asset
    duckdb_kg: DuckDBResource                # <--- 修改资源参数
):
    context.log.info("--- Starting DuckDB Relation Loading Asset ---")
    if not kg_extractions:
        context.log.warning("No KG extractions received. Skipping relation loading.")
        return

    relations_to_insert: List[Dict[str, str]] = []
    unique_relation_keys = set() # 用于在Python层面去重

    for kg_set in kg_extractions:
        for rel in kg_set.extracted_relations:
            # 从实体文本和标签生成源节点和目标节点的ID (与 duckdb_nodes_asset 中一致)
            source_node_text_norm = normalize_text_for_id(rel.head_entity_text)
            source_node_label_norm = rel.head_entity_label.upper()
            source_node_id = hashlib.md5(f"{source_node_text_norm}_{source_node_label_norm}".encode('utf-8')).hexdigest()

            target_node_text_norm = normalize_text_for_id(rel.tail_entity_text)
            target_node_label_norm = rel.tail_entity_label.upper()
            target_node_id = hashlib.md5(f"{target_node_text_norm}_{target_node_label_norm}".encode('utf-8')).hexdigest()
            
            relation_type_norm = rel.relation_type.upper()

            # 为关系本身生成一个唯一ID
            relation_unique_str = f"{source_node_id}_{relation_type_norm}_{target_node_id}"
            relation_id = hashlib.md5(relation_unique_str.encode('utf-8')).hexdigest()

            if relation_id not in unique_relation_keys:
                unique_relation_keys.add(relation_id)
                relations_to_insert.append({
                    "relation_id": relation_id,
                    "source_node_id_prop": source_node_id,
                    "target_node_id_prop": target_node_id,
                    "relation_type": relation_type_norm
                })
    
    if not relations_to_insert:
        context.log.warning("No unique relations found in extractions to load into DuckDB.")
        return

    relations_processed_count = 0
    
    # 使用 INSERT INTO ... ON CONFLICT DO NOTHING 来避免插入重复的关系 (基于 relation_id)
    insert_sql = """
    INSERT INTO KGExtractionRelation (relation_id, source_node_id_prop, target_node_id_prop, relation_type)
    VALUES (?, ?, ?, ?)
    ON CONFLICT (relation_id) DO NOTHING;
    """

    try:
        with duckdb_kg.get_connection() as conn:
            context.log.info(f"Attempting to INSERT {len(relations_to_insert)} unique relations into DuckDB KGExtractionRelation table...")
            
            for rel_data_dict in relations_to_insert:
                params = (
                    rel_data_dict["relation_id"],
                    rel_data_dict["source_node_id_prop"],
                    rel_data_dict["target_node_id_prop"],
                    rel_data_dict["relation_type"]
                )
                try:
                    conn.execute(insert_sql, params)
                    # DuckDB的execute对于INSERT ON CONFLICT DO NOTHING不直接返回是否插入
                    # 但我们可以假设它成功处理了（要么插入，要么忽略）
                    relations_processed_count += 1
                except Exception as e_insert_item:
                    context.log.error(f"Error INSERTING relation with id {rel_data_dict.get('relation_id')} into DuckDB: {e_insert_item}", exc_info=True)
            
            total_rels_after = conn.execute("SELECT COUNT(*) FROM KGExtractionRelation").fetchone()[0]
            context.log.info(f"Successfully processed {relations_processed_count} relation insert (ON CONFLICT DO NOTHING) operations.")
            context.log.info(f"Total rows in KGExtractionRelation table after inserts: {total_rels_after}")

    except Exception as e_db_rels:
        context.log.error(f"Error during DuckDB relation loading: {e_db_rels}", exc_info=True)
        raise
        
    context.add_output_metadata({
        "relations_prepared_for_insert": len(relations_to_insert),
        "relations_processed_by_insert_statement": relations_processed_count,
    })
    context.log.info("--- DuckDB Relation Loading Asset Finished ---")



@dg.asset(
    name="duckdb_vector_index", # <--- 修改资产名称
    description="Creates the HNSW vector index on the embedding column in DuckDB.",
    group_name="kg_building",
    deps=[duckdb_relations_asset]  # <--- 修改依赖
)
def duckdb_vector_index_asset(
    context: dg.AssetExecutionContext, 
    duckdb_kg: DuckDBResource # <--- 修改资源参数
):
    context.log.info("--- Starting DuckDB Vector Index Creation Asset ---")
    
    table_to_index = "ExtractedEntity"
    column_to_index = "embedding"
    # 索引名可以自定义，通常包含表名、列名和类型
    index_name = f"{table_to_index}_{column_to_index}_hnsw_idx"
    metric_type = "l2sq" # 欧氏距离的平方，与我们测试时一致

    # DuckDB 的 CREATE INDEX ... USING HNSW 语句
    # IF NOT EXISTS 确保了幂等性
    index_creation_sql = f"""
    CREATE INDEX IF NOT EXISTS {index_name} 
    ON {table_to_index} USING HNSW ({column_to_index}) 
    WITH (metric='{metric_type}');
    """

    try:
        with duckdb_kg.get_connection() as conn:
            # 在创建索引前，确保vss扩展已加载且持久化已开启 (虽然DuckDBResource的setup已做)
            try:
                conn.execute("LOAD vss;")
                conn.execute("SET hnsw_enable_experimental_persistence=true;")
                context.log.info("DuckDB: VSS extension loaded and HNSW persistence re-confirmed for index creation asset.")
            except Exception as e_vss_setup_idx:
                context.log.warning(f"DuckDB: Failed to re-confirm VSS setup for index asset: {e_vss_setup_idx}. "
                                     "Proceeding, assuming it was set by DuckDBResource.")

            context.log.info(f"Executing DuckDB vector index creation command:\n{index_creation_sql.strip()}")
            conn.execute(index_creation_sql)
            context.log.info(f"DuckDB vector index '{index_name}' creation command executed successfully (or index already existed).")

    except Exception as e_index_asset:
        context.log.error(f"Error during DuckDB vector index creation: {e_index_asset}", exc_info=True)
        raise
    
    context.log.info("--- DuckDB Vector Index Creation Asset Finished ---")


# --- 更新 all_processing_assets 列表 ---
all_processing_assets = [
    clean_chunk_text_asset,
    generate_embeddings_asset,
    vector_storage_asset,
    keyword_index_asset,
    kg_extraction_asset,
    duckdb_schema_asset,
    duckdb_nodes_asset,
    duckdb_relations_asset,
    duckdb_vector_index_asset,
]
```

File: zhz_rag_pipeline_dagster/zhz_rag_pipeline/pydantic_models_dagster.py
--------------------------------------------------------------------------
```python
# zhz_rag_pipeline/pydantic_models_dagster.py
from typing import List, Dict, Any, Union, Optional, Literal
# --- 修改：从 pydantic 导入 BaseModel 和 Field ---
from pydantic import BaseModel, Field
# --- 修改结束 ---
import uuid
# from typing import List # 这行是多余的，因为上面已经从 typing 导入了 List

class LoadedDocumentOutput(BaseModel):
    document_path: str
    file_type: str
    raw_content: Union[str, bytes]
    metadata: Dict[str, Any]

# --- 修改：在 ParsedDocumentOutput 定义之前定义其依赖的 Element 类型 ---
class DocumentElementMetadata(BaseModel):
    """通用元数据，可附加到任何文档元素上"""
    page_number: Optional[int] = None
    source_coordinates: Optional[Dict[str, float]] = None # 例如，PDF中的bbox
    custom_properties: Optional[Dict[str, Any]] = None # 其他特定于元素的属性

class TitleElement(BaseModel):
    element_type: Literal["title"] = "title"
    text: str
    level: int # 例如 1 代表 H1, 2 代表 H2
    metadata: Optional[DocumentElementMetadata] = None

class NarrativeTextElement(BaseModel): # 普通段落文本
    element_type: Literal["narrative_text"] = "narrative_text"
    text: str
    metadata: Optional[DocumentElementMetadata] = None

class ListItemElement(BaseModel):
    element_type: Literal["list_item"] = "list_item"
    text: str
    level: int = 0 # 列表嵌套层级，0代表顶层列表项
    ordered: bool = False # True代表有序列表项, False代表无序
    item_number: Optional[Union[int, str]] = None # 例如 "1", "a", "*"
    metadata: Optional[DocumentElementMetadata] = None

class TableElement(BaseModel):
    element_type: Literal["table"] = "table"
    text_representation: Optional[str] = None 
    markdown_representation: Optional[str] = None
    html_representation: Optional[str] = None
    caption: Optional[str] = None
    metadata: Optional[DocumentElementMetadata] = None

class CodeBlockElement(BaseModel):
    element_type: Literal["code_block"] = "code_block"
    code: str
    language: Optional[str] = None
    metadata: Optional[DocumentElementMetadata] = None

class ImageElement(BaseModel): 
    element_type: Literal["image"] = "image"
    alt_text: Optional[str] = None
    caption: Optional[str] = None
    metadata: Optional[DocumentElementMetadata] = None

class PageBreakElement(BaseModel):
    element_type: Literal["page_break"] = "page_break"
    metadata: Optional[DocumentElementMetadata] = None
    
class HeaderElement(BaseModel):
    element_type: Literal["header"] = "header"
    text: str
    metadata: Optional[DocumentElementMetadata] = None

class FooterElement(BaseModel):
    element_type: Literal["footer"] = "footer"
    text: str
    metadata: Optional[DocumentElementMetadata] = None

DocumentElementType = Union[
    TitleElement, 
    NarrativeTextElement, 
    ListItemElement, 
    TableElement, 
    CodeBlockElement,
    ImageElement,
    PageBreakElement,
    HeaderElement,
    FooterElement
]

class ParsedDocumentOutput(BaseModel):
    parsed_text: str = Field(description="文档内容的线性化纯文本表示，尽可能保留语义。") 
    elements: List[DocumentElementType] = Field(default_factory=list, description="从文档中解析出的结构化元素列表。")
    original_metadata: Dict[str, Any] = Field(description="关于原始文档的元数据，如文件名、路径、大小等。")
    summary: Optional[str] = None
# --- 已有模型 ---
class ChunkOutput(BaseModel):
    chunk_id: str = Field(default_factory=lambda: str(uuid.uuid4())) # 确保 Field 被导入
    chunk_text: str
    source_document_id: str 
    chunk_metadata: Dict[str, Any]

class EmbeddingOutput(BaseModel):
    chunk_id: str 
    chunk_text: str 
    embedding_vector: List[float]
    embedding_model_name: str 
    original_chunk_metadata: Dict[str, Any]

class ExtractedEntity(BaseModel):
    text: str 
    label: str 

class ExtractedRelation(BaseModel):
    head_entity_text: str
    head_entity_label: str
    relation_type: str
    tail_entity_text: str
    tail_entity_label: str

class KGTripleSetOutput(BaseModel):
    chunk_id: str 
    extracted_entities: List[ExtractedEntity] = Field(default_factory=list)
    extracted_relations: List[ExtractedRelation] = Field(default_factory=list) 
    extraction_model_name: str 
    original_chunk_metadata: Dict[str, Any]
```

File: zhz_rag_pipeline_dagster/zhz_rag_pipeline/resources.py
------------------------------------------------------------
```python
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/zhz_rag_pipeline/resources.py
import dagster as dg
from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.config import Settings
from typing import List, Dict, Any, Optional, Iterator
import httpx
import json
import litellm
import os
import shutil
from contextlib import contextmanager
from pydantic import Field as PydanticField
from pydantic import PrivateAttr
import asyncio
import time	
import gc
import duckdb

# --- START: 添加 HardwareManager 导入 ---
try:
    # 假设 hardware_manager.py 位于项目的 utils 目录下，
    # 并且 zhz_agent 是项目的根目录且在 PYTHONPATH 中
    from utils.hardware_manager import HardwareManager, HardwareInfo
except ImportError as e_hal_import:
    # 如果 utils 目录不在 zhz_agent 下，或者 PYTHONPATH 问题，可能需要调整
    # 例如，如果 hardware_manager.py 与 resources.py 在同一目录的父目录的utils下：
    # from ..utils.hardware_manager import HardwareManager, HardwareInfo
    print(f"ERROR: Failed to import HardwareManager/HardwareInfo from utils.hardware_manager: {e_hal_import}. "
          "Ensure it's in the correct path relative to this file or PYTHONPATH is set. "
          "Proceeding without HAL capabilities.")
    HardwareManager = None
    HardwareInfo = None # type: ignore
# --- END: 添加 HardwareManager 导入 ---

try:
    from zhz_rag.llm.local_model_handler import LocalModelHandler
except ImportError as e:
    print(f"FATAL: Could not import LocalModelHandler from zhz_rag.llm.local_model_handler. Error: {e}")
    raise

class GGUFEmbeddingResourceConfig(dg.Config):
    # --- 采纳外部AI建议的修改 ---
    embedding_model_path: str  # 必需字段，直接类型注解
    n_ctx: int = 2048          # 可选字段，直接赋予Python默认值
    n_gpu_layers: int = 0      # 可选字段，直接赋予Python默认值
    # --- 修改结束 ---

class GGUFEmbeddingResource(dg.ConfigurableResource):
    embedding_model_path: str
    n_ctx: int
    n_gpu_layers: int

    _model_handler: Optional[LocalModelHandler] = PrivateAttr(default=None)
    _logger: Optional[dg.DagsterLogManager] = PrivateAttr(default=None)
    _dimension: Optional[int] = PrivateAttr(default=None)
    # _dimension_lock: asyncio.Lock = PrivateAttr(default_factory=asyncio.Lock) # 在纯同步场景下暂时不需要

    def _run_async_in_new_loop(self, coro):
        # 这个辅助函数在同步方法中运行异步协程是正确的
        # 对于 Dagster 这种多进程/多线程环境，确保每次都用新循环可能更安全
        loop = asyncio.new_event_loop()
        asyncio.set_event_loop(loop)
        try:
            result = loop.run_until_complete(coro)
        finally:
            loop.close()
        return result

    def setup_for_execution(self, context: dg.InitResourceContext) -> None:
        self._logger = context.log
        self._logger.info(f"Initializing GGUFEmbeddingResource with model: {self.embedding_model_path}")
        
        if not os.path.exists(self.embedding_model_path):
            error_msg = f"GGUF embedding model file not found at: {self.embedding_model_path}"
            self._logger.error(error_msg)
            raise FileNotFoundError(error_msg)

        try:
            self._model_handler = LocalModelHandler(
                embedding_model_path=self.embedding_model_path,
                n_ctx_embed=self.n_ctx,
                n_gpu_layers_embed=self.n_gpu_layers,
                # embedding_pool_size 可以考虑从Config传入或使用默认
            )
            self._logger.info(f"GGUFEmbeddingResource initialized. LocalModelHandler created. Dimension will be fetched on first use.")
        except Exception as e:
            self._logger.error(f"Failed to initialize GGUFEmbeddingResource (LocalModelHandler creation failed): {e}", exc_info=True)
            raise

    def _ensure_dimension_is_known(self) -> int:
        if self._dimension is None:
            if self._model_handler is None: # 确保 _model_handler 已初始化
                self._logger.error("GGUFEmbeddingResource: LocalModelHandler not initialized in _ensure_dimension_is_known.")
                raise RuntimeError("LocalModelHandler not initialized, cannot get dimension.")
            
            self._logger.info("GGUFEmbeddingResource: Dimension not yet known, attempting to fetch from LocalModelHandler...")
            try:
                # _get_embedding_dimension_from_worker_once() 是 LocalModelHandler 的异步方法
                dim = self._run_async_in_new_loop(self._model_handler._get_embedding_dimension_from_worker_once())
                if dim is None or dim <= 0:
                    raise ValueError(f"LocalModelHandler returned invalid dimension: {dim}")
                self._dimension = dim
                self._logger.info(f"GGUFEmbeddingResource: Dimension fetched and cached: {self._dimension}")
            except Exception as e:
                self._logger.error(f"GGUFEmbeddingResource: Failed to fetch embedding dimension: {e}", exc_info=True)
                raise RuntimeError(f"Could not determine embedding dimension: {e}") from e
        return self._dimension

    def encode(self, texts: List[str], **kwargs: Any) -> List[List[float]]:
        if self._model_handler is None:
            self._logger.error("GGUFEmbeddingResource: LocalModelHandler not initialized in encode.")
            raise RuntimeError("GGUFEmbeddingResource is not initialized (model_handler is None).")
        
        self._ensure_dimension_is_known() 

        logger_instance = self._logger if self._logger else dg.get_dagster_logger()
        logger_instance.debug(f"GGUFEmbeddingResource: Encoding {len(texts)} texts using LocalModelHandler (async call).")
        
        try:
            embeddings = self._run_async_in_new_loop(
                self._model_handler.embed_documents(texts)
            )
            return embeddings
        except Exception as e:
             logger_instance.error(f"GGUFEmbeddingResource: Error during encode: {e}", exc_info=True)
             raise


    def get_embedding_dimension(self) -> int:
        """返回嵌入模型的维度大小。如果尚未获取，则会尝试获取。"""
        if not hasattr(self, '_model_handler') or self._model_handler is None: # 增加对 _model_handler 是否为 None 的检查
            self._logger.error("GGUFEmbeddingResource: LocalModelHandler not initialized in get_embedding_dimension.")
            raise RuntimeError("GGUFEmbeddingResource is not initialized (model_handler is None).")
        
        # 直接调用内部方法来确保维度是已知的
        # _ensure_dimension_is_known 会处理维度为 None 的情况，并主动去获取
        return self._ensure_dimension_is_known()


# --- ChromaDBResource ---
class ChromaDBResourceConfig(dg.Config):
    collection_name: str = "rag_documents"
    persist_directory: str = "/home/zhz/zhz_agent/zhz_rag/stored_data/chromadb_index/"

class ChromaDBResource(dg.ConfigurableResource):
    collection_name: str
    persist_directory: str

    _client: chromadb.Client = PrivateAttr(default=None)
    _collection: chromadb.Collection = PrivateAttr(default=None)
    _logger: Optional[dg.DagsterLogManager] = PrivateAttr(default=None)

    def setup_for_execution(self, context: dg.InitResourceContext) -> None:
        self._logger = context.log
        self._logger.info(f"Initializing ChromaDB client and collection '{self.collection_name}'...")
        self._logger.info(f"ChromaDB data will be persisted to: {self.persist_directory}")
        try:
            os.makedirs(self.persist_directory, exist_ok=True)
            self._client = chromadb.PersistentClient(path=self.persist_directory)
            self._collection = self._client.get_or_create_collection(
                name=self.collection_name,
                metadata={"hnsw:space": "cosine"}
            )
            self._logger.info(f"ChromaDB collection '{self.collection_name}' initialized/loaded. Count: {self._collection.count()}")
        except Exception as e:
            self._logger.error(f"Failed to initialize ChromaDB: {e}", exc_info=True)
            raise

    def add_embeddings(
        self, 
        ids: List[str], 
        embeddings: List[List[float]], 
        documents: Optional[List[str]] = None, # <--- 添加 documents 参数，并设为可选
        metadatas: Optional[List[Dict[str, Any]]] = None # <--- 将 metadatas 也设为可选，与ChromaDB客户端一致
    ):
        logger_instance = self._logger if self._logger else dg.get_dagster_logger()
        if self._collection is None:
            logger_instance.error("ChromaDB collection is not initialized. Cannot add embeddings.")
            raise RuntimeError("ChromaDB collection is not initialized.")
        
        # 参数长度校验
        num_ids = len(ids)
        if not (num_ids == len(embeddings) and \
                (documents is None or num_ids == len(documents)) and \
                (metadatas is None or num_ids == len(metadatas))):
            logger_instance.error(
                f"Length mismatch: ids({num_ids}), embeddings({len(embeddings)}), "
                f"documents({len(documents) if documents else 'None'}), metadatas({len(metadatas) if metadatas else 'None'})."
            )
            raise ValueError("Length of ids, embeddings, and documents/metadatas (if provided) must be the same.")

        if not ids:
            logger_instance.info("No ids provided to add_embeddings, skipping.")
            return

        logger_instance.info(f"Adding/updating {len(ids)} items to ChromaDB collection '{self.collection_name}'...")
        try:
            self._collection.add(
                ids=ids, 
                embeddings=embeddings, 
                documents=documents, # <--- 将 documents 参数传递给 collection.add
                metadatas=metadatas
            )
            logger_instance.info(f"Items added/updated. Collection count now: {self._collection.count()}")
        except Exception as e_add:
            logger_instance.error(f"Error during self._collection.add: {e_add}", exc_info=True)
            raise
        
    def query_embeddings(self, query_embeddings: List[List[float]], n_results: int = 5) -> chromadb.QueryResult:
        logger_instance = self._logger if self._logger else dg.get_dagster_logger()
        if self._collection is None:
            logger_instance.error("ChromaDB collection is not initialized. Cannot query embeddings.")
            raise RuntimeError("ChromaDB collection is not initialized.")
        logger_instance.debug(f"Querying ChromaDB collection '{self.collection_name}' with {len(query_embeddings)} vectors, n_results={n_results}.")
        return self._collection.query(query_embeddings=query_embeddings, n_results=n_results)

# --- LocalLLMAPIResource ---
class LocalLLMAPIResourceConfig(dg.Config):
    api_url: str = "http://127.0.0.1:8088/v1/chat/completions" # <--- 修改
    default_temperature: float = 0.1
    default_max_new_tokens: int = 2048

class LocalLLMAPIResource(dg.ConfigurableResource):
    # 我们将 SGLangAPIResource 重命名为 LocalLLMAPIResource
    api_url: str
    default_temperature: float
    default_max_new_tokens: int
    _logger: Optional[dg.DagsterLogManager] = PrivateAttr(default=None)

    def setup_for_execution(self, context: dg.InitResourceContext) -> None:
        self._logger = context.log
        self._logger.info(f"LocalLLMAPIResource configured with API URL: {self.api_url}") # <--- 修改日志信息

    # generate_structured_output 方法的逻辑需要调整以适配 OpenAI 兼容的 API
    async def generate_structured_output(
        self, prompt: str, json_schema: Dict[str, Any],
        temperature: Optional[float] = None, max_new_tokens: Optional[int] = None
    ) -> Dict[str, Any]:
        logger_instance = self._logger if self._logger else dg.get_dagster_logger()
        temp_to_use = temperature if temperature is not None else self.default_temperature
        tokens_to_use = max_new_tokens if max_new_tokens is not None else self.default_max_new_tokens

        messages = [{"role": "user", "content": prompt}] # 简化处理

        payload = {
            "model": "local_kg_extraction_model", # 模型名对于本地服务不重要，但需要有
            "messages": messages,
            "temperature": temp_to_use,
            "max_tokens": tokens_to_use,
            "response_format": { # 我们的 local_llm_service.py 支持这个
                "type": "json_object",
                "schema": json_schema
            }
        }
        logger_instance.debug(f"Sending request to Local LLM Service. Prompt (start): {prompt[:100]}...")

        try:
            timeout_config = httpx.Timeout(
                connect=30.0,    # 连接超时30秒
                read=300.0,      # 读取超时300秒 (5分钟)
                write=300.0,     # 写入超时300秒 (5分钟)
                pool=30.0        # 从连接池获取连接的超时30秒
            )
            async with httpx.AsyncClient(timeout=timeout_config) as client:
                response = await client.post(self.api_url, json=payload) # 使用 self.api_url
                response.raise_for_status()
                response_json = response.json()
                
                # OpenAI 兼容 API 的响应格式是 {"choices": [{"message": {"content": "..."}}]}
                if response_json.get("choices") and response_json["choices"][0].get("message"):
                    generated_text = response_json["choices"][0]["message"].get("content", "")
                    logger_instance.debug(f"Local LLM raw response text: {generated_text}")
                    try:
                        parsed_output = json.loads(generated_text)
                        return parsed_output
                    except json.JSONDecodeError as e:
                        logger_instance.error(f"Failed to decode JSON from Local LLM output: {generated_text}. Error: {e}", exc_info=True)
                        raise ValueError(f"Local LLM output was not valid JSON: {generated_text}") from e
                else:
                    raise ValueError(f"Local LLM response format is incorrect: {response_json}")
        except httpx.HTTPStatusError as e:
            logger_instance.error(f"Local LLM API HTTP error: {e.response.status_code} - {e.response.text}", exc_info=True)
            raise
        except httpx.RequestError as e:
            logger_instance.error(f"Local LLM API request error: {e}", exc_info=True)
            raise
        except Exception as e:
            logger_instance.error(f"Unexpected error during Local LLM call: {e}", exc_info=True)
            raise

# --- GeminiAPIResource ---
class GeminiAPIResourceConfig(dg.Config):
    model_name: str = PydanticField(default="gemini/gemini-1.5-flash-latest", description="Name of the Gemini model.")
    proxy_url: Optional[str] = PydanticField(default_factory=lambda: os.getenv("LITELLM_PROXY_URL"), description="Optional proxy URL for LiteLLM.")
    default_temperature: float = 0.1
    default_max_tokens: int = 2048
    
class GeminiAPIResource(dg.ConfigurableResource):
    model_name: str
    proxy_url: Optional[str]
    default_temperature: float
    default_max_tokens: int
    _api_key: Optional[str] = PrivateAttr(default=None)
    _logger: Optional[dg.DagsterLogManager] = PrivateAttr(default=None)

    def setup_for_execution(self, context: dg.InitResourceContext) -> None:
        self._logger = context.log
        self._api_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
        if self.model_name and not self.model_name.startswith("gemini/"):
            if "gemini" in self.model_name.lower():
                self._logger.info(f"Model name '{self.model_name}' auto-prefixed to 'gemini/'.")
                self.model_name = f"gemini/{self.model_name.split('/')[-1]}"
            else:
                self._logger.warning(f"Model name '{self.model_name}' does not start with 'gemini/'.")
        if not self._api_key:
            self._logger.warning("Gemini API key not found. API calls will likely fail.")
        else:
            self._logger.info(f"GeminiAPIResource initialized. Model: {self.model_name}, Proxy: {self.proxy_url or 'Not set'}")

    async def call_completion(
        self, messages: List[Dict[str, str]],
        temperature: Optional[float] = None, max_tokens: Optional[int] = None,
    ) -> Optional[str]:
        logger_instance = self._logger if self._logger else dg.get_dagster_logger()
        if not self._api_key:
            logger_instance.error("Gemini API key is not configured.")
            return None
        temp_to_use = temperature if temperature is not None else self.default_temperature
        tokens_to_use = max_tokens if max_tokens is not None else self.default_max_tokens
        litellm_params = {
            "model": self.model_name, "messages": messages, "api_key": self._api_key,
            "temperature": temp_to_use, "max_tokens": tokens_to_use,
        }
        if self.proxy_url:
            litellm_params["proxy"] = {"http": self.proxy_url, "https": self.proxy_url} # type: ignore
        logger_instance.debug(f"Calling LiteLLM (Gemini) with params (excluding messages): { {k:v for k,v in litellm_params.items() if k != 'messages'} }")
        raw_output_text: Optional[str] = None
        try:
            response = await litellm.acompletion(**litellm_params) # type: ignore
            if response and response.choices and response.choices[0].message and response.choices[0].message.content:
                raw_output_text = response.choices[0].message.content
                logger_instance.debug(f"LiteLLM (Gemini) raw response (first 300 chars): {raw_output_text[:300]}...")
            else:
                logger_instance.warning(f"LiteLLM (Gemini) returned empty/malformed response: {response}")
        except Exception as e_generic:
            logger_instance.error(f"Error calling Gemini via LiteLLM: {e_generic}", exc_info=True)
        return raw_output_text

class DuckDBResource(dg.ConfigurableResource):
    db_file_path: str = PydanticField(
        default=os.path.join(os.getenv("ZHZ_AGENT_PROJECT_ROOT", "/home/zhz/zhz_agent"), "zhz_rag", "stored_data", "duckdb_knowledge_graph.db"),
        description="Path to the DuckDB database file for the knowledge graph."
    )
    # 可以在这里添加其他配置，例如 read_only, vss_persistence 等

    _conn: Optional[duckdb.DuckDBPyConnection] = PrivateAttr(default=None)
    _logger: Optional[dg.DagsterLogManager] = PrivateAttr(default=None)

    # --- 【用这个版本覆盖原来的 setup_for_execution 方法】 ---
    def setup_for_execution(self, context: dg.InitResourceContext) -> None:
        self._logger = context.log
        db_path_to_use = self.db_file_path
        
        db_dir = os.path.dirname(db_path_to_use)
        if not os.path.exists(db_dir):
            os.makedirs(db_dir, exist_ok=True)
            self._logger.info(f"Created directory for DuckDB database: {db_dir}")

        self._logger.info(f"Attempting to connect to DuckDB at: {db_path_to_use}")
        
        try:
            # 连接数据库
            self._conn = duckdb.connect(database=db_path_to_use, read_only=False)
            self._logger.info(f"Successfully connected to DuckDB: {db_path_to_use}")
            
            # 强制加载VSS扩展
            # 即使之前已加载，重复LOAD通常是安全的，或者会快速返回
            # 关键在于确保当前连接的上下文中VSS是可用的
            try:
                self._logger.info("DuckDBResource: Forcefully attempting to INSTALL and LOAD vss extension for current connection.")
                self._conn.execute("INSTALL vss;")      # 尝试安装（如果未安装）
                self._conn.execute("LOAD vss;")         # 尝试加载
                self._logger.info("DuckDBResource: VSS extension INSTALL and LOAD sequence completed for current connection.")
                
                # 启用HNSW持久化（如果需要）
                try:
                    self._conn.execute("SET hnsw_enable_experimental_persistence=true;")
                    self._logger.info("DuckDBResource: HNSW experimental persistence enabled for current connection.")
                except Exception as e_persistence:
                    self._logger.warning(f"DuckDBResource: Failed to enable HNSW experimental persistence: {e_persistence}. This might affect index persistence if new HNSW indexes are created by this resource/asset.")
            
            except duckdb.CatalogException as e_vss_cat:
                if "already loaded" in str(e_vss_cat).lower() or "already installed" in str(e_vss_cat).lower():
                    self._logger.info(f"DuckDBResource: VSS extension reported as already installed/loaded: {e_vss_cat}")
                    # 即使已加载，也尝试再次LOAD以确保当前会话可用
                    try:
                        self._conn.execute("LOAD vss;")
                        self._logger.info("DuckDBResource: VSS extension re-LOADED successfully.")
                    except Exception as e_reload:
                        self._logger.error(f"DuckDBResource: Failed to re-LOAD VSS extension even if reported as installed/loaded: {e_reload}", exc_info=True)
                        raise RuntimeError(f"Critical: Failed to ensure VSS is loaded for DuckDB connection: {e_reload}") from e_reload
                else:
                    self._logger.error(f"DuckDBResource: CatalogException during VSS setup (not 'already loaded/installed'): {e_vss_cat}", exc_info=True)
                    raise RuntimeError(f"Failed to setup VSS extension for DuckDB: {e_vss_cat}") from e_vss_cat
            except Exception as e_vss_other:
                self._logger.error(f"DuckDBResource: Other exception during VSS setup: {e_vss_other}", exc_info=True)
                raise RuntimeError(f"Failed to setup VSS extension for DuckDB: {e_vss_other}") from e_vss_other

        except duckdb.duckdb.Error as e_connect_wal: # 捕获包括WAL重放错误在内的DuckDB连接错误
            self._logger.error(f"Failed to connect to DuckDB at {db_path_to_use} or error during WAL replay: {e_connect_wal}", exc_info=True)
            # 检查错误信息是否与VSS有关
            if "unknown index type 'HNSW'" in str(e_connect_wal):
                self._logger.error("DuckDBResource: WAL replay failed due to HNSW index. This strongly indicates VSS was not loaded prior to the operation that created/modified the index or during this connection attempt before WAL replay.")
            raise # 将原始的DuckDB连接错误重新抛出
        except Exception as e_connect_generic: # 捕获其他可能的连接错误
            self._logger.error(f"Generic error connecting to DuckDB at {db_path_to_use}: {e_connect_generic}", exc_info=True)
            raise

    def teardown_for_execution(self, context: dg.InitResourceContext) -> None:
        if self._conn:
            self._logger.info(f"Closing DuckDB connection to: {self.db_file_path}")
            self._conn.close()
            self._conn = None
        self._logger.info("DuckDBResource teardown complete.")

    @contextmanager
    def get_connection(self) -> Iterator[duckdb.DuckDBPyConnection]:
        if not self._conn:
            # 这种情况下，我们应该在 setup_for_execution 中确保连接已建立
            # 或者在这里尝试重新连接，但这会使逻辑复杂化。
            # Dagster 资源通常期望 setup_for_execution 已经准备好了资源。
            self._logger.error("DuckDB connection not established during setup. Cannot yield connection.")
            raise ConnectionError("DuckDB connection was not established by setup_for_execution.")
        
        # 简单的版本：直接 yield 已经建立的连接
        # 注意：这种共享连接的方式对于并发的 Dagster ops (如果使用非 in_process_executor) 需要小心
        # 但对于 in_process_executor 和我们的流水线是安全的。
        yield self._conn


# --- START: 定义 SystemResource ---
class SystemResource(dg.ConfigurableResource):
    """
    Dagster resource to provide system hardware information and recommendations
    based on Hardware Abstraction Layer (HAL).
    """
    _hw_manager: Optional[Any] = PrivateAttr(default=None)
    _hw_info: Optional[Any] = PrivateAttr(default=None)
    _logger: Optional[dg.DagsterLogManager] = PrivateAttr(default=None)
    
    # 可以添加一些配置项来微调HAL的行为，如果需要的话
    # 例如：safety_vram_buffer_gb_override: Optional[float] = None

    def setup_for_execution(self, context: dg.InitResourceContext) -> None:
        self._logger = context.log
        self._logger.info("Initializing SystemResource...")
        if HardwareManager:
            try:
                self._hw_manager = HardwareManager()
                self._hw_info = self._hw_manager.get_hardware_info()
                if self._hw_info:
                    self._logger.info(f"SystemResource: Hardware detection successful: {self._hw_info}")
                else:
                    self._logger.warning("SystemResource: HardwareManager did not return hardware info.")
            except Exception as e:
                self._logger.error(f"SystemResource: Error initializing HardwareManager: {e}", exc_info=True)
        else:
            self._logger.warning("SystemResource: HardwareManager class not available. HAL features will be disabled.")
        self._logger.info("SystemResource initialized.")

    def get_hardware_info(self) -> Optional[Any]:
        """Returns the detected HardwareInfo object, or None if detection failed."""
        if not self._hw_info:
            self._logger.warning("Accessing hardware info, but it was not successfully detected during initialization.")
        return self._hw_info

    def get_recommended_llm_gpu_layers(
        self, 
        model_total_layers: int, 
        model_size_on_disk_gb: float, 
        context_length_tokens: int,
        # 可以暴露HAL方法中的其他参数作为此方法的参数，或使用默认/固定值
        kv_cache_gb_per_1k_ctx: float = 0.25, 
        safety_buffer_vram_gb: float = 1.5
    ) -> int:
        """
        Delegates to HardwareManager to recommend n_gpu_layers.
        Returns 0 if HAL is not available or GPU is not suitable.
        """
        if self._hw_manager:
            try:
                return self._hw_manager.recommend_llm_gpu_layers(
                    model_total_layers=model_total_layers,
                    model_size_on_disk_gb=model_size_on_disk_gb,
                    kv_cache_gb_per_1k_ctx=kv_cache_gb_per_1k_ctx,
                    context_length_tokens=context_length_tokens,
                    safety_buffer_vram_gb=safety_buffer_vram_gb
                )
            except Exception as e:
                self._logger.error(f"Error getting GPU layer recommendation from HAL: {e}", exc_info=True)
                return 0 # Fallback to CPU on error
        self._logger.warning("HardwareManager not available, defaulting to 0 GPU layers.")
        return 0 # Fallback to CPU

    def get_recommended_concurrent_tasks(self, task_type: str = "cpu_bound_llm") -> int:
        """
        Delegates to HardwareManager to recommend concurrent task number.
        Returns 1 if HAL is not available.
        """
        if self._hw_manager:
            try:
                return self._hw_manager.recommend_concurrent_tasks(task_type=task_type)
            except Exception as e:
                self._logger.error(f"Error getting concurrent task recommendation from HAL: {e}", exc_info=True)
                return 1 # Fallback to 1 on error
        self._logger.warning("HardwareManager not available, defaulting to 1 concurrent task.")
        return 1 # Fallback
# --- END: 定义 SystemResource ---
```

File: zhz_rag_pipeline_dagster/zhz_rag_pipeline/parsers/__init__.py
-------------------------------------------------------------------
```python
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/zhz_rag_pipeline/parsers/__init__.py
import logging # 添加 logging 导入
from typing import Callable, Dict, Any, Optional, Union

# 尝试导入 Pydantic 模型，如果失败，则类型别名使用 Any
try:
    from ..pydantic_models_dagster import ParsedDocumentOutput
    _ParserOutputType = Optional[ParsedDocumentOutput]
except ImportError:
    _ParserOutputType = Optional[Any] # Fallback

# 定义一个类型别名，表示解析函数的签名
# 输入可以是路径(str)或内容(str/bytes)，元数据字典，返回Pydantic模型或字典
ParserFunction = Callable[[Union[str, bytes], Dict[str, Any]], _ParserOutputType]

# 从各个解析器模块导入主解析函数
from .md_parser import parse_markdown_to_structured_output
from .docx_parser import parse_docx_to_structured_output
from .pdf_parser import parse_pdf_to_structured_output
from .xlsx_parser import parse_xlsx_to_structured_output
from .html_parser import parse_html_to_structured_output
from .txt_parser import parse_txt_to_structured_output

logger = logging.getLogger(__name__) # 添加 logger 实例

# 创建一个解析器注册表 (合并自 parser_dispatcher.py)
PARSER_REGISTRY: Dict[str, ParserFunction] = {
    ".md": parse_markdown_to_structured_output,
    ".docx": parse_docx_to_structured_output,
    ".pdf": parse_pdf_to_structured_output,
    ".xlsx": parse_xlsx_to_structured_output,
    ".html": parse_html_to_structured_output,
    ".htm": parse_html_to_structured_output,  # Alias for html
    ".txt": parse_txt_to_structured_output,
}

def dispatch_parsing( # 合并自 parser_dispatcher.py
    file_extension: str,
    content_or_path: Union[str, bytes], # 确保这里是 Union[str, bytes]
    original_metadata: Dict[str, Any]
) -> Optional[Any]: # 返回 Optional[Any] 以匹配下游期望
    parser_func = PARSER_REGISTRY.get(file_extension.lower())
    if parser_func:
        try:
            # 调用相应的解析函数
            # txt_parser 和 md_parser, html_parser 期望 content_str
            # docx_parser, pdf_parser, xlsx_parser 期望 file_path
            # content_or_path 变量在 ingestion_assets.py 中已经根据 file_ext 做了区分
            return parser_func(content_or_path, original_metadata)
        except Exception as e:
            logger.error(f"Error calling parser for '{file_extension}' on '{original_metadata.get('source_file_path', 'N/A')}': {e}", exc_info=True)
            return None # 解析失败返回 None
    else:
        logger.warning(f"No specific parser registered for file type '{file_extension}'.")
        # 尝试一个通用的纯文本提取作为最终回退（如果适用且有实现）
        # 或者直接返回None
        return None

def get_parser(file_extension: str) -> Optional[ParserFunction]: # 保留此函数以防其他地方用到
    return PARSER_REGISTRY.get(file_extension.lower())

__all__ = [
    "parse_markdown_to_structured_output",
    "parse_docx_to_structured_output",
    "parse_pdf_to_structured_output",
    "parse_xlsx_to_structured_output",
    "parse_html_to_structured_output",
    "parse_txt_to_structured_output",
    "get_parser", # 保留
    "dispatch_parsing", # 新增导出
    "PARSER_REGISTRY",
    "ParserFunction"
]
```

File: zhz_rag_pipeline_dagster/zhz_rag_pipeline/parsers/docx_parser.py
----------------------------------------------------------------------
```python
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/zhz_rag_pipeline/parsers/docx_parser.py

import os
from typing import List, Dict, Any, Optional, Union
import re

# --- 添加：为当前模块的 logger 进行基本配置 ---
import logging
logger = logging.getLogger(__name__)
if not logger.handlers: # 避免重复添加 handler (如果模块被多次导入)
    handler = logging.StreamHandler() # 输出到 stderr，通常会被 Dagster 捕获
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    logger.setLevel(logging.INFO) # 设置希望看到的最低日志级别 (INFO, DEBUG等)
    # logger.propagate = False # 可以考虑设置，防止日志向上传播到根logger导致重复打印，但通常 Dagster 会处理好
logger.info(f"Logger for {__name__} configured in docx_parser.py.") # 确认配置生效
# --- 结束添加 ---

# --- 依赖导入与可用性检查 ---
try:
    from unstructured.partition.docx import partition_docx
    from unstructured.documents.elements import (
        Element as UnstructuredElement,
        Text, 
        NarrativeText,
        Title,
        ListItem,
        Table,
        Image as UnstructuredImage, 
        Header as UnstructuredHeader, 
        Footer as UnstructuredFooter, 
        Address,
        EmailAddress,
        FigureCaption,
        PageBreak as UnstructuredPageBreak, 
        CodeSnippet
    )
    _UNSTRUCTURED_AVAILABLE_DOCX = True
    logging.info("Successfully imported Unstructured for DOCX parsing.")
except ImportError as e_unstructured:
    logging.error(f"Failed to import Unstructured for DOCX: {e_unstructured}. DOCX parsing will have limited functionality.")
    _UNSTRUCTURED_AVAILABLE_DOCX = False
    # 创建占位符类以避免后续 NameError
    class UnstructuredElement: pass
    class Text: pass                  # type: ignore
    class NarrativeText: pass          # type: ignore
    class Title: pass                  # type: ignore
    class ListItem: pass               # type: ignore
    class Table: pass                  # type: ignore
    class UnstructuredImage: pass      # type: ignore
    class UnstructuredHeader: pass     # type: ignore
    class UnstructuredFooter: pass     # type: ignore
    class Address: pass                # type: ignore
    class EmailAddress: pass           # type: ignore
    class FigureCaption: pass          # type: ignore
    class UnstructuredPageBreak: pass  # type: ignore
    class CodeSnippet: pass            # type: ignore

try:
    from markdownify import markdownify as md # type: ignore
    _MARKDOWNIFY_AVAILABLE = True
except ImportError:
    logging.warning("markdownify library not found. HTML table to Markdown conversion will be skipped.")
    _MARKDOWNIFY_AVAILABLE = False
    def md(html_content: str) -> str: # Fallback
        return f"[Markdownify not available. HTML content: {html_content[:100]}...]"

_PYDANTIC_MODELS_AVAILABLE_DOCX = False
try:
    from ..pydantic_models_dagster import (
        ParsedDocumentOutput, DocumentElementType, TitleElement, NarrativeTextElement,
        ListItemElement, TableElement, CodeBlockElement, PageBreakElement, ImageElement,
        HeaderElement, FooterElement, DocumentElementMetadata
    )
    _PYDANTIC_MODELS_AVAILABLE_DOCX = True
except ImportError:
    class BaseModel: pass
    class DocumentElementMetadata(BaseModel): page_number: Optional[int] = None
    
    class ParsedDocumentOutput(BaseModel): parsed_text: str; elements: list; original_metadata: dict; summary: Optional[str] = None
    class TitleElement(BaseModel): element_type:str="title"; text:str; level:int; metadata: Optional[DocumentElementMetadata] = None
    class NarrativeTextElement(BaseModel): element_type:str="narrative_text"; text:str; metadata: Optional[DocumentElementMetadata] = None
    class ListItemElement(BaseModel): element_type:str="list_item"; text:str; level:int=0; ordered:bool=False; item_number:Optional[Union[int, str]]=None; metadata: Optional[DocumentElementMetadata] = None
    class TableElement(BaseModel): # <--- 修改此行
        element_type:str="table"; 
        markdown_representation:Optional[str]=None; 
        html_representation:Optional[str]=None; 
        text_representation:Optional[str]=None; # <--- 添加此字段
        caption:Optional[str]=None; 
        metadata: Optional[DocumentElementMetadata] = None
    class CodeBlockElement(BaseModel): element_type:str="code_block"; code:str; language:Optional[str]=None; metadata: Optional[DocumentElementMetadata] = None
    class PageBreakElement(BaseModel): element_type:str="page_break"; metadata: Optional[DocumentElementMetadata] = None
    class ImageElement(BaseModel): element_type:str="image"; alt_text:Optional[str]=None; caption:Optional[str]=None; metadata: Optional[DocumentElementMetadata] = None
    class HeaderElement(BaseModel): element_type:str="header"; text:str; metadata: Optional[DocumentElementMetadata] = None
    class FooterElement(BaseModel): element_type:str="footer"; text:str; metadata: Optional[DocumentElementMetadata] = None
    DocumentElementType = Any

logger = logging.getLogger(__name__)

# --- 辅助函数 ---
def _create_doc_element_metadata(unstructured_element: UnstructuredElement) -> Optional[Union[DocumentElementMetadata, Dict[str, Any]]]:
    if not hasattr(unstructured_element, 'metadata'):
        return None
        
    meta_data_dict: Dict[str, Any] = {}
    if hasattr(unstructured_element.metadata, 'page_number') and unstructured_element.metadata.page_number is not None:
        meta_data_dict['page_number'] = unstructured_element.metadata.page_number
    
    if hasattr(unstructured_element.metadata, 'filename'):
        meta_data_dict['source_filename'] = unstructured_element.metadata.filename
    if hasattr(unstructured_element.metadata, 'filetype'):
        meta_data_dict['source_filetype'] = unstructured_element.metadata.filetype

    if not meta_data_dict:
        return None

    if _PYDANTIC_MODELS_AVAILABLE_DOCX:
        return DocumentElementMetadata(**meta_data_dict)
    else:
        return meta_data_dict

def _convert_unstructured_elements_to_custom(
    unstructured_elements: List[UnstructuredElement], 
    doc_path_for_log: str # 添加一个参数用于日志记录
) -> List[Any]:
    custom_elements: List[Any] = []
    
    file_basename_for_log = os.path.basename(doc_path_for_log)

    # --- 使用 print 进行强制调试 ---
    logger.info(f"DOCX Parser ({file_basename_for_log}): _convert_unstructured_elements_to_custom received {len(unstructured_elements)} elements from unstructured.")
    if not unstructured_elements:
        logger.warning(f"DOCX Parser ({file_basename_for_log}): Unstructured returned an empty list of elements. No custom elements will be generated by this function initially.")
    
    if not _UNSTRUCTURED_AVAILABLE_DOCX:
        logger.warning(f"DOCX Parser ({file_basename_for_log}): Unstructured library is not available (should have been caught earlier).")
        # 作为回退，我们可以尝试将每个元素的文本提取为 NarrativeTextElement (如果 unstructured_elements 非空但 _UNSTRUCTURED_AVAILABLE_DOCX 意外为 False)
        for el_idx, el_fallback in enumerate(unstructured_elements):
            fallback_text = getattr(el_fallback, 'text', f"[Unstructured not fully available - Element {el_idx+1} in {file_basename_for_log}]").strip()
            if fallback_text:
                if _PYDANTIC_MODELS_AVAILABLE_DOCX:
                    custom_elements.append(NarrativeTextElement(text=fallback_text))
                else:
                    custom_elements.append({"element_type": "narrative_text", "text": fallback_text})
        return custom_elements

    for el_idx, el in enumerate(unstructured_elements):
        el_type_name = type(el).__name__
        el_id_str = getattr(el, 'id', 'N/A')
        el_text_preview = getattr(el, 'text', '')[:50].strip().replace('\n', ' ') if getattr(el, 'text', '') else "[NO TEXT]"
        # --- 修改日志级别 ---
        logger.debug( # <--- 从 info 修改为 debug
            f"DOCX Parser ({file_basename_for_log}): Processing unstructured element index {el_idx}, "
            f"Type: {el_type_name}, ID: {el_id_str}, Text Preview: '{el_text_preview}'"
        )
        
        # 打印 el.metadata.text_as_html 的预览（如果存在）
        html_preview_from_meta = getattr(el.metadata, 'text_as_html', None) if hasattr(el, 'metadata') else None
        if html_preview_from_meta:
            logger.debug( # <--- 从 info 修改为 debug
                f"  └─ ({file_basename_for_log}) Unstructured Element (idx {el_idx}, type {el_type_name}) has text_as_html (len: {len(html_preview_from_meta)}). Preview: {html_preview_from_meta[:70]}"
            )
        # --- 结束修改 ---
        
        element_metadata = _create_doc_element_metadata(el)
        el_text = el.text.strip() if hasattr(el, 'text') and el.text else ""
        custom_el: Optional[Any] = None

        if isinstance(el, Title):
            level = el.metadata.category_depth if hasattr(el.metadata, 'category_depth') and el.metadata.category_depth is not None else 1
            if el_text:
                logger.debug(f"DOCX Parser ({file_basename_for_log}): Element index {el_idx} is Title (level {level}).")
                if _PYDANTIC_MODELS_AVAILABLE_DOCX: custom_el = TitleElement(text=el_text, level=level, metadata=element_metadata)
                else: custom_el = {"element_type": "title", "text": el_text, "level": level, "metadata": element_metadata}
        
        elif isinstance(el, ListItem):
            level = el.metadata.category_depth if hasattr(el.metadata, 'category_depth') and el.metadata.category_depth is not None else 0
            # 尝试从元数据获取更精确的列表信息 (unstructured 可能提供)
            # item_number 和 ordered 的逻辑可以根据 unstructured 的实际输出来完善
            if el_text:
                logger.debug(f"DOCX Parser ({file_basename_for_log}): Element index {el_idx} is ListItem (level {level}).")
                if _PYDANTIC_MODELS_AVAILABLE_DOCX: custom_el = ListItemElement(text=el_text, level=level, metadata=element_metadata)
                else: custom_el = {"element_type": "list_item", "text": el_text, "level": level, "metadata": element_metadata}

        elif isinstance(el, Table):
            logger.info(f"DOCX Parser ({file_basename_for_log}): Element index {el_idx} IS an unstructured.documents.elements.Table object.")
            html_table = el.metadata.text_as_html if hasattr(el.metadata, 'text_as_html') else None
            
            if html_table:
                logger.info(f"DOCX Parser ({file_basename_for_log}): Found HTML table content for Table element (idx {el_idx}). Length: {len(html_table)}. Preview: {html_table[:150]}")
            else:
                logger.warning(f"DOCX Parser ({file_basename_for_log}): No HTML table content (el.metadata.text_as_html) found for Table element (idx {el_idx}) from unstructured. Element ID: {el.id if hasattr(el, 'id') else 'N/A'}")

            md_table = None
            if html_table and _MARKDOWNIFY_AVAILABLE:
                try: 
                    md_table = md(html_table)
                    logger.info(f"DOCX Parser ({file_basename_for_log}): Successfully converted HTML table (idx {el_idx}) to Markdown. MD Length: {len(md_table) if md_table else 0}")
                except Exception as e_md: 
                    logger.warning(f"DOCX Parser ({file_basename_for_log}): Failed to convert HTML table (idx {el_idx}) to Markdown: {e_md}. HTML: {html_table[:100]}")
            
            raw_table_text_fallback = el.text.strip() if hasattr(el, 'text') and el.text else None
            caption_text = None
            if hasattr(el.metadata, 'table_captions') and el.metadata.table_captions:
                    caption_obj = el.metadata.table_captions[0]
                    if hasattr(caption_obj, 'text'):
                            caption_text = caption_obj.text
            
            if not caption_text and hasattr(el.metadata, 'filename'): # Redundant if filename is always doc_path_for_log
                    caption_text = f"Table from {file_basename_for_log}" # Use basename
            final_caption = caption_text if caption_text else "Table"

            final_md_table = md_table
            final_html_table = html_table
            final_text_representation = None

            if not final_md_table and not final_html_table and raw_table_text_fallback:
                logger.info(f"DOCX Parser ({file_basename_for_log}): Table (idx {el_idx}) has no HTML/MD rep, but has raw text from unstructured: '{raw_table_text_fallback[:100]}...' Using it as text_representation.")
                final_text_representation = raw_table_text_fallback
            elif not final_md_table and not final_html_table and not raw_table_text_fallback:
                    logger.warning(f"DOCX Parser ({file_basename_for_log}): Table (idx {el_idx}) has no HTML, Markdown, or raw text representation from unstructured.")


            if _PYDANTIC_MODELS_AVAILABLE_DOCX: 
                custom_el = TableElement(
                    markdown_representation=final_md_table, 
                    html_representation=final_html_table, 
                    text_representation=final_text_representation,
                    caption=final_caption, 
                    metadata=element_metadata
                )
            else: 
                custom_el = {
                    "element_type": "table", 
                    "markdown_representation": final_md_table, 
                    "html_representation": final_html_table, 
                    "text_representation": final_text_representation,
                    "caption": final_caption, 
                    "metadata": element_metadata
                }
        
        elif isinstance(el, (NarrativeText, Text, Address, EmailAddress, FigureCaption)):
            if el_text:
                logger.debug(f"DOCX Parser ({file_basename_for_log}): Element index {el_idx} is NarrativeText/Text like.")
                if _PYDANTIC_MODELS_AVAILABLE_DOCX: custom_el = NarrativeTextElement(text=el_text, metadata=element_metadata)
                else: custom_el = {"element_type": "narrative_text", "text": el_text, "metadata": element_metadata}
        
        elif isinstance(el, UnstructuredHeader):
            if el_text:
                logger.debug(f"DOCX Parser ({file_basename_for_log}): Element index {el_idx} is Header.")
                if _PYDANTIC_MODELS_AVAILABLE_DOCX: custom_el = HeaderElement(text=el_text, metadata=element_metadata)
                else: custom_el = {"element_type": "header", "text": el_text, "metadata": element_metadata}
        
        elif isinstance(el, UnstructuredFooter):
            if el_text:
                logger.debug(f"DOCX Parser ({file_basename_for_log}): Element index {el_idx} is Footer.")
                if _PYDANTIC_MODELS_AVAILABLE_DOCX: custom_el = FooterElement(text=el_text, metadata=element_metadata)
                else: custom_el = {"element_type": "footer", "text": el_text, "metadata": element_metadata}

        elif isinstance(el, UnstructuredImage):
            alt_text = el_text if el_text else (el.metadata.filename if hasattr(el.metadata, 'filename') else "Image")
            logger.debug(f"DOCX Parser ({file_basename_for_log}): Element index {el_idx} is Image. Alt text: {alt_text}")
            if _PYDANTIC_MODELS_AVAILABLE_DOCX: custom_el = ImageElement(alt_text=alt_text, metadata=element_metadata)
            else: custom_el = {"element_type": "image", "alt_text": alt_text, "metadata": element_metadata}

        elif isinstance(el, UnstructuredPageBreak):
            logger.debug(f"DOCX Parser ({file_basename_for_log}): Element index {el_idx} is PageBreak.")
            if _PYDANTIC_MODELS_AVAILABLE_DOCX: custom_el = PageBreakElement(metadata=element_metadata)
            else: custom_el = {"element_type": "page_break", "metadata": element_metadata}
        
        elif isinstance(el, CodeSnippet):
            if el_text:
                logger.debug(f"DOCX Parser ({file_basename_for_log}): Element index {el_idx} is CodeSnippet.")
                if _PYDANTIC_MODELS_AVAILABLE_DOCX: custom_el = CodeBlockElement(code=el_text, metadata=element_metadata) # language can be inferred later if needed
                else: custom_el = {"element_type": "code_block", "code": el_text, "metadata": element_metadata}

        else: 
            # This is the catch-all for any other Unstructured element type
            # or if an element doesn't have text but we still want to represent it (though usually skipped if no text)
            if el_text: # Only create an element if there's text
                logger.warning(f"DOCX Parser ({file_basename_for_log}): Unhandled Unstructured element type: {el_type_name} at index {el_idx}. Treating as NarrativeText. Text: {el_text[:50]}")
                if _PYDANTIC_MODELS_AVAILABLE_DOCX: custom_el = NarrativeTextElement(text=el_text, metadata=element_metadata)
                else: custom_el = {"element_type": "narrative_text", "text": el_text, "_unstructured_type": el_type_name, "metadata": element_metadata}
            elif el_type_name != "CompositeElement": # CompositeElement often has no direct text but contains other elements
                    logger.debug(f"DOCX Parser ({file_basename_for_log}): Skipping Unstructured element type: {el_type_name} at index {el_idx} due to no text content.")

        if custom_el:
            custom_elements.append(custom_el)
            
    return custom_elements


def _generate_linear_text_from_custom_elements(elements: List[Any]) -> str:
    text_parts = []
    for el_data_any in elements:
        el_data: Dict[str, Any] = {}
        if _PYDANTIC_MODELS_AVAILABLE_DOCX and hasattr(el_data_any, 'model_dump'):
            el_data = el_data_any.model_dump(exclude_none=True)
        elif isinstance(el_data_any, dict):
            el_data = el_data_any
        else:
            continue

        el_type = el_data.get("element_type")
        text_content = el_data.get("text", "")
        
        current_element_text = ""
        if el_type == "title":
            current_element_text = f"\n{'#' * el_data.get('level',1)} {text_content}\n"
        elif el_type == "narrative_text":
            current_element_text = text_content + "\n"
        elif el_type == "list_item":
            prefix = f"{el_data.get('item_number', '')}. " if el_data.get('ordered') and el_data.get('item_number') else "- "
            indent = "  " * el_data.get('level', 0)
            current_element_text = f"{indent}{prefix}{text_content}\n"
        elif el_type == "table":
            caption = el_data.get('caption', 'Unnamed Table')
            md_repr = el_data.get('markdown_representation')
            if md_repr: current_element_text = f"\n[Table: {caption}]\n{md_repr}\n"
            elif el_data.get('html_representation'): current_element_text = f"\n[Table (HTML): {caption}]\n{el_data.get('html_representation')[:200]}...\n"
        elif el_type == "code_block":
            lang = el_data.get('language', "") or ""
            code_content = el_data.get('code', "")
            current_element_text = f"\n```{lang}\n{code_content}\n```\n"
        elif el_type == "page_break":
            current_element_text = "\n---\n"
        elif el_type == "header" or el_type == "footer":
            current_element_text = f"\n[{el_type.capitalize()}]: {text_content}\n"
        elif el_type == "image":
            alt_text = el_data.get('alt_text', 'Image')
            current_element_text = f"\n[Image: {alt_text}]\n"
        
        if current_element_text:
            text_parts.append(current_element_text)

    full_text = "".join(text_parts)
    full_text = re.sub(r'\n{3,}', '\n\n', full_text).strip() # Clean up excessive newlines
    return full_text


# --- 主解析函数 ---
def parse_docx_to_structured_output(
    file_path: str, 
    original_metadata: Dict[str, Any]
) -> Optional[Union[ParsedDocumentOutput, Dict[str, Any]]]:
    file_basename_for_log = os.path.basename(file_path)
    
    # --- 使用 print 进行强制调试 ---
    logger.info(f"DOCX Parser: Attempting to parse DOCX file: {file_basename_for_log} using Unstructured")
    
    if not _UNSTRUCTURED_AVAILABLE_DOCX:
        logger.error(f"DOCX Parser ({file_basename_for_log}): Unstructured library is not available. DOCX parsing cannot proceed.")
        return None
    try:
        unstructured_elements = partition_docx(
            filename=file_path, 
            strategy="fast", 
            infer_table_structure=True,
        )
        logger.info(f"DOCX Parser ({file_basename_for_log}): Unstructured partitioned DOCX. Found {len(unstructured_elements)} raw elements from partition_docx.")

        custom_elements = _convert_unstructured_elements_to_custom(unstructured_elements, file_path)
        logger.info(f"DOCX Parser ({file_basename_for_log}): _convert_unstructured_elements_to_custom created {len(custom_elements)} custom elements.")
        
        linear_text = _generate_linear_text_from_custom_elements(custom_elements)
        logger.info(f"DOCX Parser ({file_basename_for_log}): Generated linear text (len: {len(linear_text)}). Preview: {linear_text[:100].replace(chr(10), ' ')}")

        if not custom_elements and not linear_text.strip() and unstructured_elements:
            logger.warning(f"DOCX Parser ({file_basename_for_log}): partition_docx returned {len(unstructured_elements)} elements, "
                            "but no custom elements or linear text were generated. This might indicate all elements were skipped "
                            "or had no text content suitable for conversion.")
        elif not custom_elements and not linear_text.strip() and not unstructured_elements:
                logger.warning(f"DOCX Parser ({file_basename_for_log}): partition_docx returned 0 elements, "
                                "and no custom elements or linear text were generated.")

        if _PYDANTIC_MODELS_AVAILABLE_DOCX:
            return ParsedDocumentOutput(
                parsed_text=linear_text,
                elements=custom_elements, 
                original_metadata=original_metadata
            )
        else:
            return {
                "parsed_text": linear_text,
                "elements": custom_elements,
                "original_metadata": original_metadata
            }
            
    except FileNotFoundError:
        logger.error(f"DOCX Parser ({file_basename_for_log}): File not found: {file_path}")
        return None
    except ImportError as ie:
        logger.error(f"DOCX Parser ({file_basename_for_log}): ImportError during DOCX parsing with Unstructured: {ie}.")
        return None
    except Exception as e:
        logger.error(f"DOCX Parser ({file_basename_for_log}): Critical error parsing DOCX file: {e}", exc_info=True)
        error_message = f"[ERROR PARSING DOCX: {file_basename_for_log} - {type(e).__name__}: {str(e)}]"
        if _PYDANTIC_MODELS_AVAILABLE_DOCX:
            return ParsedDocumentOutput(
                parsed_text=error_message,
                elements=[],
                original_metadata=original_metadata
            )
        else:
            return {
                "parsed_text": error_message,
                "elements": [],
                "original_metadata": original_metadata
            }
```

File: zhz_rag_pipeline_dagster/zhz_rag_pipeline/parsers/html_parser.py
----------------------------------------------------------------------
```python
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/zhz_rag_pipeline/parsers/html_parser.py
import os
import logging
from typing import List, Dict, Any, Optional, Union, Set
import re

try:
    from bs4 import BeautifulSoup, Tag, NavigableString
    _BS4_AVAILABLE = True
    logging.info("Successfully imported BeautifulSoup4 for HTML parsing.")
except ImportError:
    logging.error("BeautifulSoup4 (bs4) not found. HTML parsing will not be available.")
    _BS4_AVAILABLE = False
    class BeautifulSoup: pass # Placeholder
    class Tag: pass
    class NavigableString: pass


_PYDANTIC_MODELS_AVAILABLE_HTML = False
try:
    from ..pydantic_models_dagster import (
        ParsedDocumentOutput, DocumentElementType, TitleElement, NarrativeTextElement,
        ListItemElement, TableElement, CodeBlockElement, PageBreakElement,
        DocumentElementMetadata
    )
    _PYDANTIC_MODELS_AVAILABLE_HTML = True
except ImportError:
    class BaseModel: pass
    class DocumentElementMetadata(BaseModel): page_number: Optional[int] = None
    class ParsedDocumentOutput(BaseModel): parsed_text: str; elements: list; original_metadata: dict; summary: Optional[str] = None
    class TitleElement(BaseModel): element_type:str="title"; text:str; level:int; metadata: Optional[DocumentElementMetadata] = None
    class NarrativeTextElement(BaseModel): element_type:str="narrative_text"; text:str; metadata: Optional[DocumentElementMetadata] = None
    class ListItemElement(BaseModel): element_type:str="list_item"; text:str; level:int=0; ordered:bool=False; item_number:Optional[Union[int, str]]=None; metadata: Optional[DocumentElementMetadata] = None
    class TableElement(BaseModel): element_type:str="table"; markdown_representation:Optional[str]=None; html_representation:Optional[str]=None; caption:Optional[str]=None; metadata: Optional[DocumentElementMetadata] = None
    class CodeBlockElement(BaseModel): element_type:str="code_block"; code:str; language:Optional[str]=None; metadata: Optional[DocumentElementMetadata] = None
    class PageBreakElement(BaseModel): element_type:str="page_break"; metadata: Optional[DocumentElementMetadata] = None
    DocumentElementType = Any

logger = logging.getLogger(__name__)

# Tags to typically ignore for main content extraction
IGNORE_TAGS_HTML = ['script', 'style', 'nav', 'footer', 'header', 'aside', 'form', 'meta', 'link', 'button', 'input', 'noscript', 'iframe', 'canvas', 'svg', 'path']
# Tags that define a semantic block but we want to process their children
CONTAINER_TAGS_HTML = ['div', 'section', 'article', 'main', 'body', 'figure', 'figcaption', 'details', 'summary']


def _table_to_markdown(table_tag: Tag) -> str:
    """Converts a BeautifulSoup table Tag to a Markdown string."""
    md_rows = []
    header_processed = False
    
    # Process header (thead)
    thead = table_tag.find('thead')
    if thead:
        header_rows_tags = thead.find_all('tr')
        for hr_tag in header_rows_tags:
            header_cells = hr_tag.find_all(['th', 'td'])
            if header_cells:
                header_texts = [cell.get_text(separator=' ', strip=True) for cell in header_cells]
                md_rows.append("| " + " | ".join(header_texts) + " |")
                if not header_processed: # Add separator only after the first header row group
                    md_rows.append("| " + " | ".join(["---"] * len(header_texts)) + " |")
                    header_processed = True
    
    # Process body (tbody or direct tr in table)
    tbody = table_tag.find('tbody')
    if not tbody: # If no tbody, look for tr directly under table
        rows_to_process = table_tag.find_all('tr', recursive=False)
    else:
        rows_to_process = tbody.find_all('tr')
        
    for row_tag in rows_to_process:
        # Skip if this row was already processed as part of thead (if thead was missing)
        if not header_processed and row_tag.find('th'):
            header_cells = row_tag.find_all(['th', 'td'])
            header_texts = [cell.get_text(separator=' ', strip=True) for cell in header_cells]
            md_rows.append("| " + " | ".join(header_texts) + " |")
            md_rows.append("| " + " | ".join(["---"] * len(header_texts)) + " |")
            header_processed = True
            continue
        
        cell_texts = [cell.get_text(separator=' ', strip=True) for cell in row_tag.find_all('td')]
        if cell_texts: # Only add row if it has content
            md_rows.append("| " + " | ".join(cell_texts) + " |")
            
    return "\n".join(md_rows)

def _convert_html_tag_to_elements_recursive(tag: Tag, elements_list: List[Any], processed_tags: Set[Tag], current_list_level: int = 0):
    """
    Recursively processes a BeautifulSoup Tag and its children to extract structured elements.
    Modifies elements_list in place.
    """
    if tag in processed_tags or not isinstance(tag, Tag) or tag.name in IGNORE_TAGS_HTML:
        return

    tag_name = tag.name.lower()
    element_metadata = None # Placeholder for now, can be enhanced to include source line numbers etc.
    
    created_element = False

    if tag_name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
        level = int(tag_name[1:])
        text = tag.get_text(strip=True)
        if text:
            if _PYDANTIC_MODELS_AVAILABLE_HTML: elements_list.append(TitleElement(text=text, level=level, metadata=element_metadata))
            else: elements_list.append({"element_type": "title", "text": text, "level": level, "metadata": element_metadata})
            created_element = True
    
    elif tag_name == 'p':
        text = tag.get_text(strip=True)
        if text:
            if _PYDANTIC_MODELS_AVAILABLE_HTML: elements_list.append(NarrativeTextElement(text=text, metadata=element_metadata))
            else: elements_list.append({"element_type": "narrative_text", "text": text, "metadata": element_metadata})
            created_element = True

    elif tag_name in ['ul', 'ol']:
        ordered = tag_name == 'ol'
        start_num = int(tag.get('start', '1')) if ordered else 1
        
        # Iterate over direct children that are <li>
        direct_li_children = [child for child in tag.children if isinstance(child, Tag) and child.name == 'li']
        for i, li_tag in enumerate(direct_li_children):
            if li_tag in processed_tags: continue
            
            # Extract text directly under <li>, excluding text from nested lists
            li_text_parts = []
            for content_child in li_tag.contents:
                if isinstance(content_child, NavigableString):
                    stripped_text = content_child.strip()
                    if stripped_text: li_text_parts.append(stripped_text)
                elif isinstance(content_child, Tag) and content_child.name not in ['ul', 'ol']: # Get text from non-list children
                    li_text_parts.append(content_child.get_text(strip=True))
            
            final_li_text = " ".join(li_text_parts).strip()

            if final_li_text:
                item_num_str = str(start_num + i) if ordered else None
                if _PYDANTIC_MODELS_AVAILABLE_HTML: elements_list.append(ListItemElement(text=final_li_text, level=current_list_level, ordered=ordered, item_number=item_num_str, metadata=element_metadata))
                else: elements_list.append({"element_type": "list_item", "text": final_li_text, "level": current_list_level, "ordered": ordered, "item_number": item_num_str, "metadata": element_metadata})
            
            processed_tags.add(li_tag) # Mark <li> as processed for its direct text
            # Recursively process children of this <li> for nested lists or other elements
            for child_of_li in li_tag.children:
                if isinstance(child_of_li, Tag):
                     _convert_html_tag_to_elements_recursive(child_of_li, elements_list, processed_tags, current_list_level + 1)
        created_element = True # The list itself is an element boundary

    elif tag_name == 'table':
        md_table = _table_to_markdown(tag)
        caption_tag = tag.find('caption')
        caption_text = caption_tag.get_text(strip=True) if caption_tag else None
        if md_table or caption_text : # Only add if table has content or caption
            if _PYDANTIC_MODELS_AVAILABLE_HTML: elements_list.append(TableElement(markdown_representation=md_table, html_representation=str(tag), caption=caption_text, metadata=element_metadata))
            else: elements_list.append({"element_type": "table", "markdown_representation": md_table, "html_representation": str(tag), "caption": caption_text, "metadata": element_metadata})
        created_element = True

    elif tag_name == 'pre':
        code_tag = tag.find('code')
        code_text, lang = "", None
        if code_tag:
            code_text = code_tag.get_text() # Keep original spacing and newlines
            lang_class = code_tag.get('class', [])
            if lang_class: lang = next((cls.split('language-')[-1] for cls in lang_class if cls.startswith('language-')), None)
        else:
            code_text = tag.get_text()
        
        if code_text.strip(): # Check if there's actual code
            if _PYDANTIC_MODELS_AVAILABLE_HTML: elements_list.append(CodeBlockElement(code=code_text.strip('\n'), language=lang, metadata=element_metadata))
            else: elements_list.append({"element_type": "code_block", "code": code_text.strip('\n'), "language": lang, "metadata": element_metadata})
        created_element = True

    elif tag_name == 'blockquote':
        text = tag.get_text(strip=True)
        if text:
            if _PYDANTIC_MODELS_AVAILABLE_HTML: elements_list.append(NarrativeTextElement(text=text, metadata=element_metadata))
            else: elements_list.append({"element_type": "narrative_text", "text": text, "_is_blockquote": True, "metadata": element_metadata})
        created_element = True

    elif tag_name == 'hr':
        if _PYDANTIC_MODELS_AVAILABLE_HTML: elements_list.append(PageBreakElement(metadata=element_metadata))
        else: elements_list.append({"element_type": "page_break", "metadata": element_metadata})
        created_element = True
    
    processed_tags.add(tag)
    # If the tag itself wasn't a specific block element we handled, or it's a known container,
    # process its children.
    if not created_element or tag_name in CONTAINER_TAGS_HTML:
        for child in tag.children:
            if isinstance(child, Tag):
                _convert_html_tag_to_elements_recursive(child, elements_list, processed_tags, current_list_level)
            elif isinstance(child, NavigableString): # Handle loose text not in <p> etc.
                loose_text = child.strip()
                if loose_text and tag_name not in ['ul', 'ol']: # Avoid adding list item text twice
                    if _PYDANTIC_MODELS_AVAILABLE_HTML: elements_list.append(NarrativeTextElement(text=loose_text, metadata=element_metadata))
                    else: elements_list.append({"element_type": "narrative_text", "text": loose_text, "_is_loose_text": True, "metadata": element_metadata})

def _generate_linear_text_from_html_elements(elements: List[Any]) -> str:
    # This function is identical to the one in docx_parser.py, can be refactored to common_utils later.
    text_parts = []
    for el_data_any in elements:
        el_data: Dict[str, Any] = {}
        if _PYDANTIC_MODELS_AVAILABLE_HTML and hasattr(el_data_any, 'model_dump'):
            el_data = el_data_any.model_dump(exclude_none=True)
        elif isinstance(el_data_any, dict):
            el_data = el_data_any
        else: continue

        el_type = el_data.get("element_type")
        text_content = el_data.get("text", "")
        current_element_text = ""
        if el_type == "title": current_element_text = f"\n{'#' * el_data.get('level',1)} {text_content}\n"
        elif el_type == "narrative_text": current_element_text = text_content + "\n"
        elif el_type == "list_item":
            prefix = f"{el_data.get('item_number', '')}. " if el_data.get('ordered') and el_data.get('item_number') else "- "
            indent = "  " * el_data.get('level', 0)
            current_element_text = f"{indent}{prefix}{text_content}\n"
        elif el_type == "table":
            caption = el_data.get('caption','Unnamed Table')
            md_repr = el_data.get('markdown_representation')
            if md_repr: current_element_text = f"\n[Table: {caption}]\n{md_repr}\n"
        elif el_type == "code_block":
            lang = el_data.get('language', "") or ""
            code_content = el_data.get('code', "")
            current_element_text = f"\n```{lang}\n{code_content}\n```\n"
        elif el_type == "page_break": current_element_text = "\n---\n"
        if current_element_text: text_parts.append(current_element_text)
    full_text = "".join(text_parts)
    return re.sub(r'\n{3,}', '\n\n', full_text).strip()

def parse_html_to_structured_output(
    html_content_str: str, 
    original_metadata: Dict[str, Any]
) -> Optional[Union[ParsedDocumentOutput, Dict[str, Any]]]:
    logger.info(f"Attempting to parse HTML content (length: {len(html_content_str)} chars) using BeautifulSoup4")
    if not _BS4_AVAILABLE:
        logger.error("BeautifulSoup4 (bs4) is not available. HTML parsing cannot proceed.")
        return None

    elements: List[Any] = []
    try:
        # Try lxml first, then html.parser
        try:
            soup = BeautifulSoup(html_content_str, "lxml")
        except Exception: # Fallback if lxml is not installed or fails
            logger.warning("lxml parser not available or failed, falling back to html.parser for HTML.")
            soup = BeautifulSoup(html_content_str, "html.parser")

        # Attempt to find the main content area
        main_content_area = soup.find('article') or soup.find('main') or soup.body
        if not main_content_area:
            logger.warning("Could not find <article>, <main>, or <body> tag. Parsing entire document if possible.")
            main_content_area = soup # Fallback to entire soup object

        # Remove ignored tags before processing
        for ignore_tag_name in IGNORE_TAGS_HTML:
            for tag_to_remove in main_content_area.find_all(ignore_tag_name):
                tag_to_remove.decompose()
        
        processed_tags_set: Set[Tag] = set()
        _convert_html_tag_to_elements_recursive(main_content_area, elements, processed_tags_set)
        
        logger.info(f"Converted HTML to {len(elements)} custom elements.")
        
        linear_text = _generate_linear_text_from_html_elements(elements)
        logger.info(f"Generated linear text from HTML elements (length: {len(linear_text)}). Preview: {linear_text[:200]}")

        if _PYDANTIC_MODELS_AVAILABLE_HTML:
            return ParsedDocumentOutput(
                parsed_text=linear_text,
                elements=elements, # type: ignore
                original_metadata=original_metadata
            )
        else:
            return {
                "parsed_text": linear_text,
                "elements": elements,
                "original_metadata": original_metadata
            }
    except Exception as e:
        logger.error(f"Error parsing HTML content with BeautifulSoup4: {e}", exc_info=True)
        return None
```

File: zhz_rag_pipeline_dagster/zhz_rag_pipeline/parsers/md_parser.py
--------------------------------------------------------------------
```python
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/zhz_rag_pipeline/parsers/md_parser.py
import os
from markdown_it import MarkdownIt
from markdown_it.tree import SyntaxTreeNode
import logging
from typing import List, Dict, Any, Optional, Union
import re

# --- Pydantic 模型导入和占位符定义 ---
_PARSER_PYDANTIC_AVAILABLE = False
try:
    from ..pydantic_models_dagster import (
        ParsedDocumentOutput, DocumentElementType, TitleElement, NarrativeTextElement,
        ListItemElement, TableElement, CodeBlockElement, PageBreakElement,
        DocumentElementMetadata
    )
    _PARSER_PYDANTIC_AVAILABLE = True
except ImportError:
    class BaseModel: pass
    class DocumentElementMetadata(BaseModel): page_number: Optional[int] = None
    class ParsedDocumentOutput(BaseModel): parsed_text: str; elements: list; original_metadata: dict; summary: Optional[str] = None
    class TitleElement(BaseModel): element_type:str="title"; text:str; level:int; metadata: Optional[DocumentElementMetadata] = None
    class NarrativeTextElement(BaseModel): element_type:str="narrative_text"; text:str; metadata: Optional[DocumentElementMetadata] = None
    class ListItemElement(BaseModel): element_type:str="list_item"; text:str; level:int=0; ordered:bool=False; item_number:Optional[Union[int, str]]=None; metadata: Optional[DocumentElementMetadata] = None
    class TableElement(BaseModel): element_type:str="table"; markdown_representation:Optional[str]=None; html_representation:Optional[str]=None; caption:Optional[str]=None; metadata: Optional[DocumentElementMetadata] = None
    class CodeBlockElement(BaseModel): element_type:str="code_block"; code:str; language:Optional[str]=None; metadata: Optional[DocumentElementMetadata] = None
    class PageBreakElement(BaseModel): element_type:str="page_break"; metadata: Optional[DocumentElementMetadata] = None
    DocumentElementType = Any

logger = logging.getLogger(__name__)

# --- 辅助函数 ---
def _get_node_text_content(node: SyntaxTreeNode, exclude_lists_and_tables: bool = False) -> str:
    if node.type == "text":
        return node.content
    if node.type == "softbreak":
        return " "
    if node.type == "hardbreak":
        return "\n"
    if node.type == "code_inline":
        return f"`{node.content}`"
    
    if exclude_lists_and_tables and node.type in ["bullet_list", "ordered_list", "table"]:
        return ""

    content = ""
    if node.children:
        for child in node.children:
            content += _get_node_text_content(child, exclude_lists_and_tables)
    return content

def _convert_table_node_to_markdown(table_node: SyntaxTreeNode) -> str:
    md_rows = []
    
    thead_node = next((child for child in table_node.children if child.type == 'thead'), None)
    tbody_node = next((child for child in table_node.children if child.type == 'tbody'), None)

    header_texts = []
    if thead_node:
        tr_node_header = next((child for child in thead_node.children if child.type == 'tr'), None)
        if tr_node_header:
            header_texts = [_get_node_text_content(cell).strip() for cell in tr_node_header.children if cell.type == 'th']
    elif tbody_node: 
        first_row_in_tbody = next((child for child in tbody_node.children if child.type == 'tr'), None)
        if first_row_in_tbody and all(cell.type == 'th' for cell in first_row_in_tbody.children):
             header_texts = [_get_node_text_content(cell).strip() for cell in first_row_in_tbody.children]

    if header_texts:
        md_rows.append("| " + " | ".join(header_texts) + " |")
        md_rows.append("| " + " | ".join(["---"] * len(header_texts)) + " |")

    rows_container = tbody_node if tbody_node else table_node 
    
    first_row_in_container_is_header = False
    if not header_texts and rows_container: # 只有在没有thead且容器存在时，才检查第一行是否是表头
        first_tr = next((child for child in rows_container.children if child.type == 'tr'), None)
        if first_tr and all(cell.type == 'th' for cell in first_tr.children):
            # 如果第一行全是th，作为表头处理
            header_texts_from_body = [_get_node_text_content(cell).strip() for cell in first_tr.children]
            if header_texts_from_body:
                md_rows.append("| " + " | ".join(header_texts_from_body) + " |")
                md_rows.append("| " + " | ".join(["---"] * len(header_texts_from_body)) + " |")
                first_row_in_container_is_header = True

    if rows_container: # 确保 rows_container 存在
        for row_idx, tr_node in enumerate(child for child in rows_container.children if child.type == 'tr'):
            # 如果第一行已经被作为表头处理了，则跳过它
            if first_row_in_container_is_header and row_idx == 0:
                continue
            
            # 如果已经通过 thead 处理了表头，那么 tbody/table 下的所有 tr 都应视为数据行
            # 如果没有通过 thead 处理表头，并且当前行也不是被推断为表头的 tbody 第一行，那么它也是数据行
            cell_texts = [_get_node_text_content(cell).strip() for cell in tr_node.children if cell.type == 'td']
            if cell_texts or len(tr_node.children) > 0 : 
                md_rows.append("| " + " | ".join(cell_texts) + " |")
            
    return "\n".join(md_rows)

# --- 主转换函数 ---
def _convert_md_tree_to_elements(root_node: SyntaxTreeNode) -> List[Any]: 
    elements: List[Any] = []
    
    def _process_node_recursive(node: SyntaxTreeNode, current_semantic_level: int = 0, list_ctx: Optional[Dict] = None):
        nonlocal elements
        current_metadata = None 

        node_type = node.type
        
        if node_type == "heading":
            level = int(node.tag[1:])
            text = _get_node_text_content(node).strip()
            if text or node.children: 
                if _PARSER_PYDANTIC_AVAILABLE: elements.append(TitleElement(text=text, level=level, metadata=current_metadata))
                else: elements.append({"element_type": "title", "text": text, "level": level, "metadata": current_metadata})
        
        elif node_type == "paragraph":
            text = _get_node_text_content(node).strip()
            if text:
                if _PARSER_PYDANTIC_AVAILABLE: elements.append(NarrativeTextElement(text=text, metadata=current_metadata))
                else: elements.append({"element_type": "narrative_text", "text": text, "metadata": current_metadata})

        elif node_type == "bullet_list" or node_type == "ordered_list":
            is_ordered_list = (node_type == "ordered_list")
            child_list_ctx = {
                "ordered": is_ordered_list,
                "start_num": int(node.attrs.get("start", 1)) if node.attrs and is_ordered_list else 1,
                "item_idx_in_list": 0 
            }
            for child_node in node.children:
                if child_node.type == "list_item":
                    _process_node_recursive(child_node, current_semantic_level + 1, child_list_ctx)
        
        elif node_type == "list_item":
            item_text = _get_node_text_content(node, exclude_lists_and_tables=True).strip()
            
            if item_text and list_ctx: 
                display_level = current_semantic_level - 1 
                item_number_str = None
                if list_ctx["ordered"]:
                    item_number_str = str(list_ctx["start_num"] + list_ctx["item_idx_in_list"])
                    list_ctx["item_idx_in_list"] += 1
                else: 
                    item_number_str = node.markup if node.markup else "-" 

                if _PARSER_PYDANTIC_AVAILABLE:
                    elements.append(ListItemElement(
                        text=item_text, level=display_level, 
                        ordered=list_ctx["ordered"], 
                        item_number=item_number_str, metadata=current_metadata
                    ))
                else:
                    elements.append({
                        "element_type": "list_item", "text": item_text, 
                        "level": display_level, "ordered": list_ctx["ordered"], 
                        "item_number": item_number_str, "metadata": current_metadata
                    })
            
            for child_node in node.children:
                if child_node.type in ["bullet_list", "ordered_list"]:
                    _process_node_recursive(child_node, current_semantic_level, None) # Pass current_semantic_level for nested list

        elif node_type == "table":
            md_table_representation = _convert_table_node_to_markdown(node)
            if md_table_representation:
                if _PARSER_PYDANTIC_AVAILABLE: elements.append(TableElement(markdown_representation=md_table_representation, metadata=current_metadata))
                else: elements.append({"element_type": "table", "markdown_representation": md_table_representation, "metadata": current_metadata})

        elif node_type == "fence" or node_type == "code_block":
            code_content = node.content.strip('\n') 
            lang = node.info.strip() if node.info else None
            if _PARSER_PYDANTIC_AVAILABLE: elements.append(CodeBlockElement(code=code_content, language=lang, metadata=current_metadata))
            else: elements.append({"element_type": "code_block", "code": code_content, "language": lang, "metadata": current_metadata})

        elif node_type == "hr":
            if _PARSER_PYDANTIC_AVAILABLE: elements.append(PageBreakElement(metadata=current_metadata))
            else: elements.append({"element_type": "page_break", "metadata": current_metadata})

        elif node_type == "blockquote":
            text = _get_node_text_content(node).strip()
            if text:
                if _PARSER_PYDANTIC_AVAILABLE: elements.append(NarrativeTextElement(text=text, metadata=current_metadata))
                else: elements.append({"element_type": "narrative_text", "text": text, "_is_blockquote": True, "metadata": current_metadata})
        
        elif node.children and node_type not in ["list_item", "heading", "paragraph", "table", "fence", "code_block", "blockquote", "hr", "bullet_list", "ordered_list"]: # Avoid re-processing children of already handled types
             for child in node.children:
                _process_node_recursive(child, current_semantic_level, list_ctx) # Pass context along

    _process_node_recursive(root_node) 
    return elements

def _generate_parsed_text_from_elements_internal(elements: List[Any]) -> str:
    text_parts = []
    for el_data_any in elements:
        el_data = {}
        if _PARSER_PYDANTIC_AVAILABLE and hasattr(el_data_any, 'model_dump'): el_data = el_data_any.model_dump()
        elif isinstance(el_data_any, dict): el_data = el_data_any
        else: continue
        el_type = el_data.get("element_type")
        if el_type == "title": text_parts.append(f"\n{'#' * el_data.get('level',1)} {el_data.get('text','')}\n")
        elif el_type == "narrative_text": text_parts.append(el_data.get('text','') + "\n")
        elif el_type == "list_item":
            item_num_display = str(el_data.get('item_number','-')) 
            prefix = f"{item_num_display}. " if el_data.get('ordered') else f"{item_num_display} "
            indent = "  " * el_data.get('level',0)
            text_parts.append(f"{indent}{prefix}{el_data.get('text','')}\n")
        elif el_type == "table":
            caption_text = str(el_data.get('caption')) if el_data.get('caption') is not None else 'Unnamed Table'
            if el_data.get('markdown_representation'): 
                text_parts.append(f"\n[Table: {caption_text}]\n{el_data['markdown_representation']}\n")
            elif el_data.get('text_representation'): 
                text_parts.append(f"\n[Table: {caption_text}]\n{el_data['text_representation']}\n")
        elif el_type == "code_block":
            lang = el_data.get('language', "") or ""
            text_parts.append(f"\n```{lang}\n{el_data.get('code','')}\n```\n")
        elif el_type == "page_break": text_parts.append("\n---\n")
        if el_type not in ['list_item'] and text_parts and (not text_parts[-1].endswith("\n\n") and not text_parts[-1].endswith("\n---\n\n") ) : 
             text_parts.append("\n") 
             
    raw_text = "".join(text_parts)
    cleaned_text = raw_text.strip()
    cleaned_text = cleaned_text.replace('\r\n', '\n') 
    cleaned_text = re.sub(r'\n{3,}', '\n\n', cleaned_text) 
    return cleaned_text

def parse_markdown_to_structured_output(md_content_str: str, original_metadata: Dict[str, Any]) -> Optional[Union[ParsedDocumentOutput, Dict[str, Any]]]:
    logger.info(f"Parsing Markdown content (length: {len(md_content_str)} chars) using markdown-it-py with SyntaxTreeNode...")
    try:
        md_parser = MarkdownIt("commonmark", {'linkify': True}).enable("table")
        tokens = md_parser.parse(md_content_str)
        
        root_syntax_node = SyntaxTreeNode(tokens)
        structured_elements = _convert_md_tree_to_elements(root_syntax_node) 

        linear_text = _generate_parsed_text_from_elements_internal(structured_elements)

        if _PARSER_PYDANTIC_AVAILABLE:
            return ParsedDocumentOutput(
                parsed_text=linear_text,
                elements=structured_elements, 
                original_metadata=original_metadata
            )
        else:
            return {
                "parsed_text": linear_text,
                "elements": structured_elements,
                "original_metadata": original_metadata
            }
    except Exception as e:
        logger.error(f"Error in parse_markdown_to_structured_output: {e}", exc_info=True)
        return None

# --- Placeholder for other parsers (保持不变) ---
def parse_docx_to_structured_output(file_path: str, original_metadata: Dict[str, Any]) -> Optional[Union[ParsedDocumentOutput, Dict[str, Any]]]:
    logger.info(f"DOCX parser placeholder for: {file_path}") 
    text = f"[DOCX content of {os.path.basename(file_path)}]"
    if _PARSER_PYDANTIC_AVAILABLE: return ParsedDocumentOutput(parsed_text=text, elements=[NarrativeTextElement(text=text)], original_metadata=original_metadata) 
    return {"parsed_text": text, "elements": [{"element_type":"narrative_text", "text":text}], "original_metadata": original_metadata}

def parse_pdf_to_structured_output(file_path: str, original_metadata: Dict[str, Any]) -> Optional[Union[ParsedDocumentOutput, Dict[str, Any]]]:
    logger.info(f"PDF parser placeholder for: {file_path}") 
    text = f"[PDF content of {os.path.basename(file_path)}]"
    if _PARSER_PYDANTIC_AVAILABLE: return ParsedDocumentOutput(parsed_text=text, elements=[NarrativeTextElement(text=text)], original_metadata=original_metadata) 
    return {"parsed_text": text, "elements": [{"element_type":"narrative_text", "text":text}], "original_metadata": original_metadata}

def parse_xlsx_to_structured_output(file_path: str, original_metadata: Dict[str, Any]) -> Optional[Union[ParsedDocumentOutput, Dict[str, Any]]]:
    logger.info(f"XLSX parser placeholder for: {file_path}") 
    text = f"[XLSX content of {os.path.basename(file_path)}]"
    if _PARSER_PYDANTIC_AVAILABLE: return ParsedDocumentOutput(parsed_text=text, elements=[NarrativeTextElement(text=text)], original_metadata=original_metadata) 
    return {"parsed_text": text, "elements": [{"element_type":"narrative_text", "text":text}], "original_metadata": original_metadata}
        
def parse_html_to_structured_output(html_content_str: str, original_metadata: Dict[str, Any]) -> Optional[Union[ParsedDocumentOutput, Dict[str, Any]]]:
    logger.info(f"HTML parser placeholder for content length: {len(html_content_str)}") 
    text = f"[HTML content snippet: {html_content_str[:100]}]"
    if _PARSER_PYDANTIC_AVAILABLE: return ParsedDocumentOutput(parsed_text=text, elements=[NarrativeTextElement(text=text)], original_metadata=original_metadata) 
    return {"parsed_text": text, "elements": [{"element_type":"narrative_text", "text":text}], "original_metadata": original_metadata}

def parse_txt_to_structured_output(txt_content_str: str, original_metadata: Dict[str, Any]) -> Optional[Union[ParsedDocumentOutput, Dict[str, Any]]]:
    logger.info(f"TXT parser for content length: {len(txt_content_str)}") 
    if _PARSER_PYDANTIC_AVAILABLE:
        return ParsedDocumentOutput(
            parsed_text=txt_content_str, 
            elements=[NarrativeTextElement(text=txt_content_str)], 
            original_metadata=original_metadata
        ) 
    return {
        "parsed_text": txt_content_str, 
        "elements": [{"element_type":"narrative_text", "text":txt_content_str}], 
        "original_metadata": original_metadata
    }
```

File: zhz_rag_pipeline_dagster/zhz_rag_pipeline/parsers/pdf_parser.py
---------------------------------------------------------------------
```python
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/zhz_rag_pipeline/parsers/pdf_parser.py
import os
import logging
from typing import List, Dict, Any, Optional, Union
import re

try:
    import fitz  # PyMuPDF
    _PYMUPDF_AVAILABLE = True
    logging.info("Successfully imported PyMuPDF (fitz) for PDF parsing.")
except ImportError:
    logging.error("PyMuPDF (fitz) not found. PDF parsing will not be available.")
    _PYMUPDF_AVAILABLE = False
    # 占位符，以防 fitz 未安装时代码尝试引用它
    class fitz: 
        class Document: pass
        class Page: pass
        class Rect: pass 

_PYDANTIC_MODELS_AVAILABLE_PDF = False
try:
    from ..pydantic_models_dagster import (
        ParsedDocumentOutput, DocumentElementType, NarrativeTextElement,
        DocumentElementMetadata, PageBreakElement
    )
    _PYDANTIC_MODELS_AVAILABLE_PDF = True
except ImportError:
    class BaseModel: pass
    class DocumentElementMetadata(BaseModel): page_number: Optional[int] = None
    class ParsedDocumentOutput(BaseModel): parsed_text: str; elements: list; original_metadata: dict; summary: Optional[str] = None
    class NarrativeTextElement(BaseModel): element_type:str="narrative_text"; text:str; metadata: Optional[DocumentElementMetadata] = None
    class PageBreakElement(BaseModel): element_type:str="page_break"; metadata: Optional[DocumentElementMetadata] = None
    DocumentElementType = Any

logger = logging.getLogger(__name__)

def _create_pdf_element_metadata(page_number: Optional[int] = None) -> Optional[Union[DocumentElementMetadata, Dict[str, Any]]]:
    meta_data_dict: Dict[str, Any] = {}
    if page_number is not None:
        meta_data_dict['page_number'] = page_number
    
    if not meta_data_dict:
        return None

    if _PYDANTIC_MODELS_AVAILABLE_PDF:
        return DocumentElementMetadata(**meta_data_dict)
    else:
        return meta_data_dict

def parse_pdf_to_structured_output(
    file_path: str, 
    original_metadata: Dict[str, Any]
) -> Optional[Union[ParsedDocumentOutput, Dict[str, Any]]]:
    logger.info(f"Attempting to parse PDF file: {file_path} using PyMuPDF (fitz)")
    if not _PYMUPDF_AVAILABLE:
        logger.error("PyMuPDF (fitz) is not available. PDF parsing cannot proceed.")
        return None

    elements: List[Any] = []
    full_text_parts: List[str] = []

    try:
        doc = fitz.open(file_path)
        logger.info(f"PyMuPDF opened PDF. Pages: {doc.page_count}")

        for page_num in range(doc.page_count):
            page = doc.load_page(page_num)
            
            # 使用 "dict" 模式提取文本块，并按阅读顺序排序
            page_content_blocks = page.get_text("dict", sort=True).get("blocks", [])
            
            page_text_collected = []
            
            if page_content_blocks:
                for block in page_content_blocks:
                    if block['type'] == 0: # 0 表示文本块
                        block_text_lines = []
                        for line in block.get("lines", []):
                            line_content = "".join([span.get("text", "") for span in line.get("spans", [])])
                            block_text_lines.append(line_content)
                        block_text_content = "\n".join(block_text_lines).strip()
                        if block_text_content:
                            page_text_collected.append(block_text_content)
            
            if page_text_collected:
                # 将整页的文本作为一个 NarrativeTextElement
                page_full_text = "\n\n".join(page_text_collected) # 用双换行符分隔来自不同块的文本
                element_metadata = _create_pdf_element_metadata(page_number=page_num + 1)
                if _PYDANTIC_MODELS_AVAILABLE_PDF:
                    elements.append(NarrativeTextElement(text=page_full_text, metadata=element_metadata)) # type: ignore
                else:
                    elements.append({"element_type": "narrative_text", "text": page_full_text, "metadata": element_metadata})
                full_text_parts.append(page_full_text)
            
            # 在每页（除了最后一页）之后添加一个 PageBreakElement
            if page_num < doc.page_count - 1:
                if _PYDANTIC_MODELS_AVAILABLE_PDF:
                    elements.append(PageBreakElement(metadata=_create_pdf_element_metadata(page_number=page_num + 1))) # type: ignore
                else:
                    elements.append({"element_type": "page_break", "metadata": _create_pdf_element_metadata(page_number=page_num + 1)})


        doc.close()
        
        linear_text = "\n\n--- Page Break ---\n\n".join(full_text_parts) # 用特殊标记分隔页面文本
        linear_text = re.sub(r'\n{3,}', '\n\n', linear_text).strip()

        if _PYDANTIC_MODELS_AVAILABLE_PDF:
            return ParsedDocumentOutput(
                parsed_text=linear_text,
                elements=elements, # type: ignore
                original_metadata=original_metadata
            )
        else:
            return {
                "parsed_text": linear_text,
                "elements": elements,
                "original_metadata": original_metadata
            }

    except FileNotFoundError:
        logger.error(f"PDF file not found: {file_path}")
        return None
    except Exception as e:
        logger.error(f"Error parsing PDF file {file_path} with PyMuPDF: {e}", exc_info=True)
        return None
```

File: zhz_rag_pipeline_dagster/zhz_rag_pipeline/parsers/txt_parser.py
---------------------------------------------------------------------
```python
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/zhz_rag_pipeline/parsers/txt_parser.py
import os
import logging
from typing import Dict, Any, Optional, Union

_PYDANTIC_MODELS_AVAILABLE_TXT = False
try:
    from ..pydantic_models_dagster import (
        ParsedDocumentOutput, NarrativeTextElement,
        DocumentElementMetadata
    )
    _PYDANTIC_MODELS_AVAILABLE_TXT = True
except ImportError:
    class BaseModel: pass
    class DocumentElementMetadata(BaseModel): page_number: Optional[int] = None # Not really applicable for txt
    class ParsedDocumentOutput(BaseModel): parsed_text: str; elements: list; original_metadata: dict; summary: Optional[str] = None
    class NarrativeTextElement(BaseModel): element_type:str="narrative_text"; text:str; metadata: Optional[DocumentElementMetadata] = None
    # DocumentElementType is not strictly needed here as we only create NarrativeTextElement

logger = logging.getLogger(__name__)

def parse_txt_to_structured_output(
    txt_content_str: str, # For .txt, we expect the content string directly
    original_metadata: Dict[str, Any]
) -> Optional[Union[ParsedDocumentOutput, Dict[str, Any]]]:
    logger.info(f"Attempting to parse TXT content (length: {len(txt_content_str)} chars)")

    # For .txt files, the entire content is treated as a single narrative text block.
    # No complex structure is assumed or extracted.
    
    elements = []
    element_metadata = None # No specific sub-element metadata for a single block txt file

    if _PYDANTIC_MODELS_AVAILABLE_TXT:
        elements.append(NarrativeTextElement(text=txt_content_str, metadata=element_metadata)) # type: ignore
        doc_output = ParsedDocumentOutput(
            parsed_text=txt_content_str,
            elements=elements, # type: ignore
            original_metadata=original_metadata
        )
    else:
        elements.append({"element_type": "narrative_text", "text": txt_content_str, "metadata": element_metadata})
        doc_output = {
            "parsed_text": txt_content_str,
            "elements": elements,
            "original_metadata": original_metadata
        }
    
    logger.info(f"Successfully processed TXT content into a single element.")
    return doc_output
```

File: zhz_rag_pipeline_dagster/zhz_rag_pipeline/parsers/xlsx_parser.py
----------------------------------------------------------------------
```python
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/zhz_rag_pipeline/parsers/xlsx_parser.py
import os
import logging
from typing import List, Dict, Any, Optional, Union

try:
    import pandas as pd
    _PANDAS_AVAILABLE = True
    logging.info("Successfully imported pandas for XLSX parsing.")
except ImportError:
    logging.error("pandas library not found. XLSX parsing will not be available.")
    _PANDAS_AVAILABLE = False
    class pd: # Placeholder
        @staticmethod
        def read_excel(*args, **kwargs): raise NotImplementedError("pandas not available")
        class DataFrame:
            def to_markdown(self, *args, **kwargs): raise NotImplementedError("pandas not available")
            def to_string(self, *args, **kwargs): raise NotImplementedError("pandas not available")
            @property
            def empty(self): return True

_PYDANTIC_MODELS_AVAILABLE_XLSX = False
try:
    from ..pydantic_models_dagster import (
        ParsedDocumentOutput, DocumentElementType, TableElement,
        DocumentElementMetadata
    )
    _PYDANTIC_MODELS_AVAILABLE_XLSX = True
except ImportError:
    class BaseModel: pass
    class DocumentElementMetadata(BaseModel): page_number: Optional[int] = None # page_number for xlsx might be sheet index
    class ParsedDocumentOutput(BaseModel): parsed_text: str; elements: list; original_metadata: dict; summary: Optional[str] = None
    class TableElement(BaseModel): element_type:str="table"; markdown_representation:Optional[str]=None; html_representation:Optional[str]=None; caption:Optional[str]=None; metadata: Optional[DocumentElementMetadata] = None
    DocumentElementType = Any

logger = logging.getLogger(__name__)

def _create_xlsx_element_metadata(sheet_index: Optional[int] = None) -> Optional[Union[DocumentElementMetadata, Dict[str, Any]]]:
    meta_data_dict: Dict[str, Any] = {}
    if sheet_index is not None:
        # Using page_number to represent sheet index for consistency with other parsers
        meta_data_dict['page_number'] = sheet_index 
    
    if not meta_data_dict:
        return None

    if _PYDANTIC_MODELS_AVAILABLE_XLSX:
        return DocumentElementMetadata(**meta_data_dict)
    else:
        return meta_data_dict

def parse_xlsx_to_structured_output(
    file_path: str, 
    original_metadata: Dict[str, Any]
) -> Optional[Union[ParsedDocumentOutput, Dict[str, Any]]]:
    logger.info(f"Attempting to parse XLSX file: {file_path} using pandas")
    if not _PANDAS_AVAILABLE:
        logger.error("pandas library is not available. XLSX parsing cannot proceed.")
        return None

    elements: List[Any] = []
    all_sheets_text_parts: List[str] = []

    try:
        # Read all sheets into a dictionary of DataFrames
        # Setting header=0 to use the first row as column names.
        # If a sheet has no header, pandas will infer column names like 0, 1, 2...
        xls = pd.read_excel(file_path, sheet_name=None, header=0) 
        logger.info(f"Pandas opened XLSX. Sheets: {list(xls.keys())}")

        for sheet_idx, (sheet_name, df) in enumerate(xls.items()):
            if df.empty:
                logger.info(f"Sheet '{sheet_name}' (index {sheet_idx}) is empty. Skipping.")
                continue

            sheet_md_representation = None
            sheet_text_representation = None
            
            try:
                # Attempt to convert DataFrame to Markdown
                sheet_md_representation = df.to_markdown(index=False)
                all_sheets_text_parts.append(f"Sheet: {sheet_name}\n{sheet_md_representation}")
                logger.info(f"Successfully converted sheet '{sheet_name}' to Markdown.")
            except Exception as e_markdown:
                logger.warning(f"Failed to convert sheet '{sheet_name}' to Markdown (tabulate library might be missing): {e_markdown}. Falling back to string representation.")
                try:
                    sheet_text_representation = df.to_string(index=False)
                    all_sheets_text_parts.append(f"Sheet: {sheet_name}\n{sheet_text_representation}")
                    logger.info(f"Successfully converted sheet '{sheet_name}' to string representation.")
                except Exception as e_string:
                    logger.error(f"Failed to convert sheet '{sheet_name}' even to string: {e_string}")
                    all_sheets_text_parts.append(f"Sheet: {sheet_name}\n[Error converting sheet to text]")

            element_metadata = _create_xlsx_element_metadata(sheet_index=sheet_idx)
            if _PYDANTIC_MODELS_AVAILABLE_XLSX:
                table_el = TableElement(
                    markdown_representation=sheet_md_representation,
                    # html_representation can be df.to_html(index=False) if needed later
                    caption=str(sheet_name), # Ensure caption is string
                    metadata=element_metadata # type: ignore
                )
                elements.append(table_el)
            else:
                elements.append({
                    "element_type": "table",
                    "markdown_representation": sheet_md_representation,
                    "caption": str(sheet_name),
                    "metadata": element_metadata
                })
        
        linear_text = "\n\n\n".join(all_sheets_text_parts).strip() # Use more newlines to separate sheets

        if _PYDANTIC_MODELS_AVAILABLE_XLSX:
            return ParsedDocumentOutput(
                parsed_text=linear_text,
                elements=elements, # type: ignore
                original_metadata=original_metadata
            )
        else:
            return {
                "parsed_text": linear_text,
                "elements": elements,
                "original_metadata": original_metadata
            }

    except FileNotFoundError:
        logger.error(f"XLSX file not found: {file_path}")
        return None
    except Exception as e:
        logger.error(f"Error parsing XLSX file {file_path} with pandas: {e}", exc_info=True)
        # Fallback: try to read as raw text if pandas fails catastrophically
        try:
            with open(file_path, 'r', encoding='utf-8', errors='ignore') as f_raw:
                raw_content = f_raw.read(5000) # Limit raw read
            error_text = f"[Failed to parse XLSX with pandas. Raw content preview (first 5000 chars)]:\n{raw_content}"
            if _PYDANTIC_MODELS_AVAILABLE_XLSX:
                return ParsedDocumentOutput(parsed_text=error_text, elements=[], original_metadata=original_metadata) # type: ignore
            else:
                return {"parsed_text": error_text, "elements": [], "original_metadata": original_metadata}
        except Exception as e_raw:
            logger.error(f"Failed to even read XLSX as raw text after pandas error: {e_raw}")
            return None
```


================================================================================

--- Structure for: zhz_rag/ ---
|-- api/
|-- config/
|-- core_rag/
|-- crewai_integration/
|-- evaluation/
|-- finetuning/
|-- llm/
|-- stored_data/
|-- task_management/
|-- utils/
|-- zhz_rag_core.egg-info/
|-- __init__.py
|-- setup.py
|-- crewai_integration/
    |-- __init__.py
    |-- run_crew.py
    |-- tools.py
|-- config/
    |-- constants.py
    |-- pydantic_models.py
|-- zhz_rag_core.egg-info/
    |-- PKG-INFO
    |-- SOURCES.txt
    |-- dependency_links.txt
    |-- requires.txt
    |-- top_level.txt
|-- utils/
    |-- ZHZ_AGENT_tasks.db
    |-- __init__.py
    |-- common_utils.py
    |-- db_utils.py
    |-- interaction_logger.py
    |-- load_neo4j_data.py
|-- core_rag/
    |-- __init__.py
    |-- fusion_engine.py
    |-- kg_retriever.py
    |-- retrievers/
        |-- __init__.py
        |-- chromadb_retriever.py
        |-- embedding_functions.py
        |-- file_bm25_retriever.py
|-- llm/
    |-- __init__.py
    |-- custom_crewai_llms.py
    |-- embedding_process_worker.py
    |-- llm_interface.py
    |-- local_model_handler.py
    |-- rag_prompts.py
    |-- rag_eval_data/
|-- task_management/
    |-- __init__.py
    |-- db_models.py
    |-- jobs.py
|-- finetuning/
    |-- __init__.py
    |-- refine_answer_data.py
    |-- refine_cypher_data.py
    |-- generated_data/
        |-- answer_finetune_samples_20250531.jsonl
        |-- answer_finetune_samples_review_20250531.csv
        |-- cypher_finetune_samples_20250531.jsonl
        |-- cypher_finetune_samples_review_20250531.csv
|-- stored_data/
    |-- __init__.py
    |-- duckdb_knowledge_graph.db
    |-- rag_interaction_logs/
        |-- rag_interactions_20250616.jsonl
    |-- evaluation_results_logs/
    |-- chromadb_index/
        |-- chroma.sqlite3
        |-- 4eb1ba3a-7ce7-40a3-b1d7-7f437e2495ff/
            |-- data_level0.bin
            |-- header.bin
            |-- length.bin
            |-- link_lists.bin
    |-- bm25_index/
        |-- data.csc.index.npy
        |-- doc_ids.pkl
        |-- indices.csc.index.npy
        |-- indptr.csc.index.npy
        |-- params.index.json
        |-- vocab.index.json
|-- evaluation/
    |-- __init__.py
    |-- analyze_answer.py
    |-- analyze_cypher.py
    |-- batch_eval_answer.py
    |-- batch_eval_cypher.py
    |-- evaluator.py
|-- api/
    |-- __init__.py
    |-- main_api.py
    |-- rag_api_service.py
    |-- rag_mcp_service.py
    |-- rag_service_debug.log
    |-- task_manager_api.py

--- Code Contents for: zhz_rag/ ---

File: zhz_rag/__init__.py
-------------------------
--- File is empty ---

File: zhz_rag/setup.py
----------------------
```python
# /home/zhz/zhz_agent/zhz_rag/setup.py
from setuptools import find_packages, setup

setup(
    name="zhz_rag_core",
    version="0.1.0",
    packages=find_packages(),
    install_requires=[
        # Pydantic 版本由主 requirements.txt 控制
        # LiteLLM 版本由主 requirements.txt 控制
        # ChromaDB 版本由主 requirements.txt 控制

        "protobuf>=4.25.0,<5.30.0", # 放宽 protobuf 上限，因为 pydantic 2.11.5 可能需要较新的
        "packaging>=23.2,<25.0",
        "rich>=13.7.0,<14.0.0",
        
        "fastapi>=0.110.0,<0.116.0", # 保持较新
        "starlette>=0.35.0,<0.47.0", # 保持较新

        "langchain-core>=0.1.50,<0.4.0", # 较新 langchain 可能更好兼容
        "langchain-text-splitters>=0.0.1,<0.3.0",

        "httpx>=0.27.0", # 使用较新 httpx
        "python-dotenv>=1.0.0",
        "neo4j>=5.0.0", # neo4j 驱动
        "sentence-transformers>=2.2.0", # sentence-transformers
        "transformers>=4.38.0,<4.39.0", # 固定您之前的版本或小幅更新
        "torch>=2.0.0",
        "numpy<2.0", # 保持 Numpy < 2.0
        "bm25s",
        "jieba",
        "uvicorn[standard]", # 添加 standard extras
        "pandas>=2.0.0",
        "sqlalchemy>=2.0.0",
        "databases[aiosqlite]>=0.9.0", # for async sqlite
        "apscheduler>=3.10.0",
        "pytz",
    ],
)
```

File: zhz_rag/crewai_integration/__init__.py
--------------------------------------------
--- File is empty ---

File: zhz_rag/crewai_integration/run_crew.py
--------------------------------------------
```python
# /home/zhz/zhz_agent/run_agent.py

import os
import json
import datetime

from crewai import Agent, Task, Crew, Process

# --- 导入我们自己的项目模块 (使用绝对导入) ---
from zhz_rag.crewai_integration.tools import HybridRAGTool, BaseMCPTool
from zhz_rag.config.pydantic_models import QueryRequest # 用于 RAG 工具的输入
from zhz_rag.utils.common_utils import call_mcpo_tool
from zhz_rag.llm.custom_crewai_llms import CustomGeminiLLM

# --- 环境配置 ---
from dotenv import load_dotenv
load_dotenv()

# --- CrewAI 基类和事件系统 ---
from crewai.tools import BaseTool
from pydantic import BaseModel, Field
from typing import Any, Dict, List, Optional, Type
from crewai.llms.base_llm import BaseLLM as CrewAIBaseLLM
try:
    from crewai.utilities.events.base_event_listener import BaseEventListener as CrewAIBaseCallbackHandler
    from crewai.utilities.events import LLMCallStartedEvent, LLMCallCompletedEvent
    print("Successfully imported BaseEventListener and Event Types")
except ImportError:
    print("Failed to import BaseEventListener or Event Types, using dummy classes.")
    class CrewAIBaseCallbackHandler: pass
    class LLMCallStartedEvent: pass
    class LLMCallCompletedEvent: pass

# --- LiteLLM ---
import litellm

# --- 定义简单工具以供测试 ---
class SimpleToolInput(BaseModel):
    message: str = Field(description="A simple message string for the tool.")

class MySimpleTestTool(BaseTool):
    name: str = "MySimpleTestTool"
    description: str = "A very simple test tool that takes a message and returns it."
    args_schema: Type[BaseModel] = SimpleToolInput

    def _run(self, message: str) -> str:
        print(f"MySimpleTestTool received: {message}")
        return f"MySimpleTestTool processed: {message}"

# --- 配置 Agent 使用的 LLM 实例 ---
GEMINI_MODEL_NAME = "gemini/gemini-1.5-flash-latest"
GEMINI_API_KEY = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")

if not GEMINI_API_KEY:
    print("CRITICAL ERROR: GOOGLE_API_KEY or GEMINI_API_KEY not set.")
    exit(1)

# --- 定义详细的事件监听器 ---
class MyDetailedLogger(CrewAIBaseCallbackHandler):
    def __init__(self):
        super().__init__()
        print(f"[{datetime.datetime.now()}] MyDetailedLogger: 已初始化。")

    def setup_listeners(self, crewai_event_bus):
        print(f"[{datetime.datetime.now()}] MyDetailedLogger: 正在设置监听器...")

        @crewai_event_bus.on(LLMCallStartedEvent)
        def handle_llm_start(source, event: LLMCallStartedEvent):
            self.on_llm_start_logic(source, event)

        @crewai_event_bus.on(LLMCallCompletedEvent)
        def handle_llm_completed(source, event: LLMCallCompletedEvent):
            self.on_llm_end_logic(source, event)

        print(f"[{datetime.datetime.now()}] MyDetailedLogger: 监听器设置完成。")

    def on_llm_start_logic(self, source, event: LLMCallStartedEvent):
        print(f"\n>>>> LLM 调用开始 (Event Logic) <<<<")
        llm_inputs = getattr(event, 'llm_inputs', {})
        messages = llm_inputs.get('messages')
        tools = llm_inputs.get('tools')
        print(f"来源 (Source): {source}")
        if messages:
            print("消息 (来自 event.llm_inputs):")
            if isinstance(messages, list) and len(messages) > 0:
                first_message = messages[0]
                if isinstance(first_message, dict) and 'content' in first_message:
                    content_snippet = str(first_message.get('content', ''))[:300]
                    print(f"   Role: {first_message.get('role')}, Content Snippet: {content_snippet}...")
                else:
                     print(f"  First message (raw): {first_message}")
            else:
                 print(f"  Messages (raw): {messages}")
        else:
            print("消息 (来自 event.llm_inputs): 无")
        if tools:
            print("工具 (来自 event.llm_inputs):")
            try:
                print(f"  {json.dumps(tools, indent=2, ensure_ascii=False)}")
            except Exception as e:
                print(f"  无法序列化工具为 JSON: {e}. 工具: {tools}")
        else:
            print("工具 (来自 event.llm_inputs): 无")
        print("----------------------------------")

    def on_llm_end_logic(self, source, event: LLMCallCompletedEvent):
        print(f"\n>>>> LLM 调用结束 (Event Logic) <<<<")
        response = getattr(event, 'llm_output', None)
        print(f"来源 (Source): {source}")
        if response:
            if hasattr(response, 'choices') and response.choices:
                choice = response.choices[0]
                if hasattr(choice, 'message') and choice.message:
                    print(f"  消息内容: {choice.message.content}")
                    if hasattr(choice.message, 'tool_calls') and choice.message.tool_calls:
                        print(f"  工具调用: {choice.message.tool_calls}")
                    else:
                        print(f"  工具调用: 无")
            elif hasattr(response, 'content'):
                print(f"  响应内容: {response.content}")
            else:
                print(f"  LLM 响应 (来自 event.llm_output): {str(response)[:500]}...")
        else:
            print("  在 event.llm_output 中未找到响应对象。")
        print("----------------------------------")

# --- 实例化 CustomGeminiLLM ---
custom_llm_tool_config = {"function_calling_config": {"mode": "AUTO"}}
zhz_agent_tool = HybridRAGTool()
researcher_tools = [zhz_agent_tool]

llm_for_agent = CustomGeminiLLM(
    model=GEMINI_MODEL_NAME,
    api_key=GEMINI_API_KEY,
    temperature=0.1,
    max_tokens=2048,
    tool_config=custom_llm_tool_config,
    agent_tools=researcher_tools # 传递工具列表以供缓存
)
print(f"Custom Agent LLM configured: {GEMINI_MODEL_NAME} with custom tool_config")

# --- 设置 BaseMCPTool 的调用器 ---
BaseMCPTool.set_mcpo_caller(call_mcpo_tool)

# --- 定义 Agents ---
researcher_agent = Agent(
    role='信息检索专家',
    goal='准确地回答用户查询，并且只使用提供的工具。',
    backstory=(
        "你是一位高级AI助手，专注于信息检索。"
        "你的专长在于高效地利用工具来查找最相关和最精确的答案来回应用户的查询。"
    ),
    llm=llm_for_agent,
    tools=researcher_tools,
    verbose=True,
    allow_delegation=False,
)

writer_agent = Agent(
    role='报告撰写专家',
    goal='根据提供的信息，撰写清晰、结构良好且富有洞察力的报告。',
    backstory=(
        "您是一位资深的报告撰写专家，拥有出色的分析和写作能力。"
        "您擅长将复杂的信息提炼成易于理解的报告，并能根据不同的输出状态（答案、澄清、错误）"
        "灵活调整报告内容和格式。"
    ),
    llm=llm_for_agent,
    verbose=True,
    allow_delegation=False,
)

# --- 定义 Tasks (包含上下文传递修复) ---
research_task_description = """你收到了来自用户的以下查询：

'{query}'

你应该使用提供的 `HybridRAGQueryTool` 工具来处理这个查询。
如果这个工具需要 `top_k_vector`, `top_k_kg`, 或 `top_k_bm25` 这些参数，请使用以下建议值：
top_k_vector: 5, top_k_kg: 3, top_k_bm25: 5。
在使用完必要的工具后，你的最终输出应该是（使用中文）：'我的最终答案是：' 
后跟你使用的工具返回的精确、完整的JSON字符串输出内容。"""

research_task_expected_output = "短语 '我的最终答案是：' 后跟你使用的工具返回的精确、完整的JSON字符串输出内容。"

research_task = Task(
    description=research_task_description,
    expected_output=research_task_expected_output,
    agent=researcher_agent,
)

report_writing_task = Task(
    description="""根据【前一个任务】（信息检索专家）提供的RAG工具输出（它是一个JSON字符串），生成一份报告或响应。
请仔细分析这个JSON字符串输出，它应该包含一个 'status' 字段。
1. 如果 'status' 是 'success'，则提取 'final_answer' 字段的内容，并基于此答案撰写一份简洁的报告。
2. 如果 'status' 是 'clarification_needed'，则提取 'clarification_question' 字段的内容，并向用户明确指出需要澄清的问题，例如：'系统需要澄清：[澄清问题]'。
3. 如果 'status' 是 'error'，则提取 'error_message' (或 'error') 字段的内容，并向用户报告错误，例如：'RAG服务发生错误：[错误信息]'。
你的最终输出必须是清晰、专业且符合上述情况的报告或响应。""",
    expected_output="一份清晰的报告，或者一个明确的澄清请求，或者一个错误报告。",
    agent=writer_agent,
    context=[research_task],
)

# --- 实例化监听器 ---
my_event_logger = MyDetailedLogger()

# --- 定义 Crew (添加 event_listeners) ---
office_brain_crew = Crew(
    agents=[researcher_agent, writer_agent],
    tasks=[research_task, report_writing_task],
    process=Process.sequential,
    verbose=True,
    event_listeners=[my_event_logger] # <<< --- 激活事件监听器 ---
)

# --- 启动 Crew ---
if __name__ == "__main__":
    print("--- 启动智能助手终端大脑 Crew (使用 CustomGeminiLLM 和事件监听器) ---")
    user_query_input = "公司2024年第一季度在华东和华北的总销售额一共是多少？"
    # --- 修复：kickoff inputs 只包含 query ---
    inputs = {'query': user_query_input}
    result = office_brain_crew.kickoff(inputs=inputs)
    print("\n\n=== 最终报告 ===\n")
    if hasattr(result, 'raw'):
        print(result.raw)
    else:
        print(result)
    print("\n--- Crew 任务完成 ---")
```

File: zhz_rag/crewai_integration/tools.py
-----------------------------------------
```python
# zhz_agent/custom_crewai_tools.py

import os
import json
import asyncio
import traceback
from typing import Type, List, Dict, Any, Optional, ClassVar
from pydantic import BaseModel, Field
from crewai.tools import BaseTool
import httpx

# 从 zhz_agent.pydantic_models 导入 QueryRequest
from zhz_rag.config.pydantic_models import QueryRequest # 用于 RAG 工具的输入

# MCPO 代理的基地址
MCPO_BASE_URL = os.getenv("MCPO_BASE_URL", "http://localhost:8006")

class BaseMCPTool(BaseTool):
    mcpo_base_url: str = MCPO_BASE_URL
    _call_mcpo_func: ClassVar[callable] = None

    @classmethod
    def set_mcpo_caller(cls, caller: callable):
        cls._call_mcpo_func = caller

    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    async def _call_mcpo_endpoint(self, service_and_tool_path: str, payload: dict) -> dict | str:
        api_url = f"{self.mcpo_base_url}/{service_and_tool_path}"
        cleaned_payload = {k: v for k, v in payload.items() if v is not None}
        print(f"BaseMCPTool: Calling mcpo endpoint: POST {api_url} with payload: {json.dumps(cleaned_payload, ensure_ascii=False)}")
        
        # --- [修改] 移除 proxies=None 参数 ---
        async with httpx.AsyncClient(trust_env=False) as client:
            response = None
            try:
                headers = {"Content-Type": "application/json"}
                response = await client.post(api_url, json=cleaned_payload, headers=headers, timeout=300.0)
                print(f"BaseMCPTool: mcpo status code for {service_and_tool_path}: {response.status_code}")
                print(f"BaseMCPTool: mcpo response headers for {service_and_tool_path}: {response.headers}") # <--- 新增日志
                # 尝试分块读取响应或提前获取少量内容进行日志记录，以防响应过大卡住 .text 或 .json()
                try:
                    response_text_snippet = await response.aread(num_bytes=1024) # 读取前1KB
                    print(f"BaseMCPTool: mcpo response text snippet (first 1KB) for {service_and_tool_path}: {response_text_snippet.decode(errors='ignore')}")
                except Exception as e_read:
                    print(f"BaseMCPTool: Error reading response snippet: {e_read}")

                if response.status_code == 200:
                    try:
                        # print(f"BaseMCPTool: mcpo raw response text for {service_and_tool_path}: {response.text}") # 如果怀疑内容问题，可以取消注释，但小心大响应
                        return response.json()
                    except json.JSONDecodeError:
                        print(f"BaseMCPTool Warning: mcpo returned status 200 but response is not JSON for '{service_and_tool_path}'. Returning raw text.")
                        return response.text
                else:
                    error_text = f"mcpo call to '{service_and_tool_path}' failed with status {response.status_code}. Response: {response.text[:500]}..."
                    print(f"BaseMCPTool Error: {error_text}")
                    return {"error": error_text, "status_code": response.status_code}
            except httpx.RequestError as exc:
                error_msg = f"BaseMCPTool HTTP RequestError calling mcpo tool '{service_and_tool_path}': {type(exc).__name__} - {exc}"
                print(f"BaseMCPTool Error: {error_msg}"); traceback.print_exc()
                return {"error": error_msg, "exception_type": type(exc).__name__}
            except Exception as exc:
                error_msg = f"BaseMCPTool Unexpected error calling mcpo tool '{service_and_tool_path}': {type(exc).__name__} - {exc}"
                response_text_snippet = response.text[:500] if response and hasattr(response, 'text') else "Response object not available or no text."
                print(f"BaseMCPTool Error: {error_msg}. Response snippet: {response_text_snippet}"); traceback.print_exc()
                return {"error": error_msg, "exception_type": type(exc).__name__}

    def _handle_tool_result(self, result: dict | str, tool_name_for_log: str) -> str:
        print(f"BaseMCPTool DEBUG: {tool_name_for_log} result from mcpo: {str(result)[:500]}...")
        parsed_result = result
        if isinstance(result, str):
            try:
                parsed_result = json.loads(result)
            except json.JSONDecodeError:
                if "error" in result.lower() or "failed" in result.lower() or "traceback" in result.lower():
                    return f"调用 {tool_name_for_log} 失败，返回非JSON错误文本: {result}"
                print(f"BaseMCPTool Info: Result for {tool_name_for_log} is a non-JSON string, returning as is.")
                return result
        if isinstance(parsed_result, dict):
            if "error" in parsed_result and "status_code" in parsed_result:
                return f"调用 {tool_name_for_log} 时发生HTTP错误：{parsed_result.get('error')}"
            if parsed_result.get("status") == "error":
                error_msg = parsed_result.get("error_message", "未知错误")
                error_code = parsed_result.get("error_code", "NO_CODE")
                return f"工具 {tool_name_for_log} 执行失败 (错误码: {error_code})：{error_msg}"
            try:
                return json.dumps(parsed_result, ensure_ascii=False, indent=2)
            except Exception as e:
                print(f"BaseMCPTool Error formatting successful dict result for {tool_name_for_log}: {e}")
                return str(parsed_result)
        print(f"BaseMCPTool Warning: Unexpected result format from {tool_name_for_log} mcpo call: {type(result)}, content: {str(result)[:200]}")
        return f"从 {tool_name_for_log} 服务收到的结果格式不正确或无法处理: {str(result)[:500]}"

    def _run_default_sync_wrapper(self, **kwargs) -> str:
        tool_name = getattr(self, 'name', self.__class__.__name__)
        print(f"BaseMCPTool INFO: Synchronous _run called for {tool_name} with args: {kwargs}.")
        result_str = ""
        try:
            # --- 改进的 asyncio.run 处理 ---
            try:
                loop = asyncio.get_running_loop()
            except RuntimeError:
                loop = None

            async def async_runner():
                return await self._arun(**kwargs)

            if loop and loop.is_running():
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                    future = executor.submit(asyncio.run, async_runner())
                    result = future.result(timeout=120)
            else:
                result = asyncio.run(async_runner())
            result_str = str(result)
        except asyncio.TimeoutError:
            error_message = f"Tool {tool_name} execution timed out after 120 seconds."
            print(f"BaseMCPTool ERROR_TRACE: {error_message}"); result_str = error_message
        except RuntimeError as e:
            if "cannot run event loop while another loop is running" in str(e).lower() or "event loop is already running" in str(e).lower():
                error_message = (f"BaseMCPTool Error in {tool_name} _run: Nested asyncio event loop conflict. Original error: {e}")
            else:
                error_message = f"BaseMCPTool RuntimeError in {tool_name} _run: {type(e).__name__} - {e}"
            print(f"BaseMCPTool ERROR_TRACE: {error_message}"); traceback.print_exc();
            result_str = error_message
        except Exception as e:
            error_message = f"BaseMCPTool General Exception in {tool_name} _run: {type(e).__name__} - {e}"
            print(f"BaseMCPTool ERROR_TRACE: {error_message}"); traceback.print_exc(); result_str = error_message
        return result_str

class HybridRAGTool(BaseMCPTool):
    name: str = "HybridRAGQueryTool"
    description: str = (
        "【核心RAG工具】用于通过执行混合检索增强生成 (RAG) 搜索来回答用户问题。 "
        "该工具整合了向量检索、知识图谱检索和关键词检索，并进行智能融合和重排序。 "
        "当用户需要从知识库中获取信息、回答复杂问题或生成报告时，应调用此工具。"
    )
    args_schema: Type[BaseModel] = QueryRequest
    target_mcp_service_path: str = "zhz_agent_rag_service/query_rag_v2"

    async def _arun(self, query: str, top_k_vector: int, top_k_kg: int, top_k_bm25: int, **kwargs: Any) -> str: # --- 确认有 **kwargs ---
        tool_name_for_log = getattr(self, 'name', "HybridRAGTool")
        print(f"CrewAI Tool DEBUG: {tool_name_for_log}._arun called with query='{query}', top_k_vector={top_k_vector}, top_k_kg={top_k_kg}, top_k_bm25={top_k_bm25}, additional_kwargs={kwargs}")

        security_context = kwargs.get('security_context')
        if security_context:
            print(f"CrewAI Tool INFO: Received security_context (in HybridRAGTool): {str(security_context)[:200]}...")

        payload = {
            "query": query,
            "top_k_vector": top_k_vector,
            "top_k_kg": top_k_kg,
            "top_k_bm25": top_k_bm25
        }
        result = await self._call_mcpo_endpoint(self.target_mcp_service_path, payload)
        return self._handle_tool_result(result, self.name)

    def _run(self, query: str, top_k_vector: int, top_k_kg: int, top_k_bm25: int, **kwargs: Any) -> str: # --- 确认有 **kwargs ---
        return self._run_default_sync_wrapper(query=query, top_k_vector=top_k_vector, top_k_kg=top_k_kg, top_k_bm25=top_k_bm25, **kwargs)
```

File: zhz_rag/config/constants.py
---------------------------------
```python
# zhz_agent/zhz_rag/config/constants.py

NEW_KG_SCHEMA_DESCRIPTION = """
# 知识图谱结构 (KuzuDB) 与 Cypher 查询生成规则

## 1. 节点定义:
- 节点标签: `:ExtractedEntity` (你必须且只能使用此节点标签)
- 节点属性:
    - `id_prop`: STRING (主键，实体的唯一标识)
    - `text`: STRING (实体的名称或文本内容)
    - `label`: STRING (实体类型。允许的值: "PERSON", "ORGANIZATION", "TASK")

## 2. 关系定义:
- 关系类型: `:WorksAt`
    - 结构: `(:ExtractedEntity {label:"PERSON"}) -[:WorksAt]-> (:ExtractedEntity {label:"ORGANIZATION"})`
    - 含义: 一个人 (PERSON) 在一个组织 (ORGANIZATION) 工作。
- 关系类型: `:AssignedTo`
    - 结构: `(:ExtractedEntity {label:"TASK"}) -[:AssignedTo]-> (:ExtractedEntity {label:"PERSON"})`
    - 含义: 一个任务 (TASK) 被分配给一个人 (PERSON)。

## 3. Cypher 查询生成 - 输出为 JSON 对象:

    你的【完整且唯一】的回答，必须是一个包含 "status" 和 "query" 字段的JSON对象。
    - 如果你能根据用户问题和Schema生成一个有效的Cypher查询：
        - "status" 字段应为 "success"。
        - "query" 字段应为该Cypher查询字符串。
    - 如果你无法生成有效的Cypher查询：
        - "status" 字段应为 "unable_to_generate"。
        - "query" 字段应为 "无法生成Cypher查询."。
    【不要在JSON之外或query字段内（当status为success时）包含任何解释或额外文本。】

## 4. JSON 输出格式示例:

### 示例 1 (能够生成查询):
用户问题: "任务'FixBug123'分配给了谁？"
你的【完整且唯一】的 JSON 回答:
```json
{
  "status": "success",
  "query": "MATCH (t:ExtractedEntity {text: 'FixBug123', label: 'TASK'})-[:AssignedTo]->(p:ExtractedEntity {label: 'PERSON'}) RETURN p.text AS Assignee"
}
示例 2 (无法根据Schema回答):
用户问题: "法国的首都是哪里？"
你的【完整且唯一】的 JSON 回答:
{
  "status": "unable_to_generate",
  "query": "无法生成Cypher查询."
}
"""
```

File: zhz_rag/config/pydantic_models.py
---------------------------------------
```python
# /home/zhz/zhz_agent/zhz_rag/config/pydantic_models.py
from pydantic import BaseModel, Field, root_validator
from typing import List, Dict, Any, Optional
from enum import Enum
from datetime import datetime
import uuid

# --- RAG Models ---
class QueryRequest(BaseModel):
    query: str
    # --- 修改：为所有 top_k 参数提供默认值，使其变为可选 ---
    top_k_vector: int = Field(default=3, description="Number of results to retrieve from vector search.")
    top_k_bm25: int = Field(default=3, description="Number of results to retrieve from BM25 search.")
    top_k_kg: int = Field(default=2, description="Number of results to retrieve from Knowledge Graph search.")
    top_k_final: int = Field(default=3, description="Number of final results after fusion and reranking.")
    # --- 结束修改 ---

    class Config:
        json_schema_extra = {
            "example": {
                "query": "What are the main objectives of the project?",
                "top_k_vector": 3,
                "top_k_bm25": 3,
                "top_k_kg": 2,
                "top_k_final": 3
            }
        }
        # 移除了 root_validator 和 extra='forbid' 以简化并遵循新的实践
        # 如果您仍需要严格的字段检查，可以将 extra='forbid' 放回

class RetrievedDocument(BaseModel):
    source_type: str
    content: str
    score: Optional[float] = None
    metadata: Optional[Dict[str, Any]] = None

class HybridRAGResponse(BaseModel):
    original_query: str
    answer: str
    retrieved_sources: List[RetrievedDocument]
    debug_info: Optional[Dict[str, Any]] = None


# --- Task Management Models ---
class TaskStatus(str, Enum):
    PENDING = "pending"
    ACTIVE = "active"
    COMPLETED = "completed"
    CANCELLED = "cancelled"
    FAILED = "failed"
    REMINDING = "reminding"

class ReminderMethod(str, Enum):
    NOTIFICATION = "notification"

class TaskModel(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid.uuid4()), description="任务的唯一ID (自动生成)")
    title: str = Field(description="任务标题")
    description: Optional[str] = Field(None, description="任务的详细描述")
    status: TaskStatus = Field(default=TaskStatus.PENDING, description="任务当前状态")
    created_at: datetime = Field(default_factory=datetime.utcnow, description="任务创建时间 (UTC)")
    updated_at: datetime = Field(default_factory=datetime.utcnow, description="任务最后更新时间 (UTC)")
    due_date: Optional[datetime] = Field(None, description="任务截止日期或计划执行时间 (UTC)")
    reminder_time: Optional[datetime] = Field(None, description="任务提醒时间 (UTC)")
    reminder_offset_minutes: Optional[int] = Field(None, description="提醒时间相对于due_date的提前分钟数 (例如10分钟前)")
    reminder_methods: List[ReminderMethod] = Field(default=[ReminderMethod.NOTIFICATION], description="提醒方式列表")
    priority: int = Field(default=0, description="任务优先级 (例如 0:普通, 1:重要, 2:紧急)")
    tags: List[str] = Field(default_factory=list, description="任务标签")
    action_type: Optional[str] = Field(None, description="任务到期时需要执行的动作类型 (例如 'navigate', 'send_message', 'run_report')")
    action_payload: Dict[str, Any] = Field(default_factory=dict, description="执行动作时需要的参数 (例如导航的目的地)")
    execution_result: Optional[str] = Field(None, description="任务执行后的结果或错误信息")
    last_executed_at: Optional[datetime] = Field(None, description="上次执行时间 (UTC)")

    class Config:
        use_enum_values = True
        # Pydantic v2 推荐使用 from_attributes 替代 orm_mode
        from_attributes = True


class CreateTaskRequest(BaseModel):
    title: str
    description: Optional[str] = None
    due_date: Optional[datetime] = None
    reminder_offset_minutes: Optional[int] = None
    reminder_methods: Optional[List[ReminderMethod]] = [ReminderMethod.NOTIFICATION]
    priority: Optional[int] = 0
    tags: Optional[List[str]] = None
    action_type: Optional[str] = None
    action_payload: Optional[Dict[str, Any]] = None
    
    class Config:
        extra = 'forbid'

class UpdateTaskRequest(BaseModel):
    title: Optional[str] = None
    description: Optional[str] = None
    status: Optional[TaskStatus] = None
    due_date: Optional[datetime] = None
    reminder_offset_minutes: Optional[int] = None
    reminder_methods: Optional[List[ReminderMethod]] = None
    priority: Optional[int] = None
    tags: Optional[List[str]] = None
    action_type: Optional[str] = None
    action_payload: Optional[Dict[str, Any]] = None

    class Config:
        extra = 'forbid'


class IdentifiedEntity(BaseModel):
    text: str = Field(description="识别出的实体文本。")
    label: Optional[str] = Field(None, description="推断的实体类型 (例如 PERSON, ORGANIZATION, TASK)。")

class ExtractedRelationItem(BaseModel): # 新建一个类名以避免与可能的其他同名类冲突
    head_entity_text: str
    head_entity_label: str
    relation_type: str
    tail_entity_text: str
    tail_entity_label: str

class ExtractedEntitiesAndRelationIntent(BaseModel):
    entities: List[IdentifiedEntity] = Field(default_factory=list, description="从用户查询中识别出的核心实体列表。")
    # --- 新增 "relations" 字段 ---
    relations: List[ExtractedRelationItem] = Field(default_factory=list, description="从用户查询中识别出的关系列表。")
    # --- "relation_hint" 字段可以保留，或者如果您觉得 "relations" 列表更全面，可以考虑移除或标记为废弃 ---
    relation_hint: Optional[str] = Field(None, description="[可选的旧字段] 如果用户查询暗示了实体间的特定关系，这里是关系的文本描述或关键词。新的 'relations' 列表更推荐。")
```

File: zhz_rag/utils/__init__.py
-------------------------------
--- File is empty ---

File: zhz_rag/utils/common_utils.py
-----------------------------------
```python
# zhz_rag/utils/common_utils.py

import httpx
import json
import traceback
import os
import glob
from dotenv import load_dotenv
from datetime import datetime, timezone
import uuid
import logging
import asyncio #确保 asyncio 被导入
from typing import List, Dict, Any, Optional
import re
import unicodedata
import logging

load_dotenv()

# --- Logger Configuration ---
utils_logger = logging.getLogger("UtilsLogger")
utils_logger.setLevel(logging.INFO)
if not utils_logger.hasHandlers():
    _utils_console_handler = logging.StreamHandler()
    _utils_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
    _utils_console_handler.setFormatter(_utils_formatter)
    utils_logger.addHandler(_utils_console_handler)
    utils_logger.propagate = False
utils_logger.info("--- UtilsLogger configured ---")

# --- MCP Configuration ---
MCPO_BASE_URL = os.getenv("MCPO_BASE_URL", "http://localhost:8006")

_CURRENT_FILE_DIR = os.path.dirname(os.path.abspath(__file__))
_ZHZ_RAG_PACKAGE_DIR = os.path.dirname(_CURRENT_FILE_DIR)

STORED_DATA_ROOT_DIR = os.path.join(_ZHZ_RAG_PACKAGE_DIR, 'stored_data')

RAG_INTERACTION_LOGS_DIR = os.path.join(STORED_DATA_ROOT_DIR, 'rag_interaction_logs')
EVALUATION_RESULTS_LOGS_DIR = os.path.join(STORED_DATA_ROOT_DIR, 'evaluation_results_logs')

FINETUNING_GENERATED_DATA_DIR = os.path.join(_ZHZ_RAG_PACKAGE_DIR, 'finetuning', 'generated_data')

# Ensure these directories exist
_DIRECTORIES_TO_CREATE = [
    STORED_DATA_ROOT_DIR,
    RAG_INTERACTION_LOGS_DIR,
    EVALUATION_RESULTS_LOGS_DIR,
    FINETUNING_GENERATED_DATA_DIR
]
for dir_path in _DIRECTORIES_TO_CREATE:
    if not os.path.exists(dir_path):
        try:
            os.makedirs(dir_path, exist_ok=True)
            utils_logger.info(f"Successfully created directory: {dir_path}")
        except Exception as e:
            utils_logger.error(f"Error creating directory {dir_path}: {e}. Consider creating it manually.")

# --- Log File Path Getters ---

def get_interaction_log_filepath() -> str:
    """Gets the full path for the current RAG interaction log file (daily rotation)."""
    today_str = datetime.now(timezone.utc).strftime("%Y%m%d")
    return os.path.join(RAG_INTERACTION_LOGS_DIR, f"rag_interactions_{today_str}.jsonl")

def get_evaluation_result_log_filepath(evaluation_name: str) -> str:
    """Gets the full path for an evaluation result log file (daily rotation, by evaluation name)."""
    today_str = datetime.now(timezone.utc).strftime("%Y%m%d")
    return os.path.join(EVALUATION_RESULTS_LOGS_DIR, f"eval_results_{evaluation_name}_{today_str}.jsonl")

def find_latest_rag_interaction_log(log_dir: str = RAG_INTERACTION_LOGS_DIR) -> Optional[str]:
    """
    Finds the latest RAG interaction log file (rag_interactions_*.jsonl) in the specified directory.
    Defaults to RAG_INTERACTION_LOGS_DIR.
    """
    utils_logger.debug(f"Searching for RAG interaction logs in: {log_dir}")
    rag_log_pattern = os.path.join(log_dir, "rag_interactions_*.jsonl")
    candidate_rag_logs = glob.glob(rag_log_pattern)

    if candidate_rag_logs:
        candidate_rag_logs.sort(key=os.path.getmtime, reverse=True)
        utils_logger.info(f"Automatically selected RAG interaction log: {candidate_rag_logs[0]}")
        return candidate_rag_logs[0]
    else:
        utils_logger.warning(f"No RAG interaction log files found matching pattern: {rag_log_pattern} in directory {log_dir}")
        return None

# --- Logging Function ---

async def log_interaction_data(
    interaction_data: Dict[str, Any],
    is_evaluation_result: bool = False,
    evaluation_name_for_file: Optional[str] = None
):
    """
    Asynchronously appends a single interaction data or evaluation result to a JSONL file.
    """
    if is_evaluation_result:
        if not evaluation_name_for_file:
            evaluation_name_for_file = interaction_data.get("task_type", "general_eval_result") # More specific default
        filepath = get_evaluation_result_log_filepath(evaluation_name=evaluation_name_for_file)
    else:
        filepath = get_interaction_log_filepath()

    if "timestamp_utc" not in interaction_data:
        interaction_data["timestamp_utc"] = datetime.now(timezone.utc).isoformat()
    if "interaction_id" not in interaction_data and not is_evaluation_result: # Eval results use original_interaction_id_ref
        interaction_data["interaction_id"] = str(uuid.uuid4())
    elif "interaction_id" not in interaction_data and is_evaluation_result and "original_interaction_id_ref" in interaction_data:
        # For eval results, ensure there's an ID, can be a new one for the eval log entry itself
         interaction_data["interaction_id"] = str(uuid.uuid4())

    try:
        data_to_log = interaction_data.copy()
        if 'llm_input_messages' in data_to_log and isinstance(data_to_log['llm_input_messages'], list):
            processed_messages = []
            for msg in data_to_log['llm_input_messages']:
                if isinstance(msg, dict) and msg.get('role') == 'system':
                    processed_msg = msg.copy() # 复制消息字典
                    original_content = processed_msg.get('content', '')
                    processed_msg['content'] = f"System Prompt (length: {len(original_content)}, starts with: {original_content[:70]}...)"
                    processed_messages.append(processed_msg)
                else:
                    processed_messages.append(msg)
            data_to_log['llm_input_messages'] = processed_messages # 用处理过的消息列表替换原来的
        if 'llm_input_original_prompt_if_string' in data_to_log and isinstance(data_to_log['llm_input_original_prompt_if_string'], str):
            original_prompt_str = data_to_log['llm_input_original_prompt_if_string']
            if len(original_prompt_str) > 500: # 如果原始prompt字符串太长
                data_to_log['llm_input_original_prompt_if_string'] = f"Original Prompt String (length: {len(original_prompt_str)}, starts with: {original_prompt_str[:200]}...)"
        # --- 添加结束 ---

        def _write_sync():
            log_file_dir = os.path.dirname(filepath)
            if not os.path.exists(log_file_dir):
                try:
                    os.makedirs(log_file_dir, exist_ok=True)
                    # utils_logger.info(f"Created directory for log file: {log_file_dir}") # 使用print替代，避免日志级别问题
                    print(f"COMMON_UTILS_LOG_DATA: Created directory for log file: {log_file_dir}")
                except Exception as e_mkdir:
                    # utils_logger.error(f"Error creating directory {log_file_dir} for log file: {e_mkdir}")
                    print(f"COMMON_UTILS_LOG_DATA: Error creating directory {log_file_dir} for log file: {e_mkdir}")
                    return 

            with open(filepath, 'a', encoding='utf-8') as f:
                json_string_to_write = json.dumps(data_to_log, ensure_ascii=False, default=str)
                f.write(json_string_to_write + "\n") 
        await asyncio.to_thread(_write_sync)
    except Exception as e:
        utils_logger.error(f"Failed to log interaction data to {filepath}: {e}", exc_info=True)

# --- MCP Tool Calling Utility ---

async def call_mcpo_tool(tool_name_with_prefix: str, payload: Dict[str, Any]) -> Dict[str, Any]:
    """
    异步调用MCP工具服务，并返回结构化的成功或错误响应。
    """
    api_url = f"{MCPO_BASE_URL}/{tool_name_with_prefix}"
    cleaned_payload = {k: v for k, v in (payload or {}).items() if v is not None}

    utils_logger.info(f"CALL_MCPO_TOOL: Attempting to call {api_url}") # 使用 utils_logger
    utils_logger.debug(f"CALL_MCPO_TOOL: Payload: {json.dumps(cleaned_payload, ensure_ascii=False)}") # 使用 utils_logger

    timeout_config = httpx.Timeout(120.0, connect=10.0, read=120.0, write=10.0) 
    
    async with httpx.AsyncClient(timeout=timeout_config) as client:
        response: Optional[httpx.Response] = None 
        try:
            headers = {
                "Content-Type": "application/json",
                "Accept": "application/json",
                "User-Agent": "ZhzAgent/1.0 (call_mcpo_tool)"
            }
            utils_logger.debug(f"CALL_MCPO_TOOL: Sending POST request to {api_url} with headers: {headers}") # 使用 utils_logger
            response = await client.post(api_url, json=cleaned_payload, headers=headers)
            
            utils_logger.info(f"CALL_MCPO_TOOL: Response from {api_url} - Status: {response.status_code}") # 使用 utils_logger
            utils_logger.debug(f"CALL_MCPO_TOOL: Response Headers: {response.headers}") # 使用 utils_logger
            
            try:
                response_text_snippet = response.text[:500] 
                utils_logger.debug(f"CALL_MCPO_TOOL: Response Text Snippet (first 500 chars): {response_text_snippet}") # 使用 utils_logger
            except Exception as e_read_snippet:
                utils_logger.warning(f"CALL_MCPO_TOOL: Could not read response text snippet: {e_read_snippet}") # 使用 utils_logger

            response.raise_for_status() 

            try:
                result_data = response.json()
                utils_logger.info(f"CALL_MCPO_TOOL: Successfully received and parsed JSON response from {api_url}.") # 使用 utils_logger
                if isinstance(result_data, dict) and result_data.get("isError"):
                    error_content_list = result_data.get("content", [{"type": "text", "text": "Unknown error from MCP tool"}])
                    error_text_from_mcp_payload = "Unknown error from MCP tool"
                    for item in error_content_list:
                        if item.get("type") == "text":
                            error_text_from_mcp_payload = item.get("text", error_text_from_mcp_payload)
                            break
                    utils_logger.error(f"CALL_MCPO_TOOL: MCP Tool '{tool_name_with_prefix}' reported an application-level error (isError=true): {error_text_from_mcp_payload}") # 使用 utils_logger
                    return {
                        "success": False,
                        "error": f"MCP tool '{tool_name_with_prefix}' reported failure: {error_text_from_mcp_payload}",
                        "error_type": "MCP_APPLICATION_ERROR",
                        "status_code": response.status_code
                    }
                return {
                        "success": True, 
                        "data": result_data 
                }
            except json.JSONDecodeError as e_json_decode:
                utils_logger.error(f"CALL_MCPO_TOOL: Response from {api_url} was 2xx but not valid JSON. Error: {e_json_decode}", exc_info=True) # 使用 utils_logger
                return {
                    "success": False,
                    "error": "MCP service returned a 2xx status but the response was not valid JSON.",
                    "error_type": "JSON_DECODE_ERROR",
                    "status_code": response.status_code,
                    "raw_response_snippet": response.text[:500] if response else "N/A"
                }

        except httpx.HTTPStatusError as exc_http_status:
            error_message = f"HTTP Error {exc_http_status.response.status_code} when calling {api_url}."
            utils_logger.error(f"CALL_MCPO_TOOL: {error_message} Response: {exc_http_status.response.text[:500]}", exc_info=True) # 使用 utils_logger
            error_detail_from_response = exc_http_status.response.text
            try:
                parsed_error_json = exc_http_status.response.json()
                if isinstance(parsed_error_json, dict) and "detail" in parsed_error_json:
                    error_detail_from_response = parsed_error_json["detail"]
                elif isinstance(parsed_error_json, dict) and "error" in parsed_error_json: 
                    error_detail_from_response = parsed_error_json["error"]
            except json.JSONDecodeError:
                pass 
            return {
                "success": False,
                "error": f"HTTP error from MCP service: {error_detail_from_response}",
                "error_type": "HTTP_STATUS_ERROR",
                "status_code": exc_http_status.response.status_code,
                "raw_response_snippet": exc_http_status.response.text[:500] if exc_http_status.response else "N/A"
            }
        except httpx.TimeoutException as exc_timeout:
            utils_logger.error(f"CALL_MCPO_TOOL: Timeout when calling {api_url}. Error: {exc_timeout}", exc_info=True) # 使用 utils_logger
            return {
                "success": False,
                "error": f"Request to MCP service timed out after {timeout_config.read if timeout_config else 'default'}s.",
                "error_type": "TIMEOUT_ERROR",
                "status_code": None 
            }
        except httpx.ConnectError as exc_connect:
            utils_logger.error(f"CALL_MCPO_TOOL: Connection error when calling {api_url}. Is the MCP service running at {MCPO_BASE_URL}? Error: {exc_connect}", exc_info=True) # 使用 utils_logger
            return {
                "success": False,
                "error": f"Could not connect to MCP service at {MCPO_BASE_URL}.",
                "error_type": "CONNECTION_ERROR",
                "status_code": None
            }
        except httpx.RequestError as exc_request_other: 
            utils_logger.error(f"CALL_MCPO_TOOL: Network request error when calling {api_url}. Error: {exc_request_other}", exc_info=True) # 使用 utils_logger
            return {
                "success": False,
                "error": f"A network request error occurred: {str(exc_request_other)}",
                "error_type": type(exc_request_other).__name__,
                "status_code": None
            }
        except Exception as exc_unexpected:
            utils_logger.error(f"CALL_MCPO_TOOL: Unexpected error when calling {api_url}. Error: {exc_unexpected}", exc_info=True) # 使用 utils_logger
            return {
                "success": False,
                "error": f"An unexpected error occurred during MCP call: {str(exc_unexpected)}",
                "error_type": type(exc_unexpected).__name__,
                "status_code": response.status_code if response else None, 
                "traceback": traceback.format_exc() 
            }

def load_jsonl_file(filepath: str, encoding: str = 'utf-8') -> List[Dict[str, Any]]:
    """
    从 JSONL 文件加载数据。

    Args:
        filepath (str): JSONL 文件的路径。
        encoding (str): 文件编码，默认为 'utf-8'。

    Returns:
        List[Dict[str, Any]]: 从文件中加载的字典列表。如果文件不存在或解析出错，
                              会记录错误并返回空列表。
    """
    data_list: List[Dict[str, Any]] = []
    if not os.path.exists(filepath):
        utils_logger.error(f"File not found: {filepath}") # 使用已有的 utils_logger
        return data_list

    try:
        with open(filepath, 'r', encoding=encoding) as f:
            for line_number, line in enumerate(f, 1):
                try:
                    if line.strip(): # 确保行不是空的
                        data_list.append(json.loads(line.strip()))
                except json.JSONDecodeError as e_json:
                    utils_logger.warning(f"Skipping malformed JSON line {line_number} in {filepath}: {e_json}")
                except Exception as e_line:
                    utils_logger.warning(f"Error processing line {line_number} in {filepath}: {e_line}")
    except FileNotFoundError: # 再次捕获以防万一，虽然上面已经检查了
        utils_logger.error(f"File not found during open: {filepath}")
    except Exception as e_file:
        utils_logger.error(f"Error reading or processing file {filepath}: {e_file}", exc_info=True)
        return [] # 如果文件读取层面发生严重错误，返回空列表

    utils_logger.info(f"Successfully loaded {len(data_list)} entries from {filepath}")
    return data_list


def normalize_text_for_id(text: str) -> str:
    if not isinstance(text, str):
        return str(text) 
    
    try:
        normalized_text = unicodedata.normalize('NFKD', text)
        normalized_text = normalized_text.lower()
        normalized_text = normalized_text.strip()
        normalized_text = re.sub(r'\s+', ' ', normalized_text)
        return normalized_text
    except Exception as e:
        return text
```

File: zhz_rag/utils/db_utils.py
-------------------------------
```python
# ZHZ_AGENT/database.py
import os
from databases import Database
from sqlalchemy import create_engine
from sqlalchemy.orm import declarative_base
from typing import Optional

# --- APScheduler 相关导入 ---
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.jobstores.sqlalchemy import SQLAlchemyJobStore
# --- [修改] 明确导入并使用 pytz ---
import pytz #

# --- 数据库配置 ---
ZHZ_AGENT_DIR = os.path.dirname(os.path.abspath(__file__))
DB_NAME = "ZHZ_AGENT_tasks.db"
DATABASE_FILE_PATH = os.path.join(ZHZ_AGENT_DIR, DB_NAME)
DATABASE_URL = f"sqlite+aiosqlite:///{DATABASE_FILE_PATH}"

database = Database(DATABASE_URL)
sqlalchemy_engine = create_engine(DATABASE_URL.replace("+aiosqlite", ""))
Base = declarative_base() #

# --- 全局调度器实例定义 ---
scheduler: Optional[AsyncIOScheduler] = None

def get_scheduler() -> AsyncIOScheduler:
    """获取或创建调度器实例，并配置作业存储和 UTC 时区。"""
    global scheduler
    if scheduler is None:
        jobstore_url = f"sqlite:///{DATABASE_FILE_PATH}"
        jobstores = {
            'default': SQLAlchemyJobStore(url=jobstore_url, tablename='apscheduler_jobs_v2') #
        }
        # --- [修复] 明确使用 pytz.utc 设置时区 ---
        scheduler = AsyncIOScheduler(
            jobstores=jobstores,
            timezone=pytz.utc # <--- 强制使用 pytz.utc #
        )
        import logging
        logging.getLogger('apscheduler').setLevel(logging.DEBUG)
        print(f"APScheduler initialized with timezone: {pytz.utc}") # 确认使用 pytz.utc #
    return scheduler #
```

File: zhz_rag/utils/interaction_logger.py
-----------------------------------------
```python
# 文件: zhz_rag/utils/interaction_logger.py
import os
import json
import asyncio
from typing import Dict, Any, Optional
from datetime import datetime, timezone
import logging
import uuid
import traceback

# --- 配置此模块的logger ---
# 使用一个特定的名字，以便在项目中其他地方可以按名获取，避免与根logger混淆
interaction_logger_module_logger = logging.getLogger("InteractionLoggerUtil")
# 建议从环境变量或配置文件读取日志级别，提供默认值
interaction_logger_module_logger.setLevel(os.getenv("INTERACTION_LOG_LEVEL", "INFO").upper())
# 设置propagate = False以防止日志消息被传递到父级logger（如root logger），避免重复输出
interaction_logger_module_logger.propagate = False

# 确保只添加一次处理器，防止重复配置
if not interaction_logger_module_logger.hasHandlers():
    _il_console_handler = logging.StreamHandler()
    _il_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
    _il_console_handler.setFormatter(_il_formatter)
    interaction_logger_module_logger.addHandler(_il_console_handler)
    interaction_logger_module_logger.info("--- InteractionLoggerUtil configured ---")


# --- 定义日志存储目录常量 (可以从 config.constants 导入，或在此处定义) ---
# 获取当前文件所在目录的父目录的父目录 (即 zhz_rag 的父目录，应该是 zhz_agent)
_PROJECT_ROOT_FOR_LOGS = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))

RAG_INTERACTION_LOGS_DIR_DEFAULT = os.path.join(_PROJECT_ROOT_FOR_LOGS, 'stored_data', 'rag_interaction_logs')
EVALUATION_RESULTS_LOGS_DIR_DEFAULT = os.path.join(_PROJECT_ROOT_FOR_LOGS, 'stored_data', 'evaluation_results_logs')


def _sync_write_to_jsonl_robust(filepath: str, interaction_json_string: str):
    """
    一个健壮的同步函数，用于将字符串追加到文件，并确保数据刷入磁盘。
    """
    logger_to_use = interaction_logger_module_logger
    logger_to_use.debug(f"SYNC_WRITE_ROBUST: Attempting to write to {filepath}")
    try:
        # 'a' for append. '+' is not strictly needed for 'a' as it creates the file if it doesn't exist.
        with open(filepath, 'a', encoding='utf-8') as f:
            f.write(interaction_json_string + "\n")
            # 步骤1: 确保Python应用层缓冲区的内容写入操作系统缓冲区
            f.flush()
            # 步骤2: 请求操作系统将缓冲区内容实际写入磁盘，提供最强保证
            os.fsync(f.fileno())
        logger_to_use.debug(f"SYNC_WRITE_ROBUST: Successfully wrote and synced to {filepath}")
    except Exception as e:
        # 这种底层的关键日志如果失败，需要非常明确的错误提示
        logger_to_use.error(f"CRITICAL_LOG_FAILURE in _sync_write_to_jsonl_robust: Failed to write to {filepath}. Error: {e}", exc_info=True)
        # 如果logger可能没有配置好，可以取消下面的注释作为备用方案
        # print(f"CRITICAL_LOG_FAILURE in _sync_write_to_jsonl_robust: Failed to write to {filepath}. Error: {e}")
        # traceback.print_exc()

async def log_interaction_data(
    interaction_data: Dict[str, Any],
    is_evaluation_result: bool = False,
    evaluation_name_for_file: Optional[str] = None,
    custom_log_dir: Optional[str] = None
):
    """
    异步将单条交互数据或评估结果追加到按天分割的JSONL文件中，
    此过程使用一个健壮的写入方法以确保数据持久化。

    Args:
        interaction_data (Dict[str, Any]): 要记录的交互数据字典。
        is_evaluation_result (bool): 如果为True，则记录到评估结果目录。默认为False。
        evaluation_name_for_file (Optional[str]): 如果是评估结果，用于文件名中区分评估类型。
        custom_log_dir (Optional[str]): 自定义日志目录路径，如果提供则覆盖默认目录。
    """
    logger_to_use = interaction_logger_module_logger
    filepath = "" # 初始化filepath以备在异常处理中使用
    try:
        # --- 数据准备逻辑 ---
        today_str = datetime.now(timezone.utc).strftime("%Y%m%d")
        
        if is_evaluation_result:
            base_dir = custom_log_dir if custom_log_dir else EVALUATION_RESULTS_LOGS_DIR_DEFAULT
            file_prefix = "eval_results"
            if evaluation_name_for_file:
                file_prefix += f"_{evaluation_name_for_file}"
        else:
            base_dir = custom_log_dir if custom_log_dir else RAG_INTERACTION_LOGS_DIR_DEFAULT
            file_prefix = "rag_interactions"
            
        log_filename = f"{file_prefix}_{today_str}.jsonl"
        filepath = os.path.join(base_dir, log_filename)

        if not os.path.exists(base_dir):
            try:
                os.makedirs(base_dir, exist_ok=True)
                logger_to_use.info(f"Created log directory: {base_dir}")
            except Exception as e_mkdir:
                logger_to_use.error(f"Failed to create log directory {base_dir}: {e_mkdir}", exc_info=True)
                # 如果目录创建失败，直接返回，避免后续操作引发更多错误
                return

        # 确保时间戳和ID存在
        if "timestamp_utc" not in interaction_data:
            interaction_data["timestamp_utc"] = datetime.now(timezone.utc).isoformat()
        if "interaction_id" not in interaction_data:
            interaction_data["interaction_id"] = str(uuid.uuid4())
        
        # 将字典转换为JSON字符串
        json_string = json.dumps(interaction_data, ensure_ascii=False)
        
        # 通过 asyncio.to_thread 调用我们加强版的同步写入函数
        await asyncio.to_thread(_sync_write_to_jsonl_robust, filepath, json_string)
        
        logger_to_use.debug(f"Successfully queued robust log write to {filepath}. Interaction ID: {interaction_data.get('interaction_id', 'N/A')}")

    except Exception as e:
        logger_to_use.error(f"ERROR in log_interaction_data: Failed to queue log writing for {filepath}. Error: {e}", exc_info=True)
        # 记录失败时，打印部分数据以供调试
        data_str_for_log = str(interaction_data)
        logger_to_use.error(f"Data that failed to log (first 500 chars): {data_str_for_log[:500]}")
```

File: zhz_rag/utils/load_neo4j_data.py
--------------------------------------
```python
# zhz_agent/load_neo4j_data.py
import json
import os
from neo4j import GraphDatabase, basic_auth
from dotenv import load_dotenv
import traceback

load_dotenv() # 确保加载 .env 文件中的NEO4J凭证

NEO4J_URI = os.getenv("NEO4J_URI", "bolt://localhost:7687")
NEO4J_USER = os.getenv("NEO4J_USER", "neo4j")
NEO4J_PASSWORD = os.getenv("NEO4J_PASSWORD") # 您需要确保这个密码是正确的

DATA_PATH = os.path.join(os.path.dirname(__file__), "data")
SAMPLE_KG_PATH = os.path.join(DATA_PATH, "sample_kg.json")

def clear_database(driver):
    """清除数据库中的所有节点和关系"""
    with driver.session(database="neo4j") as session:
        session.run("MATCH (n) DETACH DELETE n")
        print("Cleared all nodes and relationships from the database.")

def create_constraints(driver):
    """创建一些基本约束，确保节点属性的唯一性（如果适用）"""
    with driver.session(database="neo4j") as session:
        try:
            session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (p:Person) REQUIRE p.name IS UNIQUE")
            session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (pr:Project) REQUIRE pr.name IS UNIQUE")
            session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (r:Region) REQUIRE r.name IS UNIQUE")
            session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (prod:Product) REQUIRE prod.name IS UNIQUE")
            session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (d:Document) REQUIRE d.id IS UNIQUE")
            # SalesAmount 通常不需要唯一约束，因为它可能重复（例如不同区域同一时期的销售）
            print("Ensured constraints are created (or already exist).")
        except Exception as e:
            print(f"Error creating constraints: {e}")


def load_data(driver, kg_data):
    """根据kg_data中的facts加载数据到Neo4j"""
    facts = kg_data.get("facts", [])
    
    with driver.session(database="neo4j") as session:
        entities_to_create = set()
        node_types_from_schema = { # 定义了主要实体的标签和它们的主要标识属性
            "Person": "name", "Project": "name", "Region": "name", 
            "Product": "name", "Document": "id", "Idea": "name" # 新增Idea类型
        }

        for fact in facts:
            subject_name = fact.get("subject")
            object_name = fact.get("object")
            fact_type = fact.get("type", "")

            subject_label = None
            # 基于fact_type或其他逻辑推断subject_label
            if "person_" in fact_type: subject_label = "Person"
            elif "region_" in fact_type: subject_label = "Region"
            elif "product_" in fact_type: subject_label = "Product"
            # ... 其他类型的映射 ...
            
            if subject_label and subject_name:
                prop_name = node_types_from_schema.get(subject_label, "name")
                entities_to_create.add((subject_label, prop_name, subject_name))

            object_label = None
            if not fact_type.endswith("_amount"): # 不是直接的销售额事实
                if "_project" in fact_type: object_label = "Project"
                elif "_product" in fact_type: object_label = "Product"
                elif "_document" in fact_type: object_label = "Document"
                elif "_idea" in fact_type: object_label = "Idea" # 新增对Idea类型的处理
                # ... 其他类型的映射 ...

                if object_label and object_name:
                    prop_name = node_types_from_schema.get(object_label, "name") # Document会用id, Idea会用name
                    entities_to_create.add((object_label, prop_name, object_name))
        
        for label, prop, value in entities_to_create:
            if value is not None:
                query = f"MERGE (n:{label} {{{prop}: $value}})"
                session.run(query, value=value)
                print(f"Merged node: ({label} {{{prop}: '{value}'}})")

        for fact in facts:
            s_name = fact.get("subject")
            rel = fact.get("relation")
            o_name = fact.get("object")
            fact_type = fact.get("type", "")
            period = fact.get("period")

            if fact_type == "region_sales_amount" and period:
                session.run("MERGE (r:Region {name: $s_name})", s_name=s_name)
                try:
                    # ... (销售额解析逻辑不变) ...
                    if isinstance(o_name, str) and '万元' in o_name:
                        numeric_val_str = o_name.replace('万元', '').strip()
                        numeric_val = float(numeric_val_str)
                        unit_val = '万元'
                    # ... (其他单位解析) ...
                    else:
                        numeric_val = float(o_name) # 尝试直接转换
                        unit_val = None 
                    
                    query = """
                    MATCH (r:Region {name: $s_name})
                    CREATE (sa:SalesAmount {numeric_amount: $num_val, period: $period, unit: $unit_val})
                    CREATE (r)-[:HAS_SALES_AMOUNT]->(sa)
                    """
                    session.run(query, s_name=s_name, num_val=numeric_val, period=period, unit_val=unit_val)
                    print(f"Created SalesAmount for {s_name}, {period}: {numeric_val} {unit_val or ''}")
                except ValueError:
                    print(f"Could not parse sales amount: {o_name} for {s_name}, {period}. Skipping this SalesAmount fact.")
                
            elif s_name and rel and o_name: 
                s_label, o_label = None, None
                s_prop, o_prop = "name", "name" 

                # --- 更精确的标签和属性推断 ---
                if fact_type == "person_project" and rel == "WORKS_ON":
                    s_label, o_label = "Person", "Project"
                elif fact_type == "person_idea" and rel == "PROPOSED_IDEA": # 新增
                    s_label, o_label = "Person", "Idea"
                elif fact_type == "region_product" and rel == "HAS_SALES_PRODUCT": # 假设type是 region_product
                    s_label, o_label = "Region", "Product"
                elif fact_type == "product_document" and rel == "RELATED_TO":
                    s_label, o_label = "Product", "Document"
                    o_prop = "id" # Document用id匹配
                # 您可以根据您的fact_type添加更多精确的映射规则

                if s_label and o_label:
                    query = f"""
                    MATCH (s:{s_label} {{{s_prop}: $s_name}})
                    MATCH (o:{o_label} {{{o_prop}: $o_name}})
                    MERGE (s)-[:{rel}]->(o)
                    """
                    session.run(query, s_name=s_name, o_name=o_name)
                    print(f"Merged relationship: ({s_label} {{{s_prop}:'{s_name}'}})-[:{rel}]->({o_label} {{{o_prop}:'{o_name}'}})")
                else:
                    print(f"Could not determine labels for fact: {fact} (s_label: {s_label}, o_label: {o_label}). Relationship not created.")
            else:
                print(f"Skipping incomplete fact: {fact}")


if __name__ == "__main__":
    driver = None
    try:
        driver = GraphDatabase.driver(NEO4J_URI, auth=basic_auth(NEO4J_USER, NEO4J_PASSWORD))
        driver.verify_connectivity()
        print("Successfully connected to Neo4j.")
        
        clear_database(driver) # 清空数据库
        create_constraints(driver) # 创建约束

        with open(SAMPLE_KG_PATH, 'r', encoding='utf-8') as f:
            kg_data_to_load = json.load(f)
        
        load_data(driver, kg_data_to_load)
        
        print("\nData loading process completed.")
        print("You can now verify the data in Neo4j Browser (http://localhost:7474).")
        print("Example query to check SalesAmount:")
        print("MATCH (r:Region)-[:HAS_SALES_AMOUNT]->(sa:SalesAmount) RETURN r.name, sa.numeric_amount, sa.unit, sa.period")
        print("Example query to check Person-Project:")
        print("MATCH (p:Person)-[:WORKS_ON]->(proj:Project) RETURN p.name, proj.name")

    except Exception as e:
        print(f"An error occurred: {e}")
        traceback.print_exc()
    finally:
        if driver:
            driver.close()
            print("Neo4j connection closed.")
```

File: zhz_rag/core_rag/__init__.py
----------------------------------
```python
# zhz_agent/core_rag/__init__.py
from .kg_retriever import KGRetriever
from .fusion_engine import FusionEngine
# 如果上面 retrievers/__init__.py 也做了导出，这里也可以考虑是否进一步导出
```

File: zhz_rag/core_rag/fusion_engine.py
---------------------------------------
```python
import hashlib
import jieba
import torch
import asyncio
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from typing import List, Dict, Any, Optional
import logging
import os

# 从项目内部导入pydantic_models
from zhz_rag.config.pydantic_models import RetrievedDocument

class FusionEngine:
    def __init__(self, logger: Optional[logging.Logger] = None, use_rrf: bool = True, rrf_k: int = 60):
        """
        初始化融合引擎。

        Args:
            logger (Optional[logging.Logger]): 日志记录器实例。
            use_rrf (bool): 是否使用RRF（倒数排序融合）代替CrossEncoder进行排序。默认为 True。
            rrf_k (int): RRF算法中的平滑常数k。默认为 60。
        """
        self.reranker_model_path_from_env = os.getenv("RERANKER_MODEL_PATH")
        if not self.reranker_model_path_from_env:
            default_fallback_path = "/home/zhz/models/Qwen3-Reranker-0.6B-seq-cls"
            logging.error(f"RERANKER_MODEL_PATH not found in environment variables! Falling back to default: {default_fallback_path}")
            self.reranker_model_path_from_env = default_fallback_path

        if logger:
            self.logger = logger
        else:
            self.logger = logging.getLogger("FusionEngineLogger")
            if not self.logger.hasHandlers():
                self.logger.setLevel(logging.INFO)
                self.logger.info("FusionEngine initialized with its own basic logger (no handlers configured by default).")
            else:
                self.logger.info("FusionEngine initialized, re-using existing logger configuration for FusionEngineLogger.")

        self.reranker_tokenizer: Optional[AutoTokenizer] = None
        self.reranker_model: Optional[AutoModelForSequenceClassification] = None
        self.reranker_device = "cuda" if torch.cuda.is_available() else "cpu"
        
        self.use_rrf = use_rrf
        self.rrf_k = rrf_k
        
        self._load_reranker_model()

    def _load_reranker_model(self):
        """
        加载用于重排序的Cross-Encoder模型。
        """
        self.logger.info(f"FusionEngine: Loading reranker model from: {self.reranker_model_path_from_env} to {self.reranker_device}...")
        if not os.path.isdir(self.reranker_model_path_from_env):
            _error_msg_model_path = f"Error: Reranker model local path does not exist or is not a directory: {self.reranker_model_path_from_env}."
            self.logger.error(_error_msg_model_path)
            self.reranker_model = None
            self.reranker_tokenizer = None
            return

        try:
            self.reranker_tokenizer = AutoTokenizer.from_pretrained(self.reranker_model_path_from_env, trust_remote_code=True)
            self.reranker_model = AutoModelForSequenceClassification.from_pretrained(self.reranker_model_path_from_env, trust_remote_code=True)
            self.reranker_model.to(self.reranker_device)

            if self.reranker_device == 'cuda' and hasattr(self.reranker_model, 'half'):
                self.reranker_model.half()
                self.logger.info("FusionEngine: Reranker model loaded to GPU and using FP16.")
            else:
                self.logger.info(f"FusionEngine: Reranker model loaded to {self.reranker_device}.")
            
            self.reranker_model.eval()
            self.logger.info("FusionEngine: Reranker model loading successful!")
        except Exception as e:
            self.logger.error(f"Error: Reranker model loading failed: {e}", exc_info=True)
            self.reranker_tokenizer = None
            self.reranker_model = None

    def _rerank_documents_sync(self, query: str, documents: List[RetrievedDocument]) -> List[RetrievedDocument]:
        """
        使用Cross-Encoder模型对文档进行精细重排序（同步执行）。
        """
        if not self.reranker_model or not self.reranker_tokenizer:
            self.logger.warning("FusionEngine: Reranker model not loaded. Cannot perform fine-grained reranking. Returning documents sorted by original score.")
            return sorted(documents, key=lambda doc: doc.score if doc.score is not None else -float('inf'), reverse=True)

        if not documents:
            self.logger.info("FusionEngine: No documents to rerank.")
            return []

        pairs = []
        valid_documents_for_reranking = []
        for doc in documents:
            if isinstance(doc.content, str):
                pairs.append([query, doc.content])
                valid_documents_for_reranking.append(doc)
            else:
                self.logger.warning(f"FusionEngine: Document with non-string content skipped for reranking. ID: {doc.metadata.get('chunk_id', 'N/A')}, Type: {type(doc.content)}")
        
        if not pairs:
            self.logger.info("FusionEngine: No valid document pairs for reranking after content check.")
            return []

        self.logger.info(f"FusionEngine: Reranking {len(valid_documents_for_reranking)} documents with CrossEncoder...")
        
        try:
            with torch.no_grad():
                inputs = self.reranker_tokenizer(
                    pairs, 
                    padding=True, 
                    truncation=True, 
                    return_tensors='pt', 
                    max_length=512
                ).to(self.reranker_device)
                
                logits = self.reranker_model(**inputs).logits
                scores = logits.view(-1).float().cpu().numpy()

            for i, doc in enumerate(valid_documents_for_reranking):
                doc.score = float(scores[i])

            reranked_docs = sorted(valid_documents_for_reranking, key=lambda doc: doc.score, reverse=True)
            self.logger.info(f"FusionEngine: Reranking complete. {len(reranked_docs)} documents sorted.")
            return reranked_docs
        except Exception as e_rerank_detail:
            self.logger.error(f"FusionEngine: Detailed error during reranking with CrossEncoder: {e_rerank_detail}", exc_info=True)
            return sorted(valid_documents_for_reranking, key=lambda d: d.score if d.score is not None else -float('inf'), reverse=True)

    def _apply_rrf(self, query: str, all_retrieved_docs: List[RetrievedDocument]) -> List[RetrievedDocument]:
        """
        应用倒数排序融合（RRF）算法对文档进行排序。
        """
        if not all_retrieved_docs:
            return []

        self.logger.info(f"Applying RRF to {len(all_retrieved_docs)} documents with k={self.rrf_k}.")

        doc_scores_by_id: Dict[str, Dict[str, Any]] = {} 
        doc_objects_by_id: Dict[str, RetrievedDocument] = {}

        # 按召回源对文档进行分组
        docs_by_source: Dict[str, List[RetrievedDocument]] = {}
        for doc in all_retrieved_docs:
            source_type = doc.source_type or "unknown"
            if source_type not in docs_by_source:
                docs_by_source[source_type] = []
            docs_by_source[source_type].append(doc)

        # 为每个召回源的列表计算RRF分数贡献
        for source_type, docs_list in docs_by_source.items():
            # 假设分数越高排名越靠前
            sorted_docs = sorted(docs_list, key=lambda d: d.score if d.score is not None else -float('inf'), reverse=True)
            for rank, doc in enumerate(sorted_docs, 1): # rank 从1开始
                content_hash = hashlib.md5(doc.content.encode('utf-8')).hexdigest()
                if content_hash not in doc_objects_by_id:
                    doc_objects_by_id[content_hash] = doc 
                    doc_scores_by_id[content_hash] = {"rrf_score": 0.0}
                
                # 累加RRF分数
                doc_scores_by_id[content_hash]["rrf_score"] += 1.0 / (self.rrf_k + rank)

        # 将RRF分数更新回文档对象并排序
        final_rrf_results = []
        for content_hash, data in doc_scores_by_id.items():
            doc_obj = doc_objects_by_id[content_hash]
            doc_obj.score = data["rrf_score"]
            final_rrf_results.append(doc_obj)
        
        final_rrf_results.sort(key=lambda d: d.score if d.score is not None else 0.0, reverse=True)
        self.logger.info(f"RRF processing complete. Returning {len(final_rrf_results)} documents sorted by RRF score.")
        return final_rrf_results

    def _tokenize_text(self, text: str) -> set[str]:
        if not isinstance(text, str):
            self.logger.warning(f"FusionEngine: _tokenize_text received non-string input: {type(text)}. Returning empty set.")
            return set()
        return set(jieba.cut(text))

    def _calculate_jaccard_similarity(self, query_tokens: set[str], doc_tokens: set[str]) -> float:
        if not query_tokens or not doc_tokens:
            return 0.0
        intersection = len(query_tokens.intersection(doc_tokens))
        union = len(query_tokens.union(doc_tokens))
        return intersection / union if union > 0 else 0.0
        
    async def fuse_results(
        self,
        all_raw_retrievals: List[RetrievedDocument],
        user_query: str,
        top_n_final: int = 3
    ) -> List[RetrievedDocument]:
        self.logger.info(f"FusionEngine: Fusing {len(all_raw_retrievals)} raw retrieved documents for query: '{user_query}'. Target top_n_final: {top_n_final}")

        if not all_raw_retrievals:
            self.logger.info("FusionEngine: No documents to fuse.")
            return []

        # 1. 去重 (基于内容的哈希值)
        unique_docs_map: Dict[str, RetrievedDocument] = {}
        for doc in all_raw_retrievals:
            if not isinstance(doc.content, str) or not doc.content.strip():
                self.logger.debug(f"FusionEngine: Skipping doc with invalid content for hashing: {doc.metadata.get('chunk_id', 'N/A') if doc.metadata else 'N/A'}")
                continue
            content_hash = hashlib.md5(doc.content.encode('utf-8')).hexdigest()
            if content_hash not in unique_docs_map:
                unique_docs_map[content_hash] = doc
            else:
                if doc.score is not None and unique_docs_map[content_hash].score is not None:
                    if doc.score > unique_docs_map[content_hash].score: # type: ignore
                        unique_docs_map[content_hash] = doc
                elif doc.score is not None:
                    unique_docs_map[content_hash] = doc
                self.logger.debug(f"FusionEngine: Duplicate content hash found. Doc with score {doc.score} vs existing {unique_docs_map[content_hash].score}. Content: {doc.content[:50]}...")
        
        unique_docs = list(unique_docs_map.values())
        self.logger.info(f"FusionEngine: After deduplication (content hash): {len(unique_docs)} documents.")

        if not unique_docs:
            return []

        # 2. 初步筛选 (基于长度和Jaccard相似度)
        JACCARD_THRESHOLD = 0.02
        MIN_DOC_LENGTH_CHARS_KG = 10 
        MIN_DOC_LENGTH_CHARS_OTHER = 10
        MAX_DOC_LENGTH_CHARS = 1500

        query_tokens_set = self._tokenize_text(user_query)
        screened_results: List[RetrievedDocument] = []
        
        self.logger.info(f"FusionEngine: Starting light screening for {len(unique_docs)} unique documents. Query tokens: {query_tokens_set if query_tokens_set else 'N/A'}")
        for doc_idx, doc in enumerate(unique_docs):
            doc_content_str = str(doc.content)
            doc_length = len(doc_content_str)
            doc_id_for_log = doc.metadata.get("chunk_id", doc.metadata.get("id", f"doc_idx_{doc_idx}")) if doc.metadata else f"doc_idx_{doc_idx}"
            
            # 长度筛选
            min_len_chars = MIN_DOC_LENGTH_CHARS_KG if doc.source_type in ["knowledge_graph", "duckdb_kg"] else MIN_DOC_LENGTH_CHARS_OTHER

            if not (min_len_chars <= doc_length <= MAX_DOC_LENGTH_CHARS):
                self.logger.info(f"  Screening REJECT (Length): DocID: {doc_id_for_log}, Length: {doc_length}, Expected: [{min_len_chars}-{MAX_DOC_LENGTH_CHARS}], Type: {doc.source_type}. Content: '{doc_content_str[:100].replace(chr(10), ' ')}...'")
                continue

            # Jaccard相似度筛选
            jaccard_sim = -1.0 
            apply_jaccard_filter = True

            if doc.source_type in ["duckdb_kg", "knowledge_graph"]:
                apply_jaccard_filter = False
                self.logger.info(f"  Screening INFO (Jaccard - KG Skip): DocID: {doc_id_for_log}, Type: {doc.source_type}. Skipping Jaccard filter.")
            
            if apply_jaccard_filter and query_tokens_set:
                doc_tokens_set = self._tokenize_text(doc_content_str)
                jaccard_sim = self._calculate_jaccard_similarity(query_tokens_set, doc_tokens_set)
                
                if jaccard_sim < JACCARD_THRESHOLD:
                    self.logger.info(f"  Screening REJECT (Jaccard - Non-KG): DocID: {doc_id_for_log}, Similarity: {jaccard_sim:.4f} < {JACCARD_THRESHOLD}. Doc Tokens: {doc_tokens_set if len(doc_tokens_set) < 20 else str(list(doc_tokens_set)[:20])+'...'} Content: '{doc_content_str[:100].replace(chr(10), ' ')}...'")
                    continue
            elif apply_jaccard_filter and not query_tokens_set:
                self.logger.info(f"  Screening SKIP (Jaccard - Empty Query Tokens): DocID: {doc_id_for_log}")

            screened_results.append(doc)
            jaccard_display = f"{jaccard_sim:.4f}" if jaccard_sim != -1.0 else "N/A"
            self.logger.info(f"  Screening PASS: DocID: {doc_id_for_log}, Length: {doc_length}, Jaccard: {jaccard_display}, Type: {doc.source_type}. Content: '{doc_content_str[:100].replace(chr(10), ' ')}...'")
            
        self.logger.info(f"FusionEngine: After light screening: {len(screened_results)} documents remain.")
        
        if not screened_results:
            self.logger.info("FusionEngine: No documents remain after light screening. Returning empty list.")
            return []
        
        fused_and_ranked_results: List[RetrievedDocument]
        if self.use_rrf:
            self.logger.info("FusionEngine: Using RRF for fusion and ranking.")
            fused_and_ranked_results = await asyncio.to_thread(
                self._apply_rrf,
                query=user_query,
                all_retrieved_docs=screened_results
            )
        else:
            self.logger.info("FusionEngine: Using CrossEncoder for reranking.")
            fused_and_ranked_results = await asyncio.to_thread(
                self._rerank_documents_sync,
                query=user_query,
                documents=screened_results
            )

        self.logger.info(f"FusionEngine: After fusion/reranking: {len(fused_and_ranked_results)} documents.")
        for i_doc, doc_reranked in enumerate(fused_and_ranked_results[:top_n_final+5]):
            self.logger.debug(f"   Ranked Doc {i_doc}: type={doc_reranked.source_type}, new_score={doc_reranked.score:.4f}, content='{str(doc_reranked.content)[:100]}...'")

        # 4. 根据 top_n_final 截取最终结果
        final_output_documents = fused_and_ranked_results[:top_n_final]

        self.logger.info(f"FusionEngine: Returning final top {len(final_output_documents)} documents.")
        return final_output_documents
```

File: zhz_rag/core_rag/kg_retriever.py
--------------------------------------
```python
# 文件: zhz_rag/core_rag/kg_retriever.py
import os
import json
import duckdb
from typing import List, Dict, Any, Optional, Iterator, TYPE_CHECKING
import logging
from contextlib import contextmanager
import asyncio
if TYPE_CHECKING:
    from zhz_rag.llm.local_model_handler import LocalModelHandler

from zhz_rag.llm.llm_interface import extract_entities_for_kg_query
from zhz_rag.config.pydantic_models import ExtractedEntitiesAndRelationIntent
from zhz_rag.utils.common_utils import normalize_text_for_id

# 日志配置
kg_logger = logging.getLogger(__name__)
if not kg_logger.hasHandlers():
    kg_logger.setLevel(logging.DEBUG)
    ch = logging.StreamHandler()
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s')
    ch.setFormatter(formatter)
    kg_logger.addHandler(ch)
    kg_logger.propagate = False
kg_logger.info("KGRetriever (DuckDB) logger initialized/reconfirmed.")


class KGRetriever:
    # 使用 DuckDB 的环境变量或默认路径
    DUCKDB_KG_FILE_PATH_ENV = os.getenv(
        "DUCKDB_KG_FILE_PATH",
        os.path.join(os.getenv("ZHZ_AGENT_PROJECT_ROOT", "/home/zhz/zhz_agent"), "zhz_rag", "stored_data", "duckdb_knowledge_graph.db")
    )

    def __init__(self, db_file_path: Optional[str] = None, embedder: Optional['LocalModelHandler'] = None):
        self.db_file_path = db_file_path if db_file_path else self.DUCKDB_KG_FILE_PATH_ENV
        self._embedder = embedder
        self._retrieval_cache: Dict[str, List[Dict[str, Any]]] = {} # <--- 添加此行
        self._retrieval_cache_lock = asyncio.Lock() # <--- 添加此行
        kg_logger.info(f"KGRetriever (DuckDB) initialized. DB file path set to: {self.db_file_path}")

        if not os.path.exists(self.db_file_path):
            kg_logger.warning(f"DuckDB database file not found at {self.db_file_path}. Retrieval operations will likely fail if the DB is not created by the Dagster pipeline first.")
        
        # 健康检查
        try:
            with self._get_duckdb_connection() as conn_test:
                result = conn_test.execute("SELECT 42;").fetchone()
                if result and result[0] == 42:
                    kg_logger.info("DuckDB connection test successful and VSS setup attempted in _get_duckdb_connection.")
                else:
                    kg_logger.warning("DuckDB connection test failed to return expected result.")
        except Exception as e_init_conn_test:
            kg_logger.error(f"Error during initial DuckDB connection test: {e_init_conn_test}", exc_info=True)
    
    @contextmanager
    def _get_duckdb_connection(self) -> Iterator[duckdb.DuckDBPyConnection]:
        """
        建立并返回一个DuckDB连接的上下文管理器。
        """
        conn: Optional[duckdb.DuckDBPyConnection] = None
        kg_logger.debug(f"_get_duckdb_connection: Attempting to connect to DB at {self.db_file_path}")
        try:
            if not os.path.exists(self.db_file_path):
                raise FileNotFoundError(f"DuckDB file '{self.db_file_path}' does not exist when trying to open connection.")

            conn = duckdb.connect(database=self.db_file_path, read_only=False)
            kg_logger.debug(f"DuckDB Connection object created for path: {self.db_file_path} (read_only=False)")
            
            try:
                conn.execute("LOAD vss;")
                kg_logger.debug("DuckDB: VSS extension loaded on connection.")
            except Exception as e_vss_setup:
                kg_logger.warning(f"DuckDB: Failed to setup VSS extension on connect: {e_vss_setup}. This might be okay if already set or not needed for this operation.")
            yield conn
        except Exception as e_outer:
            kg_logger.error(f"Error in _get_duckdb_connection: {e_outer}", exc_info=True)
            raise
        finally:
            kg_logger.debug("_get_duckdb_connection: Exiting context.")
            if conn:
                kg_logger.debug("Closing DuckDB connection.")
                conn.close()

    def _execute_duckdb_sql_query_sync(self, query: str, parameters: Optional[List[Any]] = None) -> List[Dict[str, Any]]:
        """
        执行DuckDB SQL查询并返回结果列表。
        """
        kg_logger.info(f"--- Executing DuckDB SQL --- Query: {query.strip()}")
        if parameters:
            log_params = [
                str(p)[:100] + '...' if isinstance(p, list) and len(str(p)) > 100 else p
                for p in parameters
            ]
            kg_logger.info(f"Params: {log_params}")

        results_list: List[Dict[str, Any]] = []
        try:
            with self._get_duckdb_connection() as conn:
                prepared_statement = conn.execute(query, parameters)
                
                if prepared_statement.description:
                    column_names = [desc[0] for desc in prepared_statement.description]
                    raw_results = prepared_statement.fetchall()
                    
                    for row_tuple in raw_results:
                        results_list.append(dict(zip(column_names, row_tuple)))
                        
                kg_logger.info(f"DuckDB SQL executed. Records count: {len(results_list)}")
                if results_list: 
                    kg_logger.debug(f"First DuckDB record: {str(results_list[0])[:200]}")
                else:
                     kg_logger.debug("DuckDB SQL query returned 0 records.")
        except duckdb.Error as duckdb_err:
             kg_logger.error(f"DuckDB Error during SQL execution: '{query}'. Error: {duckdb_err}", exc_info=True)
        except Exception as e:
            kg_logger.error(f"Unexpected error executing DuckDB SQL query: '{query}'. Error: {e}", exc_info=True)
        return results_list

    def _format_duckdb_records_for_retrieval(
        self, 
        records: List[Dict[str, Any]], 
        query_context: str = "",
        source_type_prefix: str = "duckdb_kg"
    ) -> List[Dict[str, Any]]:
        formatted_docs = []
        if not records:
            return formatted_docs

        for record_data in records:
            content_parts = []
            entity_text = record_data.get("text") or record_data.get("target_text") or record_data.get("source_text")
            entity_label = record_data.get("label") or record_data.get("target_label") or record_data.get("source_label")
            relation_type = record_data.get("relation_type")
            
            if "source_text" in record_data and "target_text" in record_data and relation_type:
                content_parts = [
                    f"Source: {record_data['source_text']} ({record_data.get('source_label', 'Entity')})",
                    f"Relation: {relation_type}",
                    f"Target: {record_data['target_text']} ({record_data.get('target_label', 'Entity')})"
                ]
            elif entity_text:
                content_parts.append(f"Entity: {entity_text}")
                if entity_label:
                    content_parts.append(f"Label: {entity_label}")
            else:
                content_parts.append(f"Retrieved KG data: {json.dumps({k:v for k,v in record_data.items() if k != 'embedding'}, ensure_ascii=False, default=str)[:100]}")

            content_str = " | ".join(content_parts)
            
            doc_metadata = {
                "original_user_query_for_kg": query_context,
                "duckdb_retrieved_id_prop": record_data.get("id_prop") or record_data.get("source_id_prop"),
                "duckdb_retrieved_data": {k:v for k,v in record_data.items() if k != 'embedding'}
            }
            if record_data.get("_source_strategy"):
                doc_metadata["_source_strategy"] = record_data.get("_source_strategy")

            score_value = record_data.get("distance")
            if score_value is None:
                score_value = record_data.get("_score")

            if record_data.get("distance") is not None:
                similarity_score = 1.0 / (1.0 + float(score_value)) if score_value is not None else 0.5 
            elif isinstance(score_value, (int, float)):
                similarity_score = float(score_value)
            else:
                similarity_score = 0.5

            doc_data = {
                "source_type": source_type_prefix,
                "content": content_str,
                "score": similarity_score, 
                "metadata": {k: v for k, v in doc_metadata.items() if v is not None}
            }
            formatted_docs.append(doc_data)
        return formatted_docs

    async def retrieve(self, user_query: str, top_k: int = 3) -> List[Dict[str, Any]]:
        kg_logger.info(f"Starting DuckDB KG retrieval for query: '{user_query}', top_k: {top_k}")

        # --- 添加：缓存检查 ---
        cache_key = f"{user_query}_{top_k}"
        async with self._retrieval_cache_lock:
            if cache_key in self._retrieval_cache:
                kg_logger.info(f"KG CACHE HIT for key: '{cache_key[:100]}...'")
                return self._retrieval_cache[cache_key]
        kg_logger.info(f"KG CACHE MISS for key: '{cache_key[:100]}...'. Performing retrieval.")
        # --- 缓存检查结束 ---


        if not self._embedder:
            kg_logger.error("Embedder not configured for KGRetriever. Vector search will be skipped.")
        
        all_retrieved_records: List[Dict[str, Any]] = []
        processed_entity_ids = set()

        # 1. LLM提取实体和关系意图
        extracted_info: Optional[ExtractedEntitiesAndRelationIntent] = None
        try:
            extracted_info = await extract_entities_for_kg_query(user_query)
            if extracted_info:
                entities_log = [e.model_dump() for e in extracted_info.entities]
                relations_log = [r.model_dump() for r in extracted_info.relations]
                kg_logger.info(f"LLM extracted for KG: Entities: {entities_log}, Relations: {relations_log}")
            else:
                kg_logger.info("LLM did not extract specific entities/relations for KG query.")
        except Exception as e_extract:
            kg_logger.error(f"Error during entity/relation extraction for KG: {e_extract}", exc_info=True)

        # 2. 向量搜索 (可选, 作为补充)
        if self._embedder:
            try:
                kg_logger.info(f"Generating embedding for vector search text: '{user_query}'")
                query_vector_list = await self._embedder.embed_query(user_query)
                if query_vector_list:
                    vector_search_sql = "SELECT id_prop, text, label, list_distance(embedding, ?) AS distance FROM ExtractedEntity ORDER BY distance ASC LIMIT ?;"
                    kg_logger.info(f"Executing DuckDB vector search. Top K: {top_k}")
                    vector_results = self._execute_duckdb_sql_query_sync(vector_search_sql, [query_vector_list, top_k])
                    if vector_results:
                        for rec in vector_results: rec["_source_strategy"] = "vector_search"
                        all_retrieved_records.extend(vector_results)
                        processed_entity_ids.update(rec.get("id_prop") for rec in vector_results)
                        kg_logger.info(f"Retrieved {len(vector_results)} records via DuckDB vector search.")
                else:
                    kg_logger.warning("Failed to generate query embedding for vector search.")
            except Exception as e_vec_search:
                kg_logger.error(f"Error during DuckDB vector search: {e_vec_search}", exc_info=True)
        
        # 3. 基于LLM提取的实体进行精确查找
        if extracted_info and extracted_info.entities:
            for entity_info in extracted_info.entities:
                entity_text_norm = normalize_text_for_id(entity_info.text)
                entity_label_norm = entity_info.label.upper()
                exact_entity_sql = "SELECT id_prop, text, label FROM ExtractedEntity WHERE text = ? AND label = ? LIMIT 1;"
                kg_logger.info(f"Executing exact entity lookup for: text='{entity_text_norm}', label='{entity_label_norm}'")
                entity_lookup_results = self._execute_duckdb_sql_query_sync(exact_entity_sql, [entity_text_norm, entity_label_norm])
                for rec in entity_lookup_results:
                    if rec.get("id_prop") not in processed_entity_ids:
                        rec["_source_strategy"] = "exact_entity_match"
                        all_retrieved_records.append(rec)
                        processed_entity_ids.add(rec.get("id_prop"))

        # 4. 【核心修改】基于LLM提取的结构化关系进行验证和邻居查找
        if extracted_info and extracted_info.relations:
            kg_logger.info(f"Found {len(extracted_info.relations)} structured relations to process.")
            for rel_item in extracted_info.relations:
                try:
                    head_text_norm = normalize_text_for_id(rel_item.head_entity_text)
                    head_label_norm = rel_item.head_entity_label.upper()
                    tail_text_norm = normalize_text_for_id(rel_item.tail_entity_text)
                    tail_label_norm = rel_item.tail_entity_label.upper()
                    relation_type_norm = rel_item.relation_type.upper()

                    kg_logger.info(f"Processing relation: ({head_text_norm}:{head_label_norm})-[{relation_type_norm}]->({tail_text_norm}:{tail_label_norm})")

                    # 步骤 4a: 验证关系本身是否存在
                    relation_verification_sql = """
                    SELECT r.relation_type, s.id_prop AS source_id_prop, s.text AS source_text, s.label AS source_label, t.id_prop AS target_id_prop, t.text AS target_text, t.label AS target_label
                    FROM KGExtractionRelation r
                    JOIN ExtractedEntity s ON r.source_node_id_prop = s.id_prop
                    JOIN ExtractedEntity t ON r.target_node_id_prop = t.id_prop
                    WHERE s.text = ? AND s.label = ? AND t.text = ? AND t.label = ? AND r.relation_type = ? LIMIT 1;
                    """
                    relation_verification_params = [head_text_norm, head_label_norm, tail_text_norm, tail_label_norm, relation_type_norm]
                    relation_verification_results = self._execute_duckdb_sql_query_sync(relation_verification_sql, relation_verification_params)
                    
                    if relation_verification_results:
                        kg_logger.info(f"    Successfully verified relation in KG: {relation_type_norm} between '{head_text_norm}' and '{tail_text_norm}'")
                        
                        # 将参与已验证关系的核心实体（如果尚未添加）加入召回结果
                        for verified_rel_record in relation_verification_results:
                            # 添加头实体
                            head_entity_from_rel = {"id_prop": verified_rel_record.get("source_id_prop"), "text": verified_rel_record.get("source_text"), "label": verified_rel_record.get("source_label"), "_source_strategy": f"verified_relation_head_{relation_type_norm}"}
                            if head_entity_from_rel.get("id_prop") not in processed_entity_ids:
                                all_retrieved_records.append(head_entity_from_rel)
                                processed_entity_ids.add(head_entity_from_rel.get("id_prop"))
                                kg_logger.info(f"      Added head entity '{head_entity_from_rel.get('text')}' from verified relation to results.")
                            # 添加尾实体
                            tail_entity_from_rel = {"id_prop": verified_rel_record.get("target_id_prop"), "text": verified_rel_record.get("target_text"), "label": verified_rel_record.get("target_label"), "_source_strategy": f"verified_relation_tail_{relation_type_norm}"}
                            if tail_entity_from_rel.get("id_prop") not in processed_entity_ids:
                                all_retrieved_records.append(tail_entity_from_rel)
                                processed_entity_ids.add(tail_entity_from_rel.get("id_prop"))
                                kg_logger.info(f"      Added tail entity '{tail_entity_from_rel.get('text')}' from verified relation to results.")
                        
                        # --- 步骤 4b: 新增 - 查找与已验证关系相关的邻居实体 ---
                        # 1. 查找与头实体通过该关系连接的其他尾实体
                        if head_text_norm and head_label_norm and relation_type_norm:
                            find_other_tails_sql = """
                            SELECT t.id_prop, t.text, t.label, r.relation_type
                            FROM ExtractedEntity h
                            JOIN KGExtractionRelation r ON h.id_prop = r.source_node_id_prop
                            JOIN ExtractedEntity t ON r.target_node_id_prop = t.id_prop
                            WHERE h.text = ? AND h.label = ? AND r.relation_type = ? AND t.text != ?
                            LIMIT ?;
                            """
                            find_other_tails_params = [head_text_norm, head_label_norm, relation_type_norm, tail_text_norm, top_k]
                            kg_logger.info(f"      Finding other tails for ({head_text_norm})-[{relation_type_norm}]->(not {tail_text_norm})")
                            other_tails_results = self._execute_duckdb_sql_query_sync(find_other_tails_sql, find_other_tails_params)
                            for rec in other_tails_results:
                                if rec.get("id_prop") not in processed_entity_ids:
                                    rec["_source_strategy"] = f"neighbor_tail_for_{relation_type_norm}"
                                    all_retrieved_records.append(rec)
                                    processed_entity_ids.add(rec.get("id_prop"))
                                    kg_logger.info(f"        Added neighbor tail entity '{rec.get('text')}' to results.")

                        # 2. 查找与尾实体通过该关系（反向）连接的其他头实体
                        if tail_text_norm and tail_label_norm and relation_type_norm:
                            find_other_heads_sql = """
                            SELECT h.id_prop, h.text, h.label, r.relation_type
                            FROM ExtractedEntity t
                            JOIN KGExtractionRelation r ON t.id_prop = r.target_node_id_prop
                            JOIN ExtractedEntity h ON r.source_node_id_prop = h.id_prop
                            WHERE t.text = ? AND t.label = ? AND r.relation_type = ? AND h.text != ?
                            LIMIT ?;
                            """
                            find_other_heads_params = [tail_text_norm, tail_label_norm, relation_type_norm, head_text_norm, top_k]
                            kg_logger.info(f"      Finding other heads for (not {head_text_norm})-[{relation_type_norm}]->({tail_text_norm})")
                            other_heads_results = self._execute_duckdb_sql_query_sync(find_other_heads_sql, find_other_heads_params)
                            for rec in other_heads_results:
                                if rec.get("id_prop") not in processed_entity_ids:
                                    rec["_source_strategy"] = f"neighbor_head_for_{relation_type_norm}"
                                    all_retrieved_records.append(rec)
                                    processed_entity_ids.add(rec.get("id_prop"))
                                    kg_logger.info(f"        Added neighbor head entity '{rec.get('text')}' to results.")
                    else:
                        kg_logger.info(f"    Relation {relation_type_norm} between '{head_text_norm}' and '{tail_text_norm}' not found in KG via exact match.")

                except Exception as e_rel_proc:
                    kg_logger.error(f"Error processing structured relation item {rel_item.model_dump_json()}: {e_rel_proc}", exc_info=True)

        if not all_retrieved_records:
            kg_logger.info(f"No records retrieved from DuckDB KG for query: '{user_query}' after all strategies.")
            return []

        # 去重，因为多种策略可能找到相同的结果
        unique_records = []
        seen_ids = set()
        for record in all_retrieved_records:
            # 使用 id_prop 或 source_id_prop 进行去重
            record_id = record.get("id_prop") or record.get("source_id_prop")
            if record_id and record_id in seen_ids:
                continue
            unique_records.append(record)
            if record_id:
                seen_ids.add(record_id)
        
        # 格式化最终输出
        formatted_docs = self._format_duckdb_records_for_retrieval(unique_records, user_query, "duckdb_kg")
        
        # --- 添加：存储到缓存 ---
        async with self._retrieval_cache_lock:
            self._retrieval_cache[cache_key] = formatted_docs
            kg_logger.info(f"KG CACHED {len(formatted_docs)} results for key: '{cache_key[:100]}...'")
        # --- 缓存存储结束 ---

        kg_logger.info(f"KGRetriever (DuckDB) retrieve method finished. Returning {len(formatted_docs)} formatted documents for fusion.")
        return formatted_docs

    def close(self):
        kg_logger.info(f"KGRetriever (DuckDB).close() called. (No persistent DB object to close in this version as connections are per-method).")
        pass
```

File: zhz_rag/core_rag/retrievers/__init__.py
---------------------------------------------
```python
# zhz_agent/core_rag/retrievers/__init__.py
from .chromadb_retriever import ChromaDBRetriever
from .file_bm25_retriever import FileBM25Retriever
```

File: zhz_rag/core_rag/retrievers/chromadb_retriever.py
-------------------------------------------------------
```python
# 文件: zhz_rag/core_rag/retrievers/chromadb_retriever.py

import asyncio
from typing import List, Dict, Any, Optional
import chromadb
import logging
from .embedding_functions import LlamaCppEmbeddingFunction

# 配置ChromaDBRetriever的日志记录器
logger = logging.getLogger(__name__)
# 注意：在模块级别配置basicConfig可能会影响整个应用的日志行为。
# 通常建议在应用入口处统一配置。
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

class ChromaDBRetriever:
    def __init__(
        self,
        collection_name: str,
        persist_directory: str,
        embedding_function: LlamaCppEmbeddingFunction
    ):
        """
        初始化ChromaDBRetriever。

        Args:
            collection_name (str): 要查询的ChromaDB集合名称。
            persist_directory (str): ChromaDB数据持久化的目录。
            embedding_function (LlamaCppEmbeddingFunction): 用于生成嵌入的函数实例。
        """
        self.collection_name = collection_name
        self.persist_directory = persist_directory
        self._embedding_function = embedding_function
        self._client: Optional[chromadb.Client] = None
        self._collection: Optional[chromadb.Collection] = None
        self._dimension: Optional[int] = None

        # --- 新增: 初始化召回结果缓存和异步锁 ---
        self._retrieval_cache: Dict[str, List[Dict[str, Any]]] = {}
        self._retrieval_cache_lock = asyncio.Lock()

        # 初始化依然是同步的，在服务启动时执行
        self._initialize_retriever()

    def _initialize_retriever(self):
        """
        初始化ChromaDB客户端和集合。
        """
        try:
            logger.info(f"Initializing ChromaDB client from path: {self.persist_directory}")
            self._client = chromadb.PersistentClient(path=self.persist_directory)
            
            logger.info(f"Getting or creating ChromaDB collection: {self.collection_name} using provided async embedding function.")
            self._collection = self._client.get_or_create_collection(
                name=self.collection_name,
                embedding_function=self._embedding_function 
            )

            if self._collection.count() == 0:
                logger.warning(f"ChromaDB collection '{self.collection_name}' is empty!")
            else:
                logger.info(f"ChromaDB collection '{self.collection_name}' loaded. Item count: {self._collection.count()}")
        except Exception as e:
            logger.error(f"Failed to initialize ChromaDB client or collection: {e}", exc_info=True)
            raise

    async def retrieve(self, query_text: str, n_results: int = 5, include_fields: Optional[List[str]] = None) -> List[Dict[str, Any]]: 
        if self._collection is None or self._embedding_function is None:
            logger.error("Retriever is not properly initialized.")
            return []

        # --- 新增: 缓存检查 ---
        cache_key = f"{query_text}_{n_results}" # 使用查询文本和n_results作为联合键
        async with self._retrieval_cache_lock:
            if cache_key in self._retrieval_cache:
                logger.info(f"ChromaDB CACHE HIT for key: '{cache_key[:100]}...'")
                return self._retrieval_cache[cache_key]
        logger.info(f"ChromaDB CACHE MISS for key: '{cache_key[:100]}...'. Performing retrieval.")
        # --- 缓存检查结束 ---

        logger.info(f"Retrieving documents for query: '{query_text[:100]}...' with n_results={n_results}")
        
        try:
            # 1. (异步) 将查询文本向量化
            logger.debug(f"Calling _embedding_function.embed_query with query: '{query_text[:50]}...'")
            query_embedding = await self._embedding_function.embed_query(query_text)
            
            if not query_embedding: 
                logger.error(f"Failed to generate embedding for query: {query_text[:100]}")
                return []
            
            # 2. (异步) 在ChromaDB中查询
            def _blocking_query():
                include_fields_query = include_fields if include_fields is not None else ["metadatas", "documents", "distances"]
                return self._collection.query(
                    query_embeddings=[query_embedding], 
                    n_results=n_results,
                    include=include_fields_query 
                )

            results = await asyncio.to_thread(_blocking_query)
            
            # 3. 处理并格式化结果
            retrieved_docs = []
            if results and results.get("ids") and results.get("ids")[0]:
                ids_list = results["ids"][0]
                documents_list = results.get("documents", [[]])[0]
                metadatas_list = results.get("metadatas", [[]])[0] 
                distances_list = results.get("distances", [[]])[0]

                for i in range(len(ids_list)):
                    chunk_id = ids_list[i]
                    metadata = metadatas_list[i] if metadatas_list and i < len(metadatas_list) else {}
                    distance = distances_list[i] if distances_list and i < len(distances_list) else float('inf')
                    content = documents_list[i] if documents_list and i < len(documents_list) else metadata.get("chunk_text", "[Content not found]")
                    score = (1 - distance / 2.0) if distance != float('inf') and distance <= 2.0 else 0.0 

                    retrieved_docs.append({
                        "id": chunk_id,
                        "content": content,
                        "score": score,
                        "distance": distance, 
                        "metadata": metadata,
                        "source_type": "vector_chromadb"
                    })
                
                logger.info(f"Retrieved {len(retrieved_docs)} documents from ChromaDB.")
            else:
                logger.info("No documents retrieved from ChromaDB for the query.")

            # --- 新增: 存储到缓存 ---
            async with self._retrieval_cache_lock:
                self._retrieval_cache[cache_key] = retrieved_docs
                logger.info(f"ChromaDB CACHED {len(retrieved_docs)} results for key: '{cache_key[:100]}...'")
            # --- 缓存存储结束 ---
            return retrieved_docs

        except Exception as e:
            logger.error(f"Error during ChromaDB retrieval: {e}", exc_info=True)
            return []
            
    async def get_texts_by_ids(self, ids: List[str]) -> Dict[str, str]:
        """
        (异步) 根据提供的ID列表从ChromaDB集合中获取文档的文本内容。
        """
        if not self._collection:
            logger.error("ChromaDBRetriever: Collection is not initialized.")
            return {id_val: "[Error: Collection not initialized]" for id_val in ids}
        
        if not ids:
            return {}
            
        logger.info(f"ChromaDBRetriever: Getting texts for {len(ids)} IDs.")
        
        def _blocking_get():
            return self._collection.get(ids=ids, include=["documents"])

        try:
            results = await asyncio.to_thread(_blocking_get)
            retrieved_ids = results.get("ids", [])
            retrieved_docs = results.get("documents", [])
            
            texts_map = {doc_id: doc_text for doc_id, doc_text in zip(retrieved_ids, retrieved_docs)}

            for doc_id in ids:
                if doc_id not in texts_map:
                    texts_map[doc_id] = f"[Content for chunk_id {doc_id} not found in ChromaDB]"
                    logger.warning(f"ChromaDBRetriever: Content for ID '{doc_id}' not found in get() result.")

            logger.info(f"ChromaDBRetriever: Returning texts_map with {len(texts_map)} entries.")
            return texts_map

        except Exception as e:
            logger.error(f"ChromaDBRetriever: Error during get_texts_by_ids: {e}", exc_info=True)
            return {id_val: f"[Error retrieving content for ID {id_val}]" for id_val in ids}

# 简单的本地测试代码
async def main_test():
    logger.info("--- ChromaDBRetriever Async Test ---")
    
    try:
        from zhz_rag.llm.local_model_handler import LocalModelHandler
        from dotenv import load_dotenv
        import os
        load_dotenv()
        
        # 假设 LocalModelHandler 已经正确配置并可以实例化
        model_handler = LocalModelHandler(
            embedding_model_path=os.getenv("EMBEDDING_MODEL_PATH"),
            n_gpu_layers_embed=int(os.getenv("EMBEDDING_N_GPU_LAYERS", 0))
        )
        
        embed_fn = LlamaCppEmbeddingFunction(model_handler)
        
        retriever = ChromaDBRetriever(
            collection_name=os.getenv("CHROMA_COLLECTION_NAME", "rag_documents"),
            persist_directory=os.getenv("CHROMA_PERSIST_DIRECTORY"),
            embedding_function=embed_fn
        )
        
        test_query = "人工智能的应用有哪些？"
        
        print("\n--- First Retrieval ---")
        retrieved_results = await retriever.retrieve(test_query, n_results=3)
        
        if retrieved_results:
            print(f"Retrieved {len(retrieved_results)} results.")
        else:
            print("No results retrieved.")

        print("\n--- Second Retrieval (should hit cache) ---")
        retrieved_results_cached = await retriever.retrieve(test_query, n_results=3)

        if retrieved_results_cached:
            print(f"Retrieved {len(retrieved_results_cached)} results from cache.")
        else:
            print("No results retrieved from cache.")
        
        # 关闭模型处理器中的进程池
        model_handler.close_embedding_pool()
            
    except Exception as e:
        print(f"An error occurred during the test: {e}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    asyncio.run(main_test())
```

File: zhz_rag/core_rag/retrievers/embedding_functions.py
--------------------------------------------------------
```python
# 文件: zhz_rag/core_rag/retrievers/embedding_functions.py

import logging
from typing import List, Dict, TYPE_CHECKING, Optional, Sequence
import numpy as np
from chromadb import Documents, Embeddings
import asyncio # 确保 asyncio 已导入

if TYPE_CHECKING:
    from zhz_rag.llm.local_model_handler import LocalModelHandler

logger = logging.getLogger(__name__)

def l2_normalize_embeddings(embeddings: List[List[float]]) -> List[List[float]]:
    """对一批嵌入向量进行L2归一化。"""
    if not embeddings or not isinstance(embeddings, list):
        return []
    
    normalized_embeddings = []
    for emb_list in embeddings:
        if not emb_list or not isinstance(emb_list, list): 
            normalized_embeddings.append([])
            continue
        try:
            emb_array = np.array(emb_list, dtype=np.float32)
            norm = np.linalg.norm(emb_array)
            if norm == 0: 
                normalized_embeddings.append(emb_list) 
            else:
                normalized_embeddings.append((emb_array / norm).tolist())
        except Exception as e_norm:
            logger.error(f"Error during L2 normalization of an embedding: {e_norm}", exc_info=True)
            normalized_embeddings.append(emb_list) 
    return normalized_embeddings


class LlamaCppEmbeddingFunction:
    """
    一个自定义的 ChromaDB 嵌入函数，使用 LocalModelHandler (llama.cpp) 生成嵌入。
    此类的方法现在设计为异步的，以正确桥接 LocalModelHandler 的异步嵌入方法。
    ChromaDB 0.4.x 及以上版本支持异步嵌入函数。
    【新增】此类包含一个简单的内存缓存，用于避免对相同的查询文本重复进行嵌入计算。
    """
    def __init__(self, model_handler: 'LocalModelHandler'):
        if model_handler is None:
            logger.error("LlamaCppEmbeddingFunction initialized with no model_handler.")
            raise ValueError("LocalModelHandler is required.")
        if not model_handler.embedding_model_path:
            logger.error("LlamaCppEmbeddingFunction initialized with a model_handler that does not have an embedding_model_path configured.")
            raise ValueError("LocalModelHandler must have an embedding_model_path configured to be used for embeddings.")
        self.model_handler = model_handler
        self._dimension: Optional[int] = None
        
        # --- 新增：初始化查询缓存和异步锁 ---
        self._query_cache: Dict[str, List[float]] = {}
        self._cache_lock = asyncio.Lock()
        
        try:
            self._dimension = self.model_handler.get_embedding_dimension()
            if self._dimension:
                 logger.info(f"LlamaCppEmbeddingFunction initialized. Dimension from handler: {self._dimension}")
            else:
                 logger.info("LlamaCppEmbeddingFunction initialized. Dimension not immediately available, will be fetched on first embedding or via async get_dimension.")
        except Exception as e_dim_init:
            logger.warning(f"LlamaCppEmbeddingFunction: Error trying to get dimension during init: {e_dim_init}. Will fetch on first use.")

    async def __call__(self, input: Documents) -> Embeddings:
        if not isinstance(input, list):
            logger.error(f"LlamaCppEmbeddingFunction received input of type {type(input)}, expected List[str].")
            if isinstance(input, str):
                processed_texts_for_handler = [input + "<|endoftext|>" if input and not input.endswith("<|endoftext|>") else input]
            else:
                try: 
                    num_items = len(input)
                    return [[] for _ in range(num_items)]
                except TypeError:
                    return [[]] 
        elif not input: 
            return [] 
        else:
            processed_texts_for_handler: List[str] = []
            for text_item in input:
                if isinstance(text_item, str):
                    if text_item and not text_item.endswith("<|endoftext|>"):
                        processed_texts_for_handler.append(text_item + "<|endoftext|>")
                    else:
                        processed_texts_for_handler.append(text_item) 
                else:
                    logger.warning(f"LlamaCppEmbeddingFunction received non-string item in input list: {type(text_item)}. Converting to string and adding <|endoftext|>.")
                    str_item = str(text_item)
                    processed_texts_for_handler.append(str_item + "<|endoftext|>" if str_item and not str_item.endswith("<|endoftext|>") else str_item)
        
        logger.info(f"LlamaCppEmbeddingFunction: Generating embeddings for {len(processed_texts_for_handler)} processed texts (async).")
        if processed_texts_for_handler:
            logger.debug(f"LlamaCppEmbeddingFunction: First processed text for embedding: '{processed_texts_for_handler[0][:150]}...'")

        try:
            raw_embeddings_list = await self.model_handler.embed_documents(processed_texts_for_handler)
            embeddings_list = raw_embeddings_list if raw_embeddings_list else []
            
            if self._dimension is None and embeddings_list and embeddings_list[0]:
                self._dimension = len(embeddings_list[0])
                logger.info(f"LlamaCppEmbeddingFunction: Dimension updated from embedding result: {self._dimension}")
            elif embeddings_list and embeddings_list[0] and self._dimension and len(embeddings_list[0]) != self._dimension: 
                logger.warning(f"LlamaCppEmbeddingFunction: Inconsistent embedding dimension detected! Expected {self._dimension}, got {len(embeddings_list[0])}.")
            
            return embeddings_list
        except Exception as e:
            logger.error(f"LlamaCppEmbeddingFunction: Error during async embedding generation: {e}", exc_info=True)
            return [([0.0] * (self._dimension or 1024)) if self._dimension else [] for _ in input]

    async def embed_documents(self, texts: Sequence[str]) -> List[List[float]]:
        return await self.__call__(input=list(texts))

    async def embed_query(self, text: str) -> List[float]:
        if not text:
            async with self._cache_lock:
                if self._dimension is None:
                    dim_from_handler = self.model_handler.get_embedding_dimension()
                    if dim_from_handler is None:
                        dim_from_handler = await self.model_handler._get_embedding_dimension_from_worker_once()
                    self._dimension = dim_from_handler
            return [0.0] * (self._dimension or 1024) if self._dimension else []
        
        # --- 缓存检查 (Cache Check) ---
        async with self._cache_lock:
            if text in self._query_cache:
                logger.info(f"Cache HIT for query: '{text[:50]}...'")
                return self._query_cache[text]
        
        logger.info(f"Cache MISS for query: '{text[:50]}...'. Generating new embedding.")
        
        # 调用模型生成向量
        embedding_vector = await self.model_handler.embed_query(text)

        # 缓存新生成的向量
        if embedding_vector:
            async with self._cache_lock:
                if self._dimension is None:
                    self._dimension = len(embedding_vector)
                    logger.info(f"LlamaCppEmbeddingFunction: Dimension updated from query embedding result: {self._dimension}")
                elif len(embedding_vector) != self._dimension:
                    logger.warning(f"LlamaCppEmbeddingFunction: Query embedding dimension mismatch! Expected {self._dimension}, got {len(embedding_vector)}. Not caching this result.")
                    return embedding_vector # 直接返回，但不缓存

                # 存储到缓存
                self._query_cache[text] = embedding_vector
                logger.info(f"Cached new embedding for query: '{text[:50]}...'")
            return embedding_vector
        else: # embed_query 返回了 None 或空列表
            logger.warning(f"Failed to generate embedding for query: '{text[:50]}...'. Returning empty list or zero vector.")
            async with self._cache_lock:
                if self._dimension is None:
                    dim_from_handler = await self.model_handler._get_embedding_dimension_from_worker_once()
                    self._dimension = dim_from_handler
            return [0.0] * (self._dimension or 1024) if self._dimension else []
    
    async def get_dimension(self) -> Optional[int]:
        if self._dimension is None:
            async with self._cache_lock:
                # 再次检查，防止在等待锁的过程中其他协程已经设置了维度
                if self._dimension is None:
                    dim_from_handler_sync = self.model_handler.get_embedding_dimension()
                    if dim_from_handler_sync is not None:
                        self._dimension = dim_from_handler_sync
                    else:
                        logger.info("LlamaCppEmbeddingFunction: Dimension not cached, attempting async fetch from worker.")
                        dim_from_worker = await self.model_handler._get_embedding_dimension_from_worker_once()
                        if dim_from_worker is not None:
                            self._dimension = dim_from_worker
                        else:
                            logger.error("LlamaCppEmbeddingFunction: Failed to get dimension asynchronously.")
                            return None
                    if self._dimension:
                        logger.info(f"LlamaCppEmbeddingFunction: Dimension fetched/updated: {self._dimension}")
        return self._dimension
```

File: zhz_rag/core_rag/retrievers/file_bm25_retriever.py
--------------------------------------------------------
```python
# 文件: zhz_rag/core_rag/retrievers/file_bm25_retriever.py

from typing import List, Dict, Any, Optional
import jieba
import bm25s
import pickle
import os
import logging
import numpy as np
import traceback
import asyncio


# 配置日志记录器
logger = logging.getLogger(__name__)

class FileBM25Retriever:
    def __init__(
        self,
        index_directory: str,
    ):
        """
        初始化 FileBM25Retriever。

        Args:
            index_directory (str): 存储 BM25 索引文件的目录路径。
        """
        self.index_directory_path = index_directory

        self._bm25_model: Optional[bm25s.BM25] = None
        self._doc_ids: Optional[List[str]] = None
        # 用于缓存检索结果的字典
        self._retrieval_cache: Dict[str, List[Dict[str, Any]]] = {}

        self._initialize_retriever()

    def _initialize_retriever(self):
        """
        加载 BM25 模型和文档ID。
        """
        if not os.path.isdir(self.index_directory_path):
            logger.error(f"BM25 index directory not found at: {self.index_directory_path}")
            raise FileNotFoundError(f"BM25 index directory not found: {self.index_directory_path}")

        try:
            logger.info(f"Loading BM25 model from directory: {self.index_directory_path}")
            self._bm25_model = bm25s.BM25.load(
                self.index_directory_path,
                load_corpus=False,
            )
            
            if self._bm25_model is None:
                logger.error("Failed to load BM25 model (returned None).")
                raise ValueError("Failed to load BM25 model.")
            logger.info("BM25 model loaded successfully.")

            doc_ids_path = os.path.join(self.index_directory_path, "doc_ids.pkl")
            if not os.path.exists(doc_ids_path):
                logger.error(f"doc_ids.pkl not found in {self.index_directory_path}")
                raise FileNotFoundError(f"doc_ids.pkl not found in {self.index_directory_path}")
            
            with open(doc_ids_path, 'rb') as f_in:
                self._doc_ids = pickle.load(f_in)
            
            if self._doc_ids is None:
                logger.warning(f"doc_ids.pkl loaded, but it was empty or invalid.")
                self._doc_ids = []
            logger.info(f"Document IDs loaded successfully. Number of indexed documents: {len(self._doc_ids)}")

        except Exception as e:
            logger.error(f"Failed to load BM25 index or document IDs: {e}", exc_info=True)
            raise

    def retrieve(self, query_text: str, n_results: int = 5) -> List[Dict[str, Any]]:
        """
        根据查询文本使用BM25检索相关的文档块ID和分数。

        Args:
            query_text (str): 用户查询的文本。
            n_results (int): 希望返回的最大结果数量。

        Returns:
            List[Dict[str, Any]]: 检索到的文档块列表。每个字典包含：
                                'id' (chunk_id),
                                'score' (BM25分数),
                                'source_type' (固定为 "keyword_bm25")
        """
        if self._bm25_model is None or self._doc_ids is None:
            logger.error("BM25 Retriever is not properly initialized.")
            return []

        # --- 添加：缓存检查 ---
        cache_key = f"{query_text}_{n_results}"
        if cache_key in self._retrieval_cache:
            logger.info(f"BM25 CACHE HIT for key: '{cache_key[:100]}...'")
            return self._retrieval_cache[cache_key]
        logger.info(f"BM25 CACHE MISS for key: '{cache_key[:100]}...'. Performing retrieval.")
        # --- 缓存检查结束 ---

        if not self._doc_ids:
            logger.info("BM25 index is empty, no results to retrieve.")
            return []

        logger.info(f"Retrieving documents with BM25 for query: '{query_text[:100]}...' with n_results={n_results}")

        try:
            query_tokenized = list(jieba.cut_for_search(query_text))
            logger.debug(f"Tokenized query (jieba for BM25): {query_tokenized}")

            # 调用 bm25s 模型获取分数
            all_scores = self._bm25_model.get_scores(query_tokenized)
            
            actual_n_results = min(n_results, len(self._doc_ids))
            if actual_n_results <= 0:
                return []
            
            # 获取分数最高的n个结果的索引
            top_n_indices = np.argsort(all_scores)[-actual_n_results:][::-1]

            retrieved_docs = []
            for index in top_n_indices:
                if 0 <= index < len(self._doc_ids):
                    doc_id = self._doc_ids[index]
                    score = float(all_scores[index])
                    retrieved_docs.append({
                        "id": doc_id,
                        "score": score,
                        "source_type": "keyword_bm25"
                    })
                else:
                    logger.warning(f"BM25 retrieval: Index {index} out of bounds for doc_ids list (len: {len(self._doc_ids)}). Skipping.")

            # --- 添加：存储到缓存 ---
            self._retrieval_cache[cache_key] = retrieved_docs
            logger.info(f"BM25 CACHED {len(retrieved_docs)} results for key: '{cache_key[:100]}...'")
            # --- 缓存存储结束 ---
            
            logger.info(f"Retrieved {len(retrieved_docs)} documents using BM25.")
            return retrieved_docs

        except Exception as e:
            logger.error(f"Error during BM25 retrieval: {e}", exc_info=True)
            return []

def main_sync_test_runner():
    """
    用于独立测试 FileBM25Retriever 的主函数。
    """
    logger.info("--- FileBM25Retriever Sync Test Runner ---")
    try:
        # 尝试从环境变量或默认路径获取索引目录
        bm25_index_dir = os.getenv("BM25_INDEX_DIRECTORY", "/home/zhz/zhz_agent/zhz_rag/stored_data/bm25_index")
        
        if not os.path.isdir(bm25_index_dir):
            # 如果默认路径不存在，尝试备用路径
            bm25_index_dir_dagster = "/home/zhz/dagster_home/bm25_index_data/"
            if os.path.isdir(bm25_index_dir_dagster):
                bm25_index_dir = bm25_index_dir_dagster
            else:
                logger.error(f"BM25 index directory for test not found at {bm25_index_dir} or {bm25_index_dir_dagster}")
                return
        
        logger.info(f"Using BM25 index directory for test: {bm25_index_dir}")
        retriever = FileBM25Retriever(index_directory=bm25_index_dir)
        
        test_query = "人工智能的应用有哪些？"
        
        # 第一次查询，应该未命中缓存
        print("\n--- First retrieval (should be CACHE MISS) ---")
        retrieved_results = retriever.retrieve(test_query, n_results=3)
        if retrieved_results:
            print(f"--- BM25 Retrieved Results for query: '{test_query}' ---")
            for i, doc in enumerate(retrieved_results):
                print(f"Result {i+1}: ID: {doc['id']}, Score: {doc['score']:.4f}, Source: {doc.get('source_type')}")
        else:
            print(f"\nNo results for query: '{test_query}'")
        
        # 第二次查询，应该命中缓存
        print("\n--- Second retrieval (should be CACHE HIT) ---")
        retrieved_results_cached = retriever.retrieve(test_query, n_results=3)
        if retrieved_results_cached:
            print(f"--- Cached BM25 Retrieved Results for query: '{test_query}' ---")
            for i, doc in enumerate(retrieved_results_cached):
                print(f"Result {i+1}: ID: {doc['id']}, Score: {doc['score']:.4f}, Source: {doc.get('source_type')}")
        else:
            print(f"\nNo results for cached query: '{test_query}'")


    except Exception as e:
        print(f"An error occurred during the BM25 test: {e}")
        traceback.print_exc()

if __name__ == '__main__':
    # 配置顶层日志记录器，以便在独立运行时能看到日志输出
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    main_sync_test_runner()
```

File: zhz_rag/llm/__init__.py
-----------------------------
```python
# /home/zhz/zhz_agent/zhz_rag/llm/__init__.py

from .llm_interface import call_llm_via_openai_api_local_only # 导入新函数

call_sglang_llm = call_llm_via_openai_api_local_only # 别名指向新函数

from .llm_interface import (
    generate_answer_from_context,
    generate_cypher_query,
    generate_expanded_queries,
    generate_intent_classification, # 这个现在直接用 litellm 调用 Gemini
    generate_clarification_question,
    generate_clarification_options,
    NO_ANSWER_PHRASE_ANSWER_CLEAN,
    NO_ANSWER_PHRASE_KG_CLEAN
)
```

File: zhz_rag/llm/custom_crewai_llms.py
---------------------------------------
```python
#/home/zhz/zhz_agent/custom_llm.py
import os
import json
import httpx
import asyncio
import traceback
from typing import List, Dict, Any, Optional, Union, Sequence, Type 
# --- CrewAI & LiteLLM Imports ---
from crewai.tools import BaseTool
from crewai.llms.base_llm import BaseLLM as CrewAIBaseLLM
import litellm

# --- [修改] Local Imports -> 改为绝对导入 ---
from zhz_rag.llm.llm_interface import call_sglang_llm # For SGLang LLM
from dotenv import load_dotenv

load_dotenv()

# --- SGLang Config ---
SGLANG_API_URL_FOR_LLM = os.getenv("SGLANG_API_URL", "http://localhost:30000/generate")

# --- CustomGeminiLLM (from ceshi/run_agent.py with fixes) ---
class CustomGeminiLLM(CrewAIBaseLLM):
    model_name: str
    api_key: str
    max_tokens: Optional[int] = 2048
    tool_config: Optional[Dict[str, Any]] = None
    stop: Optional[List[str]] = None
    _gemini_tools_cache: Optional[List[Dict[str, Any]]] = None

    def __init__(self, model: str, api_key: str, temperature: float = 0.1, max_tokens: Optional[int] = 2048, tool_config: Optional[Dict[str, Any]] = None, stop: Optional[List[str]] = None, agent_tools: Optional[List[BaseTool]] = None, **kwargs):
        super().__init__(model=model, temperature=temperature)
        self.model_name = model
        self.api_key = api_key
        self.max_tokens = max_tokens
        self.tool_config = tool_config or {"function_calling_config": {"mode": "AUTO"}}
        self.stop = stop
        if agent_tools:
            self._gemini_tools_cache = self._convert_crewai_tools_to_gemini_format(agent_tools)
            print(f"CustomGeminiLLM __init__: Cached {len(self._gemini_tools_cache)} tools.")
        else:
            print("CustomGeminiLLM __init__: No agent_tools provided for caching.")

    def _remove_unwanted_fields(self, schema: Dict[str, Any]) -> Dict[str, Any]:
        if not isinstance(schema, dict):
            return schema

        schema.pop('title', None)

        if "properties" in schema:
            if "type" not in schema:
                schema["type"] = "object"
            for prop_name, prop_def in list(schema["properties"].items()):
                if isinstance(prop_def, dict):
                    prop_def.pop('default', None)
                    prop_def.pop('title', None)
                    self._remove_unwanted_fields(prop_def)
        elif schema.get("type") == "object" and "properties" not in schema:
            schema["properties"] = {}

        keys_to_delete = [k for k, v in schema.items() if k == 'default']
        for k in keys_to_delete:
            del schema[k]

        for k, v in schema.items():
            if isinstance(v, dict):
                self._remove_unwanted_fields(v)
            elif isinstance(v, list):
                for i, item in enumerate(v):
                    if isinstance(item, dict):
                        v[i] = self._remove_unwanted_fields(item)
        return schema

    def _convert_crewai_tools_to_gemini_format(self, tools: Optional[List[BaseTool]]) -> Optional[List[Dict[str, Any]]]:
        if not tools:
            return None
        gemini_tool_declarations = []
        for tool_instance in tools:
            tool_name = tool_instance.name
            tool_description = tool_instance.description
            if not hasattr(tool_instance, 'args_schema') or not tool_instance.args_schema:
                parameters_schema = {"type": "object", "properties": {}}
            else:
                try:
                    if hasattr(tool_instance.args_schema, 'model_json_schema'):
                        pydantic_schema = tool_instance.args_schema.model_json_schema()
                    else:
                        pydantic_schema = tool_instance.args_schema.schema()
                    cleaned_schema = self._remove_unwanted_fields(pydantic_schema.copy())
                    parameters_schema = cleaned_schema
                except Exception as e:
                    print(f"Error processing schema for tool {tool_name}: {e}")
                    parameters_schema = {"type": "object", "properties": {}}
            gemini_tool_declarations.append({
                "name": tool_name,
                "description": tool_description,
                "parameters": parameters_schema
            })
        final_tools_for_litellm = []
        for declaration in gemini_tool_declarations:
            final_tools_for_litellm.append({
                "type": "function",
                "function": declaration
            })
        return final_tools_for_litellm

    def call(self, messages: Union[str, List[Dict[str, str]]], tools: Optional[List[dict]] = None, callbacks: Optional[List[Any]] = None, **kwargs: Any) -> Union[str, Any]:
        print(f"CustomGeminiLLM CALL method invoked.")
        print(f"  CALL - Tools received by CustomLLM.call: {'Yes' if tools else 'No'}")
        print(f"  CALL - Callbacks received by CustomLLM.call: {'Yes' if callbacks else 'No'}")

        if isinstance(messages, str):
            processed_messages = [{"role": "user", "content": messages}]
        else:
            processed_messages = messages

        litellm_params = {
            "model": self.model_name,
            "messages": processed_messages,
            "api_key": self.api_key,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "stop": self.stop
        }

        # --- Proxy Addition ---
        proxy_url = os.getenv("LITELLM_PROXY_URL")
        if proxy_url:
            litellm_params["proxy"] = {
                "http": proxy_url,
                "https": proxy_url,
            }
            print(f"CustomGeminiLLM.call - Using proxy: {proxy_url}")
        else:
            print("CustomGeminiLLM.call - No proxy configured (LITELLM_PROXY_URL not set).")

        # --- Tool Handling (tools: null fix) ---
        final_tools_for_litellm = None
        received_tools_to_process = tools
        if not received_tools_to_process and self._gemini_tools_cache:
            print("  CALL - INFO: Tools argument was None, using cached tools.")
            received_tools_to_process = self._gemini_tools_cache

        if received_tools_to_process:
            cleaned_tools_for_litellm = []
            for tool_dict in received_tools_to_process:
                current_tool_def = tool_dict.copy()
                if current_tool_def.get("type") == "function" and "function" in current_tool_def:
                    func_def = current_tool_def["function"].copy()
                    if "parameters" in func_def:
                        func_def["parameters"] = self._remove_unwanted_fields(func_def["parameters"].copy())
                    current_tool_def["function"] = func_def
                    cleaned_tools_for_litellm.append(current_tool_def)
                else:
                    cleaned_tools_for_litellm.append(tool_dict)
            final_tools_for_litellm = cleaned_tools_for_litellm

        if final_tools_for_litellm:
            litellm_params["tools"] = final_tools_for_litellm
            fc_config = self.tool_config.get("function_calling_config", {})
            mode = fc_config.get("mode", "AUTO").upper()
            allowed_names = fc_config.get("allowed_function_names")

            if mode == "ANY" and allowed_names:
                litellm_params["tool_choice"] = {
                    "type": "function",
                    "function": {"name": allowed_names[0]}
                }
            elif mode in ["AUTO", "ANY", "NONE"]:
                litellm_params["tool_choice"] = mode.lower()
            else:
                litellm_params["tool_choice"] = "auto"
            print(f"CustomGeminiLLM DEBUG: Setting tool_choice to: {litellm_params['tool_choice']}")

        if callbacks:
            litellm_params["callbacks"] = callbacks

        try:
            print(f"CustomGeminiLLM.call - LiteLLM PARAMS (Preview): model={litellm_params['model']}, msgs_count={len(litellm_params['messages'])}, tools={'Yes' if 'tools' in litellm_params else 'No'}, tool_choice={litellm_params.get('tool_choice')}, proxy={'Yes' if 'proxy' in litellm_params else 'No'}")
            response = litellm.completion(**litellm_params)
        except Exception as e:
            print(f"CRITICAL ERROR: LiteLLM completion call failed: {e}")
            if callbacks:
                for handler in callbacks:
                    if hasattr(handler, 'on_llm_error'):
                        try:
                            handler.on_llm_error(error=e, llm=self, **kwargs)
                        except Exception as cb_err:
                            print(f"Error in callback on_llm_error: {cb_err}")
            raise

        llm_message_response = response.choices[0].message
        if hasattr(llm_message_response, 'tool_calls') and llm_message_response.tool_calls:
            print(f"CustomGeminiLLM.call - Detected tool_calls: {llm_message_response.tool_calls}")
            # --- ReAct Format Workaround (AttributeError fix) ---
            tool_call = llm_message_response.tool_calls[0]
            action = tool_call.function.name
            action_input = tool_call.function.arguments
            react_string = f"Action: {action}\nAction Input: {action_input}"
            print(f"CustomGeminiLLM.call - Returning ReAct string: {react_string}")
            return react_string
        else:
            print(f"CustomGeminiLLM.call - Returning text content.")
            return llm_message_response.content or ""

    def get_token_counter_instance(self):
        class GeminiTokenCounter:
            def __init__(self, model_name):
                self.model_name = model_name

            def count_tokens(self, text: Union[str, List[Dict[str,str]]]) -> int:
                try:
                    if isinstance(text, list):
                        return litellm.token_counter(model=self.model_name, messages=text)
                    return litellm.token_counter(model=self.model_name, text=str(text))
                except Exception as e:
                    print(f"Warning: Token counting failed ({e}), falling back to rough estimate.")
                    if isinstance(text, list):
                        return sum(len(str(m.get("content","")).split()) for m in text)
                    return len(str(text).split())
        return GeminiTokenCounter(model_name=self.model_name)


# --- CustomSGLangLLM (from hybrid_rag/custom_llm.py) ---
class CustomSGLangLLM(CrewAIBaseLLM):
    endpoint_url: str = SGLANG_API_URL_FOR_LLM
    model_name: str = "qwen2-3b-instruct"
    temperature: float = 0.1
    max_new_tokens_val: int = 1024

    def __init__(self, endpoint: Optional[str] = None, model: Optional[str] = None, temperature: Optional[float] = None, max_new_tokens: Optional[int] = None, **kwargs: Any):
        super().__init__(**kwargs)
        if endpoint: self.endpoint_url = endpoint
        if model: self.model_name = model
        if temperature is not None: self.temperature = temperature
        if max_new_tokens is not None: self.max_new_tokens_val = max_new_tokens
        print(f"CustomSGLangLLM initialized. Endpoint: {self.endpoint_url}, Model: {self.model_name}, Temp: {self.temperature}, MaxTokens: {self.max_new_tokens_val}")

    def _prepare_sglang_prompt(self, messages: Sequence[Dict[str, str]]) -> str:
        prompt_str = ""
        for message in messages:
            role = message.get("role")
            content = message.get("content")
            if role and content:
                prompt_str += f"<|im_start|>{role}\n{content}<|im_end|>\n"
        prompt_str += "<|im_start|>assistant\n"
        return prompt_str

    def call(self, messages: Sequence[Dict[str, str]], **kwargs: Any) -> str:
        print(f"CustomSGLangLLM.call received messages: {messages}")
        sglang_prompt = self._prepare_sglang_prompt(messages)
        print(f"CustomSGLangLLM.call prepared sglang_prompt (first 200 chars): {sglang_prompt[:200]}...")
        stop_sequences_for_sglang = kwargs.get("stop", ["<|im_end|>", "<|endoftext|>"])

        try:
            try:
                loop = asyncio.get_running_loop()
            except RuntimeError:
                loop = None

            async def async_runner():
                return await call_sglang_llm(
                    prompt=sglang_prompt,
                    temperature=self.temperature,
                    max_new_tokens=self.max_new_tokens_val,
                    stop_sequences=stop_sequences_for_sglang
                )

            if loop and loop.is_running():
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                    future = executor.submit(asyncio.run, async_runner())
                    response_text = future.result(timeout=120)
            else:
                response_text = asyncio.run(async_runner())

        except Exception as e:
            print(f"CustomSGLangLLM.call: Error during SGLang call: {type(e).__name__} - {e}")
            traceback.print_exc()
            return f"LLM_CALL_ERROR: 调用SGLang服务失败 - {str(e)}"

        if response_text is None:
            print("CustomSGLangLLM.call: SGLang returned None.")
            return "LLM_CALL_ERROR: SGLang服务未返回任何文本。"

        print(f"CustomSGLangLLM.call: SGLang returned text (first 200 chars): {response_text[:200]}...")
        return response_text

    def get_token_ids(self, text: str) -> List[int]:
        print("CustomSGLangLLM.get_token_ids: Not implemented, returning empty list.")
        return []

    @property
    def support_function_calling(self) -> bool:
        return False

    @property
    def support_stop_words(self) -> bool:
        return True

    @property
    def available_models(self) -> List[str]:
        return [self.model_name]

    @property
    def context_window(self) -> int:
        return 32768

    @property
    def identifying_params(self) -> Dict[str, Any]:
        return {
            "model": self.model_name,
            "endpoint_url": self.endpoint_url,
            "temperature": self.temperature,
            "max_new_tokens": self.max_new_tokens_val,
        }
```

File: zhz_rag/llm/embedding_process_worker.py
---------------------------------------------
```python
# 文件: zhz_rag/llm/embedding_process_worker.py

import os
import logging
from typing import List, Dict, Any, Optional
from llama_cpp import Llama
import numpy as np

# --- 全局变量，用于在子进程中缓存模型实例 ---
# 注意: 每个进程池中的工作进程会有自己的这个变量副本
_process_local_model_cache: Dict[str, Llama] = {}
_process_local_model_dimension_cache: Dict[str, int] = {}

# --- 日志配置 (与 LocalModelHandler 类似，但确保独立) ---
worker_logger = logging.getLogger("EmbeddingProcessWorker")
# 避免重复添加处理器，如果此模块被多次导入或以某种方式重新加载
if not worker_logger.hasHandlers():
    worker_logger.setLevel(logging.INFO) # 或者 DEBUG
    # 注意：在多进程环境中，日志输出到控制台可能交错。
    # 对于生产环境，可能需要更复杂的日志策略（如QueueHandler）。
    # 但对于调试，StreamHandler 也可以。
    stream_handler = logging.StreamHandler()
    formatter = logging.Formatter('%(asctime)s - %(name)s - PID:%(process)d - %(levelname)s - %(message)s')
    stream_handler.setFormatter(formatter)
    worker_logger.addHandler(stream_handler)
    worker_logger.propagate = False


def l2_normalize_embeddings_worker(embeddings: List[List[float]]) -> List[List[float]]:
    if not embeddings or not isinstance(embeddings, list):
        return []
    normalized_embeddings = []
    for emb_list in embeddings:
        if not emb_list or not isinstance(emb_list, list) or not all(isinstance(x, (float, int)) for x in emb_list):
            worker_logger.warning(f"L2_NORM_WORKER: Skipping invalid or empty inner list: {emb_list}")
            normalized_embeddings.append([])
            continue
        try:
            emb_array = np.array(emb_list, dtype=np.float32)
            norm = np.linalg.norm(emb_array)
            if norm == 0:
                normalized_embeddings.append(emb_list)
            else:
                normalized_embeddings.append((emb_array / norm).tolist())
        except Exception as e_norm:
            worker_logger.error(f"Error during L2 normalization in EmbeddingProcessWorker: {e_norm}", exc_info=True)
            normalized_embeddings.append(emb_list)
    return normalized_embeddings


def _get_embedding_model_instance_in_worker(
    model_path: str,
    n_ctx: int,
    n_gpu_layers: int,
    pooling_type: int
) -> Optional[Llama]:
    """
    在当前工作进程中获取或创建并缓存 Llama 嵌入模型实例。
    使用 model_path 作为缓存键。
    """
    global _process_local_model_cache
    global _process_local_model_dimension_cache

    if model_path in _process_local_model_cache:
        worker_logger.info(f"WORKER: Reusing cached embedding model for path: {model_path}")
        return _process_local_model_cache[model_path]

    worker_logger.info(f"WORKER: Attempting to load embedding model in process: {model_path}")
    try:
        model = Llama(
            model_path=model_path,
            n_ctx=n_ctx,
            n_gpu_layers=n_gpu_layers,
            embedding=True,
            pooling_type=pooling_type,
            verbose=False # 在工作进程中减少冗余日志
        )
        dimension = model.n_embd()
        if not dimension or dimension <= 0:
            worker_logger.error(f"WORKER: Loaded model from {model_path} but got invalid dimension {dimension}.")
            return None
        
        _process_local_model_cache[model_path] = model
        _process_local_model_dimension_cache[model_path] = dimension
        worker_logger.info(f"WORKER: Embedding model loaded and cached for {model_path}. Dimension: {dimension}, Pooling: {pooling_type}")
        return model
    except Exception as e:
        worker_logger.error(f"WORKER: Failed to load embedding model in process for path {model_path}: {e}", exc_info=True)
        return None


def embed_texts_in_subprocess(
    texts: List[str],
    embedding_model_path: str,
    n_ctx_embed: int,
    n_gpu_layers_embed: int,
    pooling_type_embed: int
) -> List[List[float]]:
    """
    在子进程中执行批量文本嵌入。
    """
    worker_logger.info(f"WORKER: embed_texts_in_subprocess received {len(texts)} texts.")
    model = _get_embedding_model_instance_in_worker(
        embedding_model_path, n_ctx_embed, n_gpu_layers_embed, pooling_type_embed
    )
    if not model:
        worker_logger.error("WORKER: Failed to get model instance in subprocess. Returning empty embeddings.")
        return [[] for _ in texts]
    
    dimension = _process_local_model_dimension_cache.get(embedding_model_path)
    if not dimension: # 应该不会发生，因为 _get_embedding_model_instance_in_worker 会设置它
        worker_logger.error("WORKER: Model dimension not found in cache after model load. Critical error.")
        return [[] for _ in texts]

    default_zero_vector = [0.0] * dimension
    
    # 与 LocalModelHandler._blocking_embed_documents_internal 类似的处理逻辑
    valid_texts_with_indices: List[tuple[int, str]] = []
    for i, text in enumerate(texts):
        if text and text.strip():
            valid_texts_with_indices.append((i, text))
        else:
            worker_logger.warning(f"WORKER: Input text at original index {i} is empty or invalid. Will use zero vector. Text: '{text}'")

    if not valid_texts_with_indices:
        return [list(default_zero_vector) for _ in texts]

    valid_texts_to_embed = [text for _, text in valid_texts_with_indices]
    raw_embeddings_for_valid_texts: List[List[float]] = []

    try:
        response = model.create_embedding(input=valid_texts_to_embed)
        # ... (此处省略与 LocalModelHandler._blocking_embed_documents_internal 中类似的详细的响应解析和错误处理逻辑)
        # 为了简洁，我们先做一个简化版的解析，假设一切顺利
        # 在实际应用中，需要复制 LocalModelHandler 中对 response 的完整健壮性检查

        if response and "data" in response and isinstance(response["data"], list):
            embeddings_data = response["data"]
            if len(embeddings_data) == len(valid_texts_to_embed):
                for item_idx, item in enumerate(embeddings_data):
                    if isinstance(item, dict) and "embedding" in item and \
                       isinstance(item["embedding"], list) and len(item["embedding"]) == dimension:
                        raw_embeddings_for_valid_texts.append([float(x) for x in item["embedding"]])
                    else:
                        worker_logger.warning(f"WORKER: Valid text at valid_idx {item_idx} got invalid embedding. Using zero vector.")
                        raw_embeddings_for_valid_texts.append(list(default_zero_vector))
            else:
                worker_logger.error(f"WORKER: Mismatch in num embeddings received. Using zero vectors.")
                raw_embeddings_for_valid_texts = [list(default_zero_vector) for _ in valid_texts_to_embed]
        else:
            worker_logger.error(f"WORKER: Invalid response from create_embedding. Using zero vectors.")
            raw_embeddings_for_valid_texts = [list(default_zero_vector) for _ in valid_texts_to_embed]
            
    except Exception as e_batch_embed:
        worker_logger.error(f"WORKER: Error during batch embedding: {e_batch_embed}", exc_info=True)
        raw_embeddings_for_valid_texts = [list(default_zero_vector) for _ in valid_texts_to_embed]

    final_embeddings_ordered: List[List[float]] = [list(default_zero_vector) for _ in texts]
    valid_embedding_idx = 0
    for original_idx, _ in valid_texts_with_indices:
        if valid_embedding_idx < len(raw_embeddings_for_valid_texts):
            final_embeddings_ordered[original_idx] = raw_embeddings_for_valid_texts[valid_embedding_idx]
            valid_embedding_idx += 1
        else:
            final_embeddings_ordered[original_idx] = list(default_zero_vector)
            
    normalized_embeddings = l2_normalize_embeddings_worker(final_embeddings_ordered)
    worker_logger.info(f"WORKER: Successfully processed and normalized {len(normalized_embeddings)} document embeddings in subprocess.")
    return normalized_embeddings


def embed_query_in_subprocess(
    text: str,
    embedding_model_path: str,
    n_ctx_embed: int,
    n_gpu_layers_embed: int,
    pooling_type_embed: int
) -> List[float]:
    """
    在子进程中执行单个查询文本嵌入。
    """
    worker_logger.info(f"WORKER: embed_query_in_subprocess received query (first 100): '{text[:100]}'")
    model = _get_embedding_model_instance_in_worker(
        embedding_model_path, n_ctx_embed, n_gpu_layers_embed, pooling_type_embed
    )
    if not model:
        worker_logger.error("WORKER: Failed to get model instance for query. Returning empty embedding.")
        return []
        
    dimension = _process_local_model_dimension_cache.get(embedding_model_path)
    if not dimension:
        worker_logger.error("WORKER: Model dimension not found in cache for query. Critical error.")
        return []
    
    default_zero_vector = [0.0] * dimension

    if not text or not text.strip():
        worker_logger.warning("WORKER: Received empty or invalid text for query embedding. Returning zero vector.")
        return list(default_zero_vector)

    try:
        # 使用 create_embedding 来保持与批量接口的一致性，即使是单个查询
        # 因为我们观察到 Llama.embed() 可能不稳定
        response = model.create_embedding(input=[text])
        if response and "data" in response and isinstance(response["data"], list) and len(response["data"]) == 1:
            item = response["data"][0]
            if isinstance(item, dict) and "embedding" in item and \
               isinstance(item["embedding"], list) and len(item["embedding"]) == dimension:
                embedding_vector = [float(x) for x in item["embedding"]]
                normalized_list_of_list = l2_normalize_embeddings_worker([embedding_vector])
                final_embedding = normalized_list_of_list[0] if normalized_list_of_list and normalized_list_of_list[0] else list(default_zero_vector)
                worker_logger.info(f"WORKER: Successfully processed query embedding in subprocess. Dimension: {len(final_embedding)}")
                return final_embedding
            else:
                worker_logger.warning(f"WORKER: Query embedding response invalid format/dim. Using zero vector.")
                return list(default_zero_vector)
        else:
            worker_logger.error(f"WORKER: Invalid or empty response from create_embedding for query. Using zero vector.")
            return list(default_zero_vector)
    except Exception as e_query_embed:
        worker_logger.error(f"WORKER: Error during query embedding: {e_query_embed}", exc_info=True)
        return list(default_zero_vector)
```

File: zhz_rag/llm/llm_interface.py
----------------------------------
```python
# zhz_agent/llm.py (renamed to llm_interface.py as per typical module naming)
# or more accurately, this is the content for llm_interface.py based on the inputs

from cachetools import TTLCache
import os
import httpx  # 用于异步HTTP请求
import json  # 用于处理JSON数据
import asyncio  # 用于 asyncio.to_thread
from typing import List, Dict, Any, Optional, Union # Added Union
from dotenv import load_dotenv
import traceback  # Ensure traceback is imported

from zhz_rag.utils.common_utils import log_interaction_data # 导入通用日志函数
from zhz_rag.config.constants import NEW_KG_SCHEMA_DESCRIPTION # <--- 确保导入这个常量

from zhz_rag.config.pydantic_models import ExtractedEntitiesAndRelationIntent
# 提示词导入
from llama_cpp import Llama, LlamaGrammar 
from zhz_rag.llm.rag_prompts import (
    get_answer_generation_messages, 
    get_clarification_question_messages,
    get_entity_relation_extraction_messages, # <--- 添加导入
    get_cypher_generation_messages_with_templates,
    KG_EXTRACTION_GBNF_STRING  
)

import logging
import re
import uuid  # 用于生成 interaction_id
from datetime import datetime, timezone  # 用于生成时间戳
import litellm # <--- 确保这个导入存在

load_dotenv()  # 确保加载.env文件

_LLM_DIR = os.path.dirname(os.path.abspath(__file__))
RAG_INTERACTION_LOGS_DIR = os.path.join(_LLM_DIR, '..', '..', 'stored_data', 'rag_interaction_logs')


if not os.path.exists(RAG_INTERACTION_LOGS_DIR):
    try:
        os.makedirs(RAG_INTERACTION_LOGS_DIR)
    except Exception:
        pass

def get_llm_log_filepath() -> str:
    """获取当前LLM交互日志文件的完整路径，按天分割。"""
    today_str = datetime.now(timezone.utc).strftime("%Y%m%d")
    return os.path.join(RAG_INTERACTION_LOGS_DIR, f"rag_interactions_{today_str}.jsonl")

async def log_llm_interaction_to_jsonl(interaction_data: Dict[str, Any]):
    """
    将单条LLM交互数据异步追加到JSONL文件中。
    (This function might be part of what log_interaction_data uses, or an alternative logger. Keeping for completeness from original llm.py)
    """
    filepath = get_llm_log_filepath()
    try:
        def _write_sync():
            with open(filepath, 'a', encoding='utf-8') as f:
                f.write(json.dumps(interaction_data, ensure_ascii=False) + "\n")
        await asyncio.to_thread(_write_sync)
        llm_py_logger.debug(f"Successfully logged LLM interaction to {filepath}")
    except Exception as e:
        llm_py_logger.error(f"Failed to log LLM interaction to {filepath}: {e}", exc_info=True)

llm_py_logger = logging.getLogger("LLMUtilsLogger")
llm_py_logger.setLevel(os.getenv("LLM_LOG_LEVEL", "INFO").upper())

if not llm_py_logger.hasHandlers():
    _llm_console_handler = logging.StreamHandler()
    _llm_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
    _llm_console_handler.setFormatter(_llm_formatter)
    llm_py_logger.addHandler(_llm_console_handler)
    llm_py_logger.propagate = False

llm_py_logger.info("--- LLMUtilsLogger configured ---")

NO_ANSWER_PHRASE_ANSWER_CLEAN = "根据目前提供的资料，我无法找到关于您问题的明确信息。"
NO_ANSWER_PHRASE_KG_CLEAN = "从知识图谱中未找到直接相关信息。"
UNIQUE_STOP_TOKEN = "<|im_endofunable|>"
NO_ANSWER_PHRASE_ANSWER_WITH_STOP_TOKEN = f"{NO_ANSWER_PHRASE_ANSWER_CLEAN}{UNIQUE_STOP_TOKEN}"
NO_ANSWER_PHRASE_KG_WITH_STOP_TOKEN = f"{NO_ANSWER_PHRASE_KG_CLEAN}{UNIQUE_STOP_TOKEN}"

# Placeholder for the schema description. Replace with actual schema.
NEW_KG_SCHEMA_DESCRIPTION = """
{
  "node_labels": ["Person", "Project", "Task", "Document", "Region", "SalesAmount", "Product"],
  "relationship_types": ["WORKS_ON", "ASSIGNED_TO", "HAS_DOCUMENT", "HAS_SALES_AMOUNT", "RELATED_TO"],
  "node_properties": {
    "Person": [{"property": "name", "type": "STRING"}, {"property": "role", "type": "STRING"}],
    "Project": [{"property": "name", "type": "STRING"}, {"property": "status", "type": "STRING"}],
    "Task": [{"property": "name", "type": "STRING"}, {"property": "status", "type": "STRING"}, {"property": "priority", "type": "STRING"}],
    "Document": [{"property": "id", "type": "STRING"}, {"property": "title", "type": "STRING"}, {"property": "type", "type": "STRING"}],
    "Region": [{"property": "name", "type": "STRING"}],
    "SalesAmount": [{"property": "period", "type": "STRING"}, {"property": "numeric_amount", "type": "FLOAT"}, {"property": "unit", "type": "STRING"}],
    "Product": [{"property": "name", "type": "STRING"}, {"property": "category", "type": "STRING"}]
  },
  "relationship_properties": {},
  "output_format_guidance": {
    "description": "Your response MUST be a JSON object with two fields: 'status' and 'query'.",
    "status_field": {
      "description": "The 'status' field can be one of two values: 'success' or 'unable_to_generate'.",
      "success": "If you can generate a Cypher query, status should be 'success'.",
      "unable_to_generate": "If you cannot generate a Cypher query based on the question and schema, status should be 'unable_to_generate'."
    },
    "query_field": {
      "description": "The 'query' field contains the Cypher query as a string if status is 'success'.",
      "success_example": "MATCH (n) RETURN n LIMIT 1",
      "unable_to_generate_example": "无法生成Cypher查询."
    }
  },
  "examples": [
    {
      "User Question": "Who is task 'FixBug123' assigned to?",
      "Your EXACT Response": {
        "status": "success",
        "query": "MATCH (t:Task {name: 'FixBug123'})<-[:ASSIGNED_TO]-(p:Person) RETURN p.name AS assignedTo"
      }
    },
    {
      "User Question": "What is the color of the sky?",
      "Your EXACT Response": {
        "status": "unable_to_generate",
        "query": "无法生成Cypher查询."
      }
    }
  ]
}
"""

LLM_API_URL = os.getenv("SGLANG_API_URL", "http://localhost:8088/v1/chat/completions")

async def call_llm_via_openai_api_local_only( # 改个名字以示区分
    prompt: Union[str, List[Dict[str, str]]], # prompt 可以是字符串或消息列表
    temperature: float = 0.2,
    max_new_tokens: Optional[int] = 1024,
    stop_sequences: Optional[List[str]] = None,
    task_type: str = "unknown_local_llm_call",
    user_query_for_log: Optional[str] = None,
    model_name_for_log: str = "local_qwen_via_openai_api_compat",
    application_version_for_log: str = "0.1.0_local_compat"
) -> Optional[str]:
    llm_py_logger.info(f"Calling LOCAL LLM ({model_name_for_log}) for task: {task_type}, Target API: {LLM_API_URL}")

    current_messages: List[Dict[str, str]]
    original_prompt_for_log: str

    if isinstance(prompt, str): # 假设旧的SGLang风格的prompt字符串
        original_prompt_for_log = prompt
        # 尝试从SGLang格式转换为OpenAI messages格式
        # 这个转换逻辑需要根据您SGLang prompt的具体格式来定
        # 一个简化的例子，可能需要调整：
        current_messages = []
        # 简单的假设：如果prompt以<|im_start|>system开头，则提取system和user部分
        if prompt.startswith("<|im_start|>system"):
            parts = prompt.split("<|im_start|>")
            for part in parts:
                if not part.strip(): continue
                role_content = part.split("<|im_end|>")[0].strip()
                if "\n" in role_content:
                    role, content = role_content.split("\n", 1)
                    current_messages.append({"role": role.strip().lower(), "content": content.strip()})
        if not current_messages: # 如果转换失败或不是SGLang格式，则认为是单个user消息
            current_messages = [{"role": "user", "content": prompt}]
    elif isinstance(prompt, list):
        current_messages = prompt
        original_prompt_for_log = "Messages list provided directly."
    else:
        llm_py_logger.error(f"Invalid 'prompt' argument type: {type(prompt)}")
        return None

    payload = {
        "model": model_name_for_log, # 这个model名会被本地服务忽略，但符合OpenAI格式
        "messages": current_messages,
        "temperature": temperature,
        "max_tokens": max_new_tokens,
    }
    if stop_sequences:
        payload["stop"] = stop_sequences

    headers = {"Content-Type": "application/json"}
    llm_parameters_for_log = {k:v for k,v in payload.items() if k not in ['messages', 'model']} # model已在顶层记录
    raw_llm_output_text = None
    error_info = None

    try:
        async with httpx.AsyncClient(timeout=120.0) as client:
            response = await client.post(LLM_API_URL, json=payload, headers=headers) # LLM_API_URL 指向本地服务
            response.raise_for_status()
            response_json = response.json()
            if response_json.get("choices") and response_json["choices"][0].get("message"):
                raw_llm_output_text = response_json["choices"][0]["message"].get("content", "")
            else:
                raw_llm_output_text = "[[LLM_RESPONSE_MALFORMED_CHOICES_OR_MESSAGE_LOCAL]]"
            llm_py_logger.info(f"FULL Local LLM Raw Output for task '{task_type}': >>>{raw_llm_output_text}<<<")

    except Exception as e:
        llm_py_logger.error(f"Error calling local LLM service: {e}", exc_info=True)
        error_info = str(e)
        # 确保记录错误
        log_error_data = {
            "interaction_id": str(uuid.uuid4()), "timestamp_utc": datetime.now(timezone.utc).isoformat(),
            "task_type": task_type + "_local_error", "user_query_for_task": user_query_for_log,
            "llm_input_messages": current_messages,
            "llm_input_original_prompt_if_string": original_prompt_for_log if isinstance(prompt, str) else None,
            "llm_parameters": llm_parameters_for_log,
            "raw_llm_output": f"Error: {error_info}. Partial raw output: {str(raw_llm_output_text)[:200] if raw_llm_output_text else 'N/A'}",
            "error_details": traceback.format_exc(), "application_version": application_version_for_log
        }
        await log_interaction_data(log_error_data)
        return None # 出错时返回None

    # 记录成功的调用
    log_success_data = {
        "interaction_id": str(uuid.uuid4()), "timestamp_utc": datetime.now(timezone.utc).isoformat(),
        "task_type": task_type, "user_query_for_task": user_query_for_log,
        "llm_input_messages": current_messages,
        "llm_input_original_prompt_if_string": original_prompt_for_log if isinstance(prompt, str) else None,
        "llm_parameters": llm_parameters_for_log,
        "raw_llm_output": raw_llm_output_text, "application_version": application_version_for_log
    }
    await log_interaction_data(log_success_data)
    return raw_llm_output_text

async def generate_cypher_query(user_question: str) -> Optional[str]: # kg_schema_description 参数可以移除了，因为它已包含在新的prompt函数中
    llm_py_logger.info(f"Attempting to generate Cypher query (template-based) for: '{user_question}' via local service.")

    messages_for_llm = get_cypher_generation_messages_with_templates(user_question)

    cypher_stop_sequences = ['<|im_end|>', '```'] # 如果输出包含markdown的json块

    llm_response_json_str = await call_llm_via_openai_api_local_only( 
        prompt=messages_for_llm,
        temperature=0.0, # 对于精确的JSON和Cypher生成，温度设为0
        max_new_tokens=1024, # 允许足够的空间输出JSON和Cypher
        stop_sequences=cypher_stop_sequences,
        task_type="cypher_generation_template_based_local_service",
        user_query_for_log=user_question,
        model_name_for_log="qwen3_gguf_cypher_template_local"
    )

    if not llm_response_json_str:
        llm_py_logger.warning(f"LLM call for Cypher (template-based) returned None or empty. User question: '{user_question}'")
        return json.dumps({"status": "unable_to_generate", "query": "无法生成Cypher查询."}) # 始终返回JSON字符串

    cleaned_json_str = llm_response_json_str.strip()
    if cleaned_json_str.startswith("```json"):
        cleaned_json_str = cleaned_json_str[len("```json"):].strip()
    if cleaned_json_str.endswith("```"):
        cleaned_json_str = cleaned_json_str[:-len("```")].strip()

    try:

        parsed_for_validation = json.loads(cleaned_json_str)
        if isinstance(parsed_for_validation, dict) and \
           "status" in parsed_for_validation and \
           "query" in parsed_for_validation:
            llm_py_logger.info(f"LLM returned valid JSON for Cypher (template-based): {cleaned_json_str}")
            return cleaned_json_str
        else:
            llm_py_logger.warning(f"LLM output for Cypher (template-based) was JSON but not expected structure: {cleaned_json_str}")
            return json.dumps({"status": "unable_to_generate", "query": "LLM输出JSON结构错误."})
    except json.JSONDecodeError:
        llm_py_logger.error(f"Failed to parse JSON response for Cypher (template-based): '{cleaned_json_str}'", exc_info=True)
        # 如果不是有效的JSON，但包含"MATCH"，可能LLM直接输出了Cypher，尝试包装它
        if "MATCH" in cleaned_json_str.upper() or "RETURN" in cleaned_json_str.upper():
             llm_py_logger.warning("LLM output for Cypher (template-based) was not JSON but looks like Cypher, wrapping it.")
             return json.dumps({"status": "success", "query": cleaned_json_str})
        return json.dumps({"status": "unable_to_generate", "query": "LLM输出非JSON格式."})

async def generate_answer_from_context(user_query: str, context_str: str) -> Optional[str]: # context 参数名改为 context_str
    llm_py_logger.info(f"Generating answer for query: '{user_query[:100]}...' using provided context.")
    messages_for_llm = get_answer_generation_messages(user_query, context_str)
    raw_answer = await call_llm_via_openai_api_local_only(
        prompt=messages_for_llm, 
        temperature=0.05,
        max_new_tokens=1024, 
        stop_sequences=['<|im_end|>', UNIQUE_STOP_TOKEN],
        task_type="answer_generation_from_context",
        user_query_for_log=user_query,
        model_name_for_log="qwen3_gguf_answer_gen"
    )
    
    if raw_answer and raw_answer.strip() and \
       raw_answer.strip() != "[[LLM_RESPONSE_MALFORMED_CHOICES_OR_MESSAGE]]" and \
       raw_answer.strip() != "[[CONTENT_NOT_FOUND]]":
        
        final_answer = raw_answer.strip()
        if final_answer == NO_ANSWER_PHRASE_ANSWER_CLEAN: # NO_ANSWER_PHRASE_ANSWER_CLEAN 可以从 rag_prompts.py 导入或在 llm_interface.py 中也定义
            llm_py_logger.info("LLM indicated unable to answer from context.")
        return final_answer
    else:
        llm_py_logger.warning(f"Answer generation returned None, empty, or placeholder. Query: {user_query}")
        return NO_ANSWER_PHRASE_ANSWER_CLEAN

async def generate_simulated_kg_query_response(user_query: str, kg_schema_description: str, kg_data_summary_for_prompt: str) -> Optional[str]:
    prompt_str = f"""<|im_start|>system
你是一个知识图谱查询助手。你的任务是根据用户提出的问题、知识图谱Schema描述和图谱中的数据摘要，直接抽取出与问题最相关的1-2个事实片段作为答案。
只输出事实片段，不要解释，不要生成Cypher语句，不要包含任何额外对话或标记。
如果找不到直接相关的事实，请**直接且完整地**回答：“{NO_ANSWER_PHRASE_KG_WITH_STOP_TOKEN}”<|im_end|>
<|im_start|>user
知识图谱Schema描述:
{kg_schema_description}

知识图谱数据摘要: 
{kg_data_summary_for_prompt}

用户问题: {user_query}<|im_end|>
<|im_start|>assistant
"""
    stop_sequences = ["<|im_end|>", UNIQUE_STOP_TOKEN]
    return await call_llm_via_openai_api_local_only(
        prompt=prompt_str,
        temperature=0.5,
        max_new_tokens=256,
        stop_sequences=stop_sequences,
        task_type="simulated_kg_query_response",
        user_query_for_log=user_query
    )

# --- 新增：为查询扩展结果定义一个缓存 ---
# 使用 TTLCache，例如缓存1小时，最多缓存100个不同的原始查询的扩展结果
# TTL (time-to-live) in seconds. 3600 seconds = 1 hour.
# maxsize is the maximum number of items the cache will hold.
_expanded_queries_cache = TTLCache(maxsize=100, ttl=3600)
_expanded_queries_cache_lock = asyncio.Lock() # 用于异步环境下的锁
# --- 缓存定义结束 ---


async def generate_expanded_queries(original_query: str) -> List[str]:
    
    # --- 添加：缓存检查 ---
    async with _expanded_queries_cache_lock:
        if original_query in _expanded_queries_cache:
            llm_py_logger.info(f"Expanded queries CACHE HIT for original query: '{original_query[:50]}...'")
            return _expanded_queries_cache[original_query]
    llm_py_logger.info(f"Expanded queries CACHE MISS for original query: '{original_query[:50]}...'. Generating new expanded queries.")
    # --- 缓存检查结束 ---

    prompt_str = f"""<|im_start|>system
你是一个专家查询分析师。根据用户提供的查询，生成3个不同但相关的子问题，以探索原始查询的不同方面。这些子问题将用于检索更全面的信息。
你的回答必须是一个JSON数组（列表），其中每个元素是一个字符串（子问题）。
只输出JSON数组，不要包含任何其他解释、对话标记或代码块。

示例:
用户查询: "公司年度财务报告和未来一年的预算规划"
助手:
[
  "公司最近的年度财务报告总结是什么？",
  "未来一年的详细预算规划有哪些主要构成？",
  "对比往年，公司财务状况有何显著变化？"
]<|im_end|>
<|im_start|>user
原始查询: {original_query}<|im_end|>
<|im_start|>assistant
"""
    stop_sequences = ["<|im_end|>"]
    
    llm_py_logger.info(f"调用LLM API进行查询扩展 (Prompt长度: {len(prompt_str)} 字符)...")
    llm_output = await call_llm_via_openai_api_local_only(
        prompt=prompt_str,
        temperature=0.1,
        max_new_tokens=512,
        stop_sequences=stop_sequences,
        task_type="query_expansion",
        user_query_for_log=original_query
    )
    expanded_queries = []
    if llm_output:
        try:
            json_str = llm_output.strip()
            if json_str.startswith("```json"):
                json_str = json_str[len("```json"):].strip()
            if json_str.endswith("```"):
                json_str = json_str[:-len("```")].strip()
            
            parsed_queries = json.loads(json_str)
            if isinstance(parsed_queries, list) and all(isinstance(q, str) for q in parsed_queries):
                expanded_queries = parsed_queries
                llm_py_logger.info(f"LLM成功生成 {len(expanded_queries)} 个扩展查询。")
            else:
                llm_py_logger.warning(f"LLM生成的扩展查询JSON格式不符合预期 (不是字符串列表): {llm_output[:200]}...")
        except json.JSONDecodeError as e:
            llm_py_logger.error(f"解析LLM扩展查询JSON失败: {e}. 原始输出: {llm_output[:200]}...", exc_info=True)
        except Exception as e:
            llm_py_logger.error(f"处理LLM扩展查询时发生未知错误: {e}. 原始输出: {llm_output[:200]}...", exc_info=True)
    else:
        llm_py_logger.warning("LLM未能生成扩展查询。")

    # Always include the original query
    if original_query not in expanded_queries:
        expanded_queries.append(original_query)
    # --- 添加：存储到缓存 ---
    async with _expanded_queries_cache_lock:
        _expanded_queries_cache[original_query] = expanded_queries
        llm_py_logger.info(f"CACHED {len(expanded_queries)} expanded queries for original query: '{original_query[:50]}...'")
    # --- 缓存存储结束 ---

    return expanded_queries


async def generate_clarification_question(original_query: str, uncertainty_reason: str) -> Optional[str]:
    llm_py_logger.info(f"调用LLM API生成澄清问题。原始查询: '{original_query}', 原因: '{uncertainty_reason}'")
    messages_for_llm = get_clarification_question_messages(original_query, uncertainty_reason)

    clarification_question_raw = await call_llm_via_openai_api_local_only(
        prompt=messages_for_llm,
        temperature=0.5,
        max_new_tokens=128,
        stop_sequences=['<|im_end|>'], # 对于Qwen系列，<|im_end|> 是一个常见的结束标记
        task_type="clarification_question_generation",
        user_query_for_log=original_query
    )
    
    if not clarification_question_raw or not clarification_question_raw.strip():
        llm_py_logger.warning("LLM未能生成澄清问题，返回默认提示。")
        return "抱歉，我不太理解您的意思，请您再具体说明一下。"  
    cleaned_question_from_llm = clarification_question_raw.strip()
    llm_py_logger.debug(f"LLM原始澄清输出 (清理后): '{cleaned_question_from_llm}'")
    potential_lines = cleaned_question_from_llm.splitlines()
    
    final_extracted_question = None

    for line in reversed(potential_lines):
        line_stripped = line.strip()
        if not line_stripped: # 跳过空行
            continue
        if line_stripped.endswith("？") or line_stripped.endswith("?"):
            if not (line_stripped.startswith("好的，") or \
                    line_stripped.startswith("首先，") or \
                    line_stripped.startswith("因此，") or \
                    line_stripped.startswith("所以，") or \
                    line_stripped.startswith("根据这个原因，") or \
                    "我需要生成一个" in line_stripped or \
                    "可能的澄清问题是" in line_stripped or \
                    "澄清问题应该是" in line_stripped or \
                    "接下来，" in line_stripped):
                final_extracted_question = line_stripped
                llm_py_logger.info(f"通过行分割和问号结尾提取到澄清问题: '{final_extracted_question}'")
                break 
        elif any(line_stripped.startswith(prefix) for prefix in ["请问您", "您对", "您具体指的是"]):
            final_extracted_question = line_stripped
            llm_py_logger.info(f"通过行分割和特定前缀提取到澄清问题: '{final_extracted_question}'")
            break

    if final_extracted_question:
        llm_py_logger.info(f"LLM成功生成并提取到最终澄清问题: {final_extracted_question}")
        return final_extracted_question
    else:
        potential_sentences = re.split(r'(?<=[。？！?])\s*', cleaned_question_from_llm)
        for sentence in reversed(potential_sentences):
            sentence_stripped = sentence.strip()
            if not sentence_stripped:
                continue
            if sentence_stripped.endswith("？") or sentence_stripped.endswith("?") or \
               any(sentence_stripped.startswith(prefix) for prefix in ["请问您", "您是想", "您具体指的是", "关于您提到的"]):
                if not (sentence_stripped.startswith("好的，") or \
                        sentence_stripped.startswith("首先，") or \
                        "我需要生成一个" in sentence_stripped or \
                        "可能的澄清问题是" in sentence_stripped): # 避免选择思考过程
                    final_extracted_question = sentence_stripped
                    llm_py_logger.info(f"通过句子分割和启发式规则提取到澄清问题: '{final_extracted_question}'")
                    break
        
        if final_extracted_question:
            llm_py_logger.info(f"LLM成功生成并提取到最终澄清问题 (后备逻辑): {final_extracted_question}")
            return final_extracted_question
        else:
            llm_py_logger.warning(f"未能通过所有启发式规则从LLM输出中提取明确的澄清问句。原始输出为: '{cleaned_question_from_llm}'。将返回默认澄清。")

            if len(cleaned_question_from_llm) < 70 and (cleaned_question_from_llm.endswith("？") or cleaned_question_from_llm.endswith("?")): # 70是个经验值
                 llm_py_logger.info(f"原始输出较短且以问号结尾，将其作为澄清问题返回: '{cleaned_question_from_llm}'")
                 return cleaned_question_from_llm
            return "抱歉，我不太理解您的意思，请您再具体说明一下。"


async def generate_clarification_options(original_query: str, uncertainty_reason: str) -> List[str]:
    prompt_str = f"""<|im_start|>system
你是一个智能助手，擅长根据用户查询的模糊性提供具体的澄清选项。
你的任务是根据用户原始查询和系统检测到的不确定性原因，生成3-5个具体的、可供用户选择的澄清选项。
每个选项都应该是一个简洁的短语或问题，帮助用户明确其意图。
你的回答必须是一个JSON数组（列表），其中每个元素是一个字符串（澄清选项）。
只输出JSON数组，不要包含任何其他解释、对话标记或代码块。

示例:
用户查询: "帮我预定机票。"
不确定性原因: "缺少出发城市、目的地、日期等信息。"
助手:
[
  "请问您想从哪个城市出发？",
  "请问您的目的地是哪里？",
  "请问您希望在哪一天出行？",
  "您有偏好的航空公司或舱位等级吗？"
]<|im_end|>
<|im_start|>user
用户原始查询: {original_query}
不确定性原因: {uncertainty_reason}

请生成澄清选项:<|im_end|>
<|im_start|>assistant
"""
    stop_sequences = ["<|im_end|>"]
    llm_py_logger.info(f"调用LLM API生成澄清选项 (Prompt长度: {len(prompt_str)} 字符)...")
    llm_output = await call_llm_via_openai_api_local_only(
        prompt=prompt_str,
        temperature=0.7,
        max_new_tokens=256,
        stop_sequences=stop_sequences,
        task_type="clarification_options_generation",
        user_query_for_log=original_query
    )

    options = []
    if llm_output:
        try:
            json_str = llm_output.strip()
            if json_str.startswith("```json"):
                json_str = json_str[len("```json"):].strip()
            if json_str.endswith("```"):
                json_str = json_str[:-len("```")].strip()
            
            parsed_options = json.loads(json_str)
            if isinstance(parsed_options, list) and all(isinstance(o, str) for o in parsed_options):
                options = parsed_options
                llm_py_logger.info(f"LLM成功生成 {len(options)} 个澄清选项。")
            else:
                llm_py_logger.warning(f"LLM生成的澄清选项JSON格式不符合预期 (不是字符串列表): {llm_output[:200]}...")
        except json.JSONDecodeError as e:
            llm_py_logger.error(f"解析LLM澄清选项JSON失败: {e}. 原始输出: {llm_output[:200]}...", exc_info=True)
        except Exception as e:
            llm_py_logger.error(f"处理LLM澄清选项时发生未知错误: {e}. 原始输出: {llm_output[:200]}...", exc_info=True)
    else:
        llm_py_logger.warning("LLM未能生成澄清选项。")
    
    if not options:
        options.append("请提供更多详细信息。")
    
    return options


INTENT_CLASSIFICATION_JSON_SCHEMA = {
    "type": "object",
    "properties": {
        "clarification_needed": {"type": "boolean"},
        "reason": {"type": "string"}
    },
    "required": ["clarification_needed", "reason"]
}

async def generate_intent_classification(user_query: str) -> Dict[str, Any]:
    llm_py_logger.info(f"Generating intent classification for query: '{user_query[:100]}...' using Gemini.")
    
    # 针对Gemini优化的Prompt，强调直接输出JSON
    system_prompt_for_intent = f"""你是一个智能意图分类器。你的任务是分析用户查询，判断该查询是否清晰明确，或者是否存在歧义、信息不足导致需要进一步澄清。
如果查询包含具体的命名实体（如人名“张三”、项目名“项目X”、产品名“新产品A”等），并且问题是关于这些实体的特定信息（例如“张三的职位是什么？”、“项目X的截止日期是哪天？”、“新产品A的功能有哪些？”），则通常认为查询是清晰的，不需要澄清。
只有当查询缺少定位关键信息所必需的核心实体，或者询问的范围过于宽泛无法直接操作时，才需要澄清。

如果查询需要澄清，请说明原因。
你的【唯一输出】必须是一个严格符合以下结构的JSON对象，不要包含任何其他文本、解释或markdown标记:
{{
  "clarification_needed": true/false,
  "reason": "如果需要澄清，请简要说明原因；如果不需要，则为空字符串。"
}}

示例1 (需要澄清 - 信息不足):
用户查询: "帮我预定明天去上海的机票。"
助手 JSON 输出:
{{
  "clarification_needed": true,
  "reason": "缺少出发城市、具体时间（上午/下午/晚上）、舱位等级等信息。"
}}

示例2 (不需要澄清 - 清晰):
用户查询: "公司最新的销售额报告在哪里可以找到？"
助手 JSON 输出:
{{
  "clarification_needed": false,
  "reason": ""
}}
"""
    
    messages_for_gemini = [
        {"role": "system", "content": system_prompt_for_intent},
        {"role": "user", "content": f"用户查询: {user_query}"}
    ]

    # 从环境变量获取Gemini配置
    gemini_model_name = os.getenv("CLOUD_LLM_MODEL_NAME_FOR_LITELLM", "gemini/gemini-1.5-flash-latest")
    gemini_api_key = os.getenv("GEMINI_API_KEY") # 或者 GOOGLE_API_KEY
    proxy_url = os.getenv("LITELLM_PROXY_URL")

    litellm_params: Dict[str, Any] = {
        "model": gemini_model_name,
        "messages": messages_for_gemini,
        "api_key": gemini_api_key,
        "temperature": 0.1, 
        "max_tokens": 256,  # 意图分类的JSON输出通常较短
        # "response_format": {"type": "json_object"} # LiteLLM的Gemini集成可能尚不支持此参数，暂时注释
    }
    if proxy_url:
        # LiteLLM 的 proxy 参数期望一个字典，或者直接是一个字符串URL (取决于LiteLLM版本和具体实现)
        # 为保险起见，我们按文档常见的字典格式提供
        litellm_params["proxy"] = {
            "http": proxy_url,
            "https": proxy_url,
        }
        # 或者，如果您的LiteLLM版本支持直接传递字符串URL作为代理：
        # litellm_params["api_base"] = proxy_url # 这会将代理用于所有请求，如果Gemini也通过此代理
        # litellm_params["base_url"] = proxy_url # 有些版本用 base_url
        # 更通用的方式是设置环境变量 HTTP_PROXY 和 HTTPS_PROXY，LiteLLM通常会读取它们
        # 但为了显式，我们这里尝试通过参数传递给litellm.acompletion

    llm_py_logger.info(f"Calling Gemini (via LiteLLM) for intent classification. Model: {gemini_model_name}")
    debug_params = {k:v for k,v in litellm_params.items() if k not in ['messages', 'api_key']}
    llm_py_logger.debug(f"LiteLLM params for intent (excluding messages & api_key): {debug_params}")
    
    raw_gemini_output_text = None
    error_info_intent = None
    parsed_result_dict: Optional[Dict[str, Any]] = None # 用于存储最终解析结果

    try:
        response = await litellm.acompletion(**litellm_params)
        if response and response.choices and response.choices[0].message and response.choices[0].message.content:
            raw_gemini_output_text = response.choices[0].message.content.strip()
            llm_py_logger.info(f"Gemini intent classification raw output: {raw_gemini_output_text[:300]}...")
            
            # 尝试解析JSON (与之前的提取逻辑类似)
            json_str_candidate = raw_gemini_output_text
            # 1. 尝试从 markdown block 中提取
            markdown_match = re.search(r"```json\s*(\{[\s\S]*?\})\s*```", json_str_candidate, re.DOTALL)
            if markdown_match:
                json_str_candidate = markdown_match.group(1)
                llm_py_logger.debug(f"Extracted JSON candidate from markdown: {json_str_candidate[:200]}...")
            
            # 2. 如果没有markdown，或者提取后仍然不是纯JSON，尝试直接解析或查找第一个 '{' 和最后一个 '}'
            try:
                parsed_result_dict = json.loads(json_str_candidate)
            except json.JSONDecodeError: # 如果直接解析失败
                first_brace = json_str_candidate.find('{')
                last_brace = json_str_candidate.rfind('}')
                if first_brace != -1 and last_brace != -1 and last_brace > first_brace:
                    json_str_candidate = json_str_candidate[first_brace : last_brace+1]
                    llm_py_logger.debug(f"Extracted JSON candidate by braces: {json_str_candidate[:200]}...")
                    try:
                        parsed_result_dict = json.loads(json_str_candidate)
                    except json.JSONDecodeError as e_json_brace:
                        error_info_intent = f"Failed to decode JSON from Gemini intent (braces): {e_json_brace}"
                        llm_py_logger.error(error_info_intent, exc_info=True)
                else: # 没有找到有效的花括号对
                    error_info_intent = "No valid JSON object found in Gemini intent output."
                    llm_py_logger.error(error_info_intent + f" Raw: {raw_gemini_output_text[:200]}")
            
            # 验证解析后的JSON结构
            if parsed_result_dict and isinstance(parsed_result_dict, dict) and \
               "clarification_needed" in parsed_result_dict and \
               "reason" in parsed_result_dict:
                llm_py_logger.info(f"Gemini successfully classified intent: {parsed_result_dict}")
                # 记录成功的调用
                log_data_intent = {
                    "interaction_id": str(uuid.uuid4()), "timestamp_utc": datetime.now(timezone.utc).isoformat(),
                    "task_type": "intent_classification_gemini", "user_query_for_task": user_query,
                    "llm_input_messages": messages_for_gemini, "llm_parameters": {k:v for k,v in litellm_params.items() if k not in ['messages', 'api_key', 'proxy']},
                    "raw_llm_output": raw_gemini_output_text, "application_version": "0.1.0_intent_gemini"
                }
                await log_interaction_data(log_data_intent)
                return parsed_result_dict
            else: # 解析成功但结构不对
                if parsed_result_dict: # 避免对None调用get
                    error_info_intent = f"Gemini intent output JSON structure mismatch. Parsed: {parsed_result_dict}"
                else: # parsed_result_dict 为 None (例如，花括号提取失败后)
                    error_info_intent = "Gemini intent output JSON structure mismatch (parsed_result_dict is None)."
                llm_py_logger.warning(error_info_intent)
        else: # response.choices[0].message.content 为空或不存在
            error_info_intent = "Gemini intent call returned empty or malformed response content."
            llm_py_logger.error(f"{error_info_intent} Full response object: {response}")

    except Exception as e_gemini_call:
        error_info_intent = f"Error calling Gemini for intent: {e_gemini_call}"
        llm_py_logger.error(error_info_intent, exc_info=True)

    # 如果执行到这里，说明出错了或者没有得到期望的JSON
    log_error_data_intent = {
        "interaction_id": str(uuid.uuid4()), "timestamp_utc": datetime.now(timezone.utc).isoformat(),
        "task_type": "intent_classification_gemini_error", "user_query_for_task": user_query,
        "llm_input_messages": messages_for_gemini, "llm_parameters": {k:v for k,v in litellm_params.items() if k not in ['messages', 'api_key', 'proxy']},
        "raw_llm_output": raw_gemini_output_text or "N/A", "error_details": error_info_intent,
        "application_version": "0.1.0_intent_gemini"
    }
    await log_interaction_data(log_error_data_intent)
    
    llm_py_logger.warning(f"Gemini failed to generate valid intent classification, defaulting to no clarification needed. Error: {error_info_intent or 'Unknown reason'}")
    return {"clarification_needed": False, "reason": f"Intent classification by Gemini failed: {error_info_intent or 'Unknown reason'}"}

# --- 新增：用于提取实体和关系意图的函数 ---
async def extract_entities_for_kg_query(user_question: str) -> Optional[ExtractedEntitiesAndRelationIntent]:
    llm_py_logger.info(f"Attempting to extract entities and relation intent for KG query (with GBNF) from: '{user_question}'")

    # --- 使用您在 test_gbnf_extraction.py 中验证成功的 One-Shot Prompt 构建逻辑 ---
    one_shot_example = """
--- 示例 ---
输入文本: "Alice在ACME公司担任工程师。"
输出JSON:
{
  "entities": [
    {"text": "Alice", "label": "PERSON"},
    {"text": "ACME公司", "label": "ORGANIZATION"},
    {"text": "工程师", "label": "TASK"}
  ],
  "relations": [
    {"head_entity_text": "Alice", "head_entity_label": "PERSON", "relation_type": "WORKS_AT", "tail_entity_text": "ACME公司", "tail_entity_label": "ORGANIZATION"}
  ]
}
--- 任务开始 ---"""
    
    # system_content 部分与您的测试脚本保持一致
    system_content_for_prompt = (
        f"你是一个严格的JSON知识图谱提取器。请根据用户提供的文本，严格按照示例格式，生成一个包含'entities'和'relations'的JSON对象。\n"
        f"{one_shot_example}"
    )

    # user_content 部分也与您的测试脚本保持一致
    user_content_for_prompt = (
        f"输入文本: \"{user_question}\"\n" # 注意：这里用的是 user_question，而不是固定的 sample_text_to_extract
        f"输出JSON:\n"
    )

    full_prompt_for_extraction = (
        f"<|im_start|>system\n{system_content_for_prompt}<|im_end|>\n"
        f"<|im_start|>user\n{user_content_for_prompt}<|im_end|>\n"
        f"<|im_start|>assistant\n"
    )
    # --- Prompt 构建结束 ---

    llm_response_str = await call_local_llm_with_gbnf(
        full_prompt=full_prompt_for_extraction,
        grammar_str=KG_EXTRACTION_GBNF_STRING, # 使用我们定义的GBNF字符串
        temperature=0.1,
        max_tokens=1024, # 与您的测试脚本一致
        repeat_penalty=1.2, # 与您的测试脚本一致
        stop_sequences=["<|im_end|>"], # Qwen的停止标记
        task_type="kg_entity_relation_extraction_gbnf",
        user_query_for_log=user_question,
        model_name_for_log="qwen3_gguf_kg_ext_gbnf"
    )

    if not llm_response_str:
        llm_py_logger.warning(f"LLM call for KG entity/relation extraction (GBNF) returned None or empty. User question: '{user_question}'")
        return None

    # GBNF应该确保输出是有效的JSON，所以我们可以直接尝试解析
    try:
        # .strip() 以防万一有额外的空白被GBNF的 space 规则匹配但未被移除
        parsed_data = json.loads(llm_response_str.strip())
        extracted_info = ExtractedEntitiesAndRelationIntent(**parsed_data)
        llm_py_logger.info(f"Successfully parsed Pydantic model from GBNF LLM output: {extracted_info.model_dump_json(indent=2)}")
        return extracted_info
    except json.JSONDecodeError as e_json:
        llm_py_logger.error(f"Failed to decode JSON from GBNF LLM output: '{llm_response_str}'. Error: {e_json}", exc_info=True)
        return None
    except Exception as e_pydantic: # Catch Pydantic validation errors
        llm_py_logger.error(f"Failed to validate Pydantic model from GBNF LLM JSON: '{llm_response_str}'. Error: {e_pydantic}", exc_info=True)
        return None
    

# 新的LLM调用函数，用于create_completion和GBNF
async def call_local_llm_with_gbnf(
    full_prompt: str,
    grammar_str: str, # GBNF语法字符串
    temperature: float = 0.1,
    max_tokens: int = 1024,
    repeat_penalty: float = 1.2, # 从您的成功脚本中获取
    stop_sequences: Optional[List[str]] = None,
    task_type: str = "gbnf_constrained_generation",
    user_query_for_log: Optional[str] = None, # 用于日志记录
    model_name_for_log: str = "local_qwen_gguf_gbnf",
    application_version_for_log: str = "0.1.0_gbnf"
) -> Optional[str]:
    llm_py_logger.info(f"Calling LOCAL LLM with GBNF for task: {task_type}. Prompt length: {len(full_prompt)}")

    # 获取模型路径 (与 test_gbnf_extraction.py 逻辑类似)
    # 注意: 这里的模型加载是临时的，理想情况下 LocalModelHandler 应该能处理这个
    # 但为了快速集成您的成功方案，我们先在这里直接加载。
    # 后续可以考虑将 create_completion 与 GBNF 的能力集成到 LocalModelHandler 中。
    model_path_from_env = os.getenv("LOCAL_LLM_GGUF_MODEL_PATH")
    if not model_path_from_env or not os.path.exists(model_path_from_env):
        llm_py_logger.error(f"LLM model path not found or not set in .env for GBNF call: {model_path_from_env}")
        # 记录错误日志
        log_error_data = {
            "interaction_id": str(uuid.uuid4()), "timestamp_utc": datetime.now(timezone.utc).isoformat(),
            "task_type": task_type + "_model_load_error", "user_query_for_task": user_query_for_log,
            "llm_input_prompt": full_prompt[:500] + "...", # 截断长prompt
            "error_details": "LLM model path not configured or invalid.",
            "application_version": application_version_for_log
        }
        await log_interaction_data(log_error_data) # 确保 log_interaction_data 已导入并可用
        return None
    
    raw_llm_output_text = None
    error_info = None
    
    try:
        # 编译GBNF语法
        compiled_grammar = LlamaGrammar.from_string(grammar_str)

        # 初始化Llama模型实例 (每次调用都初始化可能效率不高，后续优化点)
        llm_instance = Llama(
            model_path=model_path_from_env,
            n_gpu_layers=int(os.getenv("LLM_N_GPU_LAYERS", 0)),
            n_ctx=int(os.getenv("LLM_N_CTX", 4096)),
            verbose=False
        )

        def _blocking_llm_call(): # 封装阻塞操作
            response = llm_instance.create_completion(
                prompt=full_prompt,
                grammar=compiled_grammar,
                temperature=temperature,
                max_tokens=max_tokens,
                repeat_penalty=repeat_penalty,
                stop=stop_sequences
            )
            return response['choices'][0]['text']

        raw_llm_output_text = await asyncio.to_thread(_blocking_llm_call)
        llm_py_logger.info(f"GBNF Call: Raw LLM Output for task '{task_type}': >>>{raw_llm_output_text}<<<")

    except Exception as e:
        llm_py_logger.error(f"Error calling local LLM service with GBNF: {e}", exc_info=True)
        error_info = str(e)
        log_error_data = {
            "interaction_id": str(uuid.uuid4()), "timestamp_utc": datetime.now(timezone.utc).isoformat(),
            "task_type": task_type + "_error", "user_query_for_task": user_query_for_log,
            "llm_input_prompt": full_prompt[:500] + "...",
            "llm_parameters": {"temperature": temperature, "max_tokens": max_tokens, "repeat_penalty": repeat_penalty, "stop": stop_sequences},
            "raw_llm_output": f"Error: {error_info}. Partial raw output: {str(raw_llm_output_text)[:200] if raw_llm_output_text else 'N/A'}",
            "error_details": traceback.format_exc(), "application_version": application_version_for_log
        }
        await log_interaction_data(log_error_data)
        return None

    log_success_data = {
        "interaction_id": str(uuid.uuid4()), "timestamp_utc": datetime.now(timezone.utc).isoformat(),
        "task_type": task_type, "user_query_for_task": user_query_for_log,
        "llm_input_prompt": full_prompt[:500] + "...",
        "llm_parameters": {"temperature": temperature, "max_tokens": max_tokens, "repeat_penalty": repeat_penalty, "stop": stop_sequences, "grammar_used": True},
        "raw_llm_output": raw_llm_output_text, "application_version": application_version_for_log
    }
    await log_interaction_data(log_success_data)
    return raw_llm_output_text
```

File: zhz_rag/llm/local_model_handler.py
----------------------------------------
```python
# 文件: zhz_rag/llm/local_model_handler.py
import os
import logging
from typing import List, Optional, Dict, Any
import asyncio
import numpy as np
import multiprocessing
from functools import partial
from multiprocessing.pool import Pool

# 导入新的工作函数
from zhz_rag.llm.embedding_process_worker import embed_texts_in_subprocess, embed_query_in_subprocess

logger = logging.getLogger(__name__)

def l2_normalize_embeddings(embeddings: List[List[float]]) -> List[List[float]]:
    """
    对一批嵌入向量进行L2归一化。
    注意：这个函数现在主要作为备用或验证，因为归一化逻辑已移至工作进程中。
    """
    if not embeddings or not isinstance(embeddings, list):
        return []
    normalized_embeddings = []
    for emb_list in embeddings:
        if not emb_list or not isinstance(emb_list, list) or not all(isinstance(x, (float, int)) for x in emb_list):
            logger.warning(f"L2_NORM (LMH): Skipping invalid or empty inner list: {emb_list}")
            normalized_embeddings.append([]) 
            continue
        try:
            emb_array = np.array(emb_list, dtype=np.float32)
            norm = np.linalg.norm(emb_array)
            if norm == 0:
                normalized_embeddings.append(emb_list)
            else:
                normalized_embeddings.append((emb_array / norm).tolist())
        except Exception as e_norm:
            logger.error(f"Error during L2 normalization of an embedding in LocalModelHandler: {e_norm}", exc_info=True)
            normalized_embeddings.append(emb_list) 
    return normalized_embeddings

class LocalModelHandler:
    """
    管理本地GGUF模型加载和调用的句柄。
    LLM模型在主进程中加载。
    嵌入模型的操作被委托给一个独立的子进程池，以避免多线程冲突和段错误。
    """
    _instance_count = 0 

    def __init__(
        self,
        llm_model_path: Optional[str] = None,
        embedding_model_path: Optional[str] = None,
        n_ctx_llm: int = 4096,
        n_gpu_layers_llm: int = 0,
        n_ctx_embed: int = 2048,
        n_gpu_layers_embed: int = 0,
        pooling_type_embed: int = 2,
        embedding_pool_size: Optional[int] = None
    ):
        LocalModelHandler._instance_count += 1
        self.instance_id = LocalModelHandler._instance_count
        logger.info(f"LMH Instance [{self.instance_id}] __init__ called.")

        # LLM 模型加载逻辑
        self.llm_model_path = llm_model_path
        self.n_ctx_llm = n_ctx_llm
        self.n_gpu_layers_llm = n_gpu_layers_llm
        self.llm_model: Optional[Any] = None
        
        if llm_model_path:
            try:
                from llama_cpp import Llama
                logger.info(f"LMH Instance [{self.instance_id}]: Loading LLM model from: {llm_model_path}")
                self.llm_model = Llama(
                    model_path=llm_model_path,
                    n_ctx=n_ctx_llm,
                    n_gpu_layers=n_gpu_layers_llm,
                    verbose=False
                )
                logger.info(f"LMH Instance [{self.instance_id}]: LLM model loaded successfully.")
            except Exception as e:
                logger.error(f"LMH Instance [{self.instance_id}]: Failed to load LLM model from {llm_model_path}: {e}", exc_info=True)
                self.llm_model = None

        # 保存嵌入模型配置，不再直接加载
        self.embedding_model_path = embedding_model_path
        self.n_ctx_embed = n_ctx_embed
        self.n_gpu_layers_embed = n_gpu_layers_embed
        self.pooling_type_embed = pooling_type_embed
        
        self._embedding_model_dimension: Optional[int] = None
        self._dimension_lock = asyncio.Lock()

        # 初始化进程池
        self._embedding_pool: Optional[Pool] = None
        self._pool_size = embedding_pool_size if embedding_pool_size else os.cpu_count() or 1
        
        if self.embedding_model_path:
            try:
                ctx = multiprocessing.get_context('spawn')
                self._embedding_pool = ctx.Pool(processes=self._pool_size)
                logger.info(f"LMH Instance [{self.instance_id}]: Embedding subprocess pool initialized with size {self._pool_size} and context 'spawn'.")
            except Exception as e_pool:
                logger.error(f"LMH Instance [{self.instance_id}]: Failed to initialize embedding subprocess pool: {e_pool}", exc_info=True)
                self._embedding_pool = None
        else:
            logger.warning(f"LMH Instance [{self.instance_id}]: No embedding_model_path provided. Embedding pool not created.")

        if not self.llm_model and not self.embedding_model_path:
            logger.warning(f"LMH Instance [{self.instance_id}]: Initialized without LLM and no path for embedding model.")

    async def _get_embedding_dimension_from_worker_once(self) -> Optional[int]:
        """
        通过启动一个临时工作进程来获取嵌入维度，并缓存结果。
        """
        if not self.embedding_model_path or not self._embedding_pool:
            logger.error(f"LMH Instance [{self.instance_id}]: Cannot get dimension, no model path or pool.")
            return None

        dummy_text = "dimension" 
        logger.info(f"LMH Instance [{self.instance_id}]: Attempting to derive embedding dimension via a dummy query in subprocess...")
        try:
            task_args = (
                dummy_text,
                self.embedding_model_path,
                self.n_ctx_embed,
                self.n_gpu_layers_embed,
                self.pooling_type_embed
            )
            loop = asyncio.get_running_loop()
            async_result = self._embedding_pool.apply_async(embed_query_in_subprocess, args=task_args)
            result_embedding = await loop.run_in_executor(None, async_result.get, 60)

            if result_embedding and isinstance(result_embedding, list) and len(result_embedding) > 0:
                dim = len(result_embedding)
                logger.info(f"LMH Instance [{self.instance_id}]: Successfully derived embedding dimension: {dim}")
                return dim
            else:
                logger.error(f"LMH Instance [{self.instance_id}]: Failed to derive dimension. Dummy query returned: {result_embedding}")
                return None
        except multiprocessing.TimeoutError:
            logger.error(f"LMH Instance [{self.instance_id}]: Timeout while trying to get dimension from worker.")
            return None
        except Exception as e_dim:
            logger.error(f"LMH Instance [{self.instance_id}]: Error deriving embedding dimension: {e_dim}", exc_info=True)
            return None

    async def embed_documents(self, texts: List[str]) -> List[List[float]]:
        if not self._embedding_pool or not self.embedding_model_path:
            logger.error(f"LMH Instance [{self.instance_id}]: Embedding pool or model path not available. Cannot embed documents.")
            async with self._dimension_lock:
                if self._embedding_model_dimension is None:
                    logger.warning(f"LMH Instance [{self.instance_id}]: embed_documents - dimension unknown, attempting to get it first.")
                    temp_dim = await self._get_embedding_dimension_from_worker_once()
                    if temp_dim:
                        self._embedding_model_dimension = temp_dim
                    else:
                        logger.error(f"LMH Instance [{self.instance_id}]: embed_documents - Failed to get dimension, cannot proceed.")
                        return [[] for _ in texts]
            return [[0.0] * self._embedding_model_dimension if self._embedding_model_dimension else [] for _ in texts]

        if not texts:
            return []

        processed_texts = [(text + "<|endoftext|>" if text and not text.endswith("<|endoftext|>") else text) for text in texts]
        logger.info(f"LMH Instance [{self.instance_id}]: Submitting {len(processed_texts)} documents for embedding to subprocess pool...")

        try:
            task_func = partial(
                embed_texts_in_subprocess,
                embedding_model_path=self.embedding_model_path,
                n_ctx_embed=self.n_ctx_embed,
                n_gpu_layers_embed=self.n_gpu_layers_embed,
                pooling_type_embed=self.pooling_type_embed
            )
            
            loop = asyncio.get_running_loop()
            async_result = self._embedding_pool.apply_async(task_func, args=(processed_texts,))
            embeddings_list = await loop.run_in_executor(None, async_result.get, 300)

            if embeddings_list is None:
                raise RuntimeError("Embedding subprocess returned None, possibly due to timeout or unhandled error in worker.")

            logger.info(f"LMH Instance [{self.instance_id}]: Received {len(embeddings_list)} embeddings from subprocess.")
            return embeddings_list

        except multiprocessing.TimeoutError:
            logger.error(f"LMH Instance [{self.instance_id}]: Timeout embedding documents in subprocess.", exc_info=True)
            return [[0.0] * (self._embedding_model_dimension or 1024) for _ in texts]
        except Exception as e_async_embed_docs:
            logger.error(f"LMH Instance [{self.instance_id}]: Error submitting document embedding task to subprocess: {e_async_embed_docs}", exc_info=True)
            return [[0.0] * (self._embedding_model_dimension or 1024) for _ in texts]

    async def embed_query(self, text: str) -> List[float]:
        if not self._embedding_pool or not self.embedding_model_path:
            logger.error(f"LMH Instance [{self.instance_id}]: Embedding pool or model path not available. Cannot embed query.")
            async with self._dimension_lock:
                if self._embedding_model_dimension is None:
                    logger.warning(f"LMH Instance [{self.instance_id}]: embed_query - dimension unknown, attempting to get it first.")
                    temp_dim = await self._get_embedding_dimension_from_worker_once()
                    if temp_dim:
                        self._embedding_model_dimension = temp_dim
                    else:
                        logger.error(f"LMH Instance [{self.instance_id}]: embed_query - Failed to get dimension, cannot proceed.")
                        return []
            return [0.0] * self._embedding_model_dimension if self._embedding_model_dimension else []

        if not text: 
            return []

        processed_text = text + "<|endoftext|>" if not text.endswith("<|endoftext|>") else text
        logger.info(f"LMH Instance [{self.instance_id}]: Submitting query for embedding to subprocess pool (first 100): {processed_text[:100]}...")

        try:
            task_func = partial(
                embed_query_in_subprocess,
                embedding_model_path=self.embedding_model_path,
                n_ctx_embed=self.n_ctx_embed,
                n_gpu_layers_embed=self.n_gpu_layers_embed,
                pooling_type_embed=self.pooling_type_embed
            )
            loop = asyncio.get_running_loop()
            async_result = self._embedding_pool.apply_async(task_func, args=(processed_text,))
            embedding_vector = await loop.run_in_executor(None, async_result.get, 60)

            if embedding_vector is None:
                 raise RuntimeError("Embedding subprocess returned None for query, possibly due to timeout or unhandled error in worker.")

            logger.info(f"LMH Instance [{self.instance_id}]: Received query embedding from subprocess (len: {len(embedding_vector)}).")
            return embedding_vector
            
        except multiprocessing.TimeoutError:
            logger.error(f"LMH Instance [{self.instance_id}]: Timeout embedding query in subprocess.", exc_info=True)
            return [0.0] * (self._embedding_model_dimension or 1024)
        except Exception as e_async_embed_query:
            logger.error(f"LMH Instance [{self.instance_id}]: Error submitting query embedding task to subprocess: {e_async_embed_query}", exc_info=True)
            return [0.0] * (self._embedding_model_dimension or 1024)
    
    def get_embedding_dimension(self) -> Optional[int]:
        if self._embedding_model_dimension is None:
             logger.warning(f"LMH Instance [{self.instance_id}]: get_embedding_dimension() called but dimension is not yet known. "
                            "It should be fetched by calling an embedding method first or during initialization.")
        return self._embedding_model_dimension

    async def generate_text_with_local_llm(self, messages: List[Dict[str,str]], temperature: float = 0.1, max_tokens: int = 1024, stop: Optional[List[str]]=None) -> Optional[str]:
        if not self.llm_model:
            logger.error(f"LMH Instance [{self.instance_id}]: LLM model is not loaded. Cannot generate text.")
            return None
        
        logger.info(f"LMH Instance [{self.instance_id}]: Generating text with local LLM. Message count: {len(messages)}")
        
        def _blocking_llm_call():
            try:
                if not hasattr(self.llm_model, 'create_chat_completion'):
                    logger.error(f"LMH Instance [{self.instance_id}]: self.llm_model is not a valid Llama instance for generation.")
                    return None

                completion_params = {
                    "messages": messages,
                    "temperature": temperature,
                    "max_tokens": max_tokens,
                }
                if stop:
                    completion_params["stop"] = stop
                
                response = self.llm_model.create_chat_completion(**completion_params) 
                
                if response and response.get("choices") and response["choices"][0].get("message"):
                    content = response["choices"][0]["message"].get("content")
                    logger.info(f"LMH Instance [{self.instance_id}]: LLM generation successful (sync part). Output (first 100 chars): {str(content)[:100]}...")
                    return content
                else:
                    logger.warning(f"LMH Instance [{self.instance_id}]: LLM generation did not return expected content (sync part). Response: {response}")
                    return None
            except Exception as e_sync:
                logger.error(f"LMH Instance [{self.instance_id}]: Error during synchronous LLM call: {e_sync}", exc_info=True)
                return None

        try:
            generated_content = await asyncio.to_thread(_blocking_llm_call)
            return generated_content
        except Exception as e_async:
            logger.error(f"LMH Instance [{self.instance_id}]: Error in asyncio.to_thread for LLM call: {e_async}", exc_info=True)
            return None

    def close_embedding_pool(self):
        """Safely close and join the multiprocessing pool."""
        logger.info(f"LMH Instance [{self.instance_id}]: Attempting to close embedding pool...")
        if self._embedding_pool:
            try:
                self._embedding_pool.close()
                self._embedding_pool.join()
                self._embedding_pool = None
                logger.info(f"LMH Instance [{self.instance_id}]: Embedding pool closed and joined successfully.")
            except Exception as e_close:
                logger.error(f"LMH Instance [{self.instance_id}]: Error closing embedding pool: {e_close}", exc_info=True)
                if self._embedding_pool:
                    try:
                        logger.warning(f"LMH Instance [{self.instance_id}]: Pool close/join failed, attempting terminate.")
                        self._embedding_pool.terminate()
                        self._embedding_pool.join()
                    except Exception as e_term:
                        logger.error(f"LMH Instance [{self.instance_id}]: Error terminating embedding pool: {e_term}", exc_info=True)
                self._embedding_pool = None
        else:
            logger.info(f"LMH Instance [{self.instance_id}]: Embedding pool was not initialized or already closed.")

    def __del__(self):
        """Destructor to ensure the pool is closed when the object is garbage collected."""
        logger.info(f"LMH Instance [{self.instance_id}]: __del__ called. Ensuring embedding pool is closed.")
        self.close_embedding_pool()
```

File: zhz_rag/llm/rag_prompts.py
--------------------------------
```python
# /home/zhz/zhz_rag/llm/rag_prompts.py
from typing import List, Dict, Any
from zhz_rag.config.constants import NEW_KG_SCHEMA_DESCRIPTION


# 可以将 NO_ANSWER_PHRASE_ANSWER_CLEAN 也移到这里，或者从 constants.py 导入
NO_ANSWER_PHRASE_ANSWER_CLEAN = "根据目前提供的资料，我无法找到关于您问题的明确信息。" # 保持与 llm_interface.py 一致

def get_answer_generation_messages(user_query: str, context_str: str) -> List[Dict[str, str]]:
    """
    构建用于从上下文中生成答案的LLM输入messages。
    """
    system_prompt_for_answer = f"""
你是一个非常严谨、客观且专业的AI问答助手。你的核心任务是根据【上下文信息】回答【用户问题】。

**核心指令与行为准则：**

1.  **【绝对忠实于上下文】**: 你的回答【必须且只能】使用【上下文信息】中明确提供的文字和事实。严禁进行任何形式的推断、联想、猜测或引入外部知识。如果上下文信息不足或不相关，请明确指出。
2.  **【逐点核实与直接证据】**: 对于用户问题中的每一个具体信息点或子问题，你都必须在【上下文信息】中找到清晰、直接的证据来支持你的回答。如果没有直接证据，则视为无法回答该点。
3.  **【引用来源 (如果可能且适用)】**: 如果你的答案直接引用或高度依赖【上下文信息】中的特定片段，请尽可能简要地指出信息来源。例如，如果上下文片段被标记了来源（如“来源文档A第3段”），你可以说“根据文档A第3段，...”。如果上下文没有明确的来源标记，则无需强行编造。**此项为次要优先级，准确回答问题是首要的。**
4.  **【处理无法回答的情况】**:
 *   **完全无法回答**: 如果【上下文信息】完全不包含与【用户问题】相关的任何信息，或者无法找到任何直接证据来回答问题的任何部分，请【只回答】：“{NO_ANSWER_PHRASE_ANSWER_CLEAN}”
 *   **部分无法回答**: 如果【用户问题】包含多个子问题或方面，而【上下文信息】只能回答其中的一部分：
     *   请只回答你能找到直接证据支持的部分。
     *   对于【上下文信息】中没有直接证据支持的其他子问题或方面，请明确指出，例如：“关于您提到的[某子问题/方面]，提供的上下文中未包含相关信息。”
     *   **禁止**对未提供信息的部分进行任何形式的猜测或尝试回答。
5.  **【答案风格：专业、简洁、直接】**:
 *   回答应直接针对用户问题，避免不必要的寒暄或冗余信息。
 *   语言表达应专业、客观、清晰易懂。
 *   如果答案包含多个要点，可以使用简洁的列表格式。
6.  **【避免重复与冗余】**: 如果多个上下文片段提供了相同的信息，请综合它们并给出不重复的答案。
/no_think

请严格遵守以上指令，以最高的准确性和忠实度来完成回答。
"""
    messages = [
        {"role": "system", "content": system_prompt_for_answer},
        {"role": "user", "content": f"用户问题: {user_query}\n\n上下文信息:\n{context_str}"}
    ]
    return messages

def get_clarification_question_messages(original_query: str, uncertainty_reason: str) -> List[Dict[str, str]]:
    """
    构建用于生成澄清问题的LLM输入messages。
    """
    system_prompt_for_clarification = f"""你的【唯一任务】是根据用户提供的【用户原始查询】和【不确定性原因】，生成一个【简洁、明确、友好且直接的澄清问句】。
    

**【严格的输出要求】**
*   你的【最终且唯一】的输出【必须】是这个澄清问句本身。
*   【绝对禁止】输出任何思考过程、解释、前缀、后缀或任何与澄清问句无关的文字。
*   澄清问句本身不应包含用户的原始查询或不确定性原因的复述。
/no_think


**示例：**

<example>
  <user_original_query>帮我查查天气</user_original_query>
  <uncertainty_reason>缺少地点信息</uncertainty_reason>
  <assistant_clarification_question>请问您想查询哪个城市的天气呢？</assistant_clarification_question>
</example>

<example>
  <user_original_query>分析一下销售数据</user_original_query>
  <uncertainty_reason>用户没有说明具体想对销售数据做什么操作，例如是汇总、筛选还是查找特定记录。</uncertainty_reason>
  <assistant_clarification_question>请问您希望对销售数据进行哪种具体操作，例如汇总统计、筛选特定条件，还是查找某些记录？</assistant_clarification_question>
</example>

<example>
  <user_original_query>给我推荐一些关于人工智能的书籍</user_original_query>
  <uncertainty_reason>用户没有说明偏好的人工智能子领域或书籍类型（入门/进阶/技术/哲学等）。</uncertainty_reason>
  <assistant_clarification_question>您对人工智能的哪个子领域或什么类型的书籍（如入门、技术实践、哲学探讨等）更感兴趣？</assistant_clarification_question>
</example>

<example>
  <user_original_query>我们公司的年假政策是怎么样的？</user_original_query>
  <uncertainty_reason>缺少公司名称，无法定位到具体的年假政策文档。</uncertainty_reason>
  <assistant_clarification_question>请问您的公司全称是什么？</assistant_clarification_question>
</example>

<example>
  <user_original_query>处理一下这个文件。</user_original_query>
  <uncertainty_reason>用户没有说明要对文件进行何种处理，也没有指明是哪个文件。</uncertainty_reason>
  <assistant_clarification_question>请问您希望对哪个文件进行什么具体操作呢？</assistant_clarification_question>
</example>
"""
    user_content = f"""用户原始查询: {original_query}
不确定性原因: {uncertainty_reason}

你应该输出的澄清问句:""" # 改为“澄清问句”

    messages = [
        {"role": "system", "content": system_prompt_for_clarification},
        {"role": "user", "content": user_content}
    ]
    return messages

# --- 精简的Cypher模板定义 (只保留一个核心模板) ---
SIMPLIFIED_CYPHER_TEMPLATES = [
    {
        "id": "template_find_entity_attributes_by_text_label",
        "description": "根据提供的实体文本和实体标签，查找该实体的所有基本属性。",
        "template": "MATCH (n:ExtractedEntity {{text: $entity_text, label: $entity_label}}) RETURN n.text AS text, n.label AS label, n.id_prop AS id_prop LIMIT 1",
        "params_needed": ["entity_text", "entity_label"]
    }
]

def get_cypher_generation_messages_with_templates(user_question: str) -> List[Dict[str, str]]: # 函数名保持一致
    """
    构建用于（基于【单个指定模板】）生成Cypher查询的LLM输入messages。
    这个版本用于测试模型对单个模板的参数提取能力。
    """
    
    # 在这个测试版本中，我们假设总是使用第一个（也是唯一一个）模板
    selected_template = SIMPLIFIED_CYPHER_TEMPLATES[0]
    
    template_description_for_prompt = f"""你将使用以下Cypher查询模板：
Template ID: {selected_template['id']}
Description: {selected_template['description']}
Cypher Structure: {selected_template['template']}
Parameters Needed: {', '.join(selected_template['params_needed'])}
"""

    system_prompt_for_cypher = f"""你是一个精确的参数提取助手。你的任务是根据用户问题，为下面提供的【唯一Cypher查询模板】提取参数，并构建一个Cypher查询。

**【图谱Schema核心部分参考】**
(你主要关注 `:ExtractedEntity` 节点及其属性: `text`, `label`, `id_prop`。其中 `label` 的常见值是 "PERSON", "ORGANIZATION", "TASK"。)
{NEW_KG_SCHEMA_DESCRIPTION} 
# ^^^ Schema描述已包含输出JSON格式 {{"status": "success/unable_to_generate", "query": "..."}} 的指导，请严格遵循该JSON输出格式。

**【当前需要填充的Cypher查询模板】**
{template_description_for_prompt}

**【你的任务与输出要求】**
1.  仔细分析【用户问题】，理解其核心查询意图。
2.  判断该意图是否与提供的【当前需要填充的Cypher查询模板】描述相符。
3.  如果相符：
    a.  从【用户问题】中提取填充该模板所需的所有【Parameters Needed】。确保参数值与Schema中的实体文本和标签格式相符（例如，标签应为大写 "PERSON", "ORGANIZATION", "TASK"）。
    b.  将提取的参数值替换到模板的Cypher语句中（例如，`$entity_text` 替换为提取到的实体名）。
    c.  最终输出一个JSON对象，格式为：`{{"status": "success", "query": "填充好参数的Cypher语句"}}`。
4.  如果不相符（例如，用户问题意图与模板描述不符，或无法从问题中提取到模板所需的所有关键参数）：
    a.  最终输出一个JSON对象，格式为：`{{"status": "unable_to_generate", "query": "无法生成Cypher查询."}}`。
5.  【绝对禁止】输出任何除了上述指定JSON对象之外的文本、解释或思考过程。


**【处理示例】**
<example>
  <user_question>我想知道张三的详细信息。</user_question>
  <assistant_output_json>{{
    "status": "success",
    "query": "MATCH (n:ExtractedEntity {{text: '张三', label: 'PERSON'}}) RETURN n.text AS text, n.label AS label, n.id_prop AS id_prop LIMIT 1"
  }}</assistant_output_json>
</example>
<example>
  <user_question>项目Alpha的文档编写任务是什么？</user_question>
  <assistant_output_json>{{
    "status": "success",
    "query": "MATCH (n:ExtractedEntity {{text: '项目alpha的文档编写任务', label: 'TASK'}}) RETURN n.text AS text, n.label AS label, n.id_prop AS id_prop LIMIT 1"
  }}</assistant_output_json>
</example>
<example>
  <user_question>法国的首都是哪里？</user_question>
  <assistant_output_json>{{
    "status": "unable_to_generate",
    "query": "无法生成Cypher查询."
  }}</assistant_output_json>
</example>
"""
    user_content = f"用户问题: {user_question}"

    messages = [
        {"role": "system", "content": system_prompt_for_cypher},
        {"role": "user", "content": user_content}
    ]
    return messages

# --- 新增：实体与关系意图提取的提示词生成函数 ---
def get_entity_relation_extraction_messages(user_question: str) -> List[Dict[str, str]]:
    """
    构建用于从用户查询中提取核心实体和关系意图的LLM输入messages。
    目标是输出一个符合 ExtractedEntitiesAndRelationIntent Pydantic 模型结构的纯净JSON对象。
    这个版本的Prompt极度强调JSON输出格式。
    """
    import re
    match = re.search(r'label\s*:\s*STRING\s*\(实体类型。\s*允许的值\s*:\s*("([^"]+)"(?:,\s*"([^"]+)")*)\)', NEW_KG_SCHEMA_DESCRIPTION)
    allowed_entity_labels_str = "PERSON, ORGANIZATION, TASK, DOCUMENT, PROJECT, REGION, PRODUCT, OTHER"
    if match:
        labels_group = match.group(1)
        extracted_labels = re.findall(r'"([^"]+)"', labels_group)
        if extracted_labels:
            allowed_entity_labels_str = ", ".join(extracted_labels)
            if "OTHER" not in extracted_labels:
                 allowed_entity_labels_str += ", OTHER"

    # --- V3 "最最严格" Prompt ---
    system_prompt_for_entity_extraction = f"""<|im_start|>system
USER_QUERY_TO_PROCESS:
{user_question}

TASK: Analyze USER_QUERY_TO_PROCESS. Output ONLY a valid JSON object.
NO EXPLANATIONS. NO EXTRA TEXT. NO MARKDOWN. JUST JSON.

JSON_OUTPUT_SCHEMA:
{{
  "entities": [
    {{"text": "string, extracted entity text from USER_QUERY_TO_PROCESS", "label": "string, entity type from: [{allowed_entity_labels_str}], or OTHER"}}
  ],
  "relation_hint": "string, relation described in USER_QUERY_TO_PROCESS, or empty string"
}}

RULES:
1. Max 2 entities in "entities" array. If none, "entities" is `[]`.
2. "label" MUST be from the provided list or "OTHER".
3. If no relation_hint, value is `""`.
4. If USER_QUERY_TO_PROCESS yields no entities or relation, output: `{{"entities": [], "relation_hint": ""}}`

YOUR_VALID_JSON_OUTPUT_ONLY:<|im_end|>""" # <--- 结尾引导更加直接

    messages = [
        {"role": "system", "content": system_prompt_for_entity_extraction}
    ]
    return messages


# 用于Dagster流水线中，从单个文本块抽取KG的提示词
KG_EXTRACTION_SINGLE_CHUNK_PROMPT_TEMPLATE_V1 = """
你是一个信息抽取助手。请从以下提供的文本中抽取出所有的人名(PERSON)、组织机构名(ORGANIZATION)和任务(TASK)实体。
同时，请抽取出以下两种关系：
1. WORKS_AT (当一个人在一个组织工作时，例如：PERSON WORKS_AT ORGANIZATION)
2. ASSIGNED_TO (当一个任务分配给一个人时，例如：TASK ASSIGNED_TO PERSON)

请严格按照以下JSON格式进行输出，不要包含任何额外的解释或Markdown标记：
{{
  "entities": [
    {{"text": "实体1原文", "label": "实体1类型"}},
    ...
  ],
  "relations": [
    {{"head_entity_text": "头实体文本", "head_entity_label": "头实体类型", "relation_type": "关系类型", "tail_entity_text": "尾实体文本", "tail_entity_label": "尾实体类型"}},
    ...
  ]
}}
如果文本中没有可抽取的实体或关系，请返回一个空的对应列表 (例如 {{"entities": [], "relations": []}})。

文本：
"{text_to_extract}"
""" # <--- 末尾引导词已删除

# 用于Dagster流水线中，从一批文本块抽取KG的提示词
KG_EXTRACTION_BATCH_PROMPT_TEMPLATE_V1 = """
你是一个信息抽取助手。你的任务是处理下面编号的【文本块列表】中的每一个文本块。
对于列表中的【每一个文本块】，请独立地抽取出所有的人名(PERSON)、组织机构名(ORGANIZATION)和任务(TASK)实体，以及它们之间可能存在的WORKS_AT和ASSIGNED_TO关系。

【输出格式要求】:
你的最终输出【必须】是一个JSON数组。
这个数组中的每个元素都对应输入【文本块列表】中相应顺序的文本块的抽取结果。
每个元素的结构【必须】严格符合以下JSON Schema：
{{
  "entities": [ 
    {{"text": "实体原文", "label": "实体类型"}}, 
    ... 
  ],
  "relations": [
    {{"head_entity_text": "头实体文本", "head_entity_label": "头实体类型", "relation_type": "关系类型", "tail_entity_text": "尾实体文本", "tail_entity_label": "尾实体类型"}},
    ...
  ]
}}
如果某个文本块中沒有可抽取的实体或关系，则其在JSON数组中对应的元素应为：{{"entities": [], "relations": []}}。
【绝对禁止】在最终的JSON数组之外包含任何其他文本、解释或Markdown标记。

【待处理的文本块列表】:
{formatted_text_block_list}
""" 

# GBNF for Knowledge Graph Extraction (proven stable with Qwen3-1.7B and create_completion)
KG_EXTRACTION_GBNF_STRING = r"""
root ::= "{" space "\"entities\"" ":" space entities "," space "\"relations\"" ":" space relations "}"

space ::= ([ \t\n\r])*
string ::= "\"" (char)* "\""
char ::= [^"\\\x7F\x00-\x1F] | "\\\\" (["\\bfnrt] | "u" [0-9a-fA-F]{4})

entities ::= "[" space (entities-item ("," space entities-item)*)? space "]"
entities-item ::= "{" space "\"text\"" ":" space string "," space "\"label\"" ":" space string "}"

relations ::= "[" space (relations-item ("," space relations-item)*)? space "]"
relations-item ::= "{" space "\"head_entity_text\"" ":" space string "," space "\"head_entity_label\"" ":" space string "," space "\"relation_type\"" ":" space string "," space "\"tail_entity_text\"" ":" space string "," space "\"tail_entity_label\"" ":" space string "}"
"""
```

File: zhz_rag/task_management/__init__.py
-----------------------------------------
--- File is empty ---

File: zhz_rag/task_management/db_models.py
------------------------------------------
```python
# zhz_agent/database_models.py
from sqlalchemy import Column, String, DateTime, Integer, Text, Enum as SQLAlchemyEnum, ForeignKey, Boolean, JSON
from sqlalchemy.sql import func
import uuid

# --- [修改] 从 pydantic_models 导入枚举 -> 改为绝对导入 ---
from zhz_rag.config.pydantic_models import TaskStatus, ReminderMethod

# --- [修改] 从新的 database.py 导入 Base -> 改为绝对导入 ---
from zhz_rag.utils.db_utils import Base # <--- 确保只从这里导入 Base #

class TaskDB(Base): # 命名为 TaskDB 以区分 Pydantic 的 TaskModel
    __tablename__ = "tasks"

    id = Column(String, primary_key=True, index=True, default=lambda: str(uuid.uuid4()))
    title = Column(String, index=True, nullable=False)
    description = Column(Text, nullable=True) #
    status = Column(SQLAlchemyEnum(TaskStatus), default=TaskStatus.PENDING, nullable=False) #
    created_at = Column(DateTime(timezone=True), server_default=func.now(), nullable=False) #
    updated_at = Column(DateTime(timezone=True), server_default=func.now(), onupdate=func.now(), nullable=False) #
    due_date = Column(DateTime(timezone=True), nullable=True) #
    reminder_time = Column(DateTime(timezone=True), nullable=True) #
    reminder_offset_minutes = Column(Integer, nullable=True) #
    reminder_methods = Column(JSON, default=[ReminderMethod.NOTIFICATION.value], nullable=False) #
    priority = Column(Integer, default=0, nullable=False) #
    tags = Column(JSON, default=[], nullable=False) #
    action_type = Column(String, nullable=True) #
    action_payload = Column(JSON, default={}, nullable=True) #
    execution_result = Column(Text, nullable=True) #
    last_executed_at = Column(DateTime(timezone=True), nullable=True) #

    def __repr__(self):
        return f"<TaskDB(id={self.id}, title='{self.title}', status='{self.status.value}')>"
```

File: zhz_rag/task_management/jobs.py
-------------------------------------
```python
# zhz_agent/task_jobs.py
from datetime import datetime
from typing import Dict, Any
import os
import traceback
import httpx # <--- 确保 httpx 已导入
import json # <--- 确保 json 已导入

# 从 .database 导入 database 对象以便查询任务详情
# 从 .pydantic_models 导入 TaskModel 以便类型转换
# 从 .main 导入 scheduler 以便在需要时重新调度（虽然通常作业函数不直接操作调度器）
# 更好的做法是通过参数传递必要的信息，而不是依赖全局导入
WINDOWS_HOST_IP = os.getenv("WINDOWS_HOST_IP_FOR_WSL", "192.168.3.11") # <--- 请务必替换为您真实的Windows IP
LOCAL_AGENT_PORT = os.getenv("LOCAL_AGENT_PORT", "8003") # 与 local_agent_app.py 中的端口一致

# 如果 WINDOWS_HOST_IP 仍然是占位符，给出提示
if WINDOWS_HOST_IP == "在此处填写您上一步找到的Windows主机IP":
    print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
    print("REMINDER_JOB WARNING: WINDOWS_HOST_IP 未正确设置在 task_jobs.py 中!")
    print("请编辑 task_jobs.py 文件，将 '在此处填写您上一步找到的Windows主机IP' 替换为实际IP地址。")
    print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")

LOCAL_AGENT_NOTIFY_URL = f"http://{WINDOWS_HOST_IP}:{LOCAL_AGENT_PORT}/notify"

async def send_task_reminder(task_id: str, task_title: str, reminder_methods: list):
    """
    实际发送任务提醒的函数。
    """
    print(f"REMINDER_JOB: [{datetime.utcnow()}] 正在为任务 '{task_id}' - '{task_title}' 发送提醒。")
    for method in reminder_methods:
        if method == "notification": # 假设 ReminderMethod.NOTIFICATION.value 是 "notification"
            print(f"  REMINDER_JOB: 尝试通过 Local Agent 发送桌面通知: '{task_title}'")
            try:
                async with httpx.AsyncClient(timeout=10.0) as client:
                    response = await client.post(
                        LOCAL_AGENT_NOTIFY_URL,
                        json={"title": f"任务提醒: {task_title}", "message": f"任务 '{task_title}' 即将到期或需要关注。"}
                    )
                    response.raise_for_status() # Raise an exception for bad status codes
                    print(f"  REMINDER_JOB: 本地代理通知请求发送成功. 状态: {response.status_code}")
            except httpx.RequestError as e:
                print(f"  REMINDER_JOB: 通过本地代理发送通知失败 (RequestError): {e}")
                traceback.print_exc()
            except Exception as e:
                print(f"  REMINDER_JOB: 通过本地代理发送通知失败 (General Error): {e}")
                traceback.print_exc()
        # elif method == "email": #
        #     print(f"  REMINDER_JOB: 模拟发送邮件提醒...")

async def execute_task_action(task_id: str, action_type: str, action_payload: Dict[str, Any]):
    """
    实际执行任务动作的函数。
    """
    print(f"EXECUTION_JOB: [{datetime.utcnow()}] 正在为任务 '{task_id}' 执行动作 '{action_type}'。")
    print(f"  EXECUTION_JOB: 动作参数: {action_payload}")

    final_result = f"动作 '{action_type}' 已模拟执行。"
    success = True

    if action_type == "navigate":
        destination = action_payload.get("destination")
        if destination:
            print(f"  EXECUTION_JOB: 模拟导航到 '{destination}'...")
            final_result = f"已模拟为导航到 '{destination}' 准备好路线。"
        else:
            final_result = "导航动作失败：缺少目的地。"
            success = False
    elif action_type == "log_event":
        event_details = action_payload.get("event_details", "无详情")
        print(f"  EXECUTION_JOB: 记录事件: '{event_details}'")
        final_result = f"事件 '{event_details}' 已记录。"
    else:
        final_result = f"未知的动作类型: {action_type}"
        success = False

    # 更新数据库中的任务状态和结果 (需要访问数据库)
    # 这部分逻辑最好通过API调用或服务层来完成，以避免循环导入和分散DB操作
    # 这里我们只打印信息，实际应用中需要实现DB更新
    print(f"  EXECUTION_JOB: 任务 '{task_id}' 执行完毕。结果: {final_result}, 状态: {'COMPLETED' if success else 'FAILED'}")
```

File: zhz_rag/finetuning/__init__.py
------------------------------------
--- File is empty ---

File: zhz_rag/finetuning/refine_answer_data.py
----------------------------------------------
```python
# zhz_agent/refine_answer_finetune_data.py
import json
import os
import pandas as pd
from typing import List, Dict, Any, Optional
import glob
from datetime import datetime

# 假设 utils.py 和 constants.py 在同一个 zhz_agent 包内
try:
    from zhz_rag.utils.common_utils import (
    RAG_INTERACTION_LOGS_DIR,
    EVALUATION_RESULTS_LOGS_DIR,
    FINETUNING_GENERATED_DATA_DIR,
    find_latest_rag_interaction_log # 确保这个也被导入
)
    from zhz_rag.config.pydantic_models import RetrievedDocument
    # NO_ANSWER_PHRASE_ANSWER_CLEAN 将从 llm.py 导入，或者在constants.py中定义
    # 我们需要与 llm.py -> generate_answer_from_context 一致的 "无法回答" 短语
    from zhz_rag.llm.llm_interface import NO_ANSWER_PHRASE_ANSWER_CLEAN 
except ImportError as e:
    print(f"ERROR: Could not import necessary modules for refine_answer_finetune_data: {e}")
    exit(1)

import logging

# 配置此脚本的logger
refine_answer_logger = logging.getLogger("RefineAnswerFinetuneDataLogger")
refine_answer_logger.setLevel(logging.INFO)
if not refine_answer_logger.hasHandlers():
    _console_handler = logging.StreamHandler()
    _formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
    _console_handler.setFormatter(_formatter)
    refine_answer_logger.addHandler(_console_handler)
    refine_answer_logger.info("--- RefineAnswerFinetuneDataLogger configured ---")

# --- 配置 ---
RAG_LOG_DIR = "zhz_rag/stored_data/evaluation_results_logs/"
EVAL_LOG_DIR = "zhz_rag/stored_data/evaluation_results_logs/"
FINETUNE_DATA_DIR = "zhz_rag/finetuning/generated_data/"
os.makedirs(FINETUNE_DATA_DIR, exist_ok=True)

# --- 与 run_batch_answer_evaluation.py 中类似的上下文格式化函数 ---
def format_contexts_for_prompt(context_docs_raw: List[Dict[str, Any]]) -> str:
    """
    将从日志中解析出的上下文文档列表格式化为单一字符串，用于构建LLM的输入Prompt。
    这个格式应该与 llm.py -> generate_answer_from_context 中构建上下文的方式一致。
    """
    context_strings_for_llm = []
    if not context_docs_raw:
        return "No context provided."
        
    for i, doc_data in enumerate(context_docs_raw):
        try:
            # 尝试使用RetrievedDocument模型解析，如果原始日志中已经是这个结构
            # 但通常日志中可能是字典列表
            doc_content = doc_data.get("content", "[Content not available]")
            doc_source = doc_data.get("source_type", "unknown_source")
            doc_score = doc_data.get("score")
            
            # 与 rag_service.py 中准备上下文给LLM的格式保持一致
            # 在 rag_service.py 中是:
            # f"Source Type: {doc.source_type}, Score: {doc.score:.4f}\nContent: {doc.content}"
            # 我们这里也尽量模拟，但日志中的score可能不存在或格式不同
            header = f"Source Type: {doc_source}"
            if doc_score is not None:
                try:
                    header += f", Score: {float(doc_score):.4f}"
                except ValueError:
                    header += f", Score: {doc_score}" # 如果分数不是数字，直接用原始值
            
            context_strings_for_llm.append(f"{header}\nContent: {doc_content}")

        except Exception as e:
            refine_answer_logger.warning(f"Could not parse a context document fully for prompt: {doc_data}. Error: {e}")
            content = doc_data.get("content", "[Content not available]")
            context_strings_for_llm.append(f"Content: {content}") # 简化版

    return "\n\n---\n\n".join(context_strings_for_llm) if context_strings_for_llm else "No context provided."


def construct_qwen_answer_input_prompt(user_question: str, formatted_context: str) -> str:
    """
    根据用户问题和格式化的上下文构建Qwen生成答案时的完整输入Prompt。
    这个函数必须与 llm.py 中 generate_answer_from_context 内部构建Prompt的逻辑完全一致。
    """
    # --- 从 llm.py 的 generate_answer_from_context 函数复制并粘贴完整的 prompt 模板 ---
    # 注意：这里需要确保模板与 llm.py 中的完全一致
    prompt = f"""
<|im_start|>system
你是一个AI问答助手。你的任务是根据【上下文信息】回答【用户问题】。

**核心指令：**

1.  **尝试直接回答：** 请首先仔细阅读【上下文信息】，如果其中包含能直接回答【用户问题】的内容，请用上下文中的信息直接、简洁地回答。
2.  **忠实原文：** 你的回答必须严格基于【上下文信息】，禁止加入任何外部知识或个人观点。
3.  **如果无法回答：** 如果你分析了【上下文信息】后，确认其中确实没有能回答【用户问题】的明确信息，那么请只回答以下这句话：
    "根据目前提供的资料，我无法找到关于您问题的明确信息。"
    **不要添加任何其他解释、建议或反问。**

**请直接给出答案，或者只给出上述那句固定的“无法找到信息”的回复。**
<|im_start|>user
用户问题: {user_question}

上下文信息:
{formatted_context}
<|im_end|>
<|im_start|>assistant
"""
    return prompt

def load_logs_to_dict(filepath: str, key_field: str = "interaction_id") -> Dict[str, Dict[str, Any]]:
    """将JSONL文件加载到一个以指定字段为键的字典中。"""
    data_dict = {}
    if not os.path.exists(filepath):
        refine_answer_logger.error(f"Log file not found: {filepath}")
        return data_dict
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                log_entry = json.loads(line.strip())
                if key_field in log_entry and log_entry[key_field]: # 确保key_field的值不是None或空
                    data_dict[log_entry[key_field]] = log_entry
                elif key_field == "original_interaction_id_ref" and log_entry.get("original_interaction_id_ref"):
                    data_dict[log_entry["original_interaction_id_ref"]] = log_entry
            except json.JSONDecodeError:
                refine_answer_logger.warning(f"Skipping malformed JSON line in {filepath}")
    return data_dict

def generate_finetune_samples_for_answer(
    rag_interaction_logs: Dict[str, Dict[str, Any]],
    answer_evaluation_logs: Dict[str, Dict[str, Any]]
) -> List[Dict[str, str]]:
    finetune_samples = []
    processed_ids = set()

    refine_answer_logger.info(f"Processing {len(rag_interaction_logs)} RAG interaction logs and {len(answer_evaluation_logs)} Answer evaluation logs.")

    for interaction_id, rag_log in rag_interaction_logs.items():
        if rag_log.get("task_type") != "rag_query_processing_success":
            continue

        if interaction_id in processed_ids:
            continue
        processed_ids.add(interaction_id)

        user_question = rag_log.get("user_query")
        qwen_generated_answer_raw = rag_log.get("processed_llm_output") # Qwen的原始答案
        # retrieved_context_docs 在 rag_log 中可能是 "retrieved_context_docs" 或 "retrieved_documents_summary"
        # 我们需要原始的、完整的上下文文档
        retrieved_context_docs_raw = rag_log.get("retrieved_context_docs") 
        
        if not retrieved_context_docs_raw and rag_log.get("debug_info"): # 尝试从debug_info获取
             retrieved_context_docs_raw = rag_log.get("debug_info",{}).get("retrieved_context_docs")


        if qwen_generated_answer_raw is None or not qwen_generated_answer_raw.strip():
            qwen_generated_answer = NO_ANSWER_PHRASE_ANSWER_CLEAN # 空答案视为无法回答
        else:
            qwen_generated_answer = qwen_generated_answer_raw.strip()

        if not user_question or not retrieved_context_docs_raw:
            refine_answer_logger.warning(f"Skipping RAG log {interaction_id} due to missing user_question or retrieved_context_docs.")
            continue
        
        # 构建Prompt
        formatted_contexts_for_prompt = format_contexts_for_prompt(retrieved_context_docs_raw)
        qwen_answer_input_prompt = construct_qwen_answer_input_prompt(user_question, formatted_contexts_for_prompt)

        ideal_answer_output = None
        source_of_ideal = "unknown"
        gemini_scores_for_log = {}

        eval_log = answer_evaluation_logs.get(interaction_id)

        if eval_log and eval_log.get("eval_llm_processed_output_json"):
            eval_json = eval_log["eval_llm_processed_output_json"]
            summary_eval = eval_json.get("evaluation_summary", {})
            dimensions_eval = eval_json.get("dimensions", {})
            
            overall_score_str = summary_eval.get("overall_answer_quality_score")
            faithfulness_score_str = dimensions_eval.get("faithfulness", {}).get("score")
            relevance_score_str = dimensions_eval.get("relevance", {}).get("score")
            completeness_score_str = dimensions_eval.get("completeness", {}).get("score")
            context_sufficiency = dimensions_eval.get("completeness", {}).get("context_sufficiency_assessment", "Unknown")
            gemini_suggestion_answer = eval_json.get("suggestion_for_answer_improvement", "").strip()

            try:
                overall_score = int(overall_score_str) if overall_score_str is not None else 0
                faithfulness_score = int(faithfulness_score_str) if faithfulness_score_str is not None else 0
                relevance_score = int(relevance_score_str) if relevance_score_str is not None else 0
                completeness_score = int(completeness_score_str) if completeness_score_str is not None else 0
                gemini_scores_for_log = {
                    "overall": overall_score,
                    "faithfulness": faithfulness_score,
                    "relevance": relevance_score,
                    "completeness": completeness_score,
                    "context_sufficiency": context_sufficiency
                }
            except (ValueError, TypeError) as e:
                refine_answer_logger.warning(f"Could not parse one or more scores for {interaction_id}: {e}")
                overall_score = faithfulness_score = relevance_score = completeness_score = 0
                gemini_scores_for_log = { # 记录解析失败
                    "overall": "parse_error", "faithfulness": "parse_error", 
                    "relevance": "parse_error", "completeness": "parse_error",
                    "context_sufficiency": context_sufficiency
                }
                
            # --- Completion选择逻辑 (改进版) ---
            ideal_answer_output = None # 重新初始化
            source_of_ideal = "unknown" # 重新初始化

            # 规则 0: Qwen的原始答案就是标准的“无法回答”短语
            is_qwen_standard_no_answer = (qwen_generated_answer == NO_ANSWER_PHRASE_ANSWER_CLEAN)

            # 规则 1: 上下文不足，且Qwen正确地给出了标准的“无法回答”
            if is_qwen_standard_no_answer and \
               context_sufficiency == "Insufficient" and \
               overall_score >= 4: # Gemini认为Qwen的这个“无法回答”是高质量的
                ideal_answer_output = NO_ANSWER_PHRASE_ANSWER_CLEAN
                source_of_ideal = "qwen_standard_no_answer_confirmed_by_gemini_context_insufficient"
            
            # 规则 2: 上下文不足，Qwen可能没有给出标准“无法回答”，但Gemini建议应指出信息不足
            elif not is_qwen_standard_no_answer and \
                 context_sufficiency == "Insufficient" and \
                 completeness_score <=2 and \
                 ("information is not available" in gemini_suggestion_answer.lower() or \
                  "context does not contain" in gemini_suggestion_answer.lower() or \
                  "cannot be answered from the context" in gemini_suggestion_answer.lower() or \
                  "should state that the information is not found" in gemini_suggestion_answer.lower()):
                ideal_answer_output = NO_ANSWER_PHRASE_ANSWER_CLEAN
                source_of_ideal = "gemini_suggests_no_answer_due_to_insufficient_context"

            # 规则 3: Gemini 整体评分很高 (例如 overall, faithfulness, relevance 都 >= 4)
            # 并且 Qwen 的答案不是标准的“无法回答”（如果已经是，则由规则1处理）
            elif not is_qwen_standard_no_answer and \
                 overall_score >= 4 and faithfulness_score >= 4 and relevance_score >= 4:
                ideal_answer_output = qwen_generated_answer
                source_of_ideal = "qwen_high_score_by_gemini"
                # 如果此时 Gemini 仍有改进建议，可以额外标记
                if gemini_suggestion_answer and \
                   gemini_suggestion_answer != "No improvement needed." and \
                   "suggestion" not in source_of_ideal: # 避免重复标记
                    source_of_ideal += "_with_minor_gemini_suggestion"


            # 规则 4: Qwen的答案评分不高，但Gemini给出了具体的改进建议
            # 我们将这类样本标记出来，completion暂时使用Qwen的答案，供人工审核和优化
            elif overall_score < 4 and \
                 gemini_suggestion_answer and \
                 gemini_suggestion_answer != "No improvement needed." and \
                 len(gemini_suggestion_answer) > 10: # 假设太短的建议可能不具体
                ideal_answer_output = qwen_generated_answer # 保留Qwen答案作为基础
                source_of_ideal = "qwen_low_score_with_gemini_suggestion_for_review"
                refine_answer_logger.info(f"Answer log {interaction_id} (Qwen: '{qwen_generated_answer[:100]}...') marked for review due to low score but has Gemini suggestion: '{gemini_suggestion_answer[:100]}...'")
            
            # 规则 5: 如果Qwen的答案是标准“无法回答”，但上下文其实是充分的，或者Gemini认为可以回答
            # 这通常意味着Qwen可能错误地判断无法回答，或者Gemini的评估与Qwen的判断不一致
            elif is_qwen_standard_no_answer and \
                 (context_sufficiency == "Sufficient" or (context_sufficiency == "Partially Sufficient" and completeness_score >=3)) and \
                 overall_score < 4 : # Gemini不认可这个“无法回答”
                ideal_answer_output = qwen_generated_answer # 保留Qwen的“无法回答”
                source_of_ideal = "qwen_no_answer_but_gemini_disagrees_or_context_sufficient_for_review"
                refine_answer_logger.info(f"Answer log {interaction_id}: Qwen said 'no answer', but Gemini scores/context sufficiency suggest it might be answerable. Marked for review. Gemini scores: {gemini_scores_for_log}, Suggestion: '{gemini_suggestion_answer[:100]}...'")

            # 规则 6: 其他所有情况，暂时跳过，等待更明确的规则或人工审核
            else:
                refine_answer_logger.info(f"Answer log {interaction_id} (Qwen: '{qwen_generated_answer[:100]}...') did not meet current finetune criteria. Needs manual review or rule adjustment. Gemini scores: {gemini_scores_for_log}, Suggestion: '{gemini_suggestion_answer[:100]}...'")
                continue

        else: # 没有有效的Gemini评估日志
            refine_answer_logger.warning(f"No valid Gemini evaluation found for Answer log {interaction_id}. Qwen's output: '{qwen_generated_answer[:100]}...'. Skipping for finetune data.")
            continue
            
        if ideal_answer_output is not None:
            finetune_samples.append({
                "prompt": qwen_answer_input_prompt,
                "completion": ideal_answer_output.strip(),
                "original_qwen_answer": qwen_generated_answer_raw.strip() if qwen_generated_answer_raw else NO_ANSWER_PHRASE_ANSWER_CLEAN, # 记录Qwen最原始的输出
                "gemini_scores": gemini_scores_for_log,
                "gemini_suggestion": gemini_suggestion_answer if eval_log and eval_log.get("eval_llm_processed_output_json") else None,
                "source_of_ideal": source_of_ideal,
                "interaction_id": interaction_id
            })

    refine_answer_logger.info(f"Generated {len(finetune_samples)} Answer finetuning samples.")
    return finetune_samples


if __name__ == "__main__":
    rag_log_file = find_latest_rag_interaction_log(RAG_INTERACTION_LOGS_DIR)
    
    eval_log_file = None
    if rag_log_file:
        rag_log_basename = os.path.basename(rag_log_file)
        date_str_match = "".join(filter(str.isdigit, rag_log_basename))
        if len(date_str_match) >= 8:
            date_str = date_str_match[:8]
            evaluation_name = "answer_gemini_flash" # 与 evaluation.py 中一致
            eval_file_name = f"eval_results_{evaluation_name}_{date_str}.jsonl"
            eval_log_file = os.path.join(EVAL_LOG_DIR, eval_file_name)
            refine_answer_logger.info(f"Attempting to load Answer evaluation results from: {eval_log_file}")
        else:
            refine_answer_logger.error(f"Could not reliably extract date from RAG log filename: {rag_log_basename}")

    if not rag_log_file or not eval_log_file or not os.path.exists(eval_log_file):
        refine_answer_logger.error("Required log files for answer finetune data generation not found. Exiting.")
    else:
        rag_interactions = load_logs_to_dict(rag_log_file, key_field="interaction_id")
        answer_evaluations = load_logs_to_dict(eval_log_file, key_field="original_interaction_id_ref")

        if rag_interactions and answer_evaluations:
            finetune_data = generate_finetune_samples_for_answer(rag_interactions, answer_evaluations)
            
            if finetune_data:
                today_for_filename = datetime.now().strftime("%Y%m%d")
                output_filepath = os.path.join(FINETUNING_GENERATED_DATA_DIR, f"answer_finetune_samples_{today_for_filename}.jsonl")
                
                with open(output_filepath, 'w', encoding='utf-8') as f_out:
                    for sample in finetune_data:
                        f_out.write(json.dumps(sample, ensure_ascii=False) + "\n")
                refine_answer_logger.info(f"Successfully saved {len(finetune_data)} Answer finetuning samples to: {output_filepath}")
                
                try:
                    df = pd.DataFrame(finetune_data)
                    csv_output_filepath = os.path.join(FINETUNING_GENERATED_DATA_DIR, f"answer_finetune_samples_review_{today_for_filename}.csv")
                    df.to_csv(csv_output_filepath, index=False, encoding='utf-8-sig')
                    refine_answer_logger.info(f"Reviewable CSV for answers saved to: {csv_output_filepath}")
                except Exception as e_csv:
                    refine_answer_logger.error(f"Failed to save answer review CSV: {e_csv}")
            else:
                refine_answer_logger.info("No answer finetuning samples were generated.")
        else:
            refine_answer_logger.error("Failed to load data from log files for answer finetuning.")
```

File: zhz_rag/finetuning/refine_cypher_data.py
----------------------------------------------
```python
# zhz_agent/refine_cypher_finetune_data.py
import json
import os
import pandas as pd
from typing import List, Dict, Any, Optional, Tuple
import glob
from datetime import datetime

# 假设 utils.py 和 constants.py 在同一个 zhz_agent 包内
try:
    from zhz_rag.utils.common_utils import get_interaction_log_filepath, get_evaluation_result_log_filepath, find_latest_rag_interaction_log # <--- 修改这里
    from zhz_rag.config.constants import NEW_KG_SCHEMA_DESCRIPTION
except ImportError as e:
    print(f"ERROR: Could not import necessary modules: {e}")
    # ... (错误处理)
    exit(1)

import logging

# 配置此脚本的logger
refine_logger = logging.getLogger("RefineFinetuneDataLogger")
refine_logger.setLevel(logging.INFO)
if not refine_logger.hasHandlers():
    _console_handler = logging.StreamHandler()
    _formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
    _console_handler.setFormatter(_formatter)
    refine_logger.addHandler(_console_handler)
    refine_logger.info("--- RefineFinetuneDataLogger configured ---")

# --- 配置 ---
# 原始RAG交互日志的目录 (包含cypher_generation类型)
RAG_LOG_DIR = "zhz_rag/stored_data/rag_interaction_logs/"
# Gemini评估结果日志的目录
EVAL_LOG_DIR = "zhz_rag/stored_data/evaluation_results_logs/"
# 输出微调数据文件的目录
FINETUNE_DATA_DIR = "zhz_rag/finetuning/generated_data/"
os.makedirs(FINETUNE_DATA_DIR, exist_ok=True)


def load_logs_to_dict(filepath: str, key_field: str = "interaction_id") -> Dict[str, Dict[str, Any]]:
    """将JSONL文件加载到一个以指定字段为键的字典中。"""
    data_dict = {}
    if not os.path.exists(filepath):
        refine_logger.error(f"Log file not found: {filepath}")
        return data_dict
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                log_entry = json.loads(line.strip())
                if key_field in log_entry:
                    data_dict[log_entry[key_field]] = log_entry
                # 对于评估日志，我们可能需要用 original_interaction_id_ref 作为键
                elif key_field == "original_interaction_id_ref" and log_entry.get("original_interaction_id_ref"):
                    data_dict[log_entry["original_interaction_id_ref"]] = log_entry
            except json.JSONDecodeError:
                refine_logger.warning(f"Skipping malformed JSON line in {filepath}")
    return data_dict

def construct_qwen_input_prompt(user_question: str, schema_description: str) -> str:
    """
    根据用户问题和Schema描述构建Qwen生成Cypher时的完整输入Prompt。
    这个函数应该与 llm.py 中 generate_cypher_query 内部构建Prompt的逻辑一致。
    """
    # 这是我们在 llm.py -> generate_cypher_query 中使用的Prompt模板
    # 我们需要确保这里的模板与Qwen实际接收到的一致
    # 注意：这里使用了最新的V7版本（或您当前使用的版本）的Schema描述作为基础
    # 如果您的 generate_cypher_query 中的模板不同，请相应调整
    prompt = f"""<|im_start|>system
你是顶级Neo4j Cypher查询生成专家。你的任务是根据用户问题和严格提供的【知识图谱Schema】，生成一个【语法正确】、【逻辑合理】且【高效】的Cypher查询。

**【核心指令与约束 - 必须严格遵守！】**

1.  **【Schema绝对绑定 - 最高优先级】**:
    *   你生成的Cypher查询中所有用到的【节点标签】、【关系类型】、【属性名称】及其对应的【数据类型】，都**必须严格存在于**下面提供的 "知识图谱Schema描述" 中。
    *   在构建查询的每一步，都要反复与Schema核对。**严禁臆断、猜测或使用任何Schema中未明确定义的元素。**
    *   **属性名称的大小写和确切拼写必须与Schema完全一致。**
    *   **关系类型的名称和方向必须与Schema完全一致。** 例如，如果Schema定义为 `(Person)-[:WORKS_ON]->(Project)`，则查询中不能是 `(Project)-[:WORKS_ON]->(Person)`，除非Schema中也定义了反向关系。

2.  **【纯净输出格式 - 严格要求】**:
    *   如果能生成有效查询，你的回答**必须只包含纯粹的Cypher查询语句本身**。
    *   如果根据问题和Schema无法生成有效的Cypher查询（例如，问题超出了Schema表达能力，或问题本身逻辑不通），则**必须只输出固定的短语：“无法生成Cypher查询。”**
    *   **绝对禁止**在有效的Cypher语句前后添加任何前缀（如“Cypher查询: ”）、后缀、解释、注释、markdown标记（如 ```cypher ```）或任何其他多余的文本。

3.  **【属性与值的使用】**:
    *   当在`WHERE`子句中对属性进行匹配时，确保值的类型与Schema中定义的属性类型一致。例如，如果`name`是字符串，则匹配 `name: '张三'`；如果`year`是数字，则匹配 `year: 2023`。
    *   对于数值计算（如`SUM`, `AVG`），**必须**使用Schema中明确指定的数字类型属性（例如，`SalesAmount`节点的 `numeric_amount`）。

4.  **【查询构建逻辑指引】**:
    *   **实体识别**: 准确识别用户问题中的核心实体及其在Schema中对应的节点标签和属性。
    *   **关系路径**: 基于问题和Schema构建清晰的`MATCH`路径。
    *   **条件过滤**: 使用`WHERE`子句添加必要的过滤条件。
    *   **结果返回**: 使用`RETURN`子句指定需要返回的信息，并用`AS`为返回的列指定清晰、合法的别名（字母或下划线开头）。
    *   **多步查询**: 对于需要关联多个信息点的问题，合理使用`WITH`传递中间结果。
    *   **聚合**: 如需统计或汇总，正确使用`COUNT()`, `SUM()`, `COLLECT()`等聚合函数。

**【知识图谱Schema描述】**:
{schema_description}

**【查询示例 - 严格基于上述Schema】**:

*   用户问题: "张三参与了哪个项目？"
    Cypher查询: MATCH (p:Person {{name: '张三'}})-[:WORKS_ON]->(proj:Project) RETURN proj.name AS projectName

*   用户问题: "华东区域2024年第一季度的销售额是多少？"
    Cypher查询: MATCH (r:Region {{name: '华东'}})-[:HAS_SALES_AMOUNT]->(sa:SalesAmount {{period: '2024年第一季度'}}) RETURN sa.numeric_amount AS salesAmount, sa.unit AS salesUnit

*   用户问题: "查询所有产品的名称。"
    Cypher查询: MATCH (prod:Product) RETURN prod.name AS productName

*   用户问题: "项目X有哪些人参与？"
    Cypher查询: MATCH (p:Person)-[:WORKS_ON]->(proj:Project {{name: '项目X'}}) RETURN p.name AS participantName

*   用户问题: "2024年第一季度所有区域的总销售额是多少？"
    Cypher查询: MATCH (r:Region)-[:HAS_SALES_AMOUNT]->(sa:SalesAmount {{period: '2024年第一季度'}}) RETURN sum(sa.numeric_amount) AS totalSales, sa.unit AS commonUnit LIMIT 1 
    (此查询假设所有相关销售额的单位是相同的，并取第一个出现的单位作为代表)

*   用户问题: "与新产品A相关的文档ID是什么？"
    Cypher查询: MATCH (p:Product {{name: '新产品A'}})-[:RELATED_TO]->(d:Document) RETURN d.id AS documentId

*   用户问题: "公司CEO是谁？" (假设Schema中没有CEO信息)
    Cypher查询: 无法生成Cypher查询。

现在，请根据以下用户问题和上述Schema及规则生成Cypher查询。
<|im_end|>
<|im_start|>user
用户问题: {user_question}
<|im_end|>
<|im_start|>assistant
"""
    return prompt

def generate_finetune_samples_for_cypher(
    rag_interaction_logs: Dict[str, Dict[str, Any]],
    cypher_evaluation_logs: Dict[str, Dict[str, Any]]
) -> List[Dict[str, str]]:
    """
    根据原始交互日志和Gemini评估日志，生成用于Cypher微调的样本。
    返回一个列表，每个元素是 {"prompt": "...", "completion": "..."}
    """
    finetune_samples = []
    processed_ids = set()

    refine_logger.info(f"Processing {len(rag_interaction_logs)} RAG interaction logs and {len(cypher_evaluation_logs)} Cypher evaluation logs.")

    for interaction_id, rag_log in rag_interaction_logs.items():
        if rag_log.get("task_type") != "cypher_generation":
            continue

        if interaction_id in processed_ids:
            continue
        processed_ids.add(interaction_id)

        user_question = rag_log.get("user_query")
        qwen_generated_cypher_raw = rag_log.get("processed_llm_output") # 这是Qwen原始输出

        # --- 改进点: 处理Qwen输出为空或仅包含空白的情况 ---
        if qwen_generated_cypher_raw is None or not qwen_generated_cypher_raw.strip():
            qwen_generated_cypher = "无法生成Cypher查询." # 将空输出也视为无法生成
            refine_logger.info(f"Interaction {interaction_id}: Qwen output was empty/None, treating as '无法生成Cypher查询.'.")
        else:
            qwen_generated_cypher = qwen_generated_cypher_raw.strip()


        qwen_input_prompt = rag_log.get("llm_input_prompt")
        if not qwen_input_prompt:
            if user_question:
                qwen_input_prompt = construct_qwen_input_prompt(user_question, NEW_KG_SCHEMA_DESCRIPTION)
            else:
                refine_logger.warning(f"Skipping Cypher log {interaction_id} due to missing user_question for prompt reconstruction.")
                continue
        
        if not user_question: # qwen_generated_cypher 已确保非None
            refine_logger.warning(f"Skipping Cypher log {interaction_id} due to missing user_question.")
            continue

        ideal_cypher_output = None
        source_of_ideal = "unknown"
        gemini_score_for_log = None # 用于记录

        eval_log = cypher_evaluation_logs.get(interaction_id)

        if eval_log and eval_log.get("eval_llm_processed_output_json"):
            eval_json = eval_log["eval_llm_processed_output_json"]
            overall_score_str = eval_json.get("evaluation_summary", {}).get("overall_quality_score_cypher")
            gemini_suggestion_raw = eval_json.get("suggestion_for_improvement_cypher", "").strip()
            
            try:
                overall_score = int(overall_score_str)
                gemini_score_for_log = overall_score
            except (ValueError, TypeError):
                refine_logger.warning(f"Could not parse overall_quality_score_cypher for {interaction_id}: {overall_score_str}")
                overall_score = 0 # 默认给个低分
                gemini_score_for_log = 0

            # --- 规则1: Qwen自己就说无法生成 ---
            if qwen_generated_cypher == "无法生成Cypher查询.":
                # 如果Gemini也认为无法生成或评分低，那么采纳
                if "无法生成Cypher查询" in gemini_suggestion_raw or overall_score <= 2:
                    ideal_cypher_output = "无法生成Cypher查询."
                    source_of_ideal = "qwen_and_gemini_cannot_generate"
                # 如果Qwen说无法生成，但Gemini给出了高分建议，这很奇怪，需要人工看
                elif overall_score >=4 and "MATCH" in gemini_suggestion_raw.upper():
                     refine_logger.info(f"Cypher log {interaction_id}: Qwen said '无法生成', but Gemini suggested a high-score query '{gemini_suggestion_raw}'. Needs manual review.")
                     continue
                else: # Qwen说无法生成，Gemini建议不明确或中低分，也采纳Qwen的
                    ideal_cypher_output = "无法生成Cypher查询."
                    source_of_ideal = "qwen_cannot_generate_gemini_unclear"


            # --- 规则2: Qwen生成了查询，看Gemini评估 ---
            elif overall_score >= 4: # Gemini认为Qwen的输出质量高
                ideal_cypher_output = qwen_generated_cypher
                source_of_ideal = "qwen_high_score_by_gemini"

            
            # --- 规则3: Qwen的查询质量不高，但Gemini给出了具体的、看起来像Cypher的建议 ---
            elif gemini_suggestion_raw and \
                "无法生成Cypher查询" not in gemini_suggestion_raw and \
                "cannot be improved" not in gemini_suggestion_raw.lower() and \
                "needs to be extended" not in gemini_suggestion_raw.lower() and \
                ("MATCH " in gemini_suggestion_raw.upper() or \
                    "RETURN " in gemini_suggestion_raw.upper() or \
                    "CREATE " in gemini_suggestion_raw.upper() or \
                    "MERGE " in gemini_suggestion_raw.upper() or \
                    "WITH " in gemini_suggestion_raw.upper() or \
                    "OPTIONAL MATCH " in gemini_suggestion_raw.upper()
                ):

                # 简化处理：直接将 Gemini 的原始建议作为 completion 的候选
                # 清洗工作主要交给人工审核阶段
                # 我们仍然可以做非常基础的清理，比如首尾空格和常见的 markdown
                
                temp_completion = gemini_suggestion_raw.strip()
                if temp_completion.startswith("```") and temp_completion.endswith("```"):
                    temp_completion = temp_completion[3:-3].strip()
                    if temp_completion.lower().startswith("cypher"):
                        temp_completion = temp_completion[len("cypher"):].strip()
                elif temp_completion.startswith("`") and temp_completion.endswith("`"):
                    temp_completion = temp_completion[1:-1].strip()

                # 只要建议中包含核心Cypher关键字，我们就认为它有价值被审核
                core_cypher_keywords_check = ["MATCH", "RETURN", "CREATE", "MERGE", "WITH", "OPTIONAL MATCH"]
                suggestion_contains_cypher_keyword = False
                if temp_completion:
                    for core_keyword in core_cypher_keywords_check:
                        if core_keyword in temp_completion.upper():
                            suggestion_contains_cypher_keyword = True
                            break
                
                if suggestion_contains_cypher_keyword:
                    ideal_cypher_output = temp_completion # 使用初步清理后的建议
                    source_of_ideal = "gemini_suggestion_for_review" # 明确标记为需要审核
                    refine_logger.info(f"Interaction {interaction_id}: Gemini suggestion adopted for review. Raw: '{gemini_suggestion_raw[:150]}...', Processed for completion: '{ideal_cypher_output[:150]}...'")
                else:
                    refine_logger.warning(f"Interaction {interaction_id}: Gemini suggestion '{gemini_suggestion_raw[:150]}...' did not appear to contain core Cypher keywords after basic cleaning. Skipping.")
                    continue

            
            # --- 规则4: Gemini明确建议“无法生成” 或 Qwen的查询质量低且有严重问题 ---
            elif "无法生成Cypher查询" in gemini_suggestion_raw or \
                 (overall_score <= 2 and ("hallucinated" in eval_log.get("eval_llm_raw_output", "").lower() or \
                                         "schema violation" in eval_log.get("eval_llm_raw_output", "").lower() or \
                                         "syntax error" in eval_log.get("eval_llm_raw_output", "").lower())):
                ideal_cypher_output = "无法生成Cypher查询."
                source_of_ideal = "gemini_explicitly_cannot_generate_or_qwen_low_quality"
            
            # --- 规则5: 其他情况，需要人工审核 ---
            else:
                refine_logger.info(f"Cypher log {interaction_id} (Qwen: '{qwen_generated_cypher[:100]}...') needs manual review. Gemini score: {overall_score}, Suggestion: '{gemini_suggestion_raw[:100]}...'")
                continue 
        
        # --- 如果没有Gemini评估日志 ---
        else:
            refine_logger.warning(f"No valid Gemini evaluation found for Cypher log {interaction_id}. Qwen's output: '{qwen_generated_cypher[:100]}...'. Skipping for finetune data.")
            continue
            
        if ideal_cypher_output is not None:
            finetune_samples.append({
                "prompt": qwen_input_prompt,
                "completion": ideal_cypher_output.strip(), # 确保completion也strip
                "original_qwen_cypher": qwen_generated_cypher,
                "gemini_score": gemini_score_for_log,
                "source_of_ideal": source_of_ideal,
                "interaction_id": interaction_id
            })

    refine_logger.info(f"Generated {len(finetune_samples)} Cypher finetuning samples.")
    return finetune_samples


if __name__ == "__main__":
    # 1. 确定要处理的原始RAG交互日志文件 (包含cypher_generation)
    #    和对应的Gemini评估结果日志文件 (包含cypher_evaluation_result)
    
    # 自动查找最新的原始RAG交互日志
    rag_log_file = find_latest_rag_interaction_log(RAG_LOG_DIR) # utils.py中的函数
    
    # 构造对应的Gemini Cypher评估结果文件名
    # 假设评估文件名与原始日志文件名日期部分相同，且评估类型固定
    eval_log_file = None
    if rag_log_file:
        rag_log_basename = os.path.basename(rag_log_file)
        date_str_match = "".join(filter(str.isdigit, rag_log_basename)) # 提取文件名中的日期部分
        if len(date_str_match) >= 8: # 确保提取到至少YYYYMMDD
            date_str = date_str_match[:8]

            # 根据 evaluation.py 中 log_interaction_data 的 evaluation_name_for_file 参数构造
            evaluation_name = "cypher_gemini_flash" 
            eval_file_name = f"eval_results_{evaluation_name}_{date_str}.jsonl"
            
            eval_log_file = os.path.join(EVAL_LOG_DIR, eval_file_name)
            refine_logger.info(f"Attempting to load Cypher evaluation results from: {eval_log_file}")
        else:
            refine_logger.error(f"Could not reliably extract date from RAG log filename: {rag_log_basename}")
    
    if not rag_log_file or not eval_log_file or not os.path.exists(eval_log_file):
        refine_logger.error("Required log files not found. Exiting.")
        if not rag_log_file: refine_logger.error(f"RAG interaction log missing (expected pattern rag_interactions_*.jsonl in {RAG_LOG_DIR})")
        if rag_log_file and (not eval_log_file or not os.path.exists(eval_log_file)): refine_logger.error(f"Cypher evaluation result log missing (expected: {eval_log_file})")
    else:
        rag_interactions = load_logs_to_dict(rag_log_file, key_field="interaction_id")
        cypher_evaluations = load_logs_to_dict(eval_log_file, key_field="original_interaction_id_ref")

        if rag_interactions and cypher_evaluations:
            finetune_data = generate_finetune_samples_for_cypher(rag_interactions, cypher_evaluations)
            
            if finetune_data:
                # 获取当前日期用于文件名
                today_for_filename = datetime.now().strftime("%Y%m%d")
                output_filepath = os.path.join(FINETUNE_DATA_DIR, f"cypher_finetune_samples_{today_for_filename}.jsonl")
                
                with open(output_filepath, 'w', encoding='utf-8') as f_out:
                    for sample in finetune_data:
                        f_out.write(json.dumps(sample, ensure_ascii=False) + "\n")
                refine_logger.info(f"Successfully saved {len(finetune_data)} Cypher finetuning samples to: {output_filepath}")
                
                # 也可以同时保存一个CSV版本供人工审查
                try:
                    df = pd.DataFrame(finetune_data)
                    csv_output_filepath = os.path.join(FINETUNE_DATA_DIR, f"cypher_finetune_samples_review_{today_for_filename}.csv")
                    df.to_csv(csv_output_filepath, index=False, encoding='utf-8-sig')
                    refine_logger.info(f"Reviewable CSV saved to: {csv_output_filepath}")
                except Exception as e_csv:
                    refine_logger.error(f"Failed to save review CSV: {e_csv}")
            else:
                refine_logger.info("No finetuning samples were generated.")
        else:
            refine_logger.error("Failed to load data from log files.")
```

File: zhz_rag/stored_data/__init__.py
-------------------------------------
--- File is empty ---

File: zhz_rag/evaluation/__init__.py
------------------------------------
--- File is empty ---

File: zhz_rag/evaluation/analyze_answer.py
------------------------------------------
```python
# zhz_rag/evaluation/analyze_answer.py
import json
import os
import pandas as pd
from typing import List, Dict, Any, Optional
from collections import Counter
from datetime import datetime
import glob 

# --- 从项目中导入必要的模块 ---
try:
    from zhz_rag.utils.common_utils import (
        load_jsonl_file, # <--- 使用新的通用函数
        EVALUATION_RESULTS_LOGS_DIR # 导入评估日志目录常量
    )
except ImportError as e:
    print(f"ERROR: Could not import necessary modules in analyze_answer.py: {e}")
    print("Make sure this script is run in an environment where 'zhz_rag' package is accessible.")
    exit(1)

import logging

# --- 配置此脚本的logger ---
analyze_answer_logger = logging.getLogger("AnalyzeAnswerLogger")
analyze_answer_logger.setLevel(logging.INFO)
if not analyze_answer_logger.hasHandlers():
    _console_handler = logging.StreamHandler()
    _formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
    _console_handler.setFormatter(_formatter)
    analyze_answer_logger.addHandler(_console_handler)
    analyze_answer_logger.propagate = False
analyze_answer_logger.info("--- AnalyzeAnswerLogger configured ---")

# --- 核心功能函数 ---

def extract_answer_evaluation_details(log_entry: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """
    从单条已解析的答案评估日志条目中提取关键信息。
    这个函数与您之前在 analyze_answer.py 中的版本基本一致，稍作调整。
    """
    details = {}
    eval_data = log_entry.get("eval_llm_processed_output_json")

    if not eval_data or not isinstance(eval_data, dict):
        analyze_answer_logger.warning(f"Skipping log entry due to missing or invalid 'eval_llm_processed_output_json'. Interaction ID ref: {log_entry.get('original_interaction_id_ref')}")
        return None

    details["interaction_id_ref"] = log_entry.get("original_interaction_id_ref")
    details["user_question"] = log_entry.get("user_question_for_eval")
    details["generated_answer"] = log_entry.get("generated_answer_for_eval")
    
    summary = eval_data.get("evaluation_summary", {})
    dimensions = eval_data.get("dimensions", {})
    
    details["overall_answer_quality_score"] = summary.get("overall_answer_quality_score")
    details["main_strengths_answer"] = summary.get("main_strengths_answer")
    details["main_weaknesses_answer"] = summary.get("main_weaknesses_answer")
    
    faithfulness = dimensions.get("faithfulness", {})
    details["faithfulness_score"] = faithfulness.get("score")
    details["faithfulness_reasoning"] = faithfulness.get("reasoning")
    # 确保 problematic_answer_segments_faithfulness 是列表，然后 join
    problematic_segments = faithfulness.get("problematic_answer_segments_faithfulness", [])
    if isinstance(problematic_segments, list):
        details["faithfulness_problematic_segments"] = "; ".join(problematic_segments)
    else:
        details["faithfulness_problematic_segments"] = str(problematic_segments) # 以防万一不是列表

    relevance = dimensions.get("relevance", {})
    details["relevance_score"] = relevance.get("score")
    details["relevance_reasoning"] = relevance.get("reasoning")
    
    completeness = dimensions.get("completeness", {})
    details["completeness_context_sufficiency"] = completeness.get("context_sufficiency_assessment")
    details["completeness_context_reasoning"] = completeness.get("context_sufficiency_reasoning")
    details["completeness_score"] = completeness.get("score")
    details["completeness_reasoning"] = completeness.get("reasoning")
    
    coherence = dimensions.get("coherence_fluency", {}) # 键名可能与prompt中的一致
    details["coherence_score"] = coherence.get("score")
    details["coherence_reasoning"] = coherence.get("reasoning")

    actionability = dimensions.get("actionability_usability", {}) # 键名可能与prompt中的一致
    details["actionability_score"] = actionability.get("score")
    details["actionability_reasoning"] = actionability.get("reasoning")
    
    details["gemini_suggestion_answer"] = eval_data.get("suggestion_for_answer_improvement")

    return details

def perform_answer_evaluation_analysis(
    evaluation_log_filepath: str,
    output_csv_filepath: str
) -> bool:
    """
    加载答案评估日志，进行分析，并保存结果到CSV。

    Args:
        evaluation_log_filepath (str): 答案评估结果日志文件的路径。
        output_csv_filepath (str): 分析结果CSV文件的保存路径。

    Returns:
        bool: 如果分析和保存成功则返回True，否则返回False。
    """
    analyze_answer_logger.info(f"Starting Answer evaluation analysis for log file: {evaluation_log_filepath}")
    analyze_answer_logger.info(f"Analysis results will be saved to: {output_csv_filepath}")

    evaluation_logs = load_jsonl_file(evaluation_log_filepath)

    if not evaluation_logs:
        analyze_answer_logger.warning(f"No evaluation logs found or loaded from {evaluation_log_filepath}. Analysis aborted.")
        return False

    extracted_details_list = []
    for log_entry in evaluation_logs:
        if log_entry.get("task_type") == "answer_evaluation_result": # 确保是答案评估日志
            details = extract_answer_evaluation_details(log_entry)
            if details:
                extracted_details_list.append(details)
        else:
            analyze_answer_logger.debug(f"Skipping log entry with task_type '{log_entry.get('task_type')}' as it's not 'answer_evaluation_result'.")


    if not extracted_details_list:
        analyze_answer_logger.info("No valid Answer evaluation details extracted from the logs. No CSV will be generated.")
        return False

    df = pd.DataFrame(extracted_details_list)
    
    score_columns = [
        "overall_answer_quality_score", "faithfulness_score", "relevance_score",
        "completeness_score", "coherence_score", "actionability_score"
    ]
    for col in score_columns:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')

    analyze_answer_logger.info(f"\n--- Preliminary Answer Evaluation Analysis (from {len(extracted_details_list)} entries) ---")
    analyze_answer_logger.info(f"Total evaluation entries processed: {len(df)}")

    for col_name in score_columns:
        if col_name in df.columns and not df[col_name].isnull().all():
            analyze_answer_logger.info(f"\nDimension: {col_name}")
            analyze_answer_logger.info(f"{df[col_name].describe()}")
            analyze_answer_logger.info("Score Distribution:")
            analyze_answer_logger.info(f"{df[col_name].value_counts(dropna=False).sort_index()}")
        else:
            analyze_answer_logger.info(f"\nDimension: {col_name} - No data or all NaN.")
            
    if "completeness_context_sufficiency" in df.columns and not df["completeness_context_sufficiency"].isnull().all():
        analyze_answer_logger.info("\nContext Sufficiency Assessment Distribution:")
        analyze_answer_logger.info(f"{df['completeness_context_sufficiency'].value_counts(dropna=False)}")
    else:
        analyze_answer_logger.info("\nContext Sufficiency Assessment Distribution: No data.")

    try:
        os.makedirs(os.path.dirname(output_csv_filepath), exist_ok=True)
        df.to_csv(output_csv_filepath, index=False, encoding='utf-8-sig')
        analyze_answer_logger.info(f"\nAnalysis results saved to: {output_csv_filepath}")
        return True
    except Exception as e:
        analyze_answer_logger.error(f"\nFailed to save CSV file: {e}", exc_info=True)
        return False

if __name__ == "__main__":
    EVALUATION_NAME_FOR_ANSWER = "answer_gemini_flash" 
    
    # --- 动态查找最新的评估结果日志文件 ---
    eval_logs_pattern = os.path.join(EVALUATION_RESULTS_LOGS_DIR, f"eval_results_{EVALUATION_NAME_FOR_ANSWER}_*.jsonl")
    all_eval_logs = sorted(glob.glob(eval_logs_pattern), key=os.path.getmtime, reverse=True)
    
    log_file_path_answer: Optional[str] = None
    output_csv_path_answer: Optional[str] = None

    if all_eval_logs:
        log_file_path_answer = all_eval_logs[0] # 获取最新的一个
        analyze_answer_logger.info(f"Found latest Answer evaluation log for analysis: {log_file_path_answer}")
        
        # 根据找到的日志文件名构造输出的 CSV 文件名
        base_log_name = os.path.basename(log_file_path_answer)
        # 从 "eval_results_answer_gemini_flash_YYYYMMDD.jsonl" 生成 "analysis_answer_gemini_flash_YYYYMMDD.csv"
        if base_log_name.startswith(f"eval_results_{EVALUATION_NAME_FOR_ANSWER}_") and base_log_name.endswith(".jsonl"):
            date_part_from_filename = base_log_name[len(f"eval_results_{EVALUATION_NAME_FOR_ANSWER}_"):-len(".jsonl")]
            output_csv_name_answer = f"analysis_{EVALUATION_NAME_FOR_ANSWER}_{date_part_from_filename}.csv"
            output_csv_path_answer = os.path.join(EVALUATION_RESULTS_LOGS_DIR, output_csv_name_answer)
        else: # Fallback naming for CSV
            today_str = datetime.now().strftime("%Y%m%d")
            output_csv_name_answer = f"analysis_{EVALUATION_NAME_FOR_ANSWER}_{today_str}_fallback.csv"
            output_csv_path_answer = os.path.join(EVALUATION_RESULTS_LOGS_DIR, output_csv_name_answer)
        analyze_answer_logger.info(f"Analysis CSV report will be saved to: {output_csv_path_answer}")
    else:
        analyze_answer_logger.error(f"No Answer evaluation log files found matching pattern: {eval_logs_pattern}")

    if log_file_path_answer and output_csv_path_answer and os.path.exists(log_file_path_answer):
        perform_answer_evaluation_analysis(
            evaluation_log_filepath=log_file_path_answer,
            output_csv_filepath=output_csv_path_answer
        )
    else:
        analyze_answer_logger.info("Answer evaluation analysis will not run as no suitable log file was identified or output path could not be determined.")
```

File: zhz_rag/evaluation/analyze_cypher.py
------------------------------------------
```python
# zhz_rag/evaluation/analyze_cypher.py
import json
import os
import pandas as pd
from typing import List, Dict, Any, Optional
from collections import Counter
from datetime import datetime
import glob 

# --- 从项目中导入必要的模块 ---
try:
    from zhz_rag.utils.common_utils import (
        load_jsonl_file, # <--- 使用新的通用函数
        EVALUATION_RESULTS_LOGS_DIR # 导入评估日志目录常量
    )
except ImportError as e:
    print(f"ERROR: Could not import necessary modules in analyze_cypher.py: {e}")
    print("Make sure this script is run in an environment where 'zhz_rag' package is accessible.")
    exit(1)

import logging

# --- 配置此脚本的logger ---
analyze_cypher_logger = logging.getLogger("AnalyzeCypherLogger")
analyze_cypher_logger.setLevel(logging.INFO)
if not analyze_cypher_logger.hasHandlers():
    _console_handler = logging.StreamHandler()
    _formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
    _console_handler.setFormatter(_formatter)
    analyze_cypher_logger.addHandler(_console_handler)
    analyze_cypher_logger.propagate = False
analyze_cypher_logger.info("--- AnalyzeCypherLogger configured ---")

# --- 核心功能函数 ---

def extract_cypher_evaluation_details(log_entry: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """
    从单条已解析的Cypher评估日志条目中提取关键信息。
    这个函数与您之前在 analyze_cypher.py 中的版本基本一致，稍作调整以适应通用加载。
    """
    details = {}
    # eval_llm_processed_output_json 字段包含了Gemini评估的JSON输出
    eval_data = log_entry.get("eval_llm_processed_output_json")

    if not eval_data or not isinstance(eval_data, dict):
        analyze_cypher_logger.warning(f"Skipping log entry due to missing or invalid 'eval_llm_processed_output_json'. Interaction ID ref: {log_entry.get('original_interaction_id_ref')}")
        return None # 如果核心评估数据缺失，则跳过此条目

    details["interaction_id_ref"] = log_entry.get("original_interaction_id_ref")
    details["user_question"] = log_entry.get("user_question_for_eval")
    details["generated_cypher"] = log_entry.get("generated_cypher_for_eval")
    
    summary = eval_data.get("evaluation_summary", {})
    dimensions = eval_data.get("dimensions", {})
    
    details["overall_quality_score"] = summary.get("overall_quality_score_cypher")
    details["main_strength"] = summary.get("main_strength_cypher")
    details["main_weakness"] = summary.get("main_weakness_cypher")
    
    syntax = dimensions.get("syntactic_correctness", {})
    details["syntax_score"] = syntax.get("score")
    details["syntax_reasoning"] = syntax.get("reasoning")
    
    schema = dimensions.get("schema_adherence", {})
    details["schema_overall_score"] = schema.get("overall_score")
    details["schema_node_label_correct"] = schema.get("node_label_correctness", {}).get("check_result")
    details["schema_entity_type_correct"] = schema.get("entity_type_property_correctness", {}).get("check_result")
    details["schema_rel_type_correct"] = schema.get("relationship_type_correctness", {}).get("check_result")
    details["schema_prop_name_correct"] = schema.get("property_name_correctness", {}).get("check_result")
    details["schema_hallucinated_present"] = schema.get("hallucinated_schema_elements", {}).get("check_result_hallucination_present")
    details["schema_hallucinated_elements"] = ", ".join(schema.get("hallucinated_schema_elements", {}).get("elements_found", []))
    details["schema_reasoning"] = schema.get("reasoning")
    
    intent = dimensions.get("intent_accuracy", {})
    details["intent_score"] = intent.get("score")
    details["intent_explanation_cypher"] = intent.get("explanation_of_cypher_retrieval")
    details["intent_alignment_notes"] = intent.get("semantic_alignment_with_question")
    details["intent_key_elements_notes"] = intent.get("key_element_coverage_notes")
    details["intent_reasoning"] = intent.get("reasoning")
    
    details["qwen_error_patterns"] = ", ".join(eval_data.get("qwen_error_patterns_identified", []))
    details["gemini_suggestion"] = eval_data.get("suggestion_for_improvement_cypher")

    return details

def perform_cypher_evaluation_analysis(
    evaluation_log_filepath: str,
    output_csv_filepath: str
) -> bool:
    """
    加载Cypher评估日志，进行分析，并保存结果到CSV。

    Args:
        evaluation_log_filepath (str): Cypher评估结果日志文件的路径。
        output_csv_filepath (str): 分析结果CSV文件的保存路径。

    Returns:
        bool: 如果分析和保存成功则返回True，否则返回False。
    """
    analyze_cypher_logger.info(f"Starting Cypher evaluation analysis for log file: {evaluation_log_filepath}")
    analyze_cypher_logger.info(f"Analysis results will be saved to: {output_csv_filepath}")

    # 使用通用函数加载评估日志
    # 注意：evaluate_cypher_with_gemini 保存的日志中 task_type 是 "cypher_evaluation_by_gemini"
    # load_jsonl_file 不关心 task_type，它会加载所有行
    evaluation_logs = load_jsonl_file(evaluation_log_filepath)

    if not evaluation_logs:
        analyze_cypher_logger.warning(f"No evaluation logs found or loaded from {evaluation_log_filepath}. Analysis aborted.")
        return False

    extracted_details_list = []
    for log_entry in evaluation_logs:
        # 确保只处理Cypher评估结果
        if log_entry.get("task_type") == "cypher_evaluation_by_gemini":
            details = extract_cypher_evaluation_details(log_entry)
            if details: # extract_cypher_evaluation_details 可能会返回 None
                extracted_details_list.append(details)
        else:
            analyze_cypher_logger.debug(f"Skipping log entry with task_type '{log_entry.get('task_type')}' as it's not 'cypher_evaluation_by_gemini'.")


    if not extracted_details_list:
        analyze_cypher_logger.info("No valid Cypher evaluation details extracted from the logs. No CSV will be generated.")
        return False

    df = pd.DataFrame(extracted_details_list)
    
    score_columns = ["overall_quality_score", "syntax_score", "schema_overall_score", "intent_score"]
    for col in score_columns:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')

    analyze_cypher_logger.info(f"\n--- Preliminary Cypher Evaluation Analysis (from {len(extracted_details_list)} entries) ---")
    analyze_cypher_logger.info(f"Total evaluation entries processed: {len(df)}")

    if "overall_quality_score" in df.columns and not df["overall_quality_score"].isnull().all():
        analyze_cypher_logger.info("\n1. Overall Quality Score:")
        analyze_cypher_logger.info(f"{df['overall_quality_score'].describe()}")
        analyze_cypher_logger.info("\nScore Distribution:")
        analyze_cypher_logger.info(f"{df['overall_quality_score'].value_counts(dropna=False).sort_index()}")
    else:
        analyze_cypher_logger.info("\n1. Overall Quality Score: No data or all NaN.")


    if "schema_overall_score" in df.columns and not df["schema_overall_score"].isnull().all():
        analyze_cypher_logger.info("\n2. Schema Adherence Overall Score:")
        analyze_cypher_logger.info(f"{df['schema_overall_score'].describe()}")
        analyze_cypher_logger.info("\nScore Distribution:")
        analyze_cypher_logger.info(f"{df['schema_overall_score'].value_counts(dropna=False).sort_index()}")
        
        schema_sub_checks = [
            "schema_node_label_correct", "schema_entity_type_correct", 
            "schema_rel_type_correct", "schema_prop_name_correct", 
            "schema_hallucinated_present"
        ]
        analyze_cypher_logger.info("\nSchema Adherence Sub-item Issues (False means issue, Hallucinated True means issue):")
        for check in schema_sub_checks:
            if check in df.columns:
                if check == "schema_hallucinated_present":
                    issue_count = df[df[check] == True].shape[0]
                    analyze_cypher_logger.info(f"  - {check} (Hallucination Present): {issue_count} entries")
                else:
                    issue_count = df[df[check] == False].shape[0]
                    analyze_cypher_logger.info(f"  - {check} (Incorrect): {issue_count} entries")
    else:
        analyze_cypher_logger.info("\n2. Schema Adherence Overall Score: No data or all NaN.")


    if "intent_score" in df.columns and not df["intent_score"].isnull().all():
        analyze_cypher_logger.info("\n3. Intent Accuracy Score:")
        analyze_cypher_logger.info(f"{df['intent_score'].describe()}")
        analyze_cypher_logger.info("\nScore Distribution:")
        analyze_cypher_logger.info(f"{df['intent_score'].value_counts(dropna=False).sort_index()}")
    else:
        analyze_cypher_logger.info("\n3. Intent Accuracy Score: No data or all NaN.")


    if "qwen_error_patterns" in df.columns and not df["qwen_error_patterns"].isnull().all():
        analyze_cypher_logger.info("\n4. Identified Qwen Error Patterns (Top 5):")
        all_patterns = []
        for pattern_list_str in df["qwen_error_patterns"].dropna():
            if pattern_list_str and isinstance(pattern_list_str, str):
                all_patterns.extend([p.strip() for p in pattern_list_str.split(",") if p.strip()])
        pattern_counts = Counter(all_patterns)
        analyze_cypher_logger.info(f"{pattern_counts.most_common(5)}")
    else:
        analyze_cypher_logger.info("\n4. Identified Qwen Error Patterns: No data.")
        
    try:
        # Ensure output directory exists
        os.makedirs(os.path.dirname(output_csv_filepath), exist_ok=True)
        df.to_csv(output_csv_filepath, index=False, encoding='utf-8-sig')
        analyze_cypher_logger.info(f"\nAnalysis results saved to: {output_csv_filepath}")
        return True
    except Exception as e:
        analyze_cypher_logger.error(f"\nFailed to save CSV file: {e}", exc_info=True)
        return False

if __name__ == "__main__":
    EVALUATION_NAME_FOR_CYPHER = "cypher_gemini_flash" 

    eval_logs_pattern = os.path.join(EVALUATION_RESULTS_LOGS_DIR, f"eval_results_{EVALUATION_NAME_FOR_CYPHER}_*.jsonl")
    all_eval_logs = sorted(glob.glob(eval_logs_pattern), key=os.path.getmtime, reverse=True)
    
    log_file_path_cypher: Optional[str] = None
    output_csv_path_cypher: Optional[str] = None

    if all_eval_logs:
        log_file_path_cypher = all_eval_logs[0]
        analyze_cypher_logger.info(f"Found latest Cypher evaluation log for analysis: {log_file_path_cypher}")
        
        base_log_name = os.path.basename(log_file_path_cypher)
        if base_log_name.startswith(f"eval_results_{EVALUATION_NAME_FOR_CYPHER}_") and base_log_name.endswith(".jsonl"):
            date_part_from_filename = base_log_name[len(f"eval_results_{EVALUATION_NAME_FOR_CYPHER}_"):-len(".jsonl")]
            output_csv_name_cypher = f"analysis_{EVALUATION_NAME_FOR_CYPHER}_{date_part_from_filename}.csv"
            output_csv_path_cypher = os.path.join(EVALUATION_RESULTS_LOGS_DIR, output_csv_name_cypher)
        else:
            today_str = datetime.now().strftime("%Y%m%d")
            output_csv_name_cypher = f"analysis_{EVALUATION_NAME_FOR_CYPHER}_{today_str}_fallback.csv"
            output_csv_path_cypher = os.path.join(EVALUATION_RESULTS_LOGS_DIR, output_csv_name_cypher)
        analyze_cypher_logger.info(f"Analysis CSV report will be saved to: {output_csv_path_cypher}")
    else:
        analyze_cypher_logger.error(f"No Cypher evaluation log files found matching pattern: {eval_logs_pattern}")

    if log_file_path_cypher and output_csv_path_cypher and os.path.exists(log_file_path_cypher):
        perform_cypher_evaluation_analysis(
            evaluation_log_filepath=log_file_path_cypher,
            output_csv_filepath=output_csv_path_cypher
        )
    else:
        analyze_cypher_logger.info("Cypher evaluation analysis will not run as no suitable log file was identified or output path could not be determined.")
```

File: zhz_rag/evaluation/batch_eval_answer.py
---------------------------------------------
```python
# zhz_rag/evaluation/batch_eval_answer.py
import asyncio
import json
import os
from typing import List, Dict, Any, Optional, TYPE_CHECKING, Union
import glob
from datetime import datetime

try:
    from zhz_rag.evaluation.evaluator import evaluate_answer_with_gemini
    from zhz_rag.utils.common_utils import (
        find_latest_rag_interaction_log,
        load_jsonl_file,
        RAG_INTERACTION_LOGS_DIR
    )
    from zhz_rag.config.pydantic_models import RetrievedDocument
except ImportError as e:
    print(f"ERROR: Could not import necessary modules in batch_eval_answer.py: {e}")
    print("Make sure this script is run in an environment where 'zhz_rag' package is accessible.")
    exit(1)

if TYPE_CHECKING:
    from zhz_rag_pipeline_dagster.zhz_rag_pipeline.resources import GeminiAPIResource

import logging

batch_answer_eval_logger = logging.getLogger("BatchAnswerEvaluationLogger")
# 保留 DEBUG 级别，以便在需要时仍可查看详细日志，但常规 INFO 日志会更简洁
batch_answer_eval_logger.setLevel(logging.DEBUG) 
if not batch_answer_eval_logger.hasHandlers():
    _console_handler = logging.StreamHandler()
    _formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
    _console_handler.setFormatter(_formatter)
    batch_answer_eval_logger.addHandler(_console_handler)
    batch_answer_eval_logger.propagate = False
batch_answer_eval_logger.info("--- BatchAnswerEvaluationLogger configured (Level: DEBUG) ---")


def format_contexts_for_evaluation(context_docs_raw: List[Dict[str, Any]]) -> str:
    formatted_contexts = []
    if not context_docs_raw or not isinstance(context_docs_raw, list):
        batch_answer_eval_logger.warning("format_contexts_for_evaluation received no context or invalid format.")
        batch_answer_eval_logger.debug("DEBUG_FORMAT_CTX: context_docs_raw is empty or not a list.") 
        return "No context provided or context in unexpected format."

    batch_answer_eval_logger.debug(f"DEBUG_FORMAT_CTX: --- format_contexts_for_evaluation ---")
    batch_answer_eval_logger.debug(f"DEBUG_FORMAT_CTX: Received context_docs_raw length: {len(context_docs_raw)}")
    if context_docs_raw:
        batch_answer_eval_logger.debug(f"DEBUG_FORMAT_CTX: First item of context_docs_raw (type: {type(context_docs_raw[0])}): {str(context_docs_raw[0])[:500]}...")

    for i, doc_data in enumerate(context_docs_raw): 
        batch_answer_eval_logger.debug(f"DEBUG_FORMAT_CTX:   Processing doc_data item {i} (type: {type(doc_data)}): {str(doc_data)[:300]}...")
        try:
            content = doc_data.get("content", "[Content not available]") 
            source_type = doc_data.get("source_type", "unknown_source") 
            score = doc_data.get("score") 
            metadata = doc_data.get("metadata", {})
            chunk_id = metadata.get("chunk_id") or metadata.get("id") 

            context_str = f"--- Context Snippet {i+1} ---\n"
            context_str += f"Source Type: {source_type}\n"
            if score is not None:
                try:
                    context_str += f"Original Score: {float(score):.4f}\n"
                except (ValueError, TypeError):
                    context_str += f"Original Score: {score}\n" 
            if chunk_id:
                 context_str += f"Chunk ID: {chunk_id}\n"
            context_str += f"Content: {content}\n"
            formatted_contexts.append(context_str)
            batch_answer_eval_logger.debug(f"DEBUG_FORMAT_CTX:     Formatted context snippet {i+1} (content part first 100 chars): {str(content)[:100]}...")

        except Exception as e:
            batch_answer_eval_logger.warning(f"Could not parse a context document fully in format_contexts_for_evaluation: {doc_data}. Error: {e}")
            content = doc_data.get("content", "[Content not available]") 
            source_type = doc_data.get("source_type", "unknown_source") 
            formatted_contexts.append(f"--- Context Snippet {i+1} (Parsing Warning) ---\nSource Type: {source_type}\nContent: {content}\n")
    
    final_formatted_str = "\n\n".join(formatted_contexts) if formatted_contexts else "No context provided."
    batch_answer_eval_logger.debug(f"DEBUG_FORMAT_CTX: --- format_contexts_for_evaluation: Final formatted string (first 500 chars): {final_formatted_str[:500]}...")
    return final_formatted_str


async def run_answer_batch_evaluation(
    gemini_resource_for_evaluator: 'GeminiAPIResource',
    rag_interaction_log_filepath: str,
    app_version: str = "0.1.0",
    use_simulated_api: bool = False,
    api_call_delay: float = 4.1,
    target_task_types: Union[str, List[str]] = "rag_query_processing_full_log",
    field_mapping: Optional[Dict[str, Union[str, List[str]]]] = None
) -> Dict[str, int]:
    batch_answer_eval_logger.info(f"Starting batch Answer evaluation for log file: {rag_interaction_log_filepath}")
    batch_answer_eval_logger.info(f"Parameters: app_version='{app_version}', use_simulated_api={use_simulated_api}, api_call_delay={api_call_delay}s")
    batch_answer_eval_logger.info(f"Target task types: {target_task_types}")
    batch_answer_eval_logger.info(f"Field mapping: {field_mapping}")

    processed_count = 0
    evaluated_count = 0
    skipped_missing_data_count = 0
    skipped_task_type_mismatch = 0

    if use_simulated_api:
        batch_answer_eval_logger.warning("Batch Answer evaluation is USING SIMULATED Gemini responses.")
    else:
        batch_answer_eval_logger.info("Batch Answer evaluation is using REAL Gemini API calls.")

    interaction_logs = load_jsonl_file(rag_interaction_log_filepath)

    if not interaction_logs:
        batch_answer_eval_logger.error(f"No data loaded from RAG interaction log file: {rag_interaction_log_filepath}. Exiting.")
        return {"processed": 0, "evaluated": 0, "skipped_missing_data": 0, "skipped_task_type_mismatch": 0, "file_not_found_or_empty": 1}

    # --- 移除了临时筛选特定 interaction_id 的代码 ---

    if isinstance(target_task_types, str):
        target_task_types_list = [target_task_types]
    else:
        target_task_types_list = target_task_types

    default_field_map = {
        "user_query": ["user_query", "original_user_query", "query"],
        "generated_answer": ["generated_answer", "processed_llm_output", "final_answer_from_llm", "final_answer"],
        "interaction_id": ["interaction_id", "original_interaction_id"],
        "context_docs": ["final_context_docs_full", "final_context_docs_summary"]
    }
    current_field_map = default_field_map.copy()
    if field_mapping:
        for key, value in field_mapping.items():
            if isinstance(value, str):
                current_field_map[key] = [value] 
            else:
                current_field_map[key] = value

    def get_field_value(log_entry: Dict[str, Any], field_key: str) -> Any:
        for actual_field_name in current_field_map.get(field_key, []):
            if actual_field_name in log_entry:
                return log_entry[actual_field_name]
        return None

    for line_number, interaction_log in enumerate(interaction_logs, 1):
        processed_count += 1
        current_task_type = interaction_log.get("task_type")
        batch_answer_eval_logger.debug(f"Log entry {line_number} has task_type: '{current_task_type}'")
        
        if current_task_type not in target_task_types_list:
            skipped_task_type_mismatch +=1
            continue
        
        # --- 添加日志打印 interaction_log 的键，用于调试 ---
        batch_answer_eval_logger.debug(f"DEBUG_INTERACTION_LOG: Keys in interaction_log for entry {line_number}: {list(interaction_log.keys())}")
        if "final_context_docs_full" in interaction_log:
            batch_answer_eval_logger.debug(f"DEBUG_INTERACTION_LOG: interaction_log['final_context_docs_full'] (first item preview): {str(interaction_log['final_context_docs_full'][0])[:200] if interaction_log['final_context_docs_full'] else 'Empty or None'}")
        else:
            batch_answer_eval_logger.debug(f"DEBUG_INTERACTION_LOG: 'final_context_docs_full' NOT in interaction_log for entry {line_number}.")
        if "final_context_docs_summary" in interaction_log:
            batch_answer_eval_logger.debug(f"DEBUG_INTERACTION_LOG: interaction_log['final_context_docs_summary'] (first item preview): {str(interaction_log['final_context_docs_summary'][0])[:200] if interaction_log['final_context_docs_summary'] else 'Empty or None'}")
        else:
            batch_answer_eval_logger.debug(f"DEBUG_INTERACTION_LOG: 'final_context_docs_summary' NOT in interaction_log for entry {line_number}.")
        # --- 结束日志打印 ---


        user_question = get_field_value(interaction_log, "user_query")
        generated_answer = get_field_value(interaction_log, "generated_answer")
        original_id = get_field_value(interaction_log, "interaction_id")
        context_docs_raw = get_field_value(interaction_log, "context_docs")
        
        batch_answer_eval_logger.debug(
            f"Log entry {line_number}: task_type='{current_task_type}', id='{original_id}', "
            f"q_present={bool(user_question)}, ans_present={bool(generated_answer)}, ctx_present_is_list={isinstance(context_docs_raw, list) if context_docs_raw else False}"
        )

        if user_question and generated_answer and original_id and context_docs_raw and isinstance(context_docs_raw, list):
            batch_answer_eval_logger.info(f"DEBUG_EVAL: For ID {original_id}, UserQ: '{str(user_question)[:50]}...', GenAns: '{str(generated_answer)[:50]}...', Contexts count: {len(context_docs_raw)}")
            retrieved_contexts_str_for_eval = format_contexts_for_evaluation(context_docs_raw)
            
            batch_answer_eval_logger.info(f"Evaluating Answer for interaction_id: {original_id} - User Question: '{str(user_question)[:50]}...' - Generated Answer: '{str(generated_answer)[:50]}...'")
            
            evaluation_result = await evaluate_answer_with_gemini(
                gemini_resource=gemini_resource_for_evaluator,
                user_question=str(user_question),
                retrieved_contexts=retrieved_contexts_str_for_eval,
                generated_answer=str(generated_answer),
                original_interaction_id=str(original_id),
                app_version=app_version
            )

            if evaluation_result:
                evaluated_count += 1
                summary = evaluation_result.get("evaluation_summary", {})
                overall_score = summary.get("overall_answer_quality_score", "N/A")
                batch_answer_eval_logger.info(f"Successfully evaluated Answer for interaction_id: {original_id}. Overall Score: {overall_score}")
            else:
                batch_answer_eval_logger.warning(f"Answer evaluation returned None or failed for interaction_id: {original_id}")
            
            if not use_simulated_api and evaluated_count > 0 : 
                batch_answer_eval_logger.info(f"Waiting for {api_call_delay} seconds before next API call...")
                await asyncio.sleep(api_call_delay)
        else:
            skipped_missing_data_count += 1
            log_preview = {
                "interaction_id": original_id, 
                "task_type": current_task_type, 
                "user_question_found": bool(user_question),
                "generated_answer_found": bool(generated_answer),
                "context_docs_found_and_list": isinstance(context_docs_raw, list) if context_docs_raw else False
            }
            batch_answer_eval_logger.warning(f"Skipping RAG log entry {line_number} due to missing critical data. Details: {log_preview}")
        
        if processed_count > 0 and processed_count % 10 == 0:
            batch_answer_eval_logger.info(f"Progress: Processed {processed_count} log entries. Evaluated {evaluated_count} answers so far. Skipped (type mismatch): {skipped_task_type_mismatch}. Skipped (missing data): {skipped_missing_data_count}.")

    summary = {
        "total_log_entries_read": processed_count,
        "target_task_type_entries_found": processed_count - skipped_task_type_mismatch,
        "answers_evaluated_successfully": evaluated_count,
        "skipped_due_to_missing_data_in_target_entries": skipped_missing_data_count,
    }
    batch_answer_eval_logger.info(f"Batch Answer evaluation finished. Summary: {summary}")
    return summary


if __name__ == "__main__":
    try:
        from zhz_rag_pipeline_dagster.zhz_rag_pipeline.resources import GeminiAPIResource, GeminiAPIResourceConfig
        
        gemini_model_name_env = os.getenv("GEMINI_MODEL_FOR_EVAL", "gemini/gemini-1.5-flash-latest")
        gemini_proxy_url_env = os.getenv("LITELLM_PROXY_URL") 

        gemini_resource_config = GeminiAPIResourceConfig(
            model_name=gemini_model_name_env,
            proxy_url=gemini_proxy_url_env
        )
        gemini_eval_resource = GeminiAPIResource(
            model_name=gemini_resource_config.model_name,
            proxy_url=gemini_resource_config.proxy_url,
            default_temperature=gemini_resource_config.default_temperature,
            default_max_tokens=gemini_resource_config.default_max_tokens
        )
        class MockContext: 
            def __init__(self):
                self.log = batch_answer_eval_logger
        
        if hasattr(gemini_eval_resource, 'setup_for_execution'):
             gemini_eval_resource.setup_for_execution(MockContext())
        batch_answer_eval_logger.info(f"GeminiAPIResource for evaluation initialized successfully using Dagster's resource class.")

    except ImportError:
        batch_answer_eval_logger.critical("CRITICAL: Could not import GeminiAPIResource. Ensure Dagster modules are in PYTHONPATH or installed.")
        gemini_eval_resource = None 
    except Exception as e_res_init:
        batch_answer_eval_logger.critical(f"CRITICAL: Error initializing GeminiAPIResource: {e_res_init}", exc_info=True)
        gemini_eval_resource = None


    log_file_to_evaluate = find_latest_rag_interaction_log(RAG_INTERACTION_LOGS_DIR)
    use_simulated_env = os.getenv("USE_SIMULATED_GEMINI_ANSWER_EVAL", "false").lower() == "true"
    api_delay_env = float(os.getenv("GEMINI_API_CALL_DELAY_SECONDS", "4.1"))
    app_version_tag_env = os.getenv("APP_VERSION_TAG", "0.1.4_batch_answer_flexible")
    if use_simulated_env:
        app_version_tag_env += "_simulated"

    rag_service_task_types = ["rag_query_processing_full_log"]
    rag_service_field_map = {
        "user_query": "original_user_query",
        "generated_answer": "final_answer_from_llm",
        "interaction_id": "interaction_id",
        "context_docs": ["final_context_docs_full", "final_context_docs_summary"] 
    }

    if not gemini_eval_resource:
        batch_answer_eval_logger.error("Cannot proceed with evaluation as GeminiAPIResource is not available.")
    elif log_file_to_evaluate:
        batch_answer_eval_logger.info(f"Found RAG interaction log to process for answer evaluation: {log_file_to_evaluate}")
        asyncio.run(run_answer_batch_evaluation(
            gemini_resource_for_evaluator=gemini_eval_resource,
            rag_interaction_log_filepath=log_file_to_evaluate,
            app_version=app_version_tag_env,
            use_simulated_api=use_simulated_env,
            api_call_delay=api_delay_env,
            target_task_types=rag_service_task_types, 
            field_mapping=rag_service_field_map      
        ))
    elif use_simulated_env:
        batch_answer_eval_logger.warning(f"RAG interaction log file not found, but USE_SIMULATED_GEMINI_ANSWER_EVAL is true. Running with a dummy path (will process 0 entries).")
        if gemini_eval_resource: # Check again if resource is available for simulated run
            asyncio.run(run_answer_batch_evaluation(
                gemini_resource_for_evaluator=gemini_eval_resource,
                rag_interaction_log_filepath="dummy_non_existent_file.jsonl", 
                app_version=app_version_tag_env + "_no_file",
                use_simulated_api=use_simulated_env,
                api_call_delay=api_delay_env,
                target_task_types=rag_service_task_types,
                field_mapping=rag_service_field_map
            ))
        else:
            batch_answer_eval_logger.error("GeminiAPIResource for evaluation could not be initialized (even for simulated run). Aborting simulated run.")
    else:
        batch_answer_eval_logger.warning(f"No suitable RAG interaction log file found in '{RAG_INTERACTION_LOGS_DIR}' and not using simulated responses. Batch Answer evaluation will not run.")
```

File: zhz_rag/evaluation/batch_eval_cypher.py
---------------------------------------------
```python
# zhz_rag/evaluation/batch_eval_cypher.py
import asyncio
import json
import os
from typing import List, Dict, Any, Optional, TYPE_CHECKING, Union
import glob
from datetime import datetime

try:
    from zhz_rag.evaluation.evaluator import evaluate_cypher_with_gemini
    from zhz_rag.utils.common_utils import (
        find_latest_rag_interaction_log,
        load_jsonl_file,
        RAG_INTERACTION_LOGS_DIR
    )
except ImportError as e:
    print(f"ERROR: Could not import necessary modules in batch_eval_cypher.py: {e}")
    print("Make sure this script is run in an environment where 'zhz_rag' package is accessible.")
    exit(1)

if TYPE_CHECKING:
    from zhz_rag_pipeline_dagster.zhz_rag_pipeline.resources import GeminiAPIResource

import logging

batch_cypher_eval_logger = logging.getLogger("BatchCypherEvaluationLogger")
batch_cypher_eval_logger.setLevel(logging.DEBUG) # 设置为 DEBUG 以便查看详细日志
if not batch_cypher_eval_logger.hasHandlers():
    _console_handler = logging.StreamHandler()
    _formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
    _console_handler.setFormatter(_formatter)
    batch_cypher_eval_logger.addHandler(_console_handler)
    batch_cypher_eval_logger.propagate = False
batch_cypher_eval_logger.info("--- BatchCypherEvaluationLogger configured (Level: DEBUG) ---")


async def run_cypher_batch_evaluation(
    gemini_resource_for_evaluator: 'GeminiAPIResource',
    rag_interaction_log_filepath: str,
    app_version: str = "0.1.0",
    use_simulated_api: bool = False,
    api_call_delay: float = 4.1,
    target_task_types: Union[str, List[str]] = "cypher_generation_final_attempt_local_service",
    field_mapping: Optional[Dict[str, Union[str, List[str]]]] = None
) -> Dict[str, int]:
    batch_cypher_eval_logger.info(f"Starting batch Cypher evaluation for log file: {rag_interaction_log_filepath}")
    batch_cypher_eval_logger.info(f"Parameters: app_version='{app_version}', use_simulated_api={use_simulated_api}, api_call_delay={api_call_delay}s")
    batch_cypher_eval_logger.info(f"Target task types: {target_task_types}")
    batch_cypher_eval_logger.info(f"Field mapping: {field_mapping}")

    processed_count = 0
    evaluated_count = 0
    skipped_no_cypher_count = 0
    failed_to_extract_count = 0
    skipped_task_type_mismatch = 0

    if use_simulated_api:
        batch_cypher_eval_logger.warning("Batch Cypher evaluation is USING SIMULATED Gemini responses.")
    else:
        batch_cypher_eval_logger.info("Batch Cypher evaluation is using REAL Gemini API calls.")

    interaction_logs = load_jsonl_file(rag_interaction_log_filepath)

    if not interaction_logs:
        batch_cypher_eval_logger.error(f"No data loaded from RAG interaction log file: {rag_interaction_log_filepath}. Exiting.")
        return {"processed": 0, "evaluated": 0, "skipped_no_cypher":0, "failed_extract": 0, "skipped_task_type_mismatch":0, "file_not_found_or_empty": 1}

    if isinstance(target_task_types, str):
        target_task_types_list = [target_task_types]
    else:
        target_task_types_list = target_task_types

    default_field_map = {
        "user_query": ["user_query_for_task", "user_query", "original_user_query"],
        "generated_cypher": ["raw_llm_output", "processed_llm_output"], # raw_llm_output for cypher_generation_final_attempt_local_service
        "interaction_id": ["interaction_id", "original_interaction_id"]
    }
    current_field_map = default_field_map.copy()
    if field_mapping:
        for key, value in field_mapping.items():
            if isinstance(value, str):
                current_field_map[key] = [value]
            else:
                current_field_map[key] = value

    def get_field_value(log_entry: Dict[str, Any], field_key: str) -> Any:
        for actual_field_name in current_field_map.get(field_key, []):
            if actual_field_name in log_entry:
                return log_entry[actual_field_name]
        return None

    for line_number, interaction_log in enumerate(interaction_logs, 1):
        processed_count += 1
        current_task_type = interaction_log.get("task_type")

        if current_task_type not in target_task_types_list:
            skipped_task_type_mismatch +=1
            continue
            
        batch_cypher_eval_logger.debug(f"DEBUG_CYPHER_EVAL: Processing log entry {line_number} with task_type '{current_task_type}'")

        user_question = get_field_value(interaction_log, "user_query")
        generated_cypher_raw = get_field_value(interaction_log, "generated_cypher")
        original_id = get_field_value(interaction_log, "interaction_id")

        if user_question and original_id:
            generated_cypher_to_eval = None
            if isinstance(generated_cypher_raw, str):
                try:
                    # Cypher gen logs store the JSON string {"status": ..., "query": ...} in raw_llm_output
                    cypher_data = json.loads(generated_cypher_raw)
                    if isinstance(cypher_data, dict) and cypher_data.get("status") == "success":
                        generated_cypher_to_eval = cypher_data.get("query")
                    elif isinstance(cypher_data, dict) and cypher_data.get("status") == "unable_to_generate":
                        generated_cypher_to_eval = cypher_data.get("query") # Should be "无法生成Cypher查询."
                    else: # Not the expected JSON structure
                         batch_cypher_eval_logger.warning(f"Cypher log {original_id} has raw_llm_output but not in expected JSON format: {generated_cypher_raw[:100]}")
                except json.JSONDecodeError:
                    # If raw_llm_output is not JSON, it might be the Cypher directly (older log format?) or "无法生成..."
                    generated_cypher_to_eval = generated_cypher_raw 
            
            if not generated_cypher_to_eval or not str(generated_cypher_to_eval).strip():
                batch_cypher_eval_logger.info(f"Skipping evaluation for interaction_id '{original_id}' as extracted Cypher is empty.")
                skipped_no_cypher_count += 1
                continue
            
            # We will evaluate "无法生成Cypher查询." as well, Gemini should score it appropriately.
            batch_cypher_eval_logger.info(f"Evaluating Cypher for interaction_id: {original_id} - User Question: {str(user_question)[:50]}... - Cypher: {str(generated_cypher_to_eval)[:100]}...")
            
            evaluation_result = await evaluate_cypher_with_gemini(
                gemini_resource=gemini_resource_for_evaluator,
                user_question=str(user_question),
                generated_cypher=str(generated_cypher_to_eval),
                original_interaction_id=str(original_id),
                app_version=app_version
            )

            if evaluation_result:
                evaluated_count += 1
                summary = evaluation_result.get("evaluation_summary", {})
                overall_score = summary.get("overall_quality_score_cypher", "N/A")
                batch_cypher_eval_logger.info(f"Successfully evaluated Cypher for interaction_id: {original_id}. Overall Score: {overall_score}")
            else:
                batch_cypher_eval_logger.warning(f"Cypher evaluation returned None or failed for interaction_id: {original_id}")
            
            if not use_simulated_api and evaluated_count > 0:
                batch_cypher_eval_logger.info(f"Waiting for {api_call_delay} seconds before next API call...")
                await asyncio.sleep(api_call_delay)
        else:
            failed_to_extract_count += 1
            batch_cypher_eval_logger.warning(f"Skipping cypher_generation log entry {line_number} due to missing user_query or interaction_id. Log content: {str(interaction_log)[:200]}...")
        
        if processed_count > 0 and processed_count % 10 == 0:
            batch_cypher_eval_logger.info(f"Progress: Processed {processed_count} log entries. Evaluated {evaluated_count} Cypher queries. Skipped (no cypher): {skipped_no_cypher_count}. Failed extract: {failed_to_extract_count}. Type mismatch: {skipped_task_type_mismatch}")

    summary = {
        "total_log_entries_read": processed_count,
        "target_task_type_entries_found": processed_count - skipped_task_type_mismatch,
        "cypher_queries_evaluated_successfully": evaluated_count,
        "skipped_empty_or_no_cypher": skipped_no_cypher_count,
        "failed_to_extract_fields_for_eval": failed_to_extract_count
    }
    batch_cypher_eval_logger.info(f"Batch Cypher evaluation finished. Summary: {summary}")
    return summary


if __name__ == "__main__":
    try:
        from zhz_rag_pipeline_dagster.zhz_rag_pipeline.resources import GeminiAPIResource, GeminiAPIResourceConfig
        
        gemini_model_name_env = os.getenv("GEMINI_MODEL_FOR_EVAL", "gemini/gemini-1.5-flash-latest")
        gemini_proxy_url_env = os.getenv("LITELLM_PROXY_URL") 

        gemini_resource_config = GeminiAPIResourceConfig(
            model_name=gemini_model_name_env,
            proxy_url=gemini_proxy_url_env
        )
        gemini_eval_resource = GeminiAPIResource(
            model_name=gemini_resource_config.model_name,
            proxy_url=gemini_resource_config.proxy_url,
            default_temperature=gemini_resource_config.default_temperature,
            default_max_tokens=gemini_resource_config.default_max_tokens
        )
        class MockContext: 
            def __init__(self):
                self.log = batch_cypher_eval_logger
        
        if hasattr(gemini_eval_resource, 'setup_for_execution'):
             gemini_eval_resource.setup_for_execution(MockContext())
        batch_cypher_eval_logger.info(f"GeminiAPIResource for Cypher evaluation initialized successfully.")

    except ImportError:
        batch_cypher_eval_logger.critical("CRITICAL: Could not import GeminiAPIResource. Ensure Dagster modules are in PYTHONPATH or installed.")
        gemini_eval_resource = None
    except Exception as e_res_init:
        batch_cypher_eval_logger.critical(f"CRITICAL: Error initializing GeminiAPIResource: {e_res_init}", exc_info=True)
        gemini_eval_resource = None

    log_file_to_evaluate = find_latest_rag_interaction_log(RAG_INTERACTION_LOGS_DIR)
    use_simulated_env = os.getenv("USE_SIMULATED_GEMINI_CYPHER_EVAL", "false").lower() == "true"
    api_delay_env = float(os.getenv("GEMINI_API_CALL_DELAY_SECONDS", "4.1"))
    app_version_tag_env = os.getenv("APP_VERSION_TAG", "0.1.4_batch_cypher_flexible")
    if use_simulated_env:
        app_version_tag_env += "_simulated"

    # --- 配置目标 task_type 和字段映射 ---
    # llm_interface.py 中的 generate_cypher_query 记录的 task_type 是 "cypher_generation_final_attempt_local_service"
    # 其 "raw_llm_output" 字段包含的是 local_llm_service 返回的 JSON 字符串: {"status": ..., "query": ...}
    cypher_gen_task_types = ["cypher_generation_final_attempt_local_service", "cypher_generation"] # 包含旧的以防万一
    cypher_gen_field_map = {
        "user_query": "user_query_for_task", # 在 "cypher_generation_final_attempt_local_service" 中是这个
        "generated_cypher": "raw_llm_output", # 在 "cypher_generation_final_attempt_local_service" 中是这个
        "interaction_id": "interaction_id"
    }

    if not gemini_eval_resource:
        batch_cypher_eval_logger.error("Cannot proceed with Cypher evaluation as GeminiAPIResource is not available.")
    elif log_file_to_evaluate:
        batch_cypher_eval_logger.info(f"Found RAG interaction log to process for Cypher evaluation: {log_file_to_evaluate}")
        asyncio.run(run_cypher_batch_evaluation(
            gemini_resource_for_evaluator=gemini_eval_resource,
            rag_interaction_log_filepath=log_file_to_evaluate,
            app_version=app_version_tag_env,
            use_simulated_api=use_simulated_env,
            api_call_delay=api_delay_env,
            target_task_types=cypher_gen_task_types,
            field_mapping=cypher_gen_field_map
        ))
    elif use_simulated_env:
        batch_cypher_eval_logger.warning(f"RAG interaction log file not found, but USE_SIMULATED_GEMINI_CYPHER_EVAL is true. Running with a dummy path.")
        if gemini_eval_resource:
            asyncio.run(run_cypher_batch_evaluation(
                gemini_resource_for_evaluator=gemini_eval_resource,
                rag_interaction_log_filepath="dummy_non_existent_file.jsonl", 
                app_version=app_version_tag_env + "_no_file",
                use_simulated_api=use_simulated_env,
                api_call_delay=api_delay_env,
                target_task_types=cypher_gen_task_types,
                field_mapping=cypher_gen_field_map
            ))
        else:
            batch_cypher_eval_logger.error("GeminiAPIResource for Cypher evaluation could not be initialized (even for simulated run). Aborting.")
    else:
        batch_cypher_eval_logger.warning(f"No suitable RAG interaction log file found in '{RAG_INTERACTION_LOGS_DIR}' and not using simulated responses. Batch Cypher evaluation will not run.")
```

File: zhz_rag/evaluation/evaluator.py
-------------------------------------
```python
# zhz_rag/evaluation/evaluator.py
import os
import json
import traceback
from typing import Dict, Any, Optional, TYPE_CHECKING

# 导入共享的Schema描述 和 通用日志函数
from zhz_rag.config.constants import NEW_KG_SCHEMA_DESCRIPTION as KG_SCHEMA_FOR_EVALUATION
from zhz_rag.utils.common_utils import log_interaction_data

# --- 类型检查时导入资源类，避免循环导入 ---
if TYPE_CHECKING:
    from zhz_rag_pipeline_dagster.zhz_rag_pipeline.resources import GeminiAPIResource

import logging

# --- 配置此模块的logger ---
eval_logger = logging.getLogger("EvaluationLogger")
eval_logger.setLevel(logging.INFO)
eval_logger.propagate = False
if not eval_logger.hasHandlers():
    _eval_console_handler = logging.StreamHandler()
    _eval_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
    _eval_console_handler.setFormatter(_eval_formatter)
    eval_logger.addHandler(_eval_console_handler)
    eval_logger.info("--- EvaluationLogger configured ---")

CYPHER_EVALUATION_PROMPT_V1 = """You are an expert Neo4j Cypher query evaluator and debugging assistant. Your primary task is to meticulously analyze a Cypher query that was generated by another AI model (Qwen2.5-3B, a 3 billion parameter model) in response to a user's natural language question. Your evaluation must be based on the provided knowledge graph schema and the specific evaluation criteria outlined below.

**IMPORTANT SCHEMA CONTEXT (KG_SCHEMA_DESCRIPTION):**
Use code with caution.
Python
{{KG_SCHEMA_DESCRIPTION}}
**USER'S NATURAL LANGUAGE QUESTION:**
Use code with caution.
{{USER_QUESTION}}
**GENERATED CYPHER QUERY TO EVALUATE:**
Use code with caution.
{{GENERATED_CYPHER}}
**EVALUATION TASK:**

Please evaluate the "GENERATED CYPHER QUERY" based on the following criteria. Provide your evaluation in a **valid JSON format** strictly adhering to the structure specified at the end.

**EVALUATION CRITERIA:**

1.  **Syntactic Correctness:**
    *   Is the Cypher query syntactically valid and parsable by Neo4j?
    *   Score (1-5): 1 = Major syntax errors, unparsable; 3 = Minor issues, likely parsable with warnings; 5 = Syntactically perfect.
    *   Reasoning: Explain your score. If errors exist, briefly describe them.

2.  **Schema Adherence (Strictly based on the provided KG_SCHEMA_DESCRIPTION):**
    *   **Node Label Correctness:**
        *   Check: Does the query exclusively use ':ExtractedEntity' for all node patterns? (True/False)
        *   Detail: Briefly explain.
    *   **Entity Type via `label` Property Correctness:**
        *   Check: Are entity types (e.g., 'PERSON', 'ORGANIZATION', 'TASK') correctly queried using the `label` property of ':ExtractedEntity' nodes (e.g., `WHERE n.label = 'PERSON'`)? (True/False)
        *   Detail: Briefly explain.
    *   **Relationship Type and Direction Correctness:**
        *   Check: Does the query use only defined relationship types (e.g., `:WORKS_AT`, `:ASSIGNED_TO`) and their correct directions as specified in the schema? (True/False)
        *   Detail: Briefly explain.
    *   **Property Name Correctness:**
        *   Check: Does the query use only valid property names for nodes and relationships (e.g., `text`, `label` for nodes)? (True/False)
        *   Detail: Briefly explain.
    *   **Hallucinated Schema Elements:**
        *   Check: Does the query reference any labels, relationship types, or properties NOT defined in the schema? (True/False - True if hallucination is present)
        *   Detail: List any hallucinated elements.
    *   **Overall Schema Adherence Score (1-5):** Based on the sub-checks above, provide an overall schema adherence score. Explain your reasoning, considering the severity and number of any deviations. Perfect adherence means all sub-checks are True and no hallucinations are present. 1 = Gross violations; 3 = Some minor deviations; 5 = Perfectly adheres to schema.
    *   Reasoning for Overall Schema Adherence Score: Provide the explanation here.

3.  **Intent Accuracy (Semantic Correctness):**
    *   Score (1-5): 1 = Completely misses user intent; 3 = Partially addresses intent but has significant gaps or inaccuracies; 5 = Accurately and fully captures user intent.
    *   Explanation of Cypher Retrieval: In simple natural language that an average office worker could understand, what information would this Cypher query retrieve from a graph that matches the schema?
    *   Alignment with User Question: How well does this retrieved information align with what the user was asking in their "USER'S NATURAL LANGUAGE QUESTION"?
    *   Key Element Coverage: Does the Cypher query attempt to address all key entities, relationships, and conditions mentioned in the user's question? If not, what specific parts of the question seem to be missing or misinterpreted in the Cypher query?
    *   Reasoning: Explain your intent accuracy score, considering the explanation, alignment, and key element coverage.

4.  **Identification of Potential Qwen2.5-3B Error Patterns (Optional but helpful):**
    *   Based on your knowledge of smaller LLMs, does this query exhibit any common error patterns such as:
        *   Over-simplification of complex conditions?
        *   Incorrect handling of the ':ExtractedEntity' and 'label' property convention?
        *   Ignoring multiple constraints from the user question?
        *   Other (please specify)?
    *   Observed Patterns: List any observed patterns from the predefined list, or provide an empty list [] if none of the predefined patterns are clearly observed.

**OUTPUT JSON STRUCTURE (Strictly follow this format):**
```json
{
  "evaluation_summary": {
    "overall_quality_score_cypher": "<Integer score 1-5, your overall judgment of the Cypher query's quality. **Crucially, assign higher weight to 'Schema Adherence' and 'Intent Accuracy'.** A query with perfect syntax but critical flaws in schema adherence or intent accuracy should NOT receive a high overall score.>",
    "main_strength_cypher": "<Briefly describe the main strength of this Cypher query, if any>",
    "main_weakness_cypher": "<Briefly describe the main weakness or most critical issue>"
  },
  "dimensions": {
    "syntactic_correctness": {
      "score": "<Integer score 1-5>",
      "parsable_prediction": "<Boolean: true/false>",
      "reasoning": "<Text explanation>"
    },
    "schema_adherence": {
      "overall_score": "<Integer score 1-5>",
      "node_label_correctness": { "check_result": "<Boolean>", "detail": "<Text>" },
      "entity_type_property_correctness": { "check_result": "<Boolean>", "detail": "<Text>" },
      "relationship_type_correctness": { "check_result": "<Boolean>", "detail": "<Text>" },
      "property_name_correctness": { "check_result": "<Boolean>", "detail": "<Text>" },
      "hallucinated_schema_elements": { "check_result_hallucination_present": "<Boolean>", "elements_found": ["<List of strings or empty list>"] },
      "reasoning": "<Text explanation for overall schema adherence score>"
    },
    "intent_accuracy": {
      "score": "<Integer score 1-5>",
      "explanation_of_cypher_retrieval": "<Text>",
      "semantic_alignment_with_question": "<Text>",
      "key_element_coverage_notes": "<Text describing coverage of key elements, and what's missing/misinterpreted, if any>",
      "reasoning": "<Text explanation for intent accuracy score>"
    }
  },
  "qwen_error_patterns_identified": ["<List of strings describing observed patterns, or empty list>"],
  "suggestion_for_improvement_cypher": "<Actionable suggestions to improve this specific Cypher query, if applicable>"
}
```"""

async def evaluate_cypher_with_gemini(
    gemini_resource: 'GeminiAPIResource', # <--- 修改：接收 GeminiAPIResource 实例
    user_question: str,
    generated_cypher: str,
    original_interaction_id: Optional[str] = None,
    app_version: str = "0.1.0"
) -> Optional[Dict[str, Any]]:
    eval_logger.info(f"Starting Cypher evaluation. User question: '{user_question[:50]}...', Cypher: '{generated_cypher[:100]}...'")

    prompt_to_gemini = CYPHER_EVALUATION_PROMPT_V1.replace(
        "{{KG_SCHEMA_DESCRIPTION}}", KG_SCHEMA_FOR_EVALUATION
    ).replace(
        "{{USER_QUESTION}}", user_question
    ).replace(
        "{{GENERATED_CYPHER}}", generated_cypher
    )

    messages_for_gemini = [{"role": "user", "content": prompt_to_gemini}]
    evaluation_result_json: Optional[Dict[str, Any]] = None
    raw_gemini_output: Optional[str] = None
    error_info: Optional[str] = None

    # 模拟API调用的逻辑保持不变，但实际API调用将通过gemini_resource
    USE_SIMULATED_GEMINI_RESPONSE = os.getenv("USE_SIMULATED_GEMINI_CYPHER_EVAL", "false").lower() == "true"

    if USE_SIMULATED_GEMINI_RESPONSE:
        eval_logger.warning("USING SIMULATED GEMINI RESPONSE FOR CYPHER EVALUATION")
        simulated_json_output = {
            "evaluation_summary": {
                "overall_quality_score_cypher": 4,
                "main_strength_cypher": "Good attempt at capturing intent.",
                "main_weakness_cypher": "Minor schema deviation in node label."
            },
            "dimensions": {
                "syntactic_correctness": {"score": 5, "parsable_prediction": True, "reasoning": "Query appears syntactically valid."},
                "schema_adherence": {
                    "overall_score": 3,
                    "node_label_correctness": { "check_result": False, "detail": "Used :Person instead of :ExtractedEntity for one node." },
                    "entity_type_property_correctness": { "check_result": True, "detail": "Correctly used label property." },
                    "relationship_type_correctness": { "check_result": True, "detail": "Used defined relationships correctly." },
                    "property_name_correctness": { "check_result": True, "detail": "Used valid properties." },
                    "hallucinated_schema_elements": { "check_result_hallucination_present": False, "elements_found": [] },
                    "reasoning": "One instance of incorrect node label, otherwise good."
                },
                "intent_accuracy": {
                    "score": 4,
                    "explanation_of_cypher_retrieval": "The query attempts to find the organization where '张三' works.",
                    "semantic_alignment_with_question": "Well-aligned with the user's question about '张三's' workplace.",
                    "key_element_coverage_notes": "All key elements seem to be covered.",
                    "reasoning": "Good intent capture, minor improvement could be ensuring organization type is also filtered if ambiguous."
                }
            },
            "qwen_error_patterns_identified": ["Incorrect handling of the ':ExtractedEntity' and 'label' property convention"],
            "suggestion_for_improvement_cypher": "Change MATCH (p:Person...) to MATCH (p:ExtractedEntity {label: 'PERSON'})..."
        }
        raw_gemini_output = json.dumps(simulated_json_output)
        try:
            evaluation_result_json = json.loads(raw_gemini_output)
        except json.JSONDecodeError as e:
            eval_logger.error(f"Error decoding simulated JSON: {e}")
            error_info = f"Simulated JSON decode error: {str(e)}"
    else:
        try:
            eval_logger.info(f"Calling Gemini for Cypher evaluation via GeminiAPIResource. Model: {gemini_resource.model_name}. Prompt length: {len(prompt_to_gemini)}")
            # --- 修改：通过 gemini_resource 调用 ---
            raw_gemini_output = await gemini_resource.call_completion(
                messages=messages_for_gemini
                # temperature 和 max_tokens 将使用 gemini_resource 的默认值或配置值
            )
            if raw_gemini_output:
                eval_logger.info(f"Raw Gemini output for Cypher eval (first 300 chars): {raw_gemini_output[:300]}...")
                cleaned_output = raw_gemini_output.strip()
                if cleaned_output.startswith("```json"):
                    cleaned_output = cleaned_output[len("```json"):].strip()
                if cleaned_output.endswith("```"):
                    cleaned_output = cleaned_output[:-len("```")].strip()
                evaluation_result_json = json.loads(cleaned_output)
                eval_logger.info("Successfully parsed Gemini evaluation result for Cypher.")
            else:
                eval_logger.error("Gemini call via resource returned None or empty for Cypher evaluation.")
                error_info = "Gemini call via resource returned None/empty"
        except json.JSONDecodeError as e_json:
            eval_logger.error(f"Failed to decode JSON from Gemini Cypher evaluation. Raw output: {raw_gemini_output[:500] if raw_gemini_output else 'N/A'}", exc_info=True)
            error_info = f"JSONDecodeError: {str(e_json)}"
        except Exception as e_gen: # 更通用的异常捕获，因为 call_completion 可能自己处理了 LiteLLM 异常
            eval_logger.error(f"Unexpected error during Cypher evaluation with Gemini resource: {e_gen}", exc_info=True)
            error_info = f"Unexpected error with Gemini resource: {str(e_gen)}"
            if raw_gemini_output is None: # 如果 call_completion 返回 None 且没有设置 raw_gemini_output
                raw_gemini_output = error_info

    eval_log_data = {
        "task_type": "cypher_evaluation_by_gemini",
        "original_interaction_id_ref": original_interaction_id,
        "user_question_for_eval": user_question,
        "generated_cypher_for_eval": generated_cypher,
        "eval_llm_input_prompt_char_count": len(prompt_to_gemini),
        "eval_llm_model": gemini_resource.model_name, # <--- 修改：从资源获取模型名称
        "eval_llm_raw_output": raw_gemini_output,
        "eval_llm_processed_output_json": evaluation_result_json,
        "eval_error_info": error_info,
        "application_version": app_version
    }
    await log_interaction_data(
        eval_log_data,
        is_evaluation_result=True,
        evaluation_name_for_file="cypher_gemini_flash"
    )
    if evaluation_result_json:
        eval_logger.info(f"Cypher evaluation completed. Overall score: {evaluation_result_json.get('evaluation_summary', {}).get('overall_quality_score_cypher')}")
    else:
        eval_logger.warning("Cypher evaluation did not produce a valid JSON result.")
        
    return evaluation_result_json


# V1 Answer Evaluation Prompt
ANSWER_EVALUATION_PROMPT_V1 = """
You are an expert AI Answer Evaluator, specializing in assessing the quality of responses from a Retrieval Augmented Generation (RAG) system designed as an "Office Worker Assistant". Your evaluation must be objective, strictly based on the provided user question, the context the RAG system used, and the generated answer.

**USER'S NATURAL LANGUAGE QUESTION:**
Use code with caution.
Python
{{USER_QUESTION}}
**CONTEXT PROVIDED TO THE RAG SYSTEM'S GENERATION MODEL (this is the information the AI had to base its answer on):**
Use code with caution.
{{RETRIEVED_CONTEXTS}}
**AI-GENERATED ANSWER TO EVALUATE:**
Use code with caution.
{{GENERATED_ANSWER}}
**EVALUATION TASK:**

Please evaluate the "AI-GENERATED ANSWER" based on the following criteria. For each dimension, provide a score from 1 to 5 (where 5 is best) and a brief reasoning for your score. Also, if applicable, identify specific phrases or sentences in the answer that exemplify an issue.

**EVALUATION DIMENSIONS & SCORING GUIDELINES:**

1.  **Faithfulness/Traceability (Score 1-5):**
    *   Is all factual information in the answer directly supported by the "CONTEXT PROVIDED"?
    *   Does the answer avoid making up information (hallucinations) or contradicting the context?
    *   **5 (Completely Faithful):** All key factual claims in the answer are directly and accurately supported by the context. No external information introduced.
    *   **4 (Mostly Faithful):** The vast majority of factual claims are supported. May contain very minor, reasonable inferences closely tied to the context, but no direct contradictions.
    *   **3 (Partially Faithful):** Some key claims are supported, but there are noticeable unsupported claims, slight misinterpretations of the context, or minor, non-critical hallucinations.
    *   **2 (Minimally Faithful):** Significant portions of the answer are not supported by the context, or there are clear contradictions or misleading hallucinations.
    *   **1 (Not Faithful):** The answer is largely based on information 외부 from the context, contains severe hallucinations, or directly contradicts the context.

2.  **Relevance to User Query (Score 1-5):**
    *   Does the answer directly and precisely address the "USER'S NATURAL LANGUAGE QUESTION"?
    *   **5 (Perfectly Relevant):** Directly and fully answers the user's core question(s).
    *   **4 (Highly Relevant):** Accurately answers the main aspects of the question; minor aspects might be less directly addressed.
    *   **3 (Moderately Relevant):** Addresses parts of the question but may miss key aspects or include some less relevant information.
    *   **2 (Slightly Relevant):** Touches upon the topic of the question but largely misses the core intent.
    *   **1 (Not Relevant):** Does not answer the user's question at all.

3.  **Completeness (Score 1-5):**
    *   **First, assess Context Sufficiency:** Based *only* on the "CONTEXT PROVIDED", does it seem to contain enough information to fully answer the "USER'S NATURAL LANGUAGE QUESTION"? (Answer: "Sufficient", "Partially Sufficient", or "Insufficient"). Provide a brief reason for your assessment of context sufficiency.
    *   **Then, score Completeness based on the answer's performance given the context:**
        *   Does the answer address all aspects of the user's query, making good use of the relevant information available in the context?
        *   If the context was insufficient, does the answer appropriately acknowledge this or focus on what can be answered?
        *   **5 (Very Complete):** (If context sufficient) Fully addresses all aspects of the query using all relevant context. (If context insufficient) Makes the best use of available context and clearly indicates limitations.
        *   **4 (Mostly Complete):** (If context sufficient) Addresses main aspects, minor details from context might be missed. (If context insufficient) Good use of available context, fair indication of limitations.
        *   **3 (Partially Complete):** (If context sufficient) Misses some important aspects or underutilizes relevant context. (If context insufficient) Poor use of available context or unclear about limitations.
        *   **2 (Slightly Complete):** (If context sufficient) Addresses only a small part, much relevant context ignored. (If context insufficient) Very poor use of limited context.
        *   **1 (Not Complete):** Fails to address the query meaningfully, even if relevant context was available.

4.  **Coherence/Fluency (Score 1-5):**
    *   Is the answer well-written, grammatically correct, logically structured, and easy to understand?
    *   **5 (Very Fluent):** Perfectly written, clear, natural, and easy to understand. No grammatical errors.
    *   **4 (Fluent):** Well-written, mostly clear, minor or no grammatical errors.
    *   **3 (Moderately Fluent):** Understandable, but may have some awkward phrasing or minor grammatical errors that don't impede core understanding.
    *   **2 (Slightly Fluent):** Difficult to understand due to grammatical errors, awkward phrasing, or poor logical flow.
    *   **1 (Not Fluent):** Largely incomprehensible.

5.  **Actionability & Usability (for an Office Worker Assistant) (Score 1-5):**
    *   Does the answer provide clear, practical, and easy-to-understand steps, information, or suggestions that would directly help an office worker achieve their task or make a decision?
    *   **5 (Highly Actionable & Usable):** Provides clear, specific, and immediately applicable steps/information. Language is professional and easy for an office worker to understand. Format facilitates quick information retrieval.
    *   **4 (Mostly Actionable & Usable):** Provides generally clear guidance or useful information. Might require minor clarification for full actionability, or presentation could be slightly improved, but core content is helpful.
    *   **3 (Partially Actionable & Usable):** Offers some relevant information or suggestions, but lacks specific steps, is too vague for direct action, or requires significant effort to understand/apply.
    *   **2 (Minimally Actionable & Usable):** Contains some related information but no clear action plan, is impractical, or very difficult to understand/use. Offers little practical help.
    *   **1 (Not Actionable & Unusable):** Provides no actionable information, is irrelevant to practical office tasks, or is misleading/confusing.

**OUTPUT JSON STRUCTURE (Strictly follow this format):**
```json
{
  "evaluation_summary": {
    "overall_answer_quality_score": "<Integer score 1-5, your overall judgment of the answer's quality, considering all dimensions. Faithfulness and Relevance are most critical.>",
    "main_strengths_answer": "<Briefly describe the main strength(s) of this answer, if any. Be specific.>",
    "main_weaknesses_answer": "<Briefly describe the main weakness(es) or most critical issue(s) with this answer. Be specific.>"
  },
  "dimensions": {
    "faithfulness": {
      "score": "<Integer score 1-5>",
      "reasoning": "<Text explanation for faithfulness score. If not fully faithful, specify which parts are unsupported or hallucinated, referencing the answer text.>",
      "problematic_answer_segments_faithfulness": ["<List of specific phrases/sentences from the answer that are not faithful, or empty list if none>"]
    },
    "relevance": {
      "score": "<Integer score 1-5>",
      "reasoning": "<Text explanation for relevance score. Explain how well it addresses the user's core question.>"
    },
    "completeness": {
      "context_sufficiency_assessment": "<String: 'Sufficient', 'Partially Sufficient', or 'Insufficient'>",
      "context_sufficiency_reasoning": "<Brief reason for the context sufficiency assessment. If not sufficient, what key information is missing from the context?>",
      "score": "<Integer score 1-5, based on how well the answer uses the available context to address the query>",
      "reasoning": "<Text explanation for completeness score. If not fully complete, what aspects of the query were missed, or what relevant context was underutilized? If context was insufficient, how well did the answer handle this?>"
    },
    "coherence_fluency": {
      "score": "<Integer score 1-5>",
      "reasoning": "<Text explanation for coherence/fluency score. Note any grammatical issues or awkward phrasing.>"
    },
    "actionability_usability": {
      "score": "<Integer score 1-5>",
      "reasoning": "<Text explanation for actionability/usability score, considering an office worker context.>"
    }
  },
  "suggestion_for_answer_improvement": "<Provide specific, actionable suggestions on how this AI-generated answer could be improved, if applicable. If excellent, state 'No improvement needed'.>"
}
Use code with caution.
FEW-SHOT EXAMPLES:
Example 1: Good Answer
USER'S NATURAL LANGUAGE QUESTION:
如何申请本年度的带薪年假，并确保符合公司规定？
Use code with caution.
CONTEXT PROVIDED TO THE RAG SYSTEM'S GENERATION MODEL:
上下文片段1 (来源: 公司政策文档):
员工每年享有15天带薪年假。申请年假需通过公司内部HRM系统提交，并至少提前两周提出。所有年假申请必须得到直属经理的批准。未经批准的休假将按事假处理。

上下文片段2 (来源: HRM系统操作指南):
HRM系统年假申请流程：1. 登录HRM门户 (hrm.example-company.com)。 2. 导航至“我的假期”->“假期申请”。 3. 选择“年假”类型，填写休假起止日期和休假天数。 4. 在备注中简要说明休假事由（可选）。 5. 提交申请，等待经理审批。审批状态可在系统中实时查看。

上下文片段3 (来源: 知识图谱精确信息):
事实: 年假政策规定 -> 提前申请周期: 2周。
事实: 年假政策规定 -> 审批人: 直属经理。
事实: 年假政策规定 -> 年假天数上限: 15天/年。
Use code with caution.
AI-GENERATED ANSWER TO EVALUATE:
您可以按照以下步骤申请本年度的带薪年假并确保符合公司规定：

1.  **提前规划与提交：** 根据公司政策，您需要至少提前两周提交年假申请。
2.  **通过HRM系统申请：**
    *   登录公司HRM门户网站：hrm.example-company.com
    *   进入“我的假期”模块，然后选择“假期申请”。
    *   选择“年假”类型，并准确填写您希望休假的起止日期和天数。
3.  **获取经理批准：** 所有年假申请都必须得到您直属经理的批准。提交申请后，请留意系统中的审批状态。
4.  **年假额度：** 请注意，公司规定员工每年享有15天带薪年假。

请确保您的申请符合以上规定，以顺利安排您的休假。
Use code with caution.
EXPECTED EVALUATION JSON OUTPUT:
{
  "evaluation_summary": {
    "overall_answer_quality_score": 5,
    "main_strengths_answer": "答案准确、完整、步骤清晰，非常实用。",
    "main_weaknesses_answer": "无明显缺点。"
  },
  "dimensions": {
    "faithfulness": {
      "score": 5,
      "reasoning": "答案中的所有信息点（提前两周、HRM系统操作、经理批准、15天年假）均能在提供的上下文中找到明确依据，无任何捏造。",
      "problematic_answer_segments_faithfulness": []
    },
    "relevance": {
      "score": 5,
      "reasoning": "答案完美地回应了用户关于年假申请流程和合规性的问题。"
    },
    "completeness": {
      "context_sufficiency_assessment": "Sufficient",
      "context_sufficiency_reasoning": "上下文提供了申请年假所需的所有核心信息（流程、提前时间、审批人、额度）。",
      "score": 5,
      "reasoning": "答案全面覆盖了用户问题的各个方面，并充分利用了上下文中的所有关键信息，包括操作步骤和政策规定。"
    },
    "coherence_fluency": {
      "score": 5,
      "reasoning": "答案语言表达清晰、专业，逻辑步骤明确，易于理解。"
    },
    "actionability_usability": {
      "score": 5,
      "reasoning": "答案提供了非常清晰、可直接执行的申请步骤，并点明了关键的合规性要求，对办公室员工非常实用且易于操作。"
    }
  },
  "suggestion_for_answer_improvement": "No improvement needed."
}
Use code with caution.
Json
Example 2: Answer with Hallucination
USER'S NATURAL LANGUAGE QUESTION:
我们公司最新的季度IT安全演练是什么时候举行的，主要发现了哪些问题？
Use code with caution.
CONTEXT PROVIDED TO THE RAG SYSTEM'S GENERATION MODEL:
上下文片段1 (来源: 内部邮件):
主题：关于第三季度IT安全意识培训的通知
内容：各位同事，为提升全员信息安全意识，公司计划于8月15日举行第三季度IT安全意识培训，主要内容包括密码安全、钓鱼邮件识别等。请大家准时参加。

上下文片段2 (来源: IT部门公告):
IT服务台提醒：近期发现多起针对财务部门的仿冒邮件，请大家提高警惕，不要点击不明链接。如有疑问，请及时联系IT支持。演练活动将在下月进行。
Use code with caution.
AI-GENERATED ANSWER TO EVALUATE:
公司最新的季度IT安全演练已于上个月（7月）成功举行。演练主要发现了两个问题：一是部分员工的办公电脑未及时更新防病毒软件，二是演练中模拟的钓鱼邮件成功欺骗了约15%的员工。后续将加强相关培训。
Use code with caution.
EXPECTED EVALUATION JSON OUTPUT:
{
  "evaluation_summary": {
    "overall_answer_quality_score": 1,
    "main_strengths_answer": "答案结构尚可。",
    "main_weaknesses_answer": "答案内容完全是编造的（幻觉），与提供的上下文信息完全不符，具有严重误导性。"
  },
  "dimensions": {
    "faithfulness": {
      "score": 1,
      "reasoning": "答案中关于演练已于7月举行、发现的两个具体问题（未更新防病毒软件、15%员工被钓鱼邮件欺骗）在上下文中完全找不到任何依据，是严重的幻觉。",
      "problematic_answer_segments_faithfulness": ["演练已于上个月（7月）成功举行。", "演练主要发现了两个问题：一是部分员工的办公电脑未及时更新防病毒软件，二是演练中模拟的钓鱼邮件成功欺骗了约15%的员工。"]
    },
    "relevance": {
      "score": 2,
      "reasoning": "答案表面上回应了问题（演练时间和问题），但由于内容是虚假的，其实际相关性很低。"
    },
    "completeness": {
      "context_sufficiency_assessment": "Partially Sufficient",
      "context_sufficiency_reasoning": "上下文提到了计划中的培训和演练（下月进行），以及一些安全问题（仿冒邮件），但没有给出已完成演练的具体时间和发现的问题。",
      "score": 1,
      "reasoning": "答案完全没有利用上下文中的有效信息（如计划中的培训和演练），而是编造了内容。"
    },
    "coherence_fluency": {
      "score": 4,
      "reasoning": "答案的语言表达本身是通顺的，语法基本正确。"
    },
    "actionability_usability": {
      "score": 1,
      "reasoning": "虚假的信息完全不可用，且具有误导性，对办公室工作有害无益。"
    }
  },
  "suggestion_for_answer_improvement": "AI模型必须严格基于提供的上下文生成答案，严禁编造任何上下文中未提及的事实。如果上下文信息不足，应明确指出。"
}
Use code with caution.
Json
Example 3: Incomplete Answer
USER'S NATURAL LANGUAGE QUESTION:
请总结一下我们和ABC公司最近一次会议的主要议题和达成的三项关键共识。
Use code with caution.
CONTEXT PROVIDED TO THE RAG SYSTEM'S GENERATION MODEL:
上下文片段1 (来源: 会议纪要 - ABC公司会议_20250515.docx):
会议日期：2025年5月15日
与会方：我方（李明、王芳），ABC公司（张总、赵经理）
主要议题：
1.  回顾Q1合作项目进展。
2.  讨论Q2新产品联合推广计划。
3.  探讨长期战略合作框架。
关键共识：
1.  双方同意Q1项目按计划完成，成果符合预期。
2.  Q2新产品联合推广预算初定为50万，具体方案下周讨论。
3.  双方均表达了加强长期战略合作的意愿，将成立联合工作组进一步商议。
4.  下次会议暂定于6月初。
Use code with caution.
AI-GENERATED ANSWER TO EVALUATE:
我们和ABC公司最近一次会议（2025年5月15日）的主要议题包括回顾Q1项目进展和讨论Q2新产品联合推广计划。会议达成的一项关键共识是双方同意Q1项目按计划完成。
Use code with caution.
EXPECTED EVALUATION JSON OUTPUT:
{
  "evaluation_summary": {
    "overall_answer_quality_score": 3,
    "main_strengths_answer": "答案忠实于上下文，相关性较好，语言通顺。",
    "main_weaknesses_answer": "答案在完整性方面有明显不足，遗漏了多个重要议题和关键共识。"
  },
  "dimensions": {
    "faithfulness": {
      "score": 5,
      "reasoning": "答案中提到的信息点（会议日期、部分议题、一项共识）均能在上下文中找到准确依据。",
      "problematic_answer_segments_faithfulness": []
    },
    "relevance": {
      "score": 4,
      "reasoning": "答案回应了用户关于会议议题和共识的问题，但不够全面。"
    },
    "completeness": {
      "context_sufficiency_assessment": "Sufficient",
      "context_sufficiency_reasoning": "上下文详细列出了3个主要议题和4项关键共识，足以完整回答用户问题。",
      "score": 2,
      "reasoning": "答案严重不完整。议题方面遗漏了“探讨长期战略合作框架”。关键共识方面，用户要求三项，但答案只给出了一项，遗漏了“Q2推广预算初定”、“加强长期战略合作意愿将成立工作组”这两项重要共识（甚至还有第四项共识也未提及）。"
    },
    "coherence_fluency": {
      "score": 5,
      "reasoning": "答案语言表达清晰、语法正确。"
    },
    "actionability_usability": {
      "score": 3,
      "reasoning": "答案提供了一些信息，但由于信息不完整，其实用性打了折扣。用户可能需要再次查找才能获得全部关键信息。"
    }
  },
  "suggestion_for_answer_improvement": "答案应更全面地从上下文中提取信息。应完整列出所有主要议题，并至少满足用户要求的三个关键共识。例如，可以补充：'其他主要议题还包括探讨长期战略合作框架。达成的其他关键共识有：Q2新产品联合推广预算初定为50万；双方将成立联合工作组进一步商议加强长期战略合作的意愿。'"
}
Use code with caution.
Json
NOW, EVALUATE THE FOLLOWING:
USER'S NATURAL LANGUAGE QUESTION:
{{USER_QUESTION}}
Use code with caution.
CONTEXT PROVIDED TO THE RAG SYSTEM'S GENERATION MODEL (this is the information the AI had to base its answer on):
{{RETRIEVED_CONTEXTS}}
Use code with caution.
AI-GENERATED ANSWER TO EVALUATE:
{{GENERATED_ANSWER}}
Use code with caution.
YOUR EVALUATION (Strictly in the JSON format defined above):
// Your JSON output here
Use code with caution.
Json
"""

async def evaluate_answer_with_gemini(
    gemini_resource: 'GeminiAPIResource', # <--- 修改：接收 GeminiAPIResource 实例
    user_question: str,
    retrieved_contexts: str,
    generated_answer: str,
    original_interaction_id: Optional[str] = None,
    app_version: str = "0.1.0"
) -> Optional[Dict[str, Any]]:
    eval_logger.info(f"Starting Answer evaluation. User question: '{user_question[:50]}...', Answer: '{generated_answer[:50]}...'")

    prompt_to_gemini = ANSWER_EVALUATION_PROMPT_V1.replace(
        "{{USER_QUESTION}}", user_question
    ).replace(
        "{{RETRIEVED_CONTEXTS}}", retrieved_contexts
    ).replace(
        "{{GENERATED_ANSWER}}", generated_answer
    )

    messages_for_gemini = [{"role": "user", "content": prompt_to_gemini}]
    evaluation_result_json: Optional[Dict[str, Any]] = None
    raw_gemini_output: Optional[str] = None
    error_info: Optional[str] = None

    USE_SIMULATED_GEMINI_RESPONSE = os.getenv("USE_SIMULATED_GEMINI_ANSWER_EVAL", "false").lower() == "true"

    if USE_SIMULATED_GEMINI_RESPONSE:
        eval_logger.warning("USING SIMULATED GEMINI RESPONSE FOR ANSWER EVALUATION")
        simulated_json_output = {
            "evaluation_summary": {
                "overall_answer_quality_score": 4,
                "main_strengths_answer": "Answer is mostly faithful and relevant.",
                "main_weaknesses_answer": "Could be more complete by utilizing more context."
            },
            "dimensions": {
                "faithfulness": {
                    "score": 4,
                    "reasoning": "Most claims are supported by context.",
                    "problematic_answer_segments_faithfulness": []
                },
                "relevance": {
                    "score": 5,
                    "reasoning": "Directly addresses the user's question."
                },
                "completeness": {
                    "context_sufficiency_assessment": "Partially Sufficient",
                    "context_sufficiency_reasoning": "Context provides some info but lacks specific detail X.",
                    "score": 3,
                    "reasoning": "Answer uses available context but misses detail Y which was present."
                },
                "coherence_fluency": {
                    "score": 5,
                    "reasoning": "Well-written and easy to understand."
                },
                "actionability_usability": {
                    "score": 4,
                    "reasoning": "Provides useful information, could have more direct steps."
                }
            },
            "suggestion_for_answer_improvement": "Consider adding detail Y from context if relevant."
        }
        raw_gemini_output = json.dumps(simulated_json_output)
        try:
            evaluation_result_json = json.loads(raw_gemini_output)
        except json.JSONDecodeError as e:
            eval_logger.error(f"Error decoding simulated JSON for answer eval: {e}")
            error_info = f"Simulated JSON decode error for answer: {str(e)}"
    else:
        try:
            eval_logger.info(f"Calling Gemini for Answer evaluation via GeminiAPIResource. Model: {gemini_resource.model_name}. Prompt length: {len(prompt_to_gemini)}")
            # --- 修改：通过 gemini_resource 调用 ---
            raw_gemini_output = await gemini_resource.call_completion(
                messages=messages_for_gemini
            )
            if raw_gemini_output:
                eval_logger.info(f"Raw Gemini output for Answer eval (first 300 chars): {raw_gemini_output[:300]}...")
                cleaned_output = raw_gemini_output.strip()
                if cleaned_output.startswith("```json"):
                    cleaned_output = cleaned_output[len("```json"):].strip()
                if cleaned_output.endswith("```"):
                    cleaned_output = cleaned_output[:-len("```")].strip()
                evaluation_result_json = json.loads(cleaned_output)
                eval_logger.info("Successfully parsed Gemini evaluation result for Answer.")
            else:
                eval_logger.error("Gemini call via resource returned None or empty for Answer evaluation.")
                error_info = "Gemini call via resource returned None/empty for answer"
        except json.JSONDecodeError as e_json:
            eval_logger.error(f"Failed to decode JSON from Gemini Answer evaluation. Raw output: {raw_gemini_output[:500] if raw_gemini_output else 'N/A'}", exc_info=True)
            error_info = f"JSONDecodeError for answer: {str(e_json)}"
        except Exception as e_gen:
            eval_logger.error(f"Unexpected error during Answer evaluation with Gemini resource: {e_gen}", exc_info=True)
            error_info = f"Unexpected error for answer with Gemini resource: {str(e_gen)}"
            if raw_gemini_output is None:
                raw_gemini_output = error_info

    eval_log_data = {
        "task_type": "answer_evaluation_result",
        "original_interaction_id_ref": original_interaction_id,
        "user_question_for_eval": user_question,
        "retrieved_contexts_for_eval_char_count": len(retrieved_contexts),
        "generated_answer_for_eval": generated_answer,
        "eval_llm_input_prompt_char_count": len(prompt_to_gemini),
        "eval_llm_model": gemini_resource.model_name, # <--- 修改：从资源获取模型名称
        "eval_llm_raw_output": raw_gemini_output,
        "eval_llm_processed_output_json": evaluation_result_json,
        "eval_error_info": error_info,
        "application_version": app_version
    }
    await log_interaction_data(
        eval_log_data,
        is_evaluation_result=True,
        evaluation_name_for_file="answer_gemini_flash"
    )

    if evaluation_result_json:
        eval_logger.info(f"Answer evaluation completed. Overall score: {evaluation_result_json.get('evaluation_summary', {}).get('overall_answer_quality_score')}")
    else:
        eval_logger.warning("Answer evaluation did not produce a valid JSON result.")
        
    return evaluation_result_json

if __name__ == '__main__':
    import asyncio

    # --- 模拟一个 GeminiAPIResource 实例用于测试 ---
    # 在实际的 Dagster 环境中，这个资源会被正确注入
    # 为了让这里的测试能跑通，我们需要一个模拟的资源或配置
    
    class MockGeminiAPIResource:
        def __init__(self, api_key, model_name, proxy_url=None, temp=0.1, max_tokens=1024):
            self.api_key = api_key
            self.model_name = model_name
            self.proxy_url = proxy_url
            self.default_temperature = temp
            self.default_max_tokens = max_tokens
            self._logger = eval_logger # 使用eval_logger进行模拟
            self._logger.info("MockGeminiAPIResource initialized for testing.")
            if not self.api_key:
                 self._logger.warning("MockGeminiAPIResource: API key is missing!")


        async def call_completion(self, messages: List[Dict[str, str]], **kwargs) -> Optional[str]:
            self._logger.info(f"MockGeminiAPIResource.call_completion called with {len(messages)} messages.")
            # 这是一个非常简化的模拟，实际测试时可能需要更复杂的模拟响应
            # 或者直接依赖环境变量 USE_SIMULATED_GEMINI_..._EVAL 来触发 evaluator 内部的模拟
            if "cypher" in messages[0]["content"].lower():
                return json.dumps({
                    "evaluation_summary": {"overall_quality_score_cypher": 5, "main_strength_cypher": "Mocked Cypher OK", "main_weakness_cypher": "None"},
                    "dimensions": {}, "qwen_error_patterns_identified": [], "suggestion_for_improvement_cypher": "Mocked suggestion"
                })
            else:
                return json.dumps({
                     "evaluation_summary": {"overall_answer_quality_score": 5, "main_strengths_answer": "Mocked Answer OK", "main_weaknesses_answer": "None"},
                     "dimensions": {}, "suggestion_for_answer_improvement": "Mocked suggestion for answer"
                })

    async def test_evaluators_with_mock_resource():
        eval_logger.info("--- Running tests for evaluators with MockGeminiAPIResource ---")
        
        mock_api_key = os.getenv("GEMINI_API_KEY") or "MOCK_API_KEY_IF_NOT_SET"
        mock_resource = MockGeminiAPIResource(
            api_key=mock_api_key,
            model_name="gemini-1.5-flash-latest" # 与资源默认值一致
        )

        # --- Test Cypher Evaluation ---
        eval_logger.info("\n--- Testing Cypher Evaluation with Mock Resource ---")
        cypher_result = await evaluate_cypher_with_gemini(
            gemini_resource=mock_resource,
            user_question="Test Cypher Question?",
            generated_cypher="MATCH (n) RETURN n",
            original_interaction_id="test_cypher_eval_001"
        )
        if cypher_result:
            eval_logger.info(f"Mocked Cypher Eval Result: {json.dumps(cypher_result, indent=2, ensure_ascii=False)}")
        else:
            eval_logger.warning("Mocked Cypher Eval returned None.")

        # --- Test Answer Evaluation ---
        eval_logger.info("\n--- Testing Answer Evaluation with Mock Resource ---")
        answer_result = await evaluate_answer_with_gemini(
            gemini_resource=mock_resource,
            user_question="Test Answer Question?",
            retrieved_contexts="Some sample context.",
            generated_answer="A sample answer based on context.",
            original_interaction_id="test_answer_eval_001"
        )
        if answer_result:
            eval_logger.info(f"Mocked Answer Eval Result: {json.dumps(answer_result, indent=2, ensure_ascii=False)}")
        else:
            eval_logger.warning("Mocked Answer Eval returned None.")

    # 如果希望在直接运行 evaluator.py 时执行测试：
    # asyncio.run(test_evaluators_with_mock_resource())
    
    # 保留您之前的测试代码（如果需要独立运行并实际调用API，但要注意资源注入）
    # os.environ["USE_SIMULATED_GEMINI_ANSWER_EVAL"] = "true"
    # asyncio.run(test_answer_evaluation()) # 您之前的测试函数名
    pass # 通常 evaluator.py 不会直接运行，而是被其他脚本调用
```

File: zhz_rag/api/__init__.py
-----------------------------
--- File is empty ---

File: zhz_rag/api/main_api.py
-----------------------------
```python
# zhz_agent/main.py
from fastapi import FastAPI
from contextlib import asynccontextmanager
import uvicorn
import logging
from typing import Type, List, Dict, Any, Optional, ClassVar

# --- [修改] 导入 -> 改为绝对导入 ---
from zhz_rag.utils.db_utils import database, sqlalchemy_engine, Base, get_scheduler
from zhz_rag.api.task_manager_api import router as tasks_router
from zhz_rag.task_management import db_models # 确保 SQLAlchemy 模型被导入

@asynccontextmanager
async def lifespan(app_instance: FastAPI):
    print("--- Main FastAPI 应用启动 (已集成任务管理API 和 APScheduler) ---") # [修改] 更新了描述

    # --- 数据库初始化 ---
    await database.connect()
    print("数据库已连接。")
    try:
        Base.metadata.create_all(bind=sqlalchemy_engine)
        print("数据库表已执行 create_all。")
        from sqlalchemy import inspect
        inspector = inspect(sqlalchemy_engine)
        if inspector.has_table("tasks"):
            print("'tasks' 表已成功创建/存在于数据库中。")
        else:
            print("警告: 'tasks' 表在 create_all 之后仍未找到！这通常意味着模型没有在 create_all 之前被正确导入。")
            print(f"   已知的表: {inspector.get_table_names()}") # 打印所有实际创建的表
            print(f"   Base.metadata.tables: {Base.metadata.tables.keys()}") # 打印 SQLAlchemy 元数据中已注册的表
    except Exception as e:
        print(f"创建或检查数据库表时出错: {e}")
        import traceback
        traceback.print_exc() # 打印详细的异常堆栈


    # --- [修改] APScheduler 初始化 (使用 get_scheduler) ---
    current_scheduler = get_scheduler() # <--- 获取调度器实例
    try:
        logging.getLogger('apscheduler').setLevel(logging.DEBUG) # 设置为 DEBUG 级别

        if not current_scheduler.running: # <--- 只有在未运行时才启动
            current_scheduler.start()
            print("APScheduler 已启动并使用数据库作业存储。")
        else:
            print("APScheduler 已在运行。")
    except Exception as e:
        print(f"APScheduler 启动失败: {e}")

    print("RAG 组件的初始化和管理在 zhz_agent_mcp_server.py。")
    print("任务管理API已在 /tasks 路径下可用。")

    yield # FastAPI 应用在此运行

    print("--- Main FastAPI 应用关闭 ---")
    current_scheduler_on_shutdown = get_scheduler() # <--- 再次获取以确保是同一个实例
    if current_scheduler_on_shutdown and current_scheduler_on_shutdown.running:
        current_scheduler_on_shutdown.shutdown()
        print("APScheduler 已关闭。")
    await database.disconnect()
    print("数据库已断开连接。")
    print("RAG 组件的清理在 zhz_agent_mcp_server.py。")

# --- App 定义 (保持不变) ---
app = FastAPI(
    title="Hybrid RAG Backend with Task Management",
    description="主 FastAPI 应用，负责接收请求、编排 Agent，并提供任务管理API。",
    version="0.2.1",
    lifespan=lifespan
)

app.include_router(tasks_router)

@app.get("/")
async def read_root():
    return {
        "message": "Welcome to the Hybrid RAG Backend Main App.",
        "available_services": {
            "task_management": "/tasks/docs",
            "rag_via_mcpo": "mcpo proxy at port 8006 (see mcpo_servers.json)"
        }
    }

if __name__ == "__main__":
    print("--- 启动 Main FastAPI 服务器 (包含任务管理API) ---")
    uvicorn.run("zhz_agent.main:app", host="0.0.0.0", port=8000, reload=True) # Ensure correct run command
```

File: zhz_rag/api/rag_api_service.py
------------------------------------
```python
import os
import asyncio
from contextlib import asynccontextmanager
from typing import List, Dict, Any, Optional
import logging
import sys
import uvicorn
import traceback
from fastapi import FastAPI, Request, HTTPException, BackgroundTasks # <--- 修改这里
from dataclasses import dataclass
from dotenv import load_dotenv
import uuid
from datetime import datetime, timezone

# --- 添加开始：明确指定 .env 文件路径 ---
# 获取 rag_api_service.py 文件所在的目录
_current_file_dir = os.path.dirname(os.path.abspath(__file__))
# .env 文件通常在项目根目录，即 zhz_rag 包的上两级目录 (zhz_agent/)
_project_root_dir = os.path.abspath(os.path.join(_current_file_dir, "..", ".."))
_dotenv_path = os.path.join(_project_root_dir, ".env")

if os.path.exists(_dotenv_path):
    load_dotenv(dotenv_path=_dotenv_path)
    print(f"RagApiService: Successfully loaded .env file from: {_dotenv_path}")
else:
    print(f"RagApiService: .env file not found at {_dotenv_path}. Relying on system environment variables or defaults.")
    # 即使没找到，也执行一次 load_dotenv()，它可能会从其他地方加载或什么都不做
    load_dotenv()

# --- 导入应用模块 ---
from zhz_rag.config.pydantic_models import QueryRequest, HybridRAGResponse, RetrievedDocument
from zhz_rag.llm.llm_interface import generate_answer_from_context, generate_expanded_queries, NO_ANSWER_PHRASE_ANSWER_CLEAN
from zhz_rag.llm.local_model_handler import LocalModelHandler
from zhz_rag.core_rag.retrievers.chromadb_retriever import ChromaDBRetriever
from zhz_rag.core_rag.retrievers.file_bm25_retriever import FileBM25Retriever
from zhz_rag.core_rag.kg_retriever import KGRetriever
from zhz_rag.core_rag.retrievers.embedding_functions import LlamaCppEmbeddingFunction
from zhz_rag.core_rag.fusion_engine import FusionEngine
from zhz_rag.utils.interaction_logger import log_interaction_data

# --- 日志配置 ---
api_logger = logging.getLogger("RAGApiServiceLogger")
api_logger.setLevel(logging.INFO)
if not api_logger.hasHandlers():
    handler = logging.StreamHandler(sys.stdout)
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(module)s:%(lineno)d - %(message)s')
    handler.setFormatter(formatter)
    api_logger.addHandler(handler)
    api_logger.propagate = False

# --- 应用上下文 Dataclass ---
@dataclass
class RAGAppContext:
    model_handler: LocalModelHandler
    chroma_retriever: ChromaDBRetriever
    kg_retriever: KGRetriever
    file_bm25_retriever: FileBM25Retriever
    fusion_engine: FusionEngine

# --- FastAPI 生命周期管理 ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    api_logger.info("--- RAG API Service: Initializing RAG components with Qwen3 GGUF models ---")
    
    # --- 读取环境变量 ---
    llm_gguf_model_path_for_handler = os.getenv("LOCAL_LLM_GGUF_MODEL_PATH")
    embedding_gguf_model_path = os.getenv("EMBEDDING_MODEL_PATH")
    chroma_persist_dir = os.getenv("CHROMA_PERSIST_DIRECTORY")
    bm25_index_dir = os.getenv("BM25_INDEX_DIRECTORY")
    duckdb_file_path_for_api = os.getenv("DUCKDB_KG_FILE_PATH")

    embedding_pool_size_str = os.getenv("EMBEDDING_SUBPROCESS_POOL_SIZE")
    embedding_pool_size = None
    if embedding_pool_size_str and embedding_pool_size_str.isdigit():
        embedding_pool_size = int(embedding_pool_size_str)
        api_logger.info(f"RAG API Service: Embedding subprocess pool size set from env: {embedding_pool_size}")
    else:
        api_logger.info(f"RAG API Service: Embedding subprocess pool size not set or invalid in env, LocalModelHandler will use default.")

    # --- 环境变量检查 ---
    required_paths_for_log = {
        "EMBEDDING_MODEL_PATH": embedding_gguf_model_path,
        "CHROMA_PERSIST_DIRECTORY": chroma_persist_dir,
        "BM25_INDEX_DIRECTORY": bm25_index_dir,
        "DUCKDB_KG_FILE_PATH": duckdb_file_path_for_api
    }
    missing_paths = [name for name, path in required_paths_for_log.items() if not path]

    if missing_paths:
        api_logger.critical(f"Critical environment variables not set: {', '.join(missing_paths)}")
        app.state.rag_context = None
        yield
        return

    try:
        # 1. 初始化 LocalModelHandler
        api_logger.info(f"Initializing LocalModelHandler with Embedding GGUF: {embedding_gguf_model_path}")
        if llm_gguf_model_path_for_handler:
            api_logger.info(f"LocalModelHandler will also attempt to load LLM GGUF: {llm_gguf_model_path_for_handler}")
        
        model_handler = LocalModelHandler(
            llm_model_path=llm_gguf_model_path_for_handler,
            embedding_model_path=embedding_gguf_model_path,
            n_gpu_layers_embed=int(os.getenv("EMBEDDING_N_GPU_LAYERS", 0)),
            n_gpu_layers_llm=int(os.getenv("LLM_N_GPU_LAYERS", 0)),
            embedding_pool_size=embedding_pool_size
        )
        api_logger.info("LocalModelHandler instance created. Embedding operations will use subprocess pool.")

        # 2. 创建 LlamaCppEmbeddingFunction
        api_logger.info("Creating LlamaCppEmbeddingFunction...")
        custom_embed_fn = LlamaCppEmbeddingFunction(model_handler=model_handler)

        # 3. 初始化 ChromaDBRetriever
        api_logger.info(f"Initializing ChromaDBRetriever with persist_directory: {chroma_persist_dir}")
        chroma_retriever_instance = ChromaDBRetriever(
            collection_name=os.getenv("CHROMA_COLLECTION_NAME", "rag_documents"),
            persist_directory=chroma_persist_dir,
            embedding_function=custom_embed_fn
        )

        # 4. 初始化 FileBM25Retriever
        api_logger.info(f"Initializing FileBM25Retriever with index_directory: {bm25_index_dir}")
        file_bm25_retriever_instance = FileBM25Retriever(index_directory=bm25_index_dir)

        # 5. 初始化 KGRetriever
        api_logger.info(f"Initializing KGRetriever (DuckDB) with db_file_path: {duckdb_file_path_for_api}")
        kg_retriever_instance = KGRetriever(
            db_file_path=duckdb_file_path_for_api,
            embedder=model_handler 
        )
        api_logger.info("KGRetriever (DuckDB) initialized for RAG API Service.")
        
        # 6. 初始化 FusionEngine
        api_logger.info("Initializing FusionEngine...")
        use_rrf_env = os.getenv("FUSION_USE_RRF", "true").lower() == "true"
        fusion_engine_instance = FusionEngine(logger=api_logger, use_rrf=use_rrf_env, rrf_k=60)

        app.state.rag_context = RAGAppContext(
            model_handler=model_handler,
            chroma_retriever=chroma_retriever_instance,
            kg_retriever=kg_retriever_instance,
            file_bm25_retriever=file_bm25_retriever_instance,
            fusion_engine=fusion_engine_instance
        )
        api_logger.info("--- RAG components initialized successfully. Embedding uses subprocesses. Service is ready. ---")
    except Exception as e:
        api_logger.critical(f"FATAL: Failed to initialize RAG components during startup: {e}", exc_info=True)
        app.state.rag_context = None 
    
    yield
    
    # --- 应用关闭阶段 ---
    api_logger.info("--- RAG API Service: Cleaning up resources ---")
    rag_context_to_clean: Optional[RAGAppContext] = getattr(app.state, 'rag_context', None)
    
    if rag_context_to_clean:
        if rag_context_to_clean.kg_retriever and hasattr(rag_context_to_clean.kg_retriever, 'close'):
            try:
                rag_context_to_clean.kg_retriever.close() 
                api_logger.info("KGRetriever closed.")
            except Exception as e_kg_close:
                api_logger.error(f"Error closing KGRetriever: {e_kg_close}", exc_info=True)

        if rag_context_to_clean.model_handler:
            api_logger.info("Attempting to close LocalModelHandler's embedding pool...")
            try:
                rag_context_to_clean.model_handler.close_embedding_pool()
            except Exception as e_pool_close:
                api_logger.error(f"Error explicitly closing LocalModelHandler's embedding pool: {e_pool_close}", exc_info=True)
    
    api_logger.info("--- Cleanup complete. ---")

# --- FastAPI 应用实例 ---
app = FastAPI(
    title="Upgraded Standalone RAG API Service",
    description="Provides API access to the RAG framework, now powered by Qwen3 models.",
    version="2.0.0",
    lifespan=lifespan
)

async def run_with_semaphore(semaphore: asyncio.Semaphore, coro_or_func_name: str, coro_obj):
    async with semaphore:
        api_logger.debug(f"Semaphore acquired for async task: {coro_or_func_name}")
        result = await coro_obj
        api_logger.debug(f"Semaphore released for async task: {coro_or_func_name}")
        return result

async def run_sync_with_semaphore(semaphore: asyncio.Semaphore, func_name: str, func_obj, *args):
    async with semaphore:
        api_logger.debug(f"Semaphore acquired for sync task: {func_name}")
        result = await asyncio.to_thread(func_obj, *args)
        api_logger.debug(f"Semaphore released for sync task: {func_name}")
        return result

# --- API 端点 ---
@app.post("/api/v1/rag/query", response_model=HybridRAGResponse)
async def query_rag_endpoint(
    request: Request, # request 参数保持不变
    query_request: QueryRequest,
    background_tasks: BackgroundTasks # <--- 新增：添加 BackgroundTasks 依赖注入
):
    api_logger.info(f"\n--- Received RAG query: '{query_request.query}' ---")
    start_time_total = datetime.now(timezone.utc)

    CONCURRENT_CPU_OPERATIONS_LIMIT = int(os.getenv("RAG_CPU_CONCURRENCY_LIMIT", "2"))
    cpu_bound_semaphore = asyncio.Semaphore(CONCURRENT_CPU_OPERATIONS_LIMIT)
    api_logger.info(f"Using Semaphore with limit: {CONCURRENT_CPU_OPERATIONS_LIMIT} for CPU-bound tasks.")

    app_ctx: RAGAppContext = request.app.state.rag_context
    if not app_ctx:
        # 这个错误日志本身可能也无法保证写入，但尽力而为
        log_critical_error_data = {
            "interaction_id": str(uuid.uuid4()), "timestamp_utc": datetime.now(timezone.utc).isoformat(),
            "task_type": "rag_query_processing_critical_failure", "original_user_query": query_request.query,
            "error_details": "RAG service context not initialized.", "application_version": "2.0.0"
        }
        # 对于这种严重错误，我们尝试立即记录，但不依赖 BackgroundTasks，因为它可能在 app_ctx 检查失败后无法使用
        try:
            # 这里的log_interaction_data仍然是异步的，但这是启动错误，不是常规流程
            # 如果担心这里的日志也丢失，可以考虑一个同步的紧急日志函数
            asyncio.create_task(log_interaction_data(log_critical_error_data)) # 尝试作为独立任务启动
        except Exception as log_e:
            api_logger.error(f"Failed to log critical context initialization error: {log_e}")
        raise HTTPException(status_code=503, detail="RAG service is not properly initialized.")

    final_answer_for_log = "Error: Processing incomplete"
    final_context_docs_for_log: List[Dict[str, Any]] = []
    expanded_queries_for_log: Optional[List[str]] = None
    response_to_return: Optional[HybridRAGResponse] = None
    exception_occurred: Optional[Exception] = None
    interaction_id_for_log = str(uuid.uuid4()) # <--- 在try之前生成ID，以便finally中总能引用

    try:
        api_logger.info("Acquiring semaphore for 'generate_expanded_queries'...")
        expanded_queries = await run_with_semaphore(
            cpu_bound_semaphore,
            "generate_expanded_queries",
            generate_expanded_queries(query_request.query)
        )
        expanded_queries_for_log = expanded_queries
        api_logger.info(f"Query expansion completed. Expanded queries: {expanded_queries}")

        retrieval_tasks = []
        for q_text in expanded_queries:
            api_logger.debug(f"Preparing retrieval tasks for query part: '{q_text}'")
            retrieval_tasks.append(run_with_semaphore(cpu_bound_semaphore, "chroma_retriever.retrieve", app_ctx.chroma_retriever.retrieve(q_text, query_request.top_k_vector)))
            retrieval_tasks.append(run_sync_with_semaphore(cpu_bound_semaphore, "file_bm25_retriever.retrieve", app_ctx.file_bm25_retriever.retrieve, q_text, query_request.top_k_bm25))
            retrieval_tasks.append(run_with_semaphore(cpu_bound_semaphore, "kg_retriever.retrieve", app_ctx.kg_retriever.retrieve(q_text, query_request.top_k_kg)))
        
        api_logger.info(f"Gathering {len(retrieval_tasks)} retrieval tasks...")
        results_from_gather = await asyncio.gather(*retrieval_tasks, return_exceptions=True)
        api_logger.info("All retrieval tasks completed.")

        all_retrieved_docs: List[RetrievedDocument] = []
        bm25_results_to_enrich: List[Dict[str, Any]] = []

        for i, task_result_group in enumerate(results_from_gather):
            if isinstance(task_result_group, Exception):
                api_logger.error(f"An error occurred in retrieval task {i}: {task_result_group}", exc_info=task_result_group)
                continue
            if not task_result_group:
                api_logger.info(f"Retrieval task {i} returned empty result.")
                continue
            if not isinstance(task_result_group, list):
                api_logger.warning(f"Retrieval task {i} returned non-list result: {type(task_result_group)} - {str(task_result_group)[:100]}")
                continue
            for item_idx, item in enumerate(task_result_group):
                if not isinstance(item, dict):
                    api_logger.warning(f"Task {i}, Item {item_idx}: Skipping non-dict item: {type(item)} - {str(item)[:100]}")
                    continue
                # ... (内部的实体、chroma、bm25等解析和添加到all_retrieved_docs的逻辑保持不变) ...
                source_type = item.get("source_type")
                item_content = item.get("content")
                item_score = item.get("score")
                item_metadata = item.get("metadata", {})
                
                api_logger.debug(f"Task {i}, Item {item_idx}: Processing item with source_type='{source_type}', content_present={item_content is not None}, score={item_score}")

                if source_type == "duckdb_kg":
                    try:
                        if item_content is None: item_content = f"[Content missing for KG doc {item_metadata.get('duckdb_retrieved_id_prop', 'UNKNOWN_ID')}]"
                        doc_to_add = RetrievedDocument(source_type=str(source_type), content=str(item_content), score=item_score, metadata=item_metadata)
                        all_retrieved_docs.append(doc_to_add)
                    except Exception as e_kg_parse: api_logger.error(f"Task {i}, Item {item_idx} (KG): Failed to parse into RetrievedDocument: {item}, error: {e_kg_parse}")
                elif source_type == "vector_chromadb":
                    try:
                        chroma_item_id = item.get("id")
                        if item_content is None: item_content = f"[Content missing for ChromaDB doc {chroma_item_id or 'UNKNOWN_ID'}]"
                        final_metadata = item_metadata.copy()
                        if "chunk_id" not in final_metadata and chroma_item_id: final_metadata["chunk_id_from_chroma_retrieve"] = chroma_item_id
                        doc_to_add = RetrievedDocument(source_type=str(source_type), content=str(item_content), score=item_score, metadata=final_metadata)
                        all_retrieved_docs.append(doc_to_add)
                    except Exception as e_chroma_parse: api_logger.error(f"Task {i}, Item {item_idx} (ChromaDB): Failed to parse into RetrievedDocument: {item}, error: {e_chroma_parse}")
                elif source_type == "keyword_bm25":
                    bm25_item_id = item.get("id")
                    if bm25_item_id is not None and item_score is not None:
                        item_for_enrich = {"id": bm25_item_id, "score": item_score, "source_type": source_type}
                        if item_metadata: item_for_enrich["metadata"] = item_metadata
                        bm25_results_to_enrich.append(item_for_enrich)
                    else: api_logger.warning(f"Task {i}, Item {item_idx} (BM25): Item missing 'id' or 'score', skipped for enrichment: {item}")
                else: api_logger.warning(f"Task {i}, Item {item_idx}: Unknown or unhandled source_type='{source_type}'. Item skipped: {str(item)[:200]}")
        
        api_logger.info(f"Processed gather results. Docs directly added (all_retrieved_docs): {len(all_retrieved_docs)}. BM25 results to enrich: {len(bm25_results_to_enrich)}")
        
        if bm25_results_to_enrich:
            bm25_ids = list(set([res['id'] for res in bm25_results_to_enrich if 'id' in res]))
            if bm25_ids:
                api_logger.info(f"Enriching {len(bm25_ids)} unique BM25 document IDs by fetching texts from ChromaDB.")
                texts_map = await app_ctx.chroma_retriever.get_texts_by_ids(bm25_ids)
                for res in bm25_results_to_enrich:
                    chunk_id = res['id']
                    content_text = texts_map.get(chunk_id)
                    if content_text is None or "[Content for chunk_id" in content_text:
                        content_text = f"[Content for BM25 chunk_id {chunk_id} not found in ChromaDB]"
                    all_retrieved_docs.append(RetrievedDocument(source_type="keyword_bm25", content=content_text, score=res.get('score'), metadata=res.get('metadata', {"chunk_id": chunk_id})))
                api_logger.info(f"BM25 results enriched. Total docs now: {len(all_retrieved_docs)}")

        if not all_retrieved_docs:
            api_logger.info("No documents retrieved from any source after processing all tasks.")
            final_answer_for_log = NO_ANSWER_PHRASE_ANSWER_CLEAN
            response_to_return = HybridRAGResponse(answer=final_answer_for_log, original_query=query_request.query, retrieved_sources=[])

        if response_to_return is None:
            api_logger.info(f"Acquiring semaphore for 'fusion_engine.fuse_results' (total {len(all_retrieved_docs)} docs)...")
            final_context_docs_obj = await run_with_semaphore( # Renamed to avoid confusion
                cpu_bound_semaphore,
                "fusion_engine.fuse_results",
                app_ctx.fusion_engine.fuse_results(all_retrieved_docs, query_request.query, query_request.top_k_final)
            )
            final_context_docs_for_log = [doc.model_dump(exclude_none=True) for doc in final_context_docs_obj] # Use new var
            api_logger.info(f"Fusion engine processing completed. Final context docs: {len(final_context_docs_obj)}")

            if not final_context_docs_obj:
                final_answer_for_log = NO_ANSWER_PHRASE_ANSWER_CLEAN
                api_logger.info("No relevant context found after fusion. Returning no answer.")
            else:
                context_strings = []
                for doc in final_context_docs_obj:
                    score_to_display = doc.score
                    score_str = f"{score_to_display:.4f}" if isinstance(score_to_display, float) else str(score_to_display if score_to_display is not None else 'N/A')
                    context_strings.append(f"Source Type: {doc.source_type}, Score: {score_str}\nContent: {doc.content}")
                fused_context = "\n\n---\n\n".join(context_strings)
                api_logger.info(f"Fused context for LLM (length: {len(fused_context)} chars): {fused_context[:500]}...")

                api_logger.info("Acquiring semaphore for 'generate_answer_from_context'...")
                generated_final_answer = await run_with_semaphore(
                    cpu_bound_semaphore,
                    "generate_answer_from_context",
                    generate_answer_from_context(query_request.query, fused_context)
                )
                final_answer_for_log = generated_final_answer or NO_ANSWER_PHRASE_ANSWER_CLEAN
                api_logger.info(f"Answer generation completed.  {final_answer_for_log[:200]}...")
            
            response_to_return = HybridRAGResponse(answer=final_answer_for_log, original_query=query_request.query, retrieved_sources=final_context_docs_obj if final_context_docs_obj else [])

    except Exception as e:
        api_logger.error(f"Critical error in query_rag_endpoint: {e}", exc_info=True)
        exception_occurred = e
        final_answer_for_log = f"Error: Processing failed due to {type(e).__name__}" # More generic error for log
        response_to_return = HybridRAGResponse(
            answer=f"An internal error occurred: {str(e)}",
            original_query=query_request.query,
            retrieved_sources=[],
            debug_info={"error": str(e), "type": type(e).__name__}
        )

    finally:
        processing_time_seconds = (datetime.now(timezone.utc) - start_time_total).total_seconds()
        api_logger.info(f"!!!!!!!!!! FINALLY BLOCK ENTERED. Processing time so far: {processing_time_seconds:.3f}s !!!!!!!!!!")

        # Ensure all variables for logging have a sensible default if an error occurred early
        current_final_answer = final_answer_for_log if 'final_answer_for_log' in locals() and final_answer_for_log is not None else "Error: final_answer_for_log not set in finally"
        current_final_docs = final_context_docs_for_log if 'final_context_docs_for_log' in locals() and final_context_docs_for_log is not None else []
        current_expanded_q_count = len(expanded_queries_for_log) if 'expanded_queries_for_log' in locals() and expanded_queries_for_log is not None else 0
        
        interaction_log_entry = {
            "interaction_id": interaction_id_for_log, # Use pre-generated ID
            "timestamp_utc": datetime.now(timezone.utc).isoformat(),
            "task_type": "rag_query_processing_full_log",
            "original_user_query": query_request.query,
            "final_answer_from_llm": current_final_answer,
            "final_context_docs_full": current_final_docs,
            "retrieval_parameters": query_request.model_dump(),
            "expanded_queries_count": current_expanded_q_count,
            "processing_time_seconds": round(processing_time_seconds, 3)
        }

        if exception_occurred:
            interaction_log_entry["error_details"] = f"{type(exception_occurred).__name__}: {str(exception_occurred)}"
            interaction_log_entry["error_traceback"] = traceback.format_exc() if hasattr(exception_occurred, '__traceback__') else "No traceback available"

        api_logger.info(f"!!!!!!!!!! FINALLY BLOCK: interaction_log_entry BEFORE logging: {str(interaction_log_entry)[:500]}... !!!!!!!!!!")

        try:
            # --- 修改：使用 BackgroundTasks ---
            background_tasks.add_task(log_interaction_data, interaction_log_entry)
            api_logger.info(f"!!!!!!!!!! FINALLY BLOCK: Full RAG interaction log (ID: {interaction_log_entry['interaction_id']}) ADDED TO BACKGROUND TASKS. !!!!!!!!!!")
        except Exception as log_e_final:
            api_logger.error(f"!!!!!!!!!! FINALLY BLOCK CRITICAL ERROR: Failed to add log task to BackgroundTasks: {log_e_final} !!!!!!!!!!", exc_info=True)
        
        if exception_occurred:
            api_logger.error(f"!!!!!!!!!! FINALLY BLOCK: Raising HTTPException due to prior error: {str(exception_occurred)} !!!!!!!!!!")
            raise HTTPException(status_code=500, detail=f"Internal Server Error: {str(exception_occurred)}")
        
        if response_to_return is None:
             api_logger.error("!!!!!!!!!! FINALLY BLOCK CRITICAL: response_to_return was not set. !!!!!!!!!!")
             raise HTTPException(status_code=500, detail="Internal Server Error: Response generation failed unexpectedly.")

        api_logger.info(f"!!!!!!!!!! FINALLY BLOCK: Returning final response. !!!!!!!!!!")
        return response_to_return

# --- Main execution block (保持不变) ---
if __name__ == "__main__":
    api_logger.info("Starting Standalone RAG API Service...")
    uvicorn.run("zhz_rag.api.rag_api_service:app", host="0.0.0.0", port=8081, reload=False) # 确保 reload=False
```

File: zhz_rag/api/rag_mcp_service.py
------------------------------------
```python
import os
import json
import asyncio
import traceback
from contextlib import asynccontextmanager
from typing import List, Dict, Any, Optional, AsyncIterator
from dataclasses import dataclass, field # 确保导入 field
import time
import logging
import sys
import hashlib # <--- 添加
from datetime import datetime, timezone # <--- 添加
import uuid # <--- 添加


# MCP 框架导入
from mcp.server.fastmcp import FastMCP, Context

# --- 配置 rag_service 的专用日志 ---
_rag_service_py_dir = os.path.dirname(os.path.abspath(__file__))
_rag_service_log_file = os.path.join(_rag_service_py_dir, 'rag_service_debug.log')

rag_logger = logging.getLogger("RagServiceLogger")
rag_logger.setLevel(logging.DEBUG)
rag_logger.propagate = False

if rag_logger.hasHandlers():
    rag_logger.handlers.clear()

try:
    _file_handler = logging.FileHandler(_rag_service_log_file, mode='w')
    _file_handler.setLevel(logging.DEBUG)
    _formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
    _file_handler.setFormatter(_formatter)
    rag_logger.addHandler(_file_handler)
    rag_logger.info("--- RagServiceLogger configured to write to rag_service_debug.log ---")
except Exception as e:
    print(f"CRITICAL: Failed to configure RagServiceLogger: {e}")


# --- 从项目内部导入所有 RAG 模块 ---
from zhz_rag.config.pydantic_models import QueryRequest, HybridRAGResponse, RetrievedDocument
from zhz_rag.llm.llm_interface import (
    generate_answer_from_context,
    generate_expanded_queries,
    generate_cypher_query,
    generate_clarification_question,
    generate_intent_classification,
    NO_ANSWER_PHRASE_ANSWER_CLEAN
)
from zhz_rag.core_rag.retrievers.chromadb_retriever import ChromaDBRetriever
from zhz_rag.core_rag.retrievers.file_bm25_retriever import FileBM25Retriever
from zhz_rag.core_rag.kg_retriever import KGRetriever
from zhz_rag.core_rag.fusion_engine import FusionEngine
from zhz_rag.utils.common_utils import log_interaction_data

from dotenv import load_dotenv

# 加载 .env 文件
# __file__ 是当前 rag_service.py 的路径: /home/zhz/zhz_agent/rag_service.py
# os.path.dirname(os.path.abspath(__file__)) 是 /home/zhz/zhz_agent 目录
# .env 文件与 rag_service.py 在同一个目录下 (zhz_agent 目录)
current_dir = os.path.dirname(os.path.abspath(__file__))
dotenv_path = os.path.join(current_dir, '.env')

if os.path.exists(dotenv_path):
    load_dotenv(dotenv_path=dotenv_path)
    rag_logger.info(f"Loaded .env file from: {dotenv_path}")
else:
    rag_logger.warning(f".env file not found at {dotenv_path}, will rely on environment variables or defaults.")
    # 仍然尝试加载，因为python-dotenv的默认行为是查找当前工作目录和上级目录的.env
    load_dotenv()

# --- 应用上下文 Dataclass ---
@dataclass
class AppContext:
    # vector_retriever: VectorRetriever # 旧的
    chroma_retriever: Optional[ChromaDBRetriever] = None # 新的
    kg_retriever: Optional[KGRetriever] = None
    # bm25_retriever: BM25Retriever # 旧的
    file_bm25_retriever: Optional[FileBM25Retriever] = None # 新的
    fusion_engine: Optional[FusionEngine] = None
    # llm_generator: Optional[Any] = None # LLMGenerator在您的代码中没有被实例化并放入AppContext

# --- MCP 服务器生命周期管理 ---
@asynccontextmanager
async def app_lifespan_for_rag_service(server: FastMCP) -> AsyncIterator[AppContext]:
    rag_logger.info("--- RAG Service (FastMCP): 正在初始化 RAG 组件 (新版) ---")
    
    chroma_retriever_instance: Optional[ChromaDBRetriever] = None
    kg_retriever_instance: Optional[KGRetriever] = None
    file_bm25_retriever_instance: Optional[FileBM25Retriever] = None
    fusion_engine_instance: Optional[FusionEngine] = None

    # 初始化 ChromaDB Retriever
    try:
        # 这些路径和名称应该与Dagster流水线中配置的一致
        # 优先从环境变量读取，如果不存在则使用默认值（如果适用）
        chroma_persist_dir = os.getenv("CHROMA_PERSIST_DIRECTORY", "/home/zhz/dagster_home/chroma_data")
        chroma_collection_name = os.getenv("CHROMA_COLLECTION_NAME", "rag_documents")
        embedding_model_path = os.getenv("EMBEDDING_MODEL_PATH", "/home/zhz/models/bge-small-zh-v1.5")

        if not os.path.isdir(chroma_persist_dir):
                rag_logger.warning(f"ChromaDB persist directory '{chroma_persist_dir}' not found. Retrieval may fail or use an in-memory DB if ChromaDBRetriever handles this.")
        if not os.path.exists(embedding_model_path): # embedding_model_path 应该是目录
            rag_logger.warning(f"Embedding model path '{embedding_model_path}' not found. ChromaDBRetriever initialization might fail.")

        chroma_retriever_instance = ChromaDBRetriever(
            collection_name=chroma_collection_name,
            persist_directory=chroma_persist_dir,
            embedding_model_name_or_path=embedding_model_path
        )
        rag_logger.info("RAG Service: ChromaDBRetriever 初始化成功。")
    except Exception as e:
        rag_logger.error(f"RAG Service: ChromaDBRetriever 初始化失败: {e}", exc_info=True)
        # 不在此处抛出异常，允许服务在部分组件失败时仍能启动（如果设计如此）
    
    # 初始化 File BM25 Retriever
    try:
        bm25_index_dir = os.getenv("BM25_INDEX_DIRECTORY", "/home/zhz/dagster_home/bm25_index_data/")
        if not os.path.isdir(bm25_index_dir):
            rag_logger.warning(f"BM25 index directory '{bm25_index_dir}' not found. FileBM25Retriever initialization might fail.")
            
        file_bm25_retriever_instance = FileBM25Retriever(
            index_directory_path=bm25_index_dir
        )
        rag_logger.info("RAG Service: FileBM25Retriever 初始化成功。")
    except Exception as e:
        rag_logger.error(f"RAG Service: FileBM25Retriever 初始化失败: {e}", exc_info=True)

    # 初始化 KG Retriever
    try:
        # KGRetriever 内部会从环境变量读取NEO4J配置
        # generate_cypher_query 是从 zhz_agent.llm 导入的
        kg_retriever_instance = KGRetriever(llm_cypher_generator_func=generate_cypher_query)
        rag_logger.info("RAG Service: KGRetriever 初始化成功。")
    except Exception as e:
        rag_logger.error(f"RAG Service: KGRetriever 初始化失败: {e}", exc_info=True)
        if kg_retriever_instance and hasattr(kg_retriever_instance, 'close'): # 确保在失败前尝试关闭
            kg_retriever_instance.close()
            
    # 初始化 Fusion Engine
    try:
        fusion_engine_instance = FusionEngine(logger=rag_logger)
        rag_logger.info("RAG Service: FusionEngine 初始化成功。")
    except Exception as e:
        rag_logger.error(f"RAG Service: FusionEngine 初始化失败: {e}", exc_info=True)

    rag_logger.info("--- RAG Service (FastMCP): RAG 组件初始化尝试完成。---")

    ctx = AppContext(
        chroma_retriever=chroma_retriever_instance,
        kg_retriever=kg_retriever_instance,
        file_bm25_retriever=file_bm25_retriever_instance,
        fusion_engine=fusion_engine_instance
    )
    try:
        yield ctx
    finally:
        rag_logger.info("--- RAG Service (FastMCP): 正在清理资源 ---")
        if kg_retriever_instance: # 确保只在成功初始化后才调用close
            kg_retriever_instance.close() 
        rag_logger.info("--- RAG Service (FastMCP): 清理完成 ---")

# --- 初始化 FastMCP 应用 ---
rag_mcp_application = FastMCP(
    name="zhz_agent_rag_service", # 修改了服务名称以区分
    description="Upgraded Hybrid RAG 服务，使用持久化知识库。",
    lifespan=app_lifespan_for_rag_service,
)

@rag_mcp_application.tool()
async def query_rag_v2( # 重命名工具函数以避免与旧的混淆 (如果需要)
    ctx: Context,
    query: str, # 直接使用 query 作为输入，而不是 QueryRequest 对象
    top_k_vector: int = 3,
    top_k_kg: int = 2,
    top_k_bm25: int = 3,
    top_k_final: int = 3 # 最终融合后返回的文档数
) -> str: 
    rag_logger.info(f"\n--- RAG Service (query_rag_v2): 接收到查询: '{query}' ---")
    rag_logger.info(f"      Params: top_k_vector={top_k_vector}, top_k_kg={top_k_kg}, top_k_bm25={top_k_bm25}, top_k_final={top_k_final}")
    start_time_total = time.time()

    app_ctx: AppContext = ctx.request_context.lifespan_context
    response_payload = {} 
    original_query_for_response = query 
    final_json_output = ""
    # --- [新增日志变量] ---
    log_all_raw_retrievals_summary: List[Dict[str, Any]] = []
    log_final_context_docs_summary: List[Dict[str, Any]] = [] # This will store the full doc model dumps
    log_fused_context_text_for_llm_snippet: str = "N/A"
    # --- [确保所有日志变量都有初始值] ---
    log_final_answer_from_llm: str = "N/A"
    log_intent_classification_result: Optional[Dict[str, Any]] = None
    log_expanded_queries: Optional[List[str]] = None
    # --- [结束确保所有日志变量都有初始值] ---

    try:
        # --- 1. LLM 驱动的意图分类和澄清触发 ---
        rag_logger.info(f"--- [TIME] 开始意图分类 at {time.time() - start_time_total:.2f}s ---")
        start_time_intent = time.time()
        intent_classification_result = await generate_intent_classification(query)
        log_intent_classification_result = intent_classification_result # <--- 记录日志
        rag_logger.info(f"--- [TIME] 结束意图分类, 耗时: {time.time() - start_time_intent:.2f}s. Result: {intent_classification_result}")

        if intent_classification_result.get("clarification_needed"):
            uncertainty_reason = intent_classification_result.get("reason", "查询可能存在歧义或信息不足。")
            clarification_question_text = await generate_clarification_question(query, uncertainty_reason)
            response_payload = {
                "status": "clarification_needed",
                "clarification_question": clarification_question_text,
                "original_query": original_query_for_response,
                "debug_info": {"uncertainty_reason": uncertainty_reason, "source": "intent_classification"}
            }
            rag_logger.info(f"--- 需要澄清，返回: {response_payload}")
            # final_json_output will be set before finally block

        else: 
             # --- 启用查询扩展 ---
            rag_logger.info(f"--- 查询清晰，无需澄清。将对原始查询 '{query}' 进行查询扩展 ---")
            start_time_expansion = time.time()
            expanded_queries = await generate_expanded_queries(query) # <--- 取消注释
            log_expanded_queries = expanded_queries # <--- 记录实际的扩展查询
            
            if not expanded_queries or query not in expanded_queries: # 确保原始查询一定在里面
                # 如果 generate_expanded_queries 返回空或不包含原始查询，至少处理原始查询
                if query not in (expanded_queries or []): # 处理 expanded_queries 可能为 None 的情况
                    expanded_queries = [query] + (expanded_queries or [])
                elif not expanded_queries:
                    expanded_queries = [query]

            rag_logger.info(f"--- 扩展后的查询列表 (共 {len(expanded_queries)} 个): {expanded_queries}. 耗时: {time.time() - start_time_expansion:.2f}s ---")
            
            all_raw_retrievals: List[RetrievedDocument] = []
            
            queries_to_process = expanded_queries # <--- 修改：现在处理所有扩展后的查询
            rag_logger.info(f"--- [TIME] 开始并行召回 for {len(queries_to_process)} queries at {time.time() - start_time_total:.2f}s ---")
            start_time_retrieval = time.time()

            for current_query_text in queries_to_process:
                rag_logger.info(f"Processing retrievals for query: '{current_query_text}'")
                
                # 向量检索 (ChromaDB)
                if app_ctx.chroma_retriever:
                    try:
                        chroma_docs_raw = app_ctx.chroma_retriever.retrieve(query_text=current_query_text, n_results=top_k_vector)
                        rag_logger.debug(f"   ChromaDB for '{current_query_text}' raw output: {chroma_docs_raw}") 
                        for doc_raw in chroma_docs_raw:
                            retrieved_doc = RetrievedDocument(
                                source_type="vector_chroma",
                                content=doc_raw.get("text", ""),
                                score=doc_raw.get("score", 0.0),
                                metadata={**doc_raw.get("metadata", {}), "original_query_part": current_query_text}
                            )
                            all_raw_retrievals.append(retrieved_doc)
                            log_all_raw_retrievals_summary.append(retrieved_doc.model_dump()) 
                        rag_logger.info(f"   ChromaDB for '{current_query_text}': found {len(chroma_docs_raw)} docs.")
                    except Exception as e_chroma:
                        rag_logger.error(f"   Error during ChromaDB retrieval for '{current_query_text}': {e_chroma}", exc_info=True)
                
                # 关键词检索 (BM25)
                if app_ctx.file_bm25_retriever:
                    try:
                        bm25_docs_raw = app_ctx.file_bm25_retriever.retrieve(query_text=current_query_text, n_results=top_k_bm25)
                        rag_logger.debug(f"   BM25 for '{current_query_text}' raw output (IDs and scores): {bm25_docs_raw}") 
                        for doc_raw_bm25 in bm25_docs_raw:
                            bm25_chunk_id = doc_raw_bm25.get("id")
                            text_content_for_bm25 = f"[BM25: Text for ID {bm25_chunk_id} pending]"
                            found_in_chroma = False
                            for existing_doc in all_raw_retrievals: 
                                if (existing_doc.metadata and (existing_doc.metadata.get("chunk_id") == bm25_chunk_id or existing_doc.metadata.get("id") == bm25_chunk_id)):
                                    text_content_for_bm25 = existing_doc.content
                                    found_in_chroma = True
                                    break
                            if not found_in_chroma and app_ctx.chroma_retriever and bm25_chunk_id: 
                                try:
                                    specific_chroma_doc = app_ctx.chroma_retriever._collection.get(ids=[bm25_chunk_id], include=["metadatas", "documents"]) # Also fetch documents for content
                                    if specific_chroma_doc:
                                        if specific_chroma_doc.get("documents") and specific_chroma_doc.get("documents")[0]:
                                            text_content_for_bm25 = specific_chroma_doc["documents"][0]
                                        elif specific_chroma_doc.get("metadatas") and specific_chroma_doc.get("metadatas")[0]: # Fallback to chunk_text in metadata
                                            text_content_for_bm25 = specific_chroma_doc["metadatas"][0].get("chunk_text", text_content_for_bm25)

                                except Exception as e_chroma_get:
                                    rag_logger.warning(f"   Failed to get text for BM25 ID {bm25_chunk_id} from Chroma: {e_chroma_get}")
                            
                            retrieved_doc = RetrievedDocument(
                                source_type="keyword_bm25s",
                                content=text_content_for_bm25,
                                score=doc_raw_bm25.get("score", 0.0),
                                metadata={"chunk_id": bm25_chunk_id, "original_query_part": current_query_text}
                            )
                            all_raw_retrievals.append(retrieved_doc)
                            log_all_raw_retrievals_summary.append(retrieved_doc.model_dump()) 
                        rag_logger.info(f"   BM25s for '{current_query_text}': found {len(bm25_docs_raw)} potential docs.")
                    except Exception as e_bm25:
                        rag_logger.error(f"   Error during BM25 retrieval for '{current_query_text}': {e_bm25}", exc_info=True)

                # 知识图谱检索
                if app_ctx.kg_retriever:
                    try:
                        rag_logger.info(f"   Performing KG retrieval for query: '{current_query_text}'")
                        kg_docs = await app_ctx.kg_retriever.retrieve_with_llm_cypher(query=current_query_text, top_k=top_k_kg)
                        rag_logger.debug(f"   KG for '{current_query_text}' raw output: {kg_docs}") 
                        for kg_doc_data in kg_docs: # kg_docs is List[Dict], needs conversion
                            retrieved_doc = RetrievedDocument(**kg_doc_data) # Convert dict to Pydantic model
                            if retrieved_doc.metadata:
                                retrieved_doc.metadata["original_query_part"] = current_query_text
                            else:
                                retrieved_doc.metadata = {"original_query_part": current_query_text}
                            all_raw_retrievals.append(retrieved_doc)
                            log_all_raw_retrievals_summary.append(retrieved_doc.model_dump()) 
                        rag_logger.info(f"   KG Retrieval for '{current_query_text}': found {len(kg_docs)} results.")
                    except Exception as e_kg:
                        rag_logger.error(f"   Error during KG retrieval for '{current_query_text}': {e_kg}", exc_info=True)
            
            rag_logger.info(f"--- [TIME] 结束所有召回, 耗时: {time.time() - start_time_retrieval:.2f}s ---")
            rag_logger.info(f"--- 总计从各路召回（所有查询处理后）的结果数: {len(all_raw_retrievals)} ---")
            if all_raw_retrievals:
                for i_doc, doc_retrieved in enumerate(all_raw_retrievals[:3]): # 日志只打印前3条摘要
                        rag_logger.debug(f"   Raw Doc {i_doc} (Summary): type={doc_retrieved.source_type}, score={doc_retrieved.score}, content='{str(doc_retrieved.content)[:50]}...'")

            if not all_raw_retrievals: 
                response_payload = {
                    "status": "success", 
                    "final_answer": "抱歉，根据您提供的查询，未能从知识库中找到相关信息。",
                    "original_query": original_query_for_response,
                    "retrieved_context_docs": [], 
                    "debug_info": {"message": "No documents retrieved from any source."}
                }
            else:
                rag_logger.info(f"--- [TIME] 开始结果融合与重排序 at {time.time() - start_time_total:.2f}s ---")
                start_time_fusion = time.time()
                final_context_docs: List[RetrievedDocument]
                if not app_ctx.fusion_engine:
                    rag_logger.error("FusionEngine not available! Skipping fusion and reranking.")
                    final_context_docs = sorted(all_raw_retrievals, key=lambda d: d.score if d.score is not None else -float('inf'), reverse=True)[:top_k_final]
                else:
                    final_context_docs = await app_ctx.fusion_engine.fuse_results(
                        all_raw_retrievals, 
                        original_query_for_response,
                        top_n_final=top_k_final
                    ) 
                log_final_context_docs_summary = [doc.model_dump() for doc in final_context_docs] 

                # --- 新增日志，检查 model_dump 的输出 ---
                if log_final_context_docs_summary:
                    rag_logger.info(f"DEBUG_MODEL_DUMP: First item of log_final_context_docs_summary (from model_dump()): {json.dumps(log_final_context_docs_summary[0], ensure_ascii=False, default=str)}")
                # --- 结束新增日志 ---

                rag_logger.info(f"--- [TIME] 结束结果融合与重排序, 耗时: {time.time() - start_time_fusion:.2f}s. Final context docs: {len(final_context_docs)} ---")
                if final_context_docs:
                    for i_fdoc, fdoc_retrieved in enumerate(final_context_docs[:3]): # 日志只打印前3条摘要
                        rag_logger.debug(f"   Fused Doc {i_fdoc} (Summary): type={fdoc_retrieved.source_type}, score={fdoc_retrieved.score}, content='{str(fdoc_retrieved.content)[:50]}...'")
                
                if not final_context_docs: 
                    fused_context_text_for_llm = "未在知识库中找到相关信息。"
                    final_answer_from_llm = "根据现有知识，未能找到您查询的相关信息。"
                    response_payload = {
                        "status": "success",
                        "final_answer": final_answer_from_llm,
                        "original_query": original_query_for_response,
                        "retrieved_context_docs": [], 
                        "debug_info": {"message": "No relevant context found after fusion."}
                    }
                else:
                    context_strings_for_llm = []
                    for doc in final_context_docs:
                        score_str = f"{doc.score:.4f}" if isinstance(doc.score, float) else str(doc.score if doc.score is not None else 'N/A')
                        context_strings_for_llm.append(
                            f"Source Type: {doc.source_type}, Score: {score_str}\nContent: {doc.content}"
                        )
                    fused_context_text_for_llm = "\n\n---\n\n".join(context_strings_for_llm)
                    log_fused_context_text_for_llm_snippet = fused_context_text_for_llm[:500] 

                    rag_logger.info(f"\n--- FUSED CONTEXT for LLM (length: {len(fused_context_text_for_llm)} chars) ---")
                    rag_logger.info(f"{fused_context_text_for_llm[:1000]}...") 
                    rag_logger.info(f"--- END OF FUSED CONTEXT ---\n")

                    rag_logger.info(f"--- [TIME] 开始最终答案生成 at {time.time() - start_time_total:.2f}s ---")
                    start_time_answer_gen = time.time()
                    final_answer_from_llm = await generate_answer_from_context(query, fused_context_text_for_llm)
                    log_final_answer_from_llm = final_answer_from_llm or "N/A" 
                    rag_logger.info(f"--- [TIME] 结束最终答案生成, 耗时: {time.time() - start_time_answer_gen:.2f}s ---")

                    if not final_answer_from_llm or final_answer_from_llm.strip() == NO_ANSWER_PHRASE_ANSWER_CLEAN:
                        final_answer_from_llm = "根据您提供的信息，我暂时无法给出明确的回答。"
                    
                    response_payload = {
                        "status": "success",
                        "final_answer": final_answer_from_llm,
                        "original_query": original_query_for_response,
                        "retrieved_context_docs": [doc.model_dump() for doc in final_context_docs], 
                        "debug_info": {"total_raw_retrievals_count": len(all_raw_retrievals)}
                    }

        final_json_output = json.dumps(response_payload, ensure_ascii=False)
        rag_logger.info(f"--- 'query_rag_v2' 逻辑执行完毕, 总耗时: {time.time() - start_time_total:.2f}s. ---")
        
    except Exception as e_main:
        rag_logger.error(f"RAG Service CRITICAL ERROR in 'query_rag_v2' (main try-except): {type(e_main).__name__} - {str(e_main)}", exc_info=True)
        user_query_for_err_log = original_query_for_response if 'original_query_for_response' in locals() and original_query_for_response else query
        response_payload = {
            "status": "error",
            "error_code": "RAG_SERVICE_INTERNAL_ERROR",
            "error_message": f"RAG服务内部发生未预期错误: {str(e_main)}",
            "original_query": user_query_for_err_log,
            "debug_info": {"exception_type": type(e_main).__name__}
        }
        final_json_output = json.dumps(response_payload, ensure_ascii=False)
    finally: 
        interaction_id_for_log = str(uuid.uuid4())
        current_app_version = "zhz_rag_mcp_service_0.2.1" 

        processed_final_context_docs_for_log = []
        temp_log_final_context_docs = locals().get('log_final_context_docs_summary') # 安全获取

        if temp_log_final_context_docs: # 如果 RAG 流程成功并且 final_context_docs 被处理了
            for doc_dict in temp_log_final_context_docs: # temp_log_final_context_docs 是 model_dump() 后的列表
                cleaned_doc = {}
                for key, value in doc_dict.items():
                    if isinstance(value, float) and (value != value or value == float('inf') or value == float('-inf')): 
                        cleaned_doc[key] = None 
                    else:
                        cleaned_doc[key] = value
                processed_final_context_docs_for_log.append(cleaned_doc)
        # 如果 temp_log_final_context_docs 为 None (例如澄清路径)，则 processed_final_context_docs_for_log 保持为 []

        full_log_entry = {
            "timestamp_utc": datetime.now(timezone.utc).isoformat(),
            "interaction_id": interaction_id_for_log,
            "original_query_interaction_id_ref": locals().get('original_query_interaction_id'), 
            "task_type": "rag_query_processing_full_log",
            "app_version": current_app_version,
            "original_user_query": locals().get('original_query_for_response', query), # query 总是有定义的
            "query_params": {
                "top_k_vector": top_k_vector, "top_k_kg": top_k_kg, 
                "top_k_bm25": top_k_bm25, "top_k_final": top_k_final
            },
            "intent_classification_result": locals().get('log_intent_classification_result'),
            "expanded_queries": locals().get('log_expanded_queries', []), # 默认为空列表
            "all_raw_retrievals_count": len(locals().get('log_all_raw_retrievals_summary', [])),
            "final_context_docs_count": len(processed_final_context_docs_for_log), # 使用清理后列表的长度
            "final_context_docs_summary": [ 
                {
                    "source_type": doc.get("source_type"), 
                    "score": doc.get("score"), 
                    "id": (doc.get("metadata",{}).get("chunk_id") or doc.get("metadata",{}).get("id")) if doc.get("metadata") else None, 
                    "content_preview": str(doc.get("content","N/A"))[:50]+"..."
                } 
                for doc in processed_final_context_docs_for_log[:5] # 使用清理后列表的摘要
            ], 
            "final_context_docs_full": processed_final_context_docs_for_log, # <--- 使用清理后的完整列表
            "fused_context_text_for_llm_snippet": locals().get('log_fused_context_text_for_llm_snippet', "N/A"),
            "final_answer_from_llm": locals().get('log_final_answer_from_llm', "N/A"),
            "final_response_payload_status": locals().get('response_payload', {}).get("status", "Unknown"),
            "total_processing_time_seconds": round(time.time() - start_time_total, 2) if 'start_time_total' in locals() else -1,
        }

        try:
            await log_interaction_data(full_log_entry) 
            rag_logger.info(f"Full RAG interaction log (ID: {interaction_id_for_log}) has been written.")
        except Exception as e_log_final:
            rag_logger.error(f"CRITICAL: Failed to write full RAG interaction log: {e_log_final}", exc_info=True)
        
        sys.stdout.flush(); sys.stderr.flush() 
    
    return final_json_output

if __name__ == "__main__":
    rag_logger.info("--- Starting RAG Service (FastMCP for mcpo via direct run) ---")
    rag_mcp_application.run()
```

File: zhz_rag/api/task_manager_api.py
-------------------------------------
```python
# zhz_agent/task_manager_service.py
from fastapi import APIRouter, HTTPException, Depends, Body, Query, Path, status
from typing import List, Optional, Any, cast
from datetime import datetime, timedelta
import uuid
import traceback # 导入 traceback
import pytz

# --- [修改] 从 pydantic_models 导入我们定义的模型 -> 改为绝对导入 ---
from zhz_rag.config.pydantic_models import TaskModel, CreateTaskRequest, UpdateTaskRequest, TaskStatus, ReminderMethod

# --- [修改] 从 database_models 导入 SQLAlchemy 表模型 -> 改为绝对导入 ---
from zhz_rag.task_management.db_models import TaskDB

# --- [修改] 从新的 database.py 导入 database 对象 和 get_scheduler -> 改为绝对导入 ---
from zhz_rag.utils.db_utils import database, get_scheduler # 将 db_utils 修改为 database

# --- [修改] 从 .task_jobs 导入作业函数 -> 改为绝对导入 ---
from zhz_rag.task_management.jobs import send_task_reminder, execute_task_action
from apscheduler.triggers.date import DateTrigger # 用于指定精确的运行时间
from apscheduler.jobstores.base import JobLookupError # <--- [修改] 导入 JobLookupError 的正确路径

# APIRouter 实例
router = APIRouter(
    prefix="/tasks",
    tags=["Task Management"],
    responses={404: {"description": "Not found"}},
)

def _ensure_utc(dt: datetime) -> datetime:
    """确保 datetime 对象是 UTC 时区感知的。"""
    if dt.tzinfo is None:
        return pytz.utc.localize(dt) # 如果是朴素时间，假定它是UTC并设为UTC
    return dt.astimezone(pytz.utc) # 如果是其他时区，转换为UTC

def _schedule_task_jobs(task: TaskModel):
    current_scheduler = get_scheduler() # 获取 scheduler 实例
    print(f"DEBUG SCHEDULER: _schedule_task_jobs called. Scheduler instance: {current_scheduler}, Is running: {current_scheduler.running if current_scheduler else 'N/A'}")
    if not current_scheduler or not current_scheduler.running:
        print("SCHEDULER_ERROR: APScheduler 未运行，无法调度作业。")
        return

    # 提醒作业
    if task.reminder_time and task.status == TaskStatus.PENDING:
        reminder_job_id = f"reminder_{task.id}"
        try:
            reminder_methods_list = task.reminder_methods
            reminder_utc = _ensure_utc(task.reminder_time)
            print(f"SCHEDULER DEBUG: Passing reminder_methods to job: {reminder_methods_list}") # 添加日志

            current_scheduler.add_job(
                send_task_reminder,
                trigger=DateTrigger(run_date=reminder_utc),
                args=[task.id, task.title, reminder_methods_list], # <--- [修复] 直接传递列表
                id=reminder_job_id,
                name=f"Reminder for task {task.title[:20]}",
                replace_existing=True
            )
            print(f"SCHEDULER: 已为任务 '{task.id}' 添加/更新提醒作业，运行于 {task.reminder_time}")
        except Exception as e:
            print(f"SCHEDULER_ERROR: 添加提醒作业失败 for task '{task.id}': {e}")
            traceback.print_exc() # 打印详细错误堆栈

    # 执行作业
    if task.due_date and task.status == TaskStatus.PENDING:
        execution_job_id = f"execution_{task.id}"
        try:
            due_utc = _ensure_utc(task.due_date) # <--- [新增] 确保时间是 UTC 感知的
            print(f"SCHEDULER DEBUG: Adding execution job at {due_utc} ({due_utc.tzinfo})") # <--- [新增] 添加时区日志
            current_scheduler.add_job(
                execute_task_action,
                trigger=DateTrigger(run_date=due_utc),
                args=[task.id, task.action_type, task.action_payload],
                id=execution_job_id,
                name=f"Execution for task {task.title[:20]}",
                replace_existing=True
            )
            print(f"SCHEDULER: 已为任务 '{task.id}' 添加/更新执行作业，运行于 {task.due_date}")
        except Exception as e:
            print(f"SCHEDULER_ERROR: 添加执行作业失败 for task '{task.id}': {e}")

def _cancel_task_jobs(task_id: str):
    """从 APScheduler 取消作业"""
    current_scheduler = get_scheduler()
    if not current_scheduler or not current_scheduler.running:
        print("SCHEDULER_ERROR: APScheduler 未运行，无法取消作业。")
        return

    reminder_job_id = f"reminder_{task_id}"
    execution_job_id = f"execution_{task_id}"

    try:
        current_scheduler.remove_job(reminder_job_id)
        print(f"SCHEDULER: 已移除提醒作业 for task '{task_id}'")
    except JobLookupError:
        print(f"SCHEDULER_INFO: 提醒作业 '{reminder_job_id}' 未找到，无需移除。")
    except Exception as e:
        print(f"SCHEDULER_ERROR: 移除提醒作业失败 for task '{task_id}': {e}")

    try:
        current_scheduler.remove_job(execution_job_id)
        print(f"SCHEDULER: 已移除执行作业 for task '{task_id}'")
    except JobLookupError:
        print(f"SCHEDULER_INFO: 执行作业 '{execution_job_id}' 未找到，无需移除。")
    except Exception as e:
        print(f"SCHEDULER_ERROR: 移除执行作业失败 for task '{task_id}': {e}")

@router.post("/", response_model=TaskModel, status_code=status.HTTP_201_CREATED)
async def create_task(task_request: CreateTaskRequest = Body(...)):
    """
    创建一个新任务。
    """
    now = datetime.utcnow()
    task_id = str(uuid.uuid4())

    reminder_time_val = None
    if task_request.due_date and task_request.reminder_offset_minutes is not None:
        reminder_time_val = task_request.due_date - timedelta(minutes=task_request.reminder_offset_minutes)

    reminder_methods_values = [
        method.value if hasattr(method, 'value') else str(method)
        for method in (task_request.reminder_methods or [ReminderMethod.NOTIFICATION])
    ]

    insert_query = TaskDB.__table__.insert().values(
        id=task_id,
        title=task_request.title,
        description=task_request.description,
        status=TaskStatus.PENDING,
        created_at=now,
        updated_at=now,
        due_date=task_request.due_date,
        reminder_time=reminder_time_val,
        reminder_offset_minutes=task_request.reminder_offset_minutes,
        reminder_methods=reminder_methods_values, # <--- 确保存入的是字符串列表
        priority=task_request.priority or 0,
        tags=task_request.tags or [],
        action_type=task_request.action_type,
        action_payload=task_request.action_payload or {}
    )

    try:
        await database.execute(insert_query)
    except Exception as e:
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Failed to create task in database: {e}")

    created_task_db = await database.fetch_one(TaskDB.__table__.select().where(TaskDB.id == task_id))
    if not created_task_db:
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to retrieve task after creation")

    response_task = TaskModel.model_validate(dict(created_task_db))

    response_task.reminder_methods = [
        m.value if hasattr(m, 'value') else str(m)
        for m in response_task.reminder_methods
    ]

    _schedule_task_jobs(response_task)
    print(f"TASK_MANAGER: Created task '{response_task.id}' with title '{response_task.title}' in DB")
    return response_task

@router.get("/", response_model=List[TaskModel])
async def list_tasks(
    status_filter: Optional[TaskStatus] = Query(None, alias="status"),
    priority_filter: Optional[int] = Query(None, alias="priority"),
    skip: int = Query(0, ge=0),
    limit: int = Query(10, ge=1, le=100)
):
    """
    获取任务列表，支持过滤和分页。
    """
    query = TaskDB.__table__.select()
    if status_filter:
        query = query.where(TaskDB.status == status_filter)
    if priority_filter is not None:
        query = query.where(TaskDB.priority == priority_filter)

    query = query.order_by(TaskDB.created_at.desc()).offset(skip).limit(limit)

    db_tasks = await database.fetch_all(query)
    return [TaskModel.model_validate(dict(task)) for task in db_tasks]

@router.get("/{task_id}", response_model=TaskModel)
async def get_task(task_id: str = Path(..., description="要获取的任务ID")):
    """
    根据ID获取单个任务的详细信息。
    """
    query = TaskDB.__table__.select().where(TaskDB.id == task_id)
    db_task = await database.fetch_one(query)
    if not db_task:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Task not found")
    return TaskModel.model_validate(dict(db_task))

@router.put("/{task_id}", response_model=TaskModel)
async def update_task(
    task_id: str = Path(..., description="要更新的任务ID"),
    task_update: UpdateTaskRequest = Body(...)
):
    """
    更新现有任务。
    """
    existing_task_query = TaskDB.__table__.select().where(TaskDB.id == task_id)
    db_task = await database.fetch_one(existing_task_query)
    if not db_task:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Task not found")

    update_data = task_update.model_dump(exclude_unset=True)
    update_data["updated_at"] = datetime.utcnow()

    if "reminder_methods" in update_data and update_data["reminder_methods"] is not None:
        update_data["reminder_methods"] = [
            method.value if hasattr(method, 'value') else str(method)
            for method in update_data["reminder_methods"]
        ]

    current_due_date = update_data.get("due_date", cast(Optional[datetime], db_task.due_date))
    current_offset = update_data.get("reminder_offset_minutes", cast(Optional[int], db_task.reminder_offset_minutes))

    if current_due_date and current_offset is not None:
        update_data["reminder_time"] = current_due_date - timedelta(minutes=current_offset)
    elif "due_date" in update_data and current_offset is None:
         update_data["reminder_time"] = None


    update_query = TaskDB.__table__.update().where(TaskDB.id == task_id).values(**update_data)
    await database.execute(update_query)

    updated_db_task = await database.fetch_one(existing_task_query)
    if not updated_db_task:
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to retrieve task after update")

    response_task = TaskModel.model_validate(dict(updated_db_task))

    response_task.reminder_methods = [
        m.value if hasattr(m, 'value') else str(m)
        for m in response_task.reminder_methods
    ]

    _cancel_task_jobs(task_id)
    _schedule_task_jobs(response_task)
    print(f"TASK_MANAGER: Updated task '{response_task.id}' in DB")
    return response_task

@router.delete("/{task_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_task(task_id: str = Path(..., description="要删除的任务ID")):
    """
    删除一个任务。
    """
    existing_task_query = TaskDB.__table__.select().where(TaskDB.id == task_id)
    db_task = await database.fetch_one(existing_task_query)
    if not db_task:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Task not found")

    delete_query = TaskDB.__table__.delete().where(TaskDB.id == task_id)
    await database.execute(delete_query)

    _cancel_task_jobs(task_id)
    print(f"TASK_MANAGER: Deleted task '{task_id}' from DB")
    return None

@router.post("/{task_id}/complete", response_model=TaskModel)
async def mark_task_as_complete(task_id: str = Path(..., description="要标记为完成的任务ID")):
    """
    将任务标记为已完成。
    """
    existing_task_query = TaskDB.__table__.select().where(TaskDB.id == task_id)
    db_task_row = await database.fetch_one(existing_task_query)
    if not db_task_row:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Task not found")

    db_task = TaskModel.model_validate(dict(db_task_row))
    if db_task.status == TaskStatus.COMPLETED:
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="Task is already completed")

    update_data = {
        "status": TaskStatus.COMPLETED,
        "updated_at": datetime.utcnow()
    }
    update_query = TaskDB.__table__.update().where(TaskDB.id == task_id).values(**update_data)
    await database.execute(update_query)

    completed_db_task = await database.fetch_one(existing_task_query)
    if not completed_db_task:
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to retrieve task after marking complete")

    response_task = TaskModel.model_validate(dict(completed_db_task))
    _cancel_task_jobs(task_id)
    print(f"TASK_MANAGER: Marked task '{response_task.id}' as completed in DB")
    return response_task
```


================================================================================

