# /home/zhz/zhz_agent/local_agent/local_agent_app.py

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from typing import Dict, Any, List, Optional
import pandas as pd # 用于处理 Excel 数据
from pandasql import PandaSQL # 用于在 Pandas DataFrame 上执行 SQL 查询
import os
import logging # 用于日志记录
import traceback # 用于更详细的错误信息

# --- [新增] 导入 winotify 用于发送 Windows 桌面通知 ---
from winotify import Notification #

# --- 配置常量 ---
LOCAL_AGENT_PORT = 8003 # 本地代理服务监听的端口
LOG_LEVEL = logging.INFO # 日志级别

# --- 日志配置 ---
# 配置日志记录器，方便调试和追踪
logging.basicConfig(level=LOG_LEVEL, format='%(asctime)s - %(levelname)s - LocalAgent - %(message)s') #
logger = logging.getLogger(__name__) #

# --- FastAPI 应用实例 ---
app = FastAPI(
    title="ZhzAI Local Agent", #
    description="本地代理程序，提供处理本地文件（如Excel）和执行特定本地任务（如桌面通知）的MCP服务。", #
    version="0.1.1" # 更新版本号以反映添加了通知功能
)

# --- Pydantic 模型定义 ---

# 用于 Excel SQO (Structured Query Object) 操作的请求体
class ExecuteSQORequest(BaseModel): #
    sqo: Dict[str, Any] = Field(description="必需。一个结构化查询对象 (SQO) 的JSON字典，用于描述要对Excel执行的操作。") #

# Excel SQO 操作的通用响应体
class SQOResponse(BaseModel): #
    success: bool # 操作是否成功
    result: Any = None # 成功时的结果数据
    error: Optional[str] = None # 失败时的简要错误信息
    error_details: Optional[str] = None # 失败时的详细技术错误信息或堆栈

# 用于桌面通知的请求体
class NotificationRequest(BaseModel): #
    title: str = Field(default="任务提醒", description="通知的标题") #
    message: str = Field(description="通知的主要内容") #
    app_name: str = Field(default="终端大脑助手", description="显示在通知来源的应用名称") #
    # icon_path: Optional[str] = Field(None, description="通知图标的绝对路径 (可选)") #

# 桌面通知操作的响应体
class NotificationResponse(BaseModel): #
    success: bool # 操作是否成功 #
    message: str # 返回的消息，成功或失败的说明 #

# --- 核心功能函数 ---

def apply_filters_to_dataframe(df: pd.DataFrame, filters: List[Dict[str, Any]]) -> pd.DataFrame: #
    """
    将一系列过滤器应用于 Pandas DataFrame。
    这是 Excel SQO 操作的核心辅助函数，用于在执行聚合、查询等操作前筛选数据。
    """
    if not filters: #
        return df #

    df_filtered = df.copy() #
    for f_obj in filters: #
        column = f_obj.get("column") #
        operator = f_obj.get("operator") #
        value = f_obj.get("value") #
        is_date = f_obj.get("is_date", False) #

        if not all([column, operator, value is not None]): #
            logger.warning(f"Skipping invalid filter object: {f_obj}") #
            continue #

        if column not in df_filtered.columns: #
            logger.warning(f"Filter column '{column}' not found in DataFrame. Available: {df_filtered.columns.tolist()}. Skipping filter: {f_obj}") #
            continue #

        try: #
            series_to_filter = df_filtered[column] # 操作副本以避免 SettingWithCopyWarning #
            
            # 尝试转换日期，如果失败则记录警告并可能跳过
            if is_date: #
                try: #
                    series_to_filter = pd.to_datetime(series_to_filter, errors='coerce') #
                    filter_value_dt = pd.to_datetime(value, errors='coerce') #
                    # 如果日期转换失败且操作符不是处理列表的类型（列表值在后面处理）
                    if pd.isna(filter_value_dt) and not (isinstance(value, list) and operator in ["in", "is_in_list", "not in", "is_not_in_list"]): #
                        logger.warning(f"Cannot convert filter value '{value}' to date for column '{column}'. Skipping filter.") #
                        continue #
                    value = filter_value_dt # 更新 value 为日期对象或 NaT #
                except Exception as e_date: #
                    logger.warning(f"Error converting column '{column}' or value '{value}' to datetime: {e_date}. Skipping filter.") #
                    continue #
            
            condition = None #
            # 根据操作符构建筛选条件
            if operator == "equals" or operator == "==": #
                condition = (series_to_filter == value) #
            elif operator == "not_equals" or operator == "!=": #
                condition = (series_to_filter != value) #
            elif operator == "greater_than" or operator == ">": #
                condition = (series_to_filter > value) #
            elif operator == "greater_than_or_equals" or operator == ">=": #
                condition = (series_to_filter >= value) #
            elif operator == "less_than" or operator == "<": #
                condition = (series_to_filter < value) #
            elif operator == "less_than_or_equals" or operator == "<=": #
                condition = (series_to_filter <= value) #
            elif operator == "contains": #
                if is_date: #
                    logger.warning(f"'contains' operator is not directly applicable to date column '{column}'. Skipping filter.") #
                    continue #
                condition = (series_to_filter.astype(str).str.contains(str(value), case=False, na=False)) #
            elif operator == "not_contains": #
                if is_date: #
                    logger.warning(f"'not_contains' operator is not directly applicable to date column '{column}'. Skipping filter.") #
                    continue #
                condition = (~series_to_filter.astype(str).str.contains(str(value), case=False, na=False)) #
            elif operator == "is_in_list" or operator == "in": #
                if not isinstance(value, list): #
                    logger.warning(f"'is_in_list' operator expects a list value for column '{column}'. Skipping filter.") #
                    continue #
                if is_date: #
                    list_value_dt = pd.to_datetime(value, errors='coerce').dropna().tolist() #
                    if not list_value_dt: #
                        logger.warning(f"Cannot convert list values to dates for 'is_in_list' on column '{column}'. Skipping filter.") #
                        continue #
                    condition = (series_to_filter.isin(list_value_dt)) #
                else: #
                    condition = (series_to_filter.isin(value)) #
            elif operator == "is_not_in_list" or operator == "not in": #
                if not isinstance(value, list): #
                    logger.warning(f"'is_not_in_list' operator expects a list value for column '{column}'. Skipping filter.") #
                    continue #
                if is_date: #
                    list_value_dt = pd.to_datetime(value, errors='coerce').dropna().tolist() #
                    if not list_value_dt: #
                         logger.warning(f"Cannot convert list values to dates for 'is_not_in_list' on column '{column}'. Skipping filter.") #
                         continue #
                    condition = (~series_to_filter.isin(list_value_dt)) #
                else: #
                    condition = (~series_to_filter.isin(value)) #
            elif operator == "between": #
                if not (isinstance(value, list) and len(value) == 2): #
                    logger.warning(f"'between' operator expects a list of two values for column '{column}'. Skipping filter.") #
                    continue #
                val1, val2 = value #
                if is_date: #
                    val1_dt = pd.to_datetime(val1, errors='coerce') #
                    val2_dt = pd.to_datetime(val2, errors='coerce') #
                    if pd.isna(val1_dt) or pd.isna(val2_dt): #
                        logger.warning(f"Cannot convert 'between' values to dates for column '{column}'. Skipping filter.") #
                        continue #
                    condition = (series_to_filter.between(min(val1_dt, val2_dt), max(val1_dt, val2_dt))) #
                else: #
                    condition = (series_to_filter.between(min(val1, val2), max(val1, val2))) #
            elif operator == "is_null" or operator == "isnull": #
                condition = (series_to_filter.isnull()) #
            elif operator == "is_not_null" or operator == "notnull": #
                condition = (series_to_filter.notnull()) #
            else: #
                logger.warning(f"Unsupported filter operator '{operator}' for column '{column}'. Skipping filter.") #
                continue # 跳过不支持的操作符 #

            if condition is not None: #
                 df_filtered = df_filtered[condition] #
            else: #
                logger.warning(f"Condition was None for filter {f_obj}. This should not happen if operator is supported.") #

        except Exception as e_filter: #
            logger.error(f"Error applying filter {f_obj} on column '{column}': {e_filter}", exc_info=True) #
            # 根据策略，可以选择跳过此过滤器或在此处引发异常/返回错误
            # 为保持流程继续，我们选择跳过错误的过滤器
            continue #
            
    return df_filtered #

# --- FastAPI 端点 ---

@app.post("/notify", response_model=NotificationResponse) #
async def send_desktop_notification(req: NotificationRequest): #
    """
    接收通知请求并在 Windows 桌面上显示一个通知。
    """
    logger.info(f"Received notification request: Title='{req.title}', Message='{req.message}'") #
    try: #
        toast = Notification( #
            app_id=req.app_name, #
            title=req.title, #
            msg=req.message, #
            # duration="long", # 可选 #
            # icon=req.icon_path if req.icon_path and os.path.exists(req.icon_path) else "" # 可选 #
        )
        toast.show() #
        
        logger.info(f"Desktop notification successfully shown: '{req.title}'") #
        return NotificationResponse(success=True, message="Notification successfully shown.") #
    except Exception as e: #
        logger.error(f"Failed to show desktop notification: {e}", exc_info=True) #
        raise HTTPException(status_code=500, detail=f"Failed to show notification: {str(e)}") #


@app.post("/excel_sqo_mcp/execute_operation", response_model=SQOResponse) #
async def execute_excel_sqo_operation(request_data: ExecuteSQORequest): #
    """
    执行 Excel 结构化查询对象 (SQO) 操作。
    这个端点负责根据传入的 SQO 对指定的 Excel 文件进行数据读取、筛选、聚合等操作。
    """
    sqo = request_data.sqo #
    operation_type = sqo.get("operation_type") #
    file_path = sqo.get("file_path") #
    sheet_name = sqo.get("sheet_name", 0) # 默认为第一个工作表 #

    logger.info(f"Received SQO request: operation='{operation_type}', file='{file_path}', sheet='{sheet_name}'") #
    logger.debug(f"Full SQO received: {sqo}") #

    if not file_path or not operation_type: #
        logger.error("SQO missing 'file_path' or 'operation_type'.") #
        return SQOResponse(success=False, error="SQO中缺少 'file_path' 或 'operation_type' 参数。") #

    # 安全性提示：在生产环境中，需要对 file_path 进行严格校验，防止路径遍历等安全风险。
    # 例如，限制只能访问特定目录下的文件。
    if not os.path.exists(file_path): #
        logger.error(f"Excel file not found: {file_path}") #
        return SQOResponse(success=False, error=f"Excel文件未找到 '{file_path}'") #

    try: #
        # 读取 Excel 文件到 Pandas DataFrame
        df = pd.read_excel(file_path, sheet_name=sheet_name) #
        logger.info(f"Successfully loaded DataFrame. Shape: {df.shape}, Columns: {df.columns.tolist()}") #

        result_data = None #

        # --- 根据操作类型执行不同逻辑 ---

        if operation_type == "get_unique_values": #
            column_name = sqo.get("column_name") #
            if not column_name or column_name not in df.columns: #
                error_msg = f"'get_unique_values' 操作缺少有效 'column_name' 或列 '{column_name}' 不存在。可用列: {df.columns.tolist()}" #
                logger.error(error_msg) #
                return SQOResponse(success=False, error=error_msg) #
            
            df_to_process = df.copy() #
            filters_from_sqo = sqo.get("filters") #
            if filters_from_sqo and isinstance(filters_from_sqo, list): #
                logger.info(f"Applying filters for get_unique_values: {filters_from_sqo}") #
                df_to_process = apply_filters_to_dataframe(df_to_process, filters_from_sqo) #
                logger.info(f"DataFrame shape after filtering for get_unique_values: {df_to_process.shape}") #

            unique_values_series = df_to_process[column_name].unique() #
            if sqo.get('options', {}).get('drop_na', False): # 默认为 False #
                unique_values = pd.Series(unique_values_series).dropna().tolist() #
            else: #
                unique_values = unique_values_series.tolist() #
            result_data = unique_values #

        elif operation_type == "group_by_aggregate": #
            group_by_cols = sqo.get("group_by_columns") #
            agg_col = sqo.get("aggregation_column") #
            agg_func = sqo.get("aggregation_function") #
            if not (group_by_cols and agg_col and agg_func): #
                error_msg = "'group_by_aggregate' 操作缺少 'group_by_columns', 'aggregation_column', 或 'aggregation_function' 参数。" #
                logger.error(error_msg) #
                return SQOResponse(success=False, error=error_msg) #
            
            columns_to_check = [] #
            if isinstance(group_by_cols, list): columns_to_check.extend(group_by_cols) #
            elif isinstance(group_by_cols, str): columns_to_check.append(group_by_cols) #
            if agg_col: columns_to_check.append(agg_col) #
            for col in columns_to_check: #
                if col not in df.columns: #
                    error_msg = f"列 '{col}' 在Excel中未找到。可用列: {df.columns.tolist()}" #
                    logger.error(error_msg) #
                    return SQOResponse(success=False, error=error_msg) #
            
            df_to_group = df.copy() #
            filters_from_sqo = sqo.get("filters") #
            if filters_from_sqo and isinstance(filters_from_sqo, list): #
                logger.info(f"Applying filters for group_by_aggregate: {filters_from_sqo}") #
                df_to_group = apply_filters_to_dataframe(df_to_group, filters_from_sqo) #
                logger.info(f"DataFrame shape after filtering: {df_to_group.shape}") #
            elif "filters" in sqo and sqo["filters"]: #
                 logger.warning(f"SQO中的 'filters' 字段格式不正确（期望列表），将被忽略。Filters: {sqo['filters']}") #

            grouped_data = df_to_group.groupby(group_by_cols)[agg_col].agg(agg_func) #
            output_col_name = sqo.get('options', {}).get('output_column_name') #
            if output_col_name: #
                if isinstance(grouped_data, pd.Series): grouped_data = grouped_data.rename(output_col_name) #
                elif isinstance(grouped_data, pd.DataFrame) and len(grouped_data.columns)==1: grouped_data.columns = [output_col_name] #
            result_data = grouped_data.reset_index().to_dict(orient='records') #

        elif operation_type == "find_top_n_rows": #
            select_columns = sqo.get("select_columns") #
            condition_col = sqo.get("condition_column") #
            sort_order_str = sqo.get("sort_order", "descending").lower() #
            n_rows_param = sqo.get("n_rows", 1) # 使用 n_rows_param 避免与内置名称冲突 #
            if not (select_columns and condition_col): #
                error_msg = "'find_top_n_rows' 操作缺少 'select_columns' 或 'condition_column' 参数。" #
                logger.error(error_msg) #
                return SQOResponse(success=False, error=error_msg) #
            
            columns_to_check_top_n = [] # 使用不同变量名以避免作用域问题 #
            if isinstance(select_columns, list): columns_to_check_top_n.extend(select_columns) #
            elif isinstance(select_columns, str): columns_to_check_top_n.append(select_columns) #
            if condition_col: columns_to_check_top_n.append(condition_col) #
            for col_top_n in columns_to_check_top_n: # 使用不同迭代变量 #
                 if col_top_n not in df.columns: #
                    error_msg = f"列 '{col_top_n}' 在Excel中未找到。可用列: {df.columns.tolist()}" #
                    logger.error(error_msg) #
                    return SQOResponse(success=False, error=error_msg) #

            if sort_order_str not in ['ascending', 'descending']: #
                error_msg = f"无效的 sort_order: {sort_order_str}" #
                logger.error(error_msg) #
                return SQOResponse(success=False, error=error_msg) #
            if not isinstance(n_rows_param, int) or n_rows_param <= 0: #
                error_msg = f"n_rows 必须是正整数，但收到: {n_rows_param}" #
                logger.error(error_msg) #
                return SQOResponse(success=False, error=error_msg) #

            df_to_sort = df.copy() #
            filters_from_sqo_top_n = sqo.get("filters") # 使用不同变量名 #
            if filters_from_sqo_top_n and isinstance(filters_from_sqo_top_n, list): #
                logger.info(f"Applying filters for find_top_n_rows: {filters_from_sqo_top_n}") #
                df_to_sort = apply_filters_to_dataframe(df_to_sort, filters_from_sqo_top_n) #
                logger.info(f"DataFrame shape after filtering: {df_to_sort.shape}") #
            elif "filters" in sqo and sqo["filters"]: #
                 logger.warning(f"SQO中的 'filters' 字段格式不正确（期望列表），将被忽略。Filters: {sqo['filters']}") #
            
            ascending_order = True if sort_order_str == 'ascending' else False #
            sorted_df = df_to_sort.sort_values(by=condition_col, ascending=ascending_order) #
            result_df = sorted_df.head(n_rows_param) # 使用 n_rows_param #
            result_data = result_df[select_columns].to_dict(orient='records') #
        
        elif operation_type == "direct_sql_query": #
            sql_query_from_sqo = sqo.get("sql_query") #
            if not sql_query_from_sqo: #
                error_msg = "'direct_sql_query' 操作缺少 'sql_query' 参数。" #
                logger.error(error_msg) #
                return SQOResponse(success=False, error=error_msg) #
            
            pdsql = PandaSQL(persist=False) #
            logger.debug(f"Executing direct_sql_query: {sql_query_from_sqo} on columns: {df.columns.tolist()}") #
            # 在执行SQL之前，对DataFrame应用过滤器（如果提供了）
            df_for_sql = df.copy() #
            filters_for_sql = sqo.get("filters") #
            if filters_for_sql and isinstance(filters_for_sql, list): #
                logger.info(f"Applying filters before direct_sql_query: {filters_for_sql}") #
                df_for_sql = apply_filters_to_dataframe(df_for_sql, filters_for_sql) #
                logger.info(f"DataFrame shape after filtering for direct_sql_query: {df_for_sql.shape}") #

            query_result_df = pdsql(sql_query_from_sqo, env={'df': df_for_sql}) # 在过滤后的df上执行 #
            
            if query_result_df is None: #
                result_data = "SQL查询成功执行，但没有返回数据。" #
            elif query_result_df.empty: #
                result_data = "SQL查询成功执行，但未找到符合条件的数据。" #
            else: #
                result_list_of_dicts = query_result_df.to_dict(orient='records') #
                if len(result_list_of_dicts) == 1: #
                    if len(result_list_of_dicts[0]) == 1: result_data = list(result_list_of_dicts[0].values())[0] #
                    else: result_data = result_list_of_dicts[0] #
                else: #
                    if result_list_of_dicts and len(result_list_of_dicts[0]) == 1: # 添加检查 result_list_of_dicts 是否为空 #
                        single_col_name = list(result_list_of_dicts[0].keys())[0] #
                        result_data = [row[single_col_name] for row in result_list_of_dicts] #
                    else: result_data = result_list_of_dicts #
        else: #
            error_msg = f"不支持的操作类型 '{operation_type}'。" #
            logger.error(error_msg) #
            return SQOResponse(success=False, error=error_msg) #

        logger.info(f"SQO operation '{operation_type}' executed successfully.") #
        return SQOResponse(success=True, result=result_data) #

    except ImportError: #
        logger.critical("Pandas或PandaSQL未正确安装。") #
        return SQOResponse(success=False, error="执行Excel查询所需的库 (Pandas/PandaSQL) 未安装。", error_details=traceback.format_exc()) #
    except FileNotFoundError as e_fnf: # 更具体的异常捕获 #
        logger.error(f"File not found during operation: {e_fnf}", exc_info=True) #
        return SQOResponse(success=False, error=str(e_fnf), error_details=traceback.format_exc()) #
    except pd.errors.EmptyDataError as e_empty: #
        logger.error(f"Pandas EmptyDataError for file {file_path}, sheet {sheet_name}: {e_empty}", exc_info=True) #
        return SQOResponse(success=False, error=f"无法读取Excel文件 '{os.path.basename(file_path)}' (工作表: {sheet_name})，文件可能为空或格式不正确。", error_details=traceback.format_exc()) #
    except KeyError as e_key: # 捕获列名不存在等错误 #
        logger.error(f"KeyError during operation '{operation_type}': {e_key}. SQO: {sqo}", exc_info=True) #
        return SQOResponse(success=False, error=f"操作失败：列名 '{str(e_key)}' 可能不存在或不正确。请检查SQO参数和Excel列名。可用列: {df.columns.tolist() if 'df' in locals() else '未知'}", error_details=traceback.format_exc()) #
    except Exception as e: #
        error_message = f"执行SQO操作 '{operation_type}' 时发生内部错误: {type(e).__name__} - {str(e)}" #
        logger.error(error_message, exc_info=True) #
        return SQOResponse(success=False, error=error_message, error_details=traceback.format_exc()) #


# --- 用于本地直接运行测试此本地代理服务 ---
if __name__ == "__main__": #
    import uvicorn #
    logger.info(f"Starting Local Agent server on http://0.0.0.0:{LOCAL_AGENT_PORT}") #
    uvicorn.run("local_agent_app:app", host="0.0.0.0", port=LOCAL_AGENT_PORT, reload=True) # 添加 reload=True #
