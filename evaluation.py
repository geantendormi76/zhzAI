# zhz_agent/evaluation.py
import os
import json
import traceback
from typing import Dict, Any, Optional

import litellm # 用于调用Gemini API
from dotenv import load_dotenv

# 导入通用日志函数
from .utils import log_interaction_data
# 导入共享的Schema描述
from .constants import NEW_KG_SCHEMA_DESCRIPTION as KG_SCHEMA_FOR_EVALUATION

load_dotenv()

import logging

eval_logger = logging.getLogger("EvaluationLogger")
eval_logger.setLevel(logging.INFO)
eval_logger.propagate = False
if not eval_logger.hasHandlers():
    _eval_console_handler = logging.StreamHandler()
    _eval_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
    _eval_console_handler.setFormatter(_eval_formatter)
    eval_logger.addHandler(_eval_console_handler)
    eval_logger.info("--- EvaluationLogger configured ---")

CYPHER_EVALUATION_PROMPT_V1 = """You are an expert Neo4j Cypher query evaluator and debugging assistant. Your primary task is to meticulously analyze a Cypher query that was generated by another AI model (Qwen2.5-3B, a 3 billion parameter model) in response to a user's natural language question. Your evaluation must be based on the provided knowledge graph schema and the specific evaluation criteria outlined below.

**IMPORTANT SCHEMA CONTEXT (KG_SCHEMA_DESCRIPTION):**
Use code with caution.
Python
{{KG_SCHEMA_DESCRIPTION}}
**USER'S NATURAL LANGUAGE QUESTION:**
Use code with caution.
{{USER_QUESTION}}
**GENERATED CYPHER QUERY TO EVALUATE:**
Use code with caution.
{{GENERATED_CYPHER}}
**EVALUATION TASK:**

Please evaluate the "GENERATED CYPHER QUERY" based on the following criteria. Provide your evaluation in a **valid JSON format** strictly adhering to the structure specified at the end.

**EVALUATION CRITERIA:**

1.  **Syntactic Correctness:**
    *   Is the Cypher query syntactically valid and parsable by Neo4j?
    *   Score (1-5): 1 = Major syntax errors, unparsable; 3 = Minor issues, likely parsable with warnings; 5 = Syntactically perfect.
    *   Reasoning: Explain your score. If errors exist, briefly describe them.

2.  **Schema Adherence (Strictly based on the provided KG_SCHEMA_DESCRIPTION):**
    *   **Node Label Correctness:**
        *   Check: Does the query exclusively use ':ExtractedEntity' for all node patterns? (True/False)
        *   Detail: Briefly explain.
    *   **Entity Type via `label` Property Correctness:**
        *   Check: Are entity types (e.g., 'PERSON', 'ORGANIZATION', 'TASK') correctly queried using the `label` property of ':ExtractedEntity' nodes (e.g., `WHERE n.label = 'PERSON'`)? (True/False)
        *   Detail: Briefly explain.
    *   **Relationship Type and Direction Correctness:**
        *   Check: Does the query use only defined relationship types (e.g., `:WORKS_AT`, `:ASSIGNED_TO`) and their correct directions as specified in the schema? (True/False)
        *   Detail: Briefly explain.
    *   **Property Name Correctness:**
        *   Check: Does the query use only valid property names for nodes and relationships (e.g., `text`, `label` for nodes)? (True/False)
        *   Detail: Briefly explain.
    *   **Hallucinated Schema Elements:**
        *   Check: Does the query reference any labels, relationship types, or properties NOT defined in the schema? (True/False - True if hallucination is present)
        *   Detail: List any hallucinated elements.
    *   **Overall Schema Adherence Score (1-5):** Based on the sub-checks above, provide an overall schema adherence score. Explain your reasoning, considering the severity and number of any deviations. Perfect adherence means all sub-checks are True and no hallucinations are present. 1 = Gross violations; 3 = Some minor deviations; 5 = Perfectly adheres to schema.
    *   Reasoning for Overall Schema Adherence Score: Provide the explanation here.

3.  **Intent Accuracy (Semantic Correctness):**
    *   Score (1-5): 1 = Completely misses user intent; 3 = Partially addresses intent but has significant gaps or inaccuracies; 5 = Accurately and fully captures user intent.
    *   Explanation of Cypher Retrieval: In simple natural language that an average office worker could understand, what information would this Cypher query retrieve from a graph that matches the schema?
    *   Alignment with User Question: How well does this retrieved information align with what the user was asking in their "USER'S NATURAL LANGUAGE QUESTION"?
    *   Key Element Coverage: Does the Cypher query attempt to address all key entities, relationships, and conditions mentioned in the user's question? If not, what specific parts of the question seem to be missing or misinterpreted in the Cypher query?
    *   Reasoning: Explain your intent accuracy score, considering the explanation, alignment, and key element coverage.

4.  **Identification of Potential Qwen2.5-3B Error Patterns (Optional but helpful):**
    *   Based on your knowledge of smaller LLMs, does this query exhibit any common error patterns such as:
        *   Over-simplification of complex conditions?
        *   Incorrect handling of the ':ExtractedEntity' and 'label' property convention?
        *   Ignoring multiple constraints from the user question?
        *   Other (please specify)?
    *   Observed Patterns: List any observed patterns from the predefined list, or provide an empty list [] if none of the predefined patterns are clearly observed.

**OUTPUT JSON STRUCTURE (Strictly follow this format):**
```json
{
  "evaluation_summary": {
    "overall_quality_score_cypher": "<Integer score 1-5, your overall judgment of the Cypher query's quality. **Crucially, assign higher weight to 'Schema Adherence' and 'Intent Accuracy'.** A query with perfect syntax but critical flaws in schema adherence or intent accuracy should NOT receive a high overall score.>",
    "main_strength_cypher": "<Briefly describe the main strength of this Cypher query, if any>",
    "main_weakness_cypher": "<Briefly describe the main weakness or most critical issue>"
  },
  "dimensions": {
    "syntactic_correctness": {
      "score": "<Integer score 1-5>",
      "parsable_prediction": "<Boolean: true/false>",
      "reasoning": "<Text explanation>"
    },
    "schema_adherence": {
      "overall_score": "<Integer score 1-5>",
      "node_label_correctness": { "check_result": "<Boolean>", "detail": "<Text>" },
      "entity_type_property_correctness": { "check_result": "<Boolean>", "detail": "<Text>" },
      "relationship_type_correctness": { "check_result": "<Boolean>", "detail": "<Text>" },
      "property_name_correctness": { "check_result": "<Boolean>", "detail": "<Text>" },
      "hallucinated_schema_elements": { "check_result_hallucination_present": "<Boolean>", "elements_found": ["<List of strings or empty list>"] },
      "reasoning": "<Text explanation for overall schema adherence score>"
    },
    "intent_accuracy": {
      "score": "<Integer score 1-5>",
      "explanation_of_cypher_retrieval": "<Text>",
      "semantic_alignment_with_question": "<Text>",
      "key_element_coverage_notes": "<Text describing coverage of key elements, and what's missing/misinterpreted, if any>",
      "reasoning": "<Text explanation for intent accuracy score>"
    }
  },
  "qwen_error_patterns_identified": ["<List of strings describing observed patterns, or empty list>"],
  "suggestion_for_improvement_cypher": "<Actionable suggestions to improve this specific Cypher query, if applicable>"
}
```"""

async def evaluate_cypher_with_gemini(
    user_question: str,
    generated_cypher: str,
    # kg_schema_description is now imported as KG_SCHEMA_FOR_EVALUATION
    original_interaction_id: Optional[str] = None,
    app_version: str = "0.1.0"
) -> Optional[Dict[str, Any]]:
    eval_logger.info(f"Starting Cypher evaluation. User question: '{user_question[:50]}...', Cypher: '{generated_cypher[:100]}...'")

    prompt_to_gemini = CYPHER_EVALUATION_PROMPT_V1.replace(
        "{{KG_SCHEMA_DESCRIPTION}}", KG_SCHEMA_FOR_EVALUATION # Use imported constant
    ).replace(
        "{{USER_QUESTION}}", user_question
    ).replace(
        "{{GENERATED_CYPHER}}", generated_cypher
    )

    gemini_model_name = os.getenv("GEMINI_EVALUATION_MODEL", "gemini/gemini-1.5-flash-latest")
    messages_for_gemini = [{"role": "user", "content": prompt_to_gemini}]
    evaluation_result_json: Optional[Dict[str, Any]] = None
    raw_gemini_output: Optional[str] = None
    error_info: Optional[str] = None

    USE_SIMULATED_GEMINI_RESPONSE = os.getenv("USE_SIMULATED_GEMINI_CYPHER_EVAL", "false").lower() == "true"

    if USE_SIMULATED_GEMINI_RESPONSE:
        eval_logger.warning("USING SIMULATED GEMINI RESPONSE FOR CYPHER EVALUATION")
        simulated_json_output = {
            "evaluation_summary": {
                "overall_quality_score_cypher": 4,
                "main_strength_cypher": "Good attempt at capturing intent.",
                "main_weakness_cypher": "Minor schema deviation in node label."
            },
            "dimensions": {
                "syntactic_correctness": {"score": 5, "parsable_prediction": True, "reasoning": "Query appears syntactically valid."},
                "schema_adherence": {
                    "overall_score": 3,
                    "node_label_correctness": { "check_result": False, "detail": "Used :Person instead of :ExtractedEntity for one node." },
                    "entity_type_property_correctness": { "check_result": True, "detail": "Correctly used label property." },
                    "relationship_type_correctness": { "check_result": True, "detail": "Used defined relationships correctly." },
                    "property_name_correctness": { "check_result": True, "detail": "Used valid properties." },
                    "hallucinated_schema_elements": { "check_result_hallucination_present": False, "elements_found": [] },
                    "reasoning": "One instance of incorrect node label, otherwise good."
                },
                "intent_accuracy": {
                    "score": 4,
                    "explanation_of_cypher_retrieval": "The query attempts to find the organization where '张三' works.",
                    "semantic_alignment_with_question": "Well-aligned with the user's question about '张三's' workplace.",
                    "key_element_coverage_notes": "All key elements seem to be covered.",
                    "reasoning": "Good intent capture, minor improvement could be ensuring organization type is also filtered if ambiguous."
                }
            },
            "qwen_error_patterns_identified": ["Incorrect handling of the ':ExtractedEntity' and 'label' property convention"],
            "suggestion_for_improvement_cypher": "Change MATCH (p:Person...) to MATCH (p:ExtractedEntity {label: 'PERSON'})..."
        }
        raw_gemini_output = json.dumps(simulated_json_output)
        try:
            evaluation_result_json = json.loads(raw_gemini_output)
        except json.JSONDecodeError as e:
            eval_logger.error(f"Error decoding simulated JSON: {e}")
            error_info = f"Simulated JSON decode error: {str(e)}"
    else:
        try:
            eval_logger.info(f"Calling Gemini for Cypher evaluation. Model: {gemini_model_name}. Prompt length: {len(prompt_to_gemini)}")
            response = await litellm.acompletion(
                model=gemini_model_name,
                messages=messages_for_gemini,
                temperature=0.1,
                max_tokens=2048,
            )
            if response.choices and response.choices[0].message and response.choices[0].message.content:
                raw_gemini_output = response.choices[0].message.content
                eval_logger.info(f"Raw Gemini output for Cypher eval (first 300 chars): {raw_gemini_output[:300]}...")
                cleaned_output = raw_gemini_output.strip()
                if cleaned_output.startswith("```json"):
                    cleaned_output = cleaned_output[len("```json"):].strip()
                if cleaned_output.endswith("```"):
                    cleaned_output = cleaned_output[:-len("```")].strip()
                evaluation_result_json = json.loads(cleaned_output)
                eval_logger.info("Successfully parsed Gemini evaluation result for Cypher.")
            else:
                eval_logger.error("Gemini returned an empty or malformed response for Cypher evaluation.")
                raw_gemini_output = str(response)
                error_info = "Gemini empty or malformed response"
        except litellm.exceptions.APIError as e_api:
            eval_logger.error(f"LiteLLM APIError during Cypher evaluation: {e_api}", exc_info=True)
            error_info = f"LiteLLM APIError: {str(e_api)}"
            raw_gemini_output = error_info
        except json.JSONDecodeError as e_json:
            eval_logger.error(f"Failed to decode JSON from Gemini Cypher evaluation. Raw output: {raw_gemini_output[:500] if raw_gemini_output else 'N/A'}", exc_info=True)
            error_info = f"JSONDecodeError: {str(e_json)}"
        except Exception as e_gen:
            eval_logger.error(f"Unexpected error during Cypher evaluation with Gemini: {e_gen}", exc_info=True)
            error_info = f"Unexpected error: {str(e_gen)}"
            if raw_gemini_output is None:
                raw_gemini_output = error_info

    eval_log_data = {
        "task_type": "cypher_evaluation_by_gemini",
        "original_interaction_id_ref": original_interaction_id,
        "user_question_for_eval": user_question,
        "generated_cypher_for_eval": generated_cypher,
        "eval_llm_input_prompt_char_count": len(prompt_to_gemini),
        "eval_llm_model": gemini_model_name,
        "eval_llm_raw_output": raw_gemini_output,
        "eval_llm_processed_output_json": evaluation_result_json,
        "eval_error_info": error_info,
        "application_version": app_version
    }
    await log_interaction_data(eval_log_data)

    if evaluation_result_json:
        eval_logger.info(f"Cypher evaluation completed. Overall score: {evaluation_result_json.get('evaluation_summary', {}).get('overall_quality_score_cypher')}")
    else:
        eval_logger.warning("Cypher evaluation did not produce a valid JSON result.")
        
    return evaluation_result_json

if __name__ == '__main__':
    import asyncio # Add asyncio import for the test block

    async def test_cypher_evaluation():
        print("DEBUG: test_cypher_evaluation function CALLED") 
        eval_logger.info("--- Running test_cypher_evaluation ---")
        
        # KG_SCHEMA_FOR_EVALUATION is now imported from .constants
        # No need to mock it here if constants.py is correctly set up.

        test_cases = [
            {
                "user_question": "张三在哪里工作？",
                "generated_cypher": "MATCH (p:ExtractedEntity {text: '张三', label: 'PERSON'})-[:WORKS_AT]->(org:ExtractedEntity {label: 'ORGANIZATION'}) RETURN org.text AS organizationName",
                "id": "test_case_1_good"
            },
            {
                "user_question": "李四负责什么项目？",
                "generated_cypher": "MATCH (p:Person {name: '李四'})-[:RESPONSIBLE_FOR]->(proj:Project) RETURN proj.name",
                "id": "test_case_2_schema_error"
            },
            {
                "user_question": "所有任务的截止日期是什么时候？",
                "generated_cypher": "MATCH (t:ExtractedEntity {label: 'TASK'}) RETURN t.deadline",
                "id": "test_case_3_hallucination"
            }
        ]

        for i, case in enumerate(test_cases):
            eval_logger.info(f"\n--- Evaluating test case {i+1}: {case['id']} ---")
            result = await evaluate_cypher_with_gemini(
                user_question=case["user_question"],
                generated_cypher=case["generated_cypher"],
                # kg_schema_description is now handled by the imported KG_SCHEMA_FOR_EVALUATION
                original_interaction_id=case["id"]
            )
            if result:
                eval_logger.info(f"Evaluation result for {case['id']}:\n{json.dumps(result, indent=2, ensure_ascii=False)}")
            else:
                eval_logger.warning(f"Evaluation failed or returned None for {case['id']}")

    os.environ["USE_SIMULATED_GEMINI_CYPHER_EVAL"] = "true" # Enable for simulation
    asyncio.run(test_cypher_evaluation()) # Uncomment to run tests
