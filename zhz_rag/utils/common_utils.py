# zhz_rag/utils/common_utils.py

import httpx
import json
import traceback
import os
import glob
from dotenv import load_dotenv
from datetime import datetime, timezone
import uuid
import logging
import asyncio #确保 asyncio 被导入
from typing import List, Dict, Any, Optional

load_dotenv()

# --- Logger Configuration ---
utils_logger = logging.getLogger("UtilsLogger")
utils_logger.setLevel(logging.INFO)
if not utils_logger.hasHandlers():
    _utils_console_handler = logging.StreamHandler()
    _utils_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
    _utils_console_handler.setFormatter(_utils_formatter)
    utils_logger.addHandler(_utils_console_handler)
    utils_logger.propagate = False
utils_logger.info("--- UtilsLogger configured ---")

# --- MCP Configuration ---
MCPO_BASE_URL = os.getenv("MCPO_BASE_URL", "http://localhost:8006")

# --- Standard Project Directory Paths ---
# _COMMON_UTILS_FILE_PATH = os.path.abspath(__file__)
# _UTILS_DIR = os.path.dirname(_COMMON_UTILS_FILE_PATH) # zhz_rag/utils/
# _ZHZ_RAG_PACKAGE_DIR = os.path.dirname(_UTILS_DIR)     # zhz_rag/

# More robust way to get zhz_rag package directory if common_utils.py is in zhz_rag/utils/
_CURRENT_FILE_DIR = os.path.dirname(os.path.abspath(__file__))
# Assuming this file (common_utils.py) is in zhz_rag/utils/
# Then _ZHZ_RAG_PACKAGE_DIR should be the parent of _CURRENT_FILE_DIR
_ZHZ_RAG_PACKAGE_DIR = os.path.dirname(_CURRENT_FILE_DIR)


# Root directory for all data generated and used at runtime by the RAG system (logs, indexes etc.)
# This will be zhz_rag/stored_data/
STORED_DATA_ROOT_DIR = os.path.join(_ZHZ_RAG_PACKAGE_DIR, 'stored_data')

# Specific subdirectories within stored_data
RAG_INTERACTION_LOGS_DIR = os.path.join(STORED_DATA_ROOT_DIR, 'rag_interaction_logs')
EVALUATION_RESULTS_LOGS_DIR = os.path.join(STORED_DATA_ROOT_DIR, 'evaluation_results_logs')
# Example for other potential stored data:
# CHROMA_DB_DIR = os.path.join(STORED_DATA_ROOT_DIR, 'chromadb_index')
# BM25_INDEX_DIR = os.path.join(STORED_DATA_ROOT_DIR, 'bm25_index')


# Root directory for data generated by the finetuning preparation scripts
# This will be zhz_rag/finetuning/generated_data/
FINETUNING_GENERATED_DATA_DIR = os.path.join(_ZHZ_RAG_PACKAGE_DIR, 'finetuning', 'generated_data')

# Ensure these directories exist
_DIRECTORIES_TO_CREATE = [
    STORED_DATA_ROOT_DIR,
    RAG_INTERACTION_LOGS_DIR,
    EVALUATION_RESULTS_LOGS_DIR,
    FINETUNING_GENERATED_DATA_DIR
]
for dir_path in _DIRECTORIES_TO_CREATE:
    if not os.path.exists(dir_path):
        try:
            os.makedirs(dir_path, exist_ok=True)
            utils_logger.info(f"Successfully created directory: {dir_path}")
        except Exception as e:
            utils_logger.error(f"Error creating directory {dir_path}: {e}. Consider creating it manually.")

# --- Log File Path Getters ---

def get_interaction_log_filepath() -> str:
    """Gets the full path for the current RAG interaction log file (daily rotation)."""
    today_str = datetime.now(timezone.utc).strftime("%Y%m%d")
    return os.path.join(RAG_INTERACTION_LOGS_DIR, f"rag_interactions_{today_str}.jsonl")

def get_evaluation_result_log_filepath(evaluation_name: str) -> str:
    """Gets the full path for an evaluation result log file (daily rotation, by evaluation name)."""
    today_str = datetime.now(timezone.utc).strftime("%Y%m%d")
    return os.path.join(EVALUATION_RESULTS_LOGS_DIR, f"eval_results_{evaluation_name}_{today_str}.jsonl")

def find_latest_rag_interaction_log(log_dir: str = RAG_INTERACTION_LOGS_DIR) -> Optional[str]:
    """
    Finds the latest RAG interaction log file (rag_interactions_*.jsonl) in the specified directory.
    Defaults to RAG_INTERACTION_LOGS_DIR.
    """
    utils_logger.debug(f"Searching for RAG interaction logs in: {log_dir}")
    rag_log_pattern = os.path.join(log_dir, "rag_interactions_*.jsonl")
    candidate_rag_logs = glob.glob(rag_log_pattern)

    if candidate_rag_logs:
        candidate_rag_logs.sort(key=os.path.getmtime, reverse=True)
        utils_logger.info(f"Automatically selected RAG interaction log: {candidate_rag_logs[0]}")
        return candidate_rag_logs[0]
    else:
        utils_logger.warning(f"No RAG interaction log files found matching pattern: {rag_log_pattern} in directory {log_dir}")
        return None

# --- Logging Function ---

async def log_interaction_data(
    interaction_data: Dict[str, Any],
    is_evaluation_result: bool = False,
    evaluation_name_for_file: Optional[str] = None
):
    """
    Asynchronously appends a single interaction data or evaluation result to a JSONL file.
    """
    if is_evaluation_result:
        if not evaluation_name_for_file:
            evaluation_name_for_file = interaction_data.get("task_type", "general_eval_result") # More specific default
        filepath = get_evaluation_result_log_filepath(evaluation_name=evaluation_name_for_file)
    else:
        filepath = get_interaction_log_filepath()

    if "timestamp_utc" not in interaction_data:
        interaction_data["timestamp_utc"] = datetime.now(timezone.utc).isoformat()
    if "interaction_id" not in interaction_data and not is_evaluation_result: # Eval results use original_interaction_id_ref
        interaction_data["interaction_id"] = str(uuid.uuid4())
    elif "interaction_id" not in interaction_data and is_evaluation_result and "original_interaction_id_ref" in interaction_data:
        # For eval results, ensure there's an ID, can be a new one for the eval log entry itself
         interaction_data["interaction_id"] = str(uuid.uuid4())


    try:
        def _write_sync():
            # Ensure the directory for the specific log file exists
            log_file_dir = os.path.dirname(filepath)
            if not os.path.exists(log_file_dir):
                try:
                    os.makedirs(log_file_dir, exist_ok=True)
                    utils_logger.info(f"Created directory for log file: {log_file_dir}")
                except Exception as e_mkdir:
                    utils_logger.error(f"Error creating directory {log_file_dir} for log file: {e_mkdir}")
                    return # Prevent writing if directory creation fails

            with open(filepath, 'a', encoding='utf-8') as f:
                f.write(json.dumps(interaction_data, ensure_ascii=False) + "\n")
        
        await asyncio.to_thread(_write_sync)
        # utils_logger.debug(f"Successfully logged data (type: {interaction_data.get('task_type', 'N/A')}) to {filepath}")
    except Exception as e:
        utils_logger.error(f"Failed to log interaction data to {filepath}: {e}", exc_info=True)

# --- MCP Tool Calling Utility ---

async def call_mcpo_tool(tool_name_with_prefix: str, payload: Dict[str, Any]) -> Dict[str, Any]:
    """
    Asynchronously calls an MCP tool service and returns the JSON response or an error dictionary.
    """
    api_url = f"{MCPO_BASE_URL}/{tool_name_with_prefix}"
    # Ensure payload is a dictionary, even if it's empty
    cleaned_payload = {k: v for k, v in (payload or {}).items() if v is not None}


    utils_logger.info(f"Calling MCP endpoint: POST {api_url} with payload: {json.dumps(cleaned_payload, ensure_ascii=False)}")

    async with httpx.AsyncClient() as client:
        response = None
        try:
            headers = {"Content-Type": "application/json"}
            response = await client.post(api_url, json=cleaned_payload, headers=headers, timeout=120.0)
            utils_logger.info(f"MCP status code for {tool_name_with_prefix}: {response.status_code}")

            if response.status_code == 200:
                try:
                    result_data = response.json()
                    # MCP specific error handling (if MCP wraps tool errors)
                    if isinstance(result_data, dict) and result_data.get("isError"):
                        error_content_list = result_data.get("content", [{"type": "text", "text": "Unknown error from MCP tool"}])
                        error_text = "Unknown error from MCP tool"
                        for item in error_content_list:
                            if item.get("type") == "text":
                                error_text = item.get("text", error_text)
                                break
                        utils_logger.error(f"MCP Tool '{tool_name_with_prefix}' execution failed (isError=true): {error_text}")
                        try:
                            # Try to parse if the error_text itself is a JSON string with more details
                            parsed_mcp_error = json.loads(error_text)
                            if isinstance(parsed_mcp_error, dict) and "error" in parsed_mcp_error:
                                return {"error": f"Tool '{tool_name_with_prefix}' failed via MCP: {parsed_mcp_error['error']}"}
                        except json.JSONDecodeError:
                            pass 
                        return {"error": f"Tool '{tool_name_with_prefix}' failed via MCP: {error_text}"}
                    return result_data
                except json.JSONDecodeError:
                    utils_logger.warning(f"MCP call to '{tool_name_with_prefix}' returned status 200 but response is not JSON. Raw text: {response.text[:500]}...")
                    # Return a structured error if possible, or the raw text wrapped
                    return {"error": "Non-JSON response from MCP tool", "raw_response": response.text}
            else:
                error_text = f"MCP call to '{tool_name_with_prefix}' failed with status {response.status_code}. Response: {response.text[:500]}..."
                utils_logger.error(error_text)
                return {"error": error_text, "status_code": response.status_code}

        except httpx.RequestError as exc:
            error_msg = f"HTTP RequestError calling MCP tool '{tool_name_with_prefix}': {type(exc).__name__} - {exc}"
            utils_logger.error(error_msg, exc_info=True)
            return {"error": error_msg, "exception_type": type(exc).__name__}
        except Exception as exc:
            error_msg = f"Unexpected error calling MCP tool '{tool_name_with_prefix}': {type(exc).__name__} - {exc}"
            response_text_snippet = response.text[:500] if response and hasattr(response, 'text') else "Response object not available or no text."
            utils_logger.error(f"{error_msg}. Response snippet: {response_text_snippet}", exc_info=True)
            return {"error": error_msg, "exception_type": type(exc).__name__}
        
def load_jsonl_file(filepath: str, encoding: str = 'utf-8') -> List[Dict[str, Any]]:
    """
    从 JSONL 文件加载数据。

    Args:
        filepath (str): JSONL 文件的路径。
        encoding (str): 文件编码，默认为 'utf-8'。

    Returns:
        List[Dict[str, Any]]: 从文件中加载的字典列表。如果文件不存在或解析出错，
                              会记录错误并返回空列表。
    """
    data_list: List[Dict[str, Any]] = []
    if not os.path.exists(filepath):
        utils_logger.error(f"File not found: {filepath}") # 使用已有的 utils_logger
        return data_list

    try:
        with open(filepath, 'r', encoding=encoding) as f:
            for line_number, line in enumerate(f, 1):
                try:
                    if line.strip(): # 确保行不是空的
                        data_list.append(json.loads(line.strip()))
                except json.JSONDecodeError as e_json:
                    utils_logger.warning(f"Skipping malformed JSON line {line_number} in {filepath}: {e_json}")
                except Exception as e_line:
                    utils_logger.warning(f"Error processing line {line_number} in {filepath}: {e_line}")
    except FileNotFoundError: # 再次捕获以防万一，虽然上面已经检查了
        utils_logger.error(f"File not found during open: {filepath}")
    except Exception as e_file:
        utils_logger.error(f"Error reading or processing file {filepath}: {e_file}", exc_info=True)
        return [] # 如果文件读取层面发生严重错误，返回空列表

    utils_logger.info(f"Successfully loaded {len(data_list)} entries from {filepath}")
    return data_list