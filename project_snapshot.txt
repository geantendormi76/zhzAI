Project Root: /home/zhz/zhz_agent
--- Project Structure and File Contents ---

Directory: /home/zhz/zhz_agent/
  File (skipped, extension not included): .env
  File (skipped, extension not included): .envrc
  File (skipped, extension not included): .gitignore
  File: README.md
    --- START OF FILE CONTENT ---
# zhz_agent
后端：RAG增强+多agent+dagster

    --- END OF FILE CONTENT ---

  File: __init__.py
    --- File is empty ---

  File: agent_orchestrator_service.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/agent_orchestrator_service.py

import os
import asyncio
import traceback
import json
from typing import List, Dict, Any, Optional, Union
from contextlib import asynccontextmanager
import re
import uvicorn
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from datetime import datetime 

import logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(module)s:%(lineno)d - %(message)s')
service_logger = logging.getLogger(__name__)

from crewai import Agent, Task, Crew, Process
from crewai.tools import BaseTool

from core.llm_manager import get_llm_instance, CustomLiteLLMWrapper
from core.tools.enhanced_rag_tool import EnhancedRAGTool
from core.tools.excel_tool import ExcelOperationTool
from core.tools.search_tool import WebSearchTool
from core.tools.time_tool import GetCurrentTimeTool
from core.tools.calculator_tool import CalculateTool


AGENT_SERVICE_PORT = int(os.getenv("AGENT_SERVICE_PORT", 8090))
AGENT_SERVICE_HOST = "0.0.0.0"

class AgentTaskRequest(BaseModel):
    user_query: str = Field(description="用户的原始文本查询。")

class AgentTaskResponse(BaseModel):
    answer: str = Field(description="Agent 系统生成的最终答案或响应。")
    status: str = Field(default="success", description="执行状态: 'success', 'needs_clarification', 'error', 'processing_plan', 'task_created'.")
    intermediate_plan: Optional[Dict[str, Any]] = Field(None, description="如果status是'processing_plan', 这里包含Manager Agent的规划结果。")
    task_id: Optional[str] = Field(None, description="如果任务是异步的，返回任务ID。")
    error_message: Optional[str] = Field(None, description="如果发生错误，此字段包含错误信息。")
    debug_info: Optional[Dict[str, Any]] = Field(None, description="包含执行过程中的调试信息。")
    token_usage: Optional[Dict[str, Any]] = Field(None, description="LLM token 使用情况统计。")

class SubTaskDefinitionForManagerOutput(BaseModel):
    task_description: str = Field(description="用户的原始请求原文。")
    reasoning_for_plan: Optional[str] = Field(None, description="Manager Agent的决策思考过程。")
    selected_tool_names: List[str] = Field(description="选定的工具名称列表。如果直接回答，则为空列表。")
    direct_answer_content: Optional[str] = Field(None, description="如果选择直接回答，这里是答案内容。")
    tool_input_args: Optional[Dict[str, Any]] = Field(None, description="如果选择使用非Excel工具，这里是传递给该工具的参数。")
    excel_sqo_payload: Optional[List[Dict[str, Any]]] = Field(None, description="如果选择使用Excel工具，这里是SQO操作字典的列表。")

manager_llm: Optional[CustomLiteLLMWrapper] = None
worker_llm: Optional[CustomLiteLLMWrapper] = None
manager_agent_instance: Optional[Agent] = None
worker_agent_instance: Optional[Agent] = None
core_tools_instances: List[BaseTool] = []

# --- 覆盖开始 ---
CORE_TOOLS_ZHZ_AGENT = {
    "enhanced_rag_tool": "【核心RAG工具】用于从本地知识库查找信息、回答复杂问题，整合了向量、关键词和图谱检索。",
    "excel_operation_tool": "【Excel操作工具】通过结构化查询对象(SQO)对Excel文件执行复杂的数据查询、筛选、聚合等操作。此工具通过本地代理在Windows上运行。",
    "web_search_tool": "【网络搜索工具】使用DuckDuckGo搜索引擎在互联网上查找与用户查询相关的信息。此工具通过MCP调用。",
    "get_current_time_tool": "【时间工具】获取当前的日期和时间，可指定时区。此工具在Agent的Python环境中直接执行。",
    "calculate_tool": "【计算器工具】执行数学表达式的计算并返回数值结果。此工具在Agent的Python环境中直接执行。"
}
# --- 覆盖结束 ---
CORE_TOOL_NAMES_LIST = list(CORE_TOOLS_ZHZ_AGENT.keys()) # 这行会自动更新
TOOL_OPTIONS_STR_FOR_MANAGER = "\n".join( # 这行会自动更新
    [f"- '{name}': {desc}" for name, desc in CORE_TOOLS_ZHZ_AGENT.items()]
)

@asynccontextmanager
async def lifespan(app: FastAPI):
    global manager_llm, worker_llm, manager_agent_instance, worker_agent_instance, core_tools_instances
    # --- [修改] 为了让 Manager 的 prompt 也只看到 RAG 工具，我们需要在函数作用域内临时修改这些 ---
    global CORE_TOOL_NAMES_LIST, TOOL_OPTIONS_STR_FOR_MANAGER 
    
    print("--- Agent Orchestrator Service: Lifespan startup ---")

    # --- 步骤 1: 初始化核心工具实例 ---
    print("Initializing core tool instances...")
    enhanced_rag_tool_instance = None 
    excel_operation_tool_instance = None
    web_search_tool_instance = None
    get_current_time_tool_instance = None
    calculate_tool_instance = None
    try:
        enhanced_rag_tool_instance = EnhancedRAGTool()
        excel_operation_tool_instance = ExcelOperationTool() # 尝试实例化
        web_search_tool_instance = WebSearchTool()       # 尝试实例化
        get_current_time_tool_instance = GetCurrentTimeTool()
        calculate_tool_instance = CalculateTool()

        core_tools_instances = []
        if enhanced_rag_tool_instance:
            core_tools_instances.append(enhanced_rag_tool_instance)
        if get_current_time_tool_instance:
            core_tools_instances.append(get_current_time_tool_instance)
        if calculate_tool_instance:
            core_tools_instances.append(calculate_tool_instance)
        if excel_operation_tool_instance:
            core_tools_instances.append(excel_operation_tool_instance)
        if web_search_tool_instance:
            core_tools_instances.append(web_search_tool_instance)

        if not core_tools_instances:
            print("CRITICAL ERROR: No core tools could be initialized. Aborting LLM/Agent setup.")
            # 在这种情况下，后续的LLM和Agent初始化可能会失败或无意义
            # 可以考虑在这里抛出异常或设置一个全局状态阻止服务启动
        else:
            print(f"Successfully initialized tools: {[tool.name for tool in core_tools_instances]}")
        print(f"--- FULL TOOL CONFIGURATION WILL BE USED BY MANAGER (if successfully initialized) ---")
        print(f"Effective CORE_TOOL_NAMES_LIST for Manager (based on global def): {CORE_TOOL_NAMES_LIST}") # 这是全局的
        print(f"Effective TOOL_OPTIONS_STR_FOR_MANAGER for Manager (based on global def):\n{TOOL_OPTIONS_STR_FOR_MANAGER}") # 这是全局的
        print(f"Actually initialized tools for Worker: {[tool.name for tool in core_tools_instances]}")
        print(f"--- END OF TOOL CONFIGURATION ---")
        
    except Exception as e:
        print(f"ERROR during core tool initialization: {e}", exc_info=True)
        core_tools_instances = [] # 确保出错时为空列表
        # 后续LLM/Agent初始化时，如果 core_tools_instances 为空，它们可能需要特殊处理或报错

    # ... 后续的LLM和Agent初始化代码 ...
    # 确保 Manager Agent 的 tools=[] (它不直接调用工具)
    # 确保 Worker Agent 的 tools=core_tools_instances (它需要所有可用的工具实例)


    # --- 步骤 2: 初始化 LLM 实例 ---
    # LLM 初始化时，agent_tools 参数将使用上面步骤中已更新（且只包含RAG工具）的 core_tools_instances
    print("Initializing LLM instances...")
    try:
        gemini_tool_config = {"function_calling_config": {"mode": "AUTO"}}
        # Manager LLM 初始化
        manager_llm = get_llm_instance(
            llm_type="cloud_gemini", 
            temperature=0.1, 
            max_tokens=4096, 
            tool_config=gemini_tool_config,
            agent_tools=core_tools_instances # 使用已更新的 core_tools_instances
        )
        if not manager_llm:
            print("Failed to initialize Manager LLM (Cloud Gemini). Attempting fallback...")
            manager_llm = get_llm_instance(
                llm_type="local_qwen", 
                temperature=0.1, 
                max_tokens=3072, 
                tool_config=gemini_tool_config,
                agent_tools=core_tools_instances 
            )
        
        # Worker LLM 初始化
        print("Initializing Worker LLM (attempting Cloud Gemini first)...")
        worker_gemini_tool_config = {"function_calling_config": {"mode": "AUTO"}} 
        worker_llm = get_llm_instance(
            llm_type="cloud_gemini", 
            temperature=0.5, 
            max_tokens=3072,
            tool_config=worker_gemini_tool_config,
            agent_tools=core_tools_instances # 使用已更新的 core_tools_instances
        )
        if not worker_llm:
            print("Failed to initialize Worker LLM (Cloud Gemini). Attempting fallback to local_qwen...")
            worker_llm = get_llm_instance(
                llm_type="local_qwen", 
                temperature=0.6, 
                max_tokens=3072,
                agent_tools=core_tools_instances
            )

        if manager_llm: print(f"Manager LLM initialized: {manager_llm.model_name}")
        else: print("CRITICAL: Failed to initialize Manager LLM.")
        if worker_llm: print(f"Worker LLM initialized: {worker_llm.model_name}")
        else: print("CRITICAL: Failed to initialize Worker LLM.")
    except Exception as e:
        print(f"FATAL ERROR during LLM initialization: {e}") 
        traceback.print_exc() 
        manager_llm = None; worker_llm = None

    # --- 步骤 3: 初始化 Agent 实例 ---
    if manager_llm:
        manager_agent_instance = Agent(
            role='资深AI任务分解与Excel查询规划师 (Senior AI Task Decomposition and Excel Query Planner)',
            goal=f"""你的核心任务是分析用户的请求（该请求将在后续的任务描述中提供），并决定最佳的处理路径。你必须严格按照以下【工具选择规则和优先级】以及【示例】进行决策。

**【决策规则与优先级】**

1.  **【规则1：时间查询 - 强制使用工具】**
    *   如果用户查询明确是关于【获取当前日期或时间】。
    *   **行动**：【必须选择】`"get_current_time_tool"`。不要尝试自己回答“我没有实时时钟”。
    *   **参数**：为 `tool_input_args` 准备可选的 `timezone`。

2.  **【规则2：数学计算 - 强制使用工具】**
    *   如果用户查询明确是要求【执行一个数学表达式的计算】。
    *   **行动**：【必须选择】`"calculate_tool"`。不要尝试自己计算，即使很简单。
    *   **参数**：为 `tool_input_args` 准备 `expression`。

3.  **【规则3：实时/最新信息查询 - 网络搜索强制】**
    *   如果用户查询明显需要【实时、最新的、动态变化的、或广泛的外部互联网信息】（例如：今天的天气、最新的新闻、当前股价等）。
    *   **行动**：【必须且只能】选择 `"web_search_tool"`。
    *   **参数**：为 `tool_input_args` 准备 `query` 和可选的 `max_results`。
    *   **禁止**：不要使用 "enhanced_rag_tool" 查找此类信息。

4.  **【规则4：内部知识/文档深度查询 - RAG】**
    *   如果用户查询的是关于【已归档的、静态的公司内部信息、特定文档的详细内容、历史数据分析、或不需要实时更新的深度专业知识】。
    *   **行动**：选择 `"enhanced_rag_tool"`。
    *   **参数**：为 `tool_input_args` 准备 `query` 等。

5.  **【规则5：Excel文件操作 - 强制使用工具】**
    *   如果用户明确要求或其意图明显指向需要对【Excel文件进行复杂操作】。
    *   **行动**：【必须选择】`"excel_operation_tool"`。
    *   **任务**：为其构建【结构化查询对象 (SQO) 的JSON列表】到 `excel_sqo_payload`。SQO不含 "file_path" 或 "sheet_name"。

6.  **【规则6：LLM直接回答（在工具不适用后）】**
    *   **仅当**以上所有规则都不适用，并且你判断可以基于你【已有的知识和常识】直接、准确、完整地回答用户的全部请求时。
    *   **行动**：`selected_tool_names` 设为 `[]`，在 `direct_answer_content` 提供答案。

7.  **【规则7：无法处理/需要澄清（最终回退）】**
    *   如果所有规则都不适用，且你也无法直接回答。
    *   **行动**：`selected_tool_names` 设为 `[]`。在 `reasoning_for_plan` 中解释，如果合适，在 `direct_answer_content` 中礼貌回复。

**【可用工具的参考描述】：**
{TOOL_OPTIONS_STR_FOR_MANAGER}

**【决策示例 - 你必须学习并模仿这些示例的决策逻辑和输出格式】**

<example>
  <user_query>现在几点了？</user_query>
  <thought>用户明确询问当前时间。根据规则1，必须使用get_current_time_tool。</thought>
  <output_json>{{
    "task_description": "现在几点了？",
    "reasoning_for_plan": "用户询问当前时间，根据规则1，应使用时间工具。",
    "selected_tool_names": ["get_current_time_tool"],
    "direct_answer_content": null,
    "tool_input_args": {{"timezone": "Asia/Shanghai"}},
    "excel_sqo_payload": null
  }}</output_json>
</example>

<example>
  <user_query>计算 5 * (10 + 3)</user_query>
  <thought>用户要求进行数学计算。根据规则2，必须使用calculate_tool。</thought>
  <output_json>{{
    "task_description": "计算 5 * (10 + 3)",
    "reasoning_for_plan": "用户要求数学计算，根据规则2，应使用计算器工具。",
    "selected_tool_names": ["calculate_tool"],
    "direct_answer_content": null,
    "tool_input_args": {{"expression": "5 * (10 + 3)"}},
    "excel_sqo_payload": null
  }}</output_json>
</example>

<example>
  <user_query>今天上海的天气怎么样？</user_query>
  <thought>用户询问“今天”的天气，这是时效性信息。根据规则3，必须使用web_search_tool。</thought>
  <output_json>{{
    "task_description": "今天上海的天气怎么样？",
    "reasoning_for_plan": "查询今日天气，需要实时信息，根据规则3选择网络搜索。",
    "selected_tool_names": ["web_search_tool"],
    "direct_answer_content": null,
    "tool_input_args": {{"query": "今天上海天气"}},
    "excel_sqo_payload": null
  }}</output_json>
</example>

<example>
  <user_query>我们公司的报销政策是什么？</user_query>
  <thought>用户查询公司内部政策，属于内部知识库范畴。根据规则4，应使用enhanced_rag_tool。</thought>
  <output_json>{{
    "task_description": "我们公司的报销政策是什么？",
    "reasoning_for_plan": "查询公司内部政策，根据规则4选择RAG工具。",
    "selected_tool_names": ["enhanced_rag_tool"],
    "direct_answer_content": null,
    "tool_input_args": {{"query": "公司报销政策"}},
    "excel_sqo_payload": null
  }}</output_json>
</example>

<example>
  <user_query>中国的首都是哪里？</user_query>
  <thought>这是一个常见的常识性问题，我的内部知识足以回答，无需使用工具。根据规则6直接回答。</thought>
  <output_json>{{
    "task_description": "中国的首都是哪里？",
    "reasoning_for_plan": "常识性问题，可直接回答。",
    "selected_tool_names": [],
    "direct_answer_content": "中国的首都是北京。",
    "tool_input_args": null,
    "excel_sqo_payload": null
  }}</output_json>
</example>

<example>
  <user_query>请帮我分析 "sales_report_Q3.xlsx" 文件中 "产品类别" 列的销售额总和，并按 "区域" 进行分组。</user_query>
  <thought>用户明确要求对Excel文件进行分组聚合操作。根据规则5，必须使用excel_operation_tool，并生成SQO列表。</thought>
  <output_json>{{
    "task_description": "请帮我分析 \"sales_report_Q3.xlsx\" 文件中 \"产品类别\" 列的销售额总和，并按 \"区域\" 进行分组。",
    "reasoning_for_plan": "用户要求对Excel进行分组聚合，根据规则5选择Excel工具并生成SQO。",
    "selected_tool_names": ["excel_operation_tool"],
    "direct_answer_content": null,
    "tool_input_args": null,
    "excel_sqo_payload": [
      {{
        "operation_type": "group_by_aggregate",
        "group_by_columns": ["区域", "产品类别"],
        "aggregation_column": "销售额",
        "aggregation_function": "sum"
      }}
    ]
  }}</output_json>
</example>

**【输出格式要求 - 必须严格遵守！】**
你的唯一输出必须是一个JSON对象，符合 `SubTaskDefinitionForManagerOutput` Pydantic模型，包含：
*   `task_description`: (字符串) 用户的原始请求。
*   `reasoning_for_plan`: (字符串) 你做出决策的思考过程，清晰说明你遵循了上述哪条规则和哪个示例。
*   `selected_tool_names`: (字符串列表) 选定的工具名称列表。
*   `direct_answer_content`: (可选字符串) 仅在规则6适用时填充。
*   `tool_input_args`: (可选对象) 仅在规则1, 2, 3, 4适用时，为对应工具填充参数。
*   `excel_sqo_payload`: (可选SQO列表) 仅在规则5适用时填充。

我【不】自己执行任何工具操作。我的职责是精准规划并输出结构化的任务定义。
""",
            # --- 添加/恢复 backstory 参数 ---
            backstory="""我是一位经验丰富且高度智能的AI任务调度官和数据分析规划专家。我的核心使命是精确解读用户的每一个请求，并为其匹配最高效、最准确的处理路径。我的工作流程严谨细致：

1.  **【请求深度解析与意图识别】**：我会首先对用户的原始请求进行彻底的语义分析和意图识别。我会判断请求的性质：是简单问答？是需要内部知识检索？是需要实时外部信息？还是需要对特定数据文件（如Excel）进行操作？

2.  **【决策优先级：优先自主解决】**：在考虑动用任何外部工具之前，我会首先评估我的内部知识库和推理能力是否足以直接、完整且准确地回答用户的问题。只有当我确认无法自主解决时，我才会启动工具选择流程。

3.  **【工具选择的智慧：精准匹配，而非盲目调用】**：
    *   **时效性是关键**：对于新闻、天气、实时数据等具有强时效性的查询，我会毫不犹豫地选择【网络搜索工具】(`web_search_tool`)。
    *   **内部知识优先**：对于公司政策、历史项目资料、特定存档文档等内部信息查询，我会优先使用【增强型RAG工具】(`enhanced_rag_tool`)，因为它能从我们精心构建的本地知识库中提取精确信息。
    *   **Excel事务专家**：任何涉及Excel文件（.xlsx, .csv）的复杂数据操作——无论是读取、计算、筛选、聚合还是修改——我都会委派给【Excel操作工具】(`excel_operation_tool`)。此时，我的核心任务是为该工具生成一个或多个清晰、准确的【结构化查询对象 (SQO) 的JSON列表】，放入 `excel_sqo_payload` 字段。我深知SQO的质量直接影响执行结果，因此我会仔细构造每一个SQO的操作类型 (`operation_type`) 和所需参数，并且我【绝不会】在SQO中包含文件路径 (`file_path`) 或工作表名 (`sheet_name`)，这些将由后续流程处理。
    *   **基础计算与时间查询**：对于明确的数学表达式计算，我会选择【计算器工具】(`calculate_tool`)；对于获取当前日期时间的需求，我会选择【时间工具】(`get_current_time_tool`)。
    *   **审慎对待无法处理的请求**：如果用户请求过于宽泛、模糊，或者超出了当前所有可用工具和我的知识范围，我不会强行匹配工具或给出猜测性答复。我会选择不使用任何工具，并在我的思考过程（`reasoning_for_plan`）中解释原因，或者在 `direct_answer_content` 中礼貌地请求用户提供更多信息或说明无法处理。

4.  **【结构化输出：一切为了清晰执行】**：我的最终输出永远是一个严格符合 `SubTaskDefinitionForManagerOutput` Pydantic模型规范的JSON对象。这个JSON对象不仅包含了用户的原始请求 (`task_description`) 和我的决策理由 (`reasoning_for_plan`)，更重要的是，它清晰地指明了选定的工具 (`selected_tool_names`) 以及调用这些工具所需的一切参数（在 `tool_input_args` 或 `excel_sqo_payload` 中）。

我从不亲自执行任务的细节，我的价值在于运筹帷幄，确保每一个用户请求都能被分配到最合适的处理单元，从而实现高效、准确的问题解决。
""",
            # --- backstory 参数结束 ---
            llm=manager_llm,
            verbose=True,
            allow_delegation=False,
            tools=[] 
        )
        print(f"Manager Agent initialized with LLM: {manager_llm.model_name}")

    if worker_llm:
        worker_agent_instance = Agent(
            role='任务执行专家 (Task Execution Expert)',
            goal="根据Manager分配的具体任务描述和指定的工具，高效地执行任务并提供结果。",
            backstory="""我是一个AI执行者，专注于使用【Manager明确授权给我的工具】来解决问题。
                        我会严格遵循任务指令。如果任务是调用Excel工具并提供了SQO列表，我会按顺序迭代处理这些SQO，并整合结果。
                        对于像 'get_current_time_tool' 和 'calculate_tool' 这样的本地Python工具，我会直接在我的环境中执行它们。
                        对于其他工具，我会使用工具的名称（例如 'enhanced_rag_tool', 'excel_operation_tool', 'web_search_tool'）来调用它们。""",
            llm=worker_llm,
            verbose=True,
            allow_delegation=False,
            tools=core_tools_instances # Worker Agent 使用已更新（只含RAG工具）的 core_tools_instances
        )
        print(f"Worker Agent initialized with LLM: {worker_llm.model_name} and tools: {[t.name for t in worker_agent_instance.tools]}")

    if not manager_agent_instance or not worker_agent_instance:
        print("CRITICAL: One or more core agents failed to initialize. Service functionality will be severely limited.")
    elif not core_tools_instances and worker_agent_instance : 
        print("WARNING: Worker Agent initialized, but no core tools were successfully instantiated. Tool-based tasks will fail.")

    print("--- Agent Orchestrator Service: Lifespan startup complete ---")
    yield
    print("--- Agent Orchestrator Service: Lifespan shutdown ---")

app = FastAPI(
    title="Agent Orchestrator Service",
    description="接收用户请求，通过Manager/Worker Agent模型进行任务规划和执行。",
    version="0.1.1", # 版本更新
    lifespan=lifespan
)

@app.post("/v1/execute_task", response_model=AgentTaskResponse)
async def execute_task_endpoint(request: AgentTaskRequest):
    global manager_agent_instance, worker_agent_instance, core_tools_instances # 确保能访问全局 Agent 和工具实例
    
    print(f"Received agent task request: User Query='{request.user_query}'") # 使用 print 替代 logger

    if not manager_agent_instance or not worker_agent_instance:
        print("CRITICAL ERROR: Core agents are not initialized. Cannot process task.")
        raise HTTPException(status_code=503, detail="Service not ready: Core agents failed to initialize.")
    if not core_tools_instances:
        print("WARNING: Core tools are not initialized. Tool-based tasks may fail.")
    # --- 构建 Manager Task 的描述 ---
    # 我们将用户请求和可用的核心工具列表传递给 Manager Agent
    # Manager Agent 的 goal 和 backstory 已经包含了大部分指令
    # Task 的 description 主要用于传递动态信息，如当前用户查询
    manager_task_description_for_crewai = f"""
    请仔细分析以下用户请求：
    '{request.user_query}'

    你的目标是：
    1.  理解用户的核心意图。
    2.  **优先判断**：你能否基于你现有的知识直接、准确地回答这个问题？
        - 如果是，请在输出的JSON中填充 `direct_answer_content` 字段，并将 `selected_tool_names` 设为空列表。
    3.  **如果不能直接回答**：你当前只有一个核心工具可用：'enhanced_rag_tool'。请判断是否需要使用它。
    4.  如果选择了 'enhanced_rag_tool'，请准备好传递给该工具的参数，并将其放入 `tool_input_args` 字段。
    5.  严格按照 `SubTaskDefinitionForManagerOutput` 的JSON格式输出你的规划。`task_description` 字段必须是用户的原始请求原文: '{request.user_query}'。同时提供你的 `reasoning_for_plan`。
    """
    
    # --- Manager Task 的期望输出格式说明 ---
    manager_task_expected_output_description = f"""一个JSON对象，必须严格符合以下Pydantic模型的结构（你不需要输出 "SubTaskDefinitionForManagerOutput" 这个词本身）：
    {{
      "task_description": "string (固定为用户的原始请求: '{request.user_query}')",
      "reasoning_for_plan": "string (你的决策思考过程)",
      "selected_tool_names": ["list of strings (选定的工具名称列表。如果直接回答，则为空列表。如果使用工具，则为 ['enhanced_rag_tool'])"],
      "direct_answer_content": "string (可选, 仅当 selected_tool_names 为空列表时，这里是你的答案内容)",
      "tool_input_args": {{ "key": "value" }} (可选, 仅当 selected_tool_names 包含'enhanced_rag_tool'时，这里是给该工具的参数字典),
      "excel_sqo_payload": null # 当前Excel工具不可用，此字段应为null
    }}

    【重要输出规则】:
    - 如果你选择【直接回答】：`selected_tool_names` 必须是空列表 `[]`，`direct_answer_content` 必须包含你的答案，`tool_input_args` 和 `excel_sqo_payload` 应该为 `null` 或不存在。
    - 如果你选择使用【enhanced_rag_tool】：`selected_tool_names` 必须包含 `"enhanced_rag_tool"`，`direct_answer_content` 应该为 `null` 或不存在，`tool_input_args` 必须包含调用该工具所需的参数 (例如 `{{ "query": "{request.user_query}" }}` )，`excel_sqo_payload` 应该为 `null` 或不存在。

    示例输出 (直接回答):
    {{
      "task_description": "{request.user_query}",
      "reasoning_for_plan": "这是一个常识性问题，我可以根据我的内部知识直接回答。",
      "selected_tool_names": [],
      "direct_answer_content": "中国的首都是北京。",
      "tool_input_args": null,
      "excel_sqo_payload": null
    }}

    示例输出 (使用RAG工具):
    {{
      "task_description": "{request.user_query}",
      "reasoning_for_plan": "用户询问关于公司政策的问题，这需要从知识库中查找。",
      "selected_tool_names": ["enhanced_rag_tool"],
      "direct_answer_content": null,
      "tool_input_args": {{"query": "{request.user_query}", "top_k_vector": 5, "top_k_kg": 3, "top_k_bm25": 3}},
      "excel_sqo_payload": null
    }}
    请严格按照此JSON格式输出。
    """

    manager_task = Task(
        description=manager_task_description_for_crewai,
        expected_output=manager_task_expected_output_description,
        agent=manager_agent_instance,
        async_execution=False,
        output_pydantic=SubTaskDefinitionForManagerOutput
    )

    # --- 步骤 2: 执行 Manager Task ---
    print("Executing Manager Task...")
    manager_plan_object: Optional[SubTaskDefinitionForManagerOutput] = None
    manager_raw_output: Optional[str] = None

    try:
        manager_crew = Crew(
            agents=[manager_agent_instance],
            tasks=[manager_task],
            process=Process.sequential,
            verbose=True 
        )
        print(f"[{datetime.now()}] About to call manager_crew.kickoff()...") # <--- 添加日志
        manager_task_raw_result = manager_crew.kickoff(inputs={})
        print(f"[{datetime.now()}] manager_crew.kickoff() returned.") # <--- 添加日志

        # --- 新的解析逻辑，处理 CrewOutput ---
        actual_task_output: Optional[Any] = None # 用于存储实际的 TaskOutput 或字符串

        if hasattr(manager_task_raw_result, 'tasks_output') and isinstance(manager_task_raw_result.tasks_output, list) and manager_task_raw_result.tasks_output:
            # CrewOutput.tasks_output 是一个 TaskOutput 对象的列表
            actual_task_output = manager_task_raw_result.tasks_output[0] # 我们只有一个 manager_task
            print(f"Extracted TaskOutput from CrewOutput: {type(actual_task_output)}")
        elif hasattr(manager_task_raw_result, 'raw_output'): # 兼容直接返回 TaskOutput 的情况 (旧版或特定配置)
            actual_task_output = manager_task_raw_result
            print(f"kickoff returned a TaskOutput-like object directly: {type(actual_task_output)}")
        elif isinstance(manager_task_raw_result, str): # 直接返回字符串
            actual_task_output = manager_task_raw_result
            print(f"kickoff returned a raw string.")
        else:
            print(f"Warning: manager_crew.kickoff() returned an unexpected type: {type(manager_task_raw_result)}")
            actual_task_output = str(manager_task_raw_result)
        # --- 结束新的解析逻辑 ---


        # --- 后续的解析逻辑，现在基于 actual_task_output ---
        print(f"DEBUG: Type of actual_task_output: {type(actual_task_output)}")
        
        manager_raw_llm_output_str: Optional[str] = None
        if hasattr(actual_task_output, 'raw') and isinstance(actual_task_output.raw, str):
            manager_raw_llm_output_str = actual_task_output.raw.strip()
            print(f"DEBUG: actual_task_output.raw (LLM's original string output) IS:\n---\n{manager_raw_llm_output_str}\n---")
        else:
            print(f"DEBUG: actual_task_output.raw is not a string or does not exist. Value: {getattr(actual_task_output, 'raw', 'Attribute .raw not found')}")
            manager_raw_llm_output_str = str(actual_task_output) # Fallback

        # 优先尝试使用 CrewAI 已经解析好的 Pydantic 对象
        if hasattr(actual_task_output, 'pydantic_output') and \
           actual_task_output.pydantic_output is not None and \
           isinstance(actual_task_output.pydantic_output, SubTaskDefinitionForManagerOutput):
            print("DEBUG: Successfully using actual_task_output.pydantic_output.")
            manager_plan_object = actual_task_output.pydantic_output
        
        # 如果 Pydantic 对象不可用，但我们从 .raw 成功获取了字符串，则尝试解析它
        # 并且确保 manager_plan_object 之前没有被成功赋值
        elif manager_plan_object is None and manager_raw_llm_output_str: # <--- 添加 manager_plan_object is None 条件
            print(f"DEBUG: pydantic_output not available. Attempting to parse string from actual_task_output.raw:\n---\n{manager_raw_llm_output_str}\n---")
            json_to_parse_from_raw = manager_raw_llm_output_str # 使用我们从 .raw 获取的
            try:
                final_answer_marker = "## Final Answer:" # 虽然日志显示Qwen没输出这个，但保留以防万一
                if final_answer_marker in json_to_parse_from_raw:
                    json_to_parse_from_raw = json_to_parse_from_raw.split(final_answer_marker, 1)[-1].strip()
                
                cleaned_json_str = json_to_parse_from_raw.strip()
                if cleaned_json_str.startswith("```json"): cleaned_json_str = cleaned_json_str[len("```json"):].strip()
                if cleaned_json_str.endswith("```"): cleaned_json_str = cleaned_json_str[:-len("```")].strip()
                
                if not cleaned_json_str: raise ValueError("Cleaned JSON string (from .raw) is empty.")
                manager_plan_object = SubTaskDefinitionForManagerOutput(**json.loads(cleaned_json_str))
                print(f"Parsed Manager Plan (from actual_task_output.raw): {manager_plan_object.model_dump_json(indent=2)}")
            except (json.JSONDecodeError, ValueError, Exception) as e:
                print(f"Error parsing JSON from actual_task_output.raw: {e}. Raw content used: {json_to_parse_from_raw}")
        
        # 在所有尝试之后，如果 manager_plan_object 仍然是 None，才报告最终的解析失败
        if not manager_plan_object:
             # manager_raw_output 现在应该引用我们尝试过的 manager_raw_llm_output_str
             final_raw_output_for_debug = manager_raw_llm_output_str if manager_raw_llm_output_str else str(actual_task_output)
             print(f"Manager Task did not produce a valid Pydantic object after all attempts. Final Raw for debug: {final_raw_output_for_debug}")
             return AgentTaskResponse(answer="无法解析Manager Agent的规划结果。", status="error",
                                      error_message="Failed to parse manager plan after all attempts.",
                                      debug_info={"manager_raw_output_tried": final_raw_output_for_debug})
        
        # 如果 manager_plan_object 成功解析，则打印日志（这行已经在您的代码中）
        print(f"Parsed Manager Plan: {manager_plan_object.model_dump_json(indent=2)}")
        
    except Exception as e:
        print(f"Error executing Manager Task or its Crew: {e}") # <--- 修改后的第一行
        traceback.print_exc() # <--- 修改后的第二行
        return AgentTaskResponse(
            answer="执行Manager Agent任务时发生错误。", status="error", error_message=str(e),
            debug_info={"traceback": traceback.format_exc(), "manager_raw_output": manager_raw_output}
        )

    # --- 步骤 2: 根据 Manager 的规划执行后续操作 ---
    if not manager_plan_object.selected_tool_names and manager_plan_object.direct_answer_content:
        print(f"Manager decided to answer directly. Answer: {manager_plan_object.direct_answer_content}")
        return AgentTaskResponse(
            answer=manager_plan_object.direct_answer_content,
            status="success",
            debug_info={"manager_plan": manager_plan_object.model_dump()}
        )
    elif not manager_plan_object.selected_tool_names and not manager_plan_object.direct_answer_content:
        # 如果 Manager 既没有选择工具，也没有提供直接答案，这可能是一个规划错误
         print(f"Manager Warning: No tool selected and no direct answer provided. Reasoning: {manager_plan_object.reasoning_for_plan}")
         return AgentTaskResponse(
            answer=manager_plan_object.reasoning_for_plan or "Manager 未能提供明确的行动计划或答案。",
            status="success", # 或者 "error" 取决于您如何定义这种情况
            debug_info={"manager_plan": manager_plan_object.model_dump()}
        )

    # --- 步骤 3: 如果 Manager 规划使用工具，则创建并执行 Worker Task ---
    selected_tool_name = manager_plan_object.selected_tool_names[0] if manager_plan_object.selected_tool_names else None

    if not selected_tool_name:
        return AgentTaskResponse(
            answer="Manager规划使用工具但未在selected_tool_names中指定工具名称。", status="error",
            error_message="Tool name missing in manager plan's selected_tool_names.",
            debug_info={"manager_plan": manager_plan_object.model_dump()}
        )

    # 找到对应的工具实例
    tool_instance_for_worker = next((tool for tool in core_tools_instances if tool.name == selected_tool_name), None)

    if not tool_instance_for_worker:
        return AgentTaskResponse(
            answer=f"系统中未找到Manager规划使用的工具: {selected_tool_name}。", status="error",
            error_message=f"Tool '{selected_tool_name}' not found in core_tools_instances.",
            debug_info={"manager_plan": manager_plan_object.model_dump(), "available_tools": [t.name for t in core_tools_instances]}
        )

    print(f"Manager planned to use tool: {selected_tool_name}. Instance found: {tool_instance_for_worker is not None}")

    worker_task_description = ""
    worker_task_inputs = {} # 用于传递给 task.execute(inputs=...)

    if selected_tool_name == "enhanced_rag_tool":
        rag_query = manager_plan_object.tool_input_args.get("query", request.user_query) if manager_plan_object.tool_input_args else request.user_query
        top_k_v = manager_plan_object.tool_input_args.get("top_k_vector", 5) if manager_plan_object.tool_input_args else 5
        top_k_kg = manager_plan_object.tool_input_args.get("top_k_kg", 3) if manager_plan_object.tool_input_args else 3
        top_k_b = manager_plan_object.tool_input_args.get("top_k_bm25", 3) if manager_plan_object.tool_input_args else 3
        
        worker_task_description = f"请使用增强RAG工具回答以下问题：'{rag_query}'。\n使用以下参数进行检索：top_k_vector={top_k_v}, top_k_kg={top_k_kg}, top_k_bm25={top_k_b}。"
        worker_task_inputs = {
            "query": rag_query, 
            "top_k_vector": top_k_v, 
            "top_k_kg": top_k_kg, 
            "top_k_bm25": top_k_b
        }

    elif selected_tool_name == "excel_operation_tool":
        excel_sqo_list = manager_plan_object.excel_sqo_payload
        if not excel_sqo_list:
            return AgentTaskResponse(
                answer="Manager选择Excel工具但未提供SQO列表。", status="error",
                error_message="excel_sqo_payload is missing for excel_operation_tool.",
                debug_info={"manager_plan": manager_plan_object.model_dump()}
            )
        # --- 添加 elif 分支来处理 web_search_tool ---
    elif selected_tool_name == "web_search_tool": 
        search_query = manager_plan_object.tool_input_args.get("query", request.user_query) if manager_plan_object.tool_input_args else request.user_query
        max_results = manager_plan_object.tool_input_args.get("max_results", 5) if manager_plan_object.tool_input_args else 5
        worker_task_description = f"请使用网络搜索工具查找关于 '{search_query}' 的信息，返回最多 {max_results} 条结果。"
        worker_task_inputs = {"query": search_query, "max_results": max_results}
    # --- 添加结束 ---
    # --- 添加开始 ---
    elif selected_tool_name == "get_current_time_tool":
        timezone_str = manager_plan_object.tool_input_args.get("timezone", "Asia/Shanghai") if manager_plan_object.tool_input_args else "Asia/Shanghai"
        worker_task_description = f"请使用时间工具获取当前时间。时区参数为: '{timezone_str}'。"
        worker_task_inputs = {"timezone_str": timezone_str} # 注意参数名与工具类 _run 方法的参数名一致

    elif selected_tool_name == "calculate_tool":
        expression_str = manager_plan_object.tool_input_args.get("expression", "") if manager_plan_object.tool_input_args else ""
        if not expression_str:
            return AgentTaskResponse(
                answer="Manager选择计算器工具但未提供表达式。", status="error",
                error_message="expression is missing for calculate_tool.",
                debug_info={"manager_plan": manager_plan_object.model_dump()}
            )
        worker_task_description = f"请使用计算器工具计算以下表达式: '{expression_str}'。"
        worker_task_inputs = {"expression": expression_str} # 注意参数名与工具类 _run 方法的参数名一致
        # --- 添加结束 ---
        # TODO: 从用户原始请求 manager_plan_object.task_description 中提取 file_path 和 sheet_name

        excel_file_path = "/home/zhz/zhz_agent/data/test.xlsx" # 临时硬编码，后续需要动态获取
        excel_sheet_name: Union[str, int] = 0 # 临时硬编码

        worker_task_description = (
            f"你需要处理一个Excel文件相关的任务。文件路径是 '{excel_file_path}'，工作表是 '{excel_sheet_name}'。\n"
            f"请严格按照以下SQO操作列表，顺序执行每一个操作，并整合所有操作的结果形成最终答案。\n"
            f"SQO操作列表: {json.dumps(excel_sqo_list, ensure_ascii=False)}\n\n"
            f"对于列表中的每一个SQO字典，你需要调用 '{tool_instance_for_worker.name}' 工具一次，"
            f"将该SQO字典作为 'sqo_dict' 参数，同时传递 'file_path': '{excel_file_path}' 和 'sheet_name': '{excel_sheet_name}'。"
        )
        # Worker Task的inputs将是整个SQO列表以及文件和工作表信息
        # Worker Agent的Prompt需要指导它如何迭代处理这个列表
        worker_task_inputs = {
            "excel_sqo_list_to_execute": excel_sqo_list,
            "target_excel_file_path": excel_file_path,
            "target_excel_sheet_name": excel_sheet_name
        }
        
    elif selected_tool_name == "web_search_tool":
        search_query = manager_plan_object.tool_input_args.get("query", request.user_query) if manager_plan_object.tool_input_args else request.user_query
        max_results = manager_plan_object.tool_input_args.get("max_results", 5) if manager_plan_object.tool_input_args else 5

        worker_task_description = f"请使用网络搜索工具查找关于 '{search_query}' 的信息，返回最多 {max_results} 条结果。"
        worker_task_inputs = {"query": search_query, "max_results": max_results}
        
    else:
        return AgentTaskResponse(
            answer=f"未知的工具名称 '{selected_tool_name}' 被Manager规划。", status="error",
            error_message=f"Unknown tool '{selected_tool_name}' planned by manager.",
            debug_info={"manager_plan": manager_plan_object.model_dump()}
        )

    # 创建 Worker Task
    worker_task = Task(
        description=worker_task_description,
        expected_output="任务的执行结果，通常是一个字符串，其中包含答案或操作的状态。",
        agent=worker_agent_instance,
        tools=[tool_instance_for_worker], # 只给 Worker 当前任务需要的工具
        async_execution=False # Worker Task 通常也是同步的
    )

    print(f"Executing Worker Task with tool: {selected_tool_name}")
    print(f"Worker Task Description: {worker_task_description}")
    print(f"Worker Task Inputs: {worker_task_inputs}")
    
    worker_final_result: str = ""
    worker_crew_usage_metrics: Optional[Dict[str, Any]] = None # 用于存储 token usage
    token_usage_for_response: Optional[Dict[str, Any]] = None # <--- 在这里初始化
    try:
        # --- [关键修改] 为 Worker Task 创建并运行一个 Crew ---
        worker_crew = Crew(
            agents=[worker_agent_instance], # Worker Agent
            tasks=[worker_task],             # 它要执行的单个任务
            process=Process.sequential,
            verbose=True # 可以设为 True 或 2 来查看 Worker Crew 的详细日志
        )
        
        # 如果 worker_task_inputs 为空，则 inputs={}
        # CrewAI 的 kickoff 方法期望 inputs 是一个字典
        task_execution_inputs = worker_task_inputs if worker_task_inputs else {}
            
        # 执行 Worker Crew
        # kickoff 返回的是 CrewOutput 对象，或者在某些旧版本/配置下可能直接是结果字符串或 TaskOutput
        worker_crew_output = worker_crew.kickoff(inputs=task_execution_inputs)
        
        # 从 Worker Crew 的输出中提取结果
        # 这与我们处理 Manager Crew 输出的逻辑类似
        actual_worker_task_output: Optional[Any] = None
        if hasattr(worker_crew_output, 'tasks_output') and isinstance(worker_crew_output.tasks_output, list) and worker_crew_output.tasks_output:
            actual_worker_task_output = worker_crew_output.tasks_output[0] # 我们只有一个 worker_task
        elif hasattr(worker_crew_output, 'raw_output'): # 兼容直接返回 TaskOutput
            actual_worker_task_output = worker_crew_output
        elif isinstance(worker_crew_output, str): # 直接返回字符串
            actual_worker_task_output = worker_crew_output
        else: # 其他意外情况
            actual_worker_task_output = str(worker_crew_output)

        # 从 actual_worker_task_output 中提取最终的字符串结果
        if hasattr(actual_worker_task_output, 'raw') and isinstance(actual_worker_task_output.raw, str):
            worker_final_result = actual_worker_task_output.raw.strip()
        elif isinstance(actual_worker_task_output, str):
            worker_final_result = actual_worker_task_output.strip()
        else: # Fallback
            worker_final_result = str(actual_worker_task_output)
        
        print(f"Worker Task executed. Result: {worker_final_result}")
        
        # 获取 Worker Crew 的 token usage
        if hasattr(worker_crew, 'usage_metrics'):
            worker_crew_usage_metrics = worker_crew.usage_metrics
            print(f"DEBUG AGENT_ORCH: Raw worker_crew.usage_metrics object: {worker_crew_usage_metrics}")
            print(f"DEBUG AGENT_ORCH: Type of worker_crew.usage_metrics: {type(worker_crew_usage_metrics)}")
            if hasattr(worker_crew_usage_metrics, 'model_dump'):
                print(f"DEBUG AGENT_ORCH: worker_crew.usage_metrics.model_dump(): {worker_crew_usage_metrics.model_dump()}")
            elif isinstance(worker_crew_usage_metrics, dict):
                    print(f"DEBUG AGENT_ORCH: worker_crew.usage_metrics (is dict): {worker_crew_usage_metrics}")
            else:
                print(f"DEBUG AGENT_ORCH: worker_crew.usage_metrics is not a dict and has no model_dump.")
        else:
            print("DEBUG AGENT_ORCH: worker_crew does not have usage_metrics attribute.")
            worker_crew_usage_metrics = None # 确保它被定义
            # --- [结束关键修改] ---

            # 准备 token_usage_for_response
            token_usage_for_response = None
            if worker_crew_usage_metrics:
                if hasattr(worker_crew_usage_metrics, 'model_dump'):
                    token_usage_for_response = worker_crew_usage_metrics.model_dump()
                elif isinstance(worker_crew_usage_metrics, dict):
                    token_usage_for_response = worker_crew_usage_metrics
                else:
                    # 如果不是 Pydantic 模型或字典，尝试转换为字符串记录，但不作为结构化数据返回
                    service_logger.warning(f"Unexpected type for worker_crew_usage_metrics: {type(worker_crew_usage_metrics)}. Will not be included in structured token_usage.")

        return AgentTaskResponse(
                answer=worker_final_result,
                status="success",
                debug_info={
                    "manager_plan": manager_plan_object.model_dump(),
                    "worker_tool_used": selected_tool_name,
                    "worker_task_inputs": worker_task_inputs 
                },
                token_usage=token_usage_for_response # 使用处理后的 token_usage_for_response
            )

    except Exception as e:
    # 使用 traceback 打印详细错误
        print(f"Error executing Worker Task for tool {selected_tool_name}: {e}")
        traceback.print_exc() 
        return AgentTaskResponse(
            answer=f"执行工具 '{selected_tool_name}' 时发生错误。",
            status="error",
            error_message=str(e),
            debug_info={
                "manager_plan": manager_plan_object.model_dump(),
                "worker_tool_used": selected_tool_name,
                "worker_task_inputs": worker_task_inputs,
                "traceback": traceback.format_exc()
            }
        )

if __name__ == "__main__":
    print(f"--- Starting Agent Orchestrator FastAPI Service on {AGENT_SERVICE_HOST}:{AGENT_SERVICE_PORT} ---")
    uvicorn.run("agent_orchestrator_service:app", host=AGENT_SERVICE_HOST, port=AGENT_SERVICE_PORT, reload=True) # 确保模块名正确
    --- END OF FILE CONTENT ---

  File: config.yaml
    --- START OF FILE CONTENT ---
# config.yaml

model_list:
  - model_name: gemini/gemini-1.5-flash-latest # 您已有的 Gemini 配置 (如果存在)
    litellm_params:
      model: gemini/gemini-1.5-flash-latest
      api_key: os.environ/GEMINI_API_KEY
      # api_base: http://localhost:YOUR_CLASH_PORT # 如果需要

  # --- 修改我们本地的 Qwen3 1.7B 模型配置 ---
  - model_name: local/qwen3-1.7b-gguf
    litellm_params:
      model: openai/my-local-qwen # 使用 "openai/" 前缀，后面可以跟一个自定义的描述符
      api_base: http://localhost:8088/v1
      api_key: nokey

# ... (其他 LiteLLM 配置) ...
    --- END OF FILE CONTENT ---

  File: debug_kuzu_visibility.py
    --- START OF FILE CONTENT ---
# debug_kuzu_visibility.py
import kuzu
import os
import shutil
import time
import pandas as pd

DB_PATH = "./zhz_rag/stored_data/kuzu_test_visibility_db" # 使用一个新的测试路径

def setup_and_verify_schema():
    print(f"--- Phase 1: Setup and Initial Verification ---")
    if os.path.exists(DB_PATH):
        print(f"Removing existing database at {DB_PATH}")
        shutil.rmtree(DB_PATH)
    
    db_setup = None
    conn_setup = None
    try:
        print(f"Creating new database at {DB_PATH}")
        db_setup = kuzu.Database(DB_PATH, read_only=False)
        conn_setup = kuzu.Connection(db_setup)
        print("Database and connection created for setup.")

        ddl_queries = [
            "CREATE NODE TABLE IF NOT EXISTS ExtractedEntity (id_prop STRING, text STRING, label STRING, PRIMARY KEY (id_prop))",
            "CREATE REL TABLE IF NOT EXISTS WorksAt (FROM ExtractedEntity TO ExtractedEntity)",
            "CREATE REL TABLE IF NOT EXISTS AssignedTo (FROM ExtractedEntity TO ExtractedEntity)"
        ]
        for query in ddl_queries:
            print(f"Executing DDL: {query}")
            conn_setup.execute(query)
        print("DDL execution complete.")

        print("Executing CHECKPOINT...")
        conn_setup.execute("CHECKPOINT;")
        print("CHECKPOINT complete.")

        # Phase 1 Verification
        print("Verifying tables immediately after creation (same session)...")
        try:
            # 尝试 SHOW TABLES;
            print("Attempting 'SHOW TABLES;' in Phase 1...")
            result = conn_setup.execute("SHOW TABLES;") # <--- 修改这里
            df = pd.DataFrame(result.get_as_df())
            print(f"Tables found using 'SHOW TABLES;' (Phase 1):\n{df}")
            # 检查 'name' 列是否存在，并且 'ExtractedEntity' 是否在其中
            if not df.empty and 'name' in df.columns and "ExtractedEntity" in df["name"].tolist():
                print("Phase 1 Verification (SHOW TABLES;): ExtractedEntity table FOUND.")
            elif not df.empty and 'name' not in df.columns:
                print("Phase 1 Verification (SHOW TABLES;): 'name' column not found in SHOW TABLES result.")
                print(f"Columns available in SHOW TABLES result: {df.columns.tolist()}")
            else: # df is empty or ExtractedEntity not in name column
                print("Phase 1 Verification (SHOW TABLES;): ExtractedEntity table NOT FOUND or result was empty.")
        except Exception as e:
            print(f"Error during Phase 1 verification with 'SHOW TABLES;': {e}")
            print("Falling back to direct query for ExtractedEntity in Phase 1...")
            try:
                result_direct = conn_setup.execute("MATCH (e:ExtractedEntity) RETURN count(e) AS entity_count;")
                df_direct = pd.DataFrame(result_direct.get_as_df())
                print(f"Direct query result (Phase 1):\n{df_direct}")
                count_val = df_direct['entity_count'].iloc[0] if not df_direct.empty else -1
                print(f"Phase 1 Verification (Direct Query): ExtractedEntity table SEEMS TO EXIST (count: {count_val}).")
            except Exception as e_direct_phase1:
                print(f"Error during Phase 1 direct query verification: {e_direct_phase1}")

    except Exception as e:
        print(f"Error during Phase 1 setup: {e}")
    finally:
        if conn_setup:
            print("Closing setup connection.")
            # conn_setup.close() # Kuzu Connection 没有显式 close
        if db_setup:
            print("Deleting setup database instance reference (will trigger close).")
            del db_setup # 依赖 __del__
        print("--- Phase 1 Complete ---")

def verify_in_new_session():
    print(f"\n--- Phase 2: Verification in a New Session ---")
    if not os.path.exists(DB_PATH):
        print(f"Database at {DB_PATH} does not exist. Cannot perform Phase 2.")
        return

    db_verify = None
    conn_verify = None
    try:
        print(f"Opening existing database at {DB_PATH} for verification.")
        db_verify = kuzu.Database(DB_PATH, read_only=False) # 打开同一个数据库
        conn_verify = kuzu.Connection(db_verify)
        print("Database and connection created for verification.")

        # Phase 2 Verification
        print("Verifying tables in new session...")
        try:
            # 尝试 SHOW TABLES;
            print("Attempting 'SHOW TABLES;' in Phase 2...")
            result = conn_verify.execute("SHOW TABLES;") # <--- 修改这里
            df = pd.DataFrame(result.get_as_df())
            print(f"Tables found using 'SHOW TABLES;' (Phase 2):\n{df}")
            if not df.empty and 'name' in df.columns and "ExtractedEntity" in df["name"].tolist():
                print("Phase 2 Verification (SHOW TABLES;): ExtractedEntity table FOUND.")
            elif not df.empty and 'name' not in df.columns:
                print("Phase 2 Verification (SHOW TABLES;): 'name' column not found in SHOW TABLES result.")
                print(f"Columns available in SHOW TABLES result: {df.columns.tolist()}")
            else: # df is empty or ExtractedEntity not in name column
                print("Phase 2 Verification (SHOW TABLES;): ExtractedEntity table NOT FOUND or result was empty.")
        except Exception as e:
            print(f"Error during Phase 2 verification with 'SHOW TABLES;': {e}")
            print("Falling back to direct query for ExtractedEntity in Phase 2...")
            try:
                result_direct = conn_verify.execute("MATCH (e:ExtractedEntity) RETURN count(e) AS entity_count;")
                df_direct = pd.DataFrame(result_direct.get_as_df())
                print(f"Direct query result (Phase 2):\n{df_direct}")
                count_val = df_direct['entity_count'].iloc[0] if not df_direct.empty else -1
                print(f"Phase 2 Verification (Direct Query): ExtractedEntity table SEEMS TO EXIST (count: {count_val}).")
            except Exception as e_direct_phase2:
                print(f"Error during Phase 2 direct query verification: {e_direct_phase2}")

    except Exception as e:
        print(f"Error during Phase 2 setup: {e}")
    finally:
        if conn_verify:
            print("Closing verification connection.")
        if db_verify:
            print("Deleting verification database instance reference.")
            del db_verify
        print("--- Phase 2 Complete ---")

if __name__ == "__main__":
    print(f"Kuzu Python client version: {kuzu.__version__}")
    setup_and_verify_schema()
    print("\nWaiting a moment before trying to open in a new session (simulating process switch)...")
    time.sleep(2) # 短暂等待，模拟进程切换的间隙
    verify_in_new_session()
    --- END OF FILE CONTENT ---

  File: llm_setup.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/llm_setup.py

import os
from typing import Any, Optional, List, Dict

# LiteLLM 本身通常是直接调用其 API，例如 litellm.completion
# 如果要将其适配给 CrewAI 的 Agent，我们需要将其包装成一个符合 CrewAI BaseLLM 接口的类。
# CrewAI 自身也提供了通过 LiteLLM 使用各种模型的集成，我们也可以研究直接使用 CrewAI 的方式。
# 为简化起见，我们先创建一个函数，返回配置好的 LiteLLM 调用参数，或者一个简单的包装器。

# --- 从 agent_orchestrator_service.py 中获取的配置常量 ---
# 本地 LLM 服务地址 (Qwen3)
LOCAL_QWEN_API_BASE = os.getenv("LOCAL_LLM_API_BASE", "http://localhost:8088/v1") # 确保与 local_llm_service.py 的端口一致
LOCAL_QWEN_MODEL_NAME_FOR_LITELLM = os.getenv("LOCAL_LLM_MODEL_NAME", "local/qwen3-1.7b-gguf") # 这个是给 litellm 的 model 参数，需要与 LiteLLM 配置或调用方式对应

# 云端 LiteLLM 网关地址 (用于 Gemini 等)
CLOUD_LITELLM_GW_API_BASE = os.getenv("CLOUD_LITELLM_GW_API_BASE", "YOUR_CLOUD_LITELLM_GATEWAY_URL_HERE/v1") # 假设您的网关也提供 /v1 路径
GEMINI_MODEL_NAME_FOR_LITELLM = os.getenv("CLOUD_LLM_MODEL_NAME", "gemini/gemini-1.5-flash-latest") # 或您希望通过网关调用的模型

# API Keys (通常 LiteLLM 会在其自己的配置或环境变量中处理，但这里可以作为参考)
# GEMINI_API_KEY = os.getenv("GEMINI_API_KEY") # 如果直接调用 Gemini 而非通过网关
# 注意：本地模型的 API Key 通常是 "nokey" 或在服务中不强制

# --- CrewAI LLM 包装器 (如果需要直接传递给 CrewAI Agent) ---
# CrewAI 期望一个 LLM 对象，我们可以创建一个简单的包装器，或者使用 CrewAI 提供的 LiteLLM 集成
# from crewai.llms.base_llm import BaseLLM as CrewAIBaseLLM
# import litellm
#
# class LiteLLMWrapperForCrewAI(CrewAIBaseLLM):
#     model: str
#     api_base: Optional[str] = None
#     api_key: Optional[str] = None
#     custom_llm_provider: Optional[str] = None # 例如 "openai" for OpenAI-compatible endpoints
#     litellm_kwargs: Dict[str, Any] = {}
#
#     def __init__(self, model: str, api_base: Optional[str] = None, api_key: Optional[str] = None, custom_llm_provider: Optional[str] = None, **kwargs):
#         super().__init__(model=model) # CrewAI BaseLLM 需要 model 参数
#         self.model = model
#         self.api_base = api_base
#         self.api_key = api_key
#         self.custom_llm_provider = custom_llm_provider
#         self.litellm_kwargs = kwargs
#
#     def call(self, messages: List[Dict[str, str]], **kwargs: Any) -> str:
#         # CrewAI 的 BaseLLM.call 方法通常期望返回一个字符串
#         # LiteLLM 的 completion 返回一个 ModelResponse 对象
#         params = {
#             "model": self.model,
#             "messages": messages,
#             "api_base": self.api_base,
#             "api_key": self.api_key,
#             "custom_llm_provider": self.custom_llm_provider,
#             **self.litellm_kwargs, # 包含 temperature, max_tokens 等
#             **kwargs # 运行时可能传递的额外参数
#         }
#         # 移除值为 None 的参数，因为 litellm.completion 不喜欢 None 的 api_key 等
#         params_cleaned = {k: v for k, v in params.items() if v is not None}
#
#         response = litellm.completion(**params_cleaned)
#
#         # 从 LiteLLM 响应中提取内容
#         content = ""
#         if response.choices and response.choices[0].message and response.choices[0].message.content:
#             content = response.choices[0].message.content
#         return content
#
#     def get_token_ids(self, text: str) -> List[int]:
#         # LiteLLM 通常不直接暴露 tokenizer，但可以尝试通过 litellm.token_counter (如果它返回ids)
#         # 或者如果使用特定模型，可以尝试加载其 tokenizer
#         # 为简单起见，我们先返回空列表或引发 NotImplementedError
#         # return litellm.encode(model=self.model, text=text) # 如果 litellm.encode 可用
#         raise NotImplementedError("get_token_ids is not implemented for this LiteLLM wrapper.")


def get_local_qwen_llm_instance(
    temperature: float = 0.7,
    max_tokens: int = 2048,
    # crewai_wrapper: bool = False # 是否返回 CrewAI 兼容的包装器
) -> Any: # 返回 Any 以便后续决定是直接返回配置字典还是包装器实例
    """
    获取配置好的本地 Qwen3 LLM 实例（通过 LiteLLM 调用）。
    """
    print(f"LLM Setup: Configuring Local Qwen LLM via LiteLLM.")
    print(f"  Model: {LOCAL_QWEN_MODEL_NAME_FOR_LITELLM}")
    print(f"  API Base: {LOCAL_QWEN_API_BASE}")
    # if crewai_wrapper:
    #     return LiteLLMWrapperForCrewAI(
    #         model=LOCAL_QWEN_MODEL_NAME_FOR_LITELLM,
    #         api_base=LOCAL_QWEN_API_BASE,
    #         api_key="nokey", # 本地服务通常不需要 key
    #         custom_llm_provider="openai", # 因为我们的本地服务是 OpenAI 兼容的
    #         temperature=temperature,
    #         max_tokens=max_tokens
    #     )
    # else:
    # 返回一个配置字典，调用方可以使用 litellm.completion(**config, messages=...)
    return {
        "model": LOCAL_QWEN_MODEL_NAME_FOR_LITELLM,
        "api_base": LOCAL_QWEN_API_BASE,
        "api_key": "nokey",
        "custom_llm_provider": "openai",
        "temperature": temperature,
        "max_tokens": max_tokens
    }


def get_cloud_gemini_llm_instance(
    temperature: float = 0.5,
    max_tokens: int = 4096,
    # crewai_wrapper: bool = False
) -> Any:
    """
    获取配置好的云端 Gemini LLM 实例（通过云端 LiteLLM 网关调用）。
    """
    print(f"LLM Setup: Configuring Cloud Gemini LLM via LiteLLM Gateway.")
    print(f"  Model: {GEMINI_MODEL_NAME_FOR_LITELLM}")
    print(f"  API Base (Gateway): {CLOUD_LITELLM_GW_API_BASE}")

    if CLOUD_LITELLM_GW_API_BASE == "YOUR_CLOUD_LITELLM_GATEWAY_URL_HERE/v1":
        print("LLM Setup WARNING: CLOUD_LITELLM_GW_API_BASE is not configured. Cloud LLM calls will likely fail.")
        # 可以选择返回 None 或者一个无效的配置，让调用方处理
        return None

    # if crewai_wrapper:
    #     return LiteLLMWrapperForCrewAI(
    #         model=GEMINI_MODEL_NAME_FOR_LITELLM,
    #         api_base=CLOUD_LITELLM_GW_API_BASE,
    #         # API Key 通常由云端 LiteLLM 网关管理，客户端调用网关时可能不需要直接提供
    #         # 或者网关本身可能需要某种形式的认证 key
    #         api_key=os.getenv("CLOUD_LITELLM_GATEWAY_API_KEY"), # 假设网关可能需要一个 key
    #         # custom_llm_provider 可能不需要，因为网关会处理到具体云服务的转换
    #         temperature=temperature,
    #         max_tokens=max_tokens
    #     )
    # else:
    return {
        "model": GEMINI_MODEL_NAME_FOR_LITELLM,
        "api_base": CLOUD_LITELLM_GW_API_BASE,
        "api_key": os.getenv("CLOUD_LITELLM_GATEWAY_API_KEY"), # 网关本身的key
        "temperature": temperature,
        "max_tokens": max_tokens
    }

# --- 示例用法 (可选，用于测试此文件) ---
async def main_test_llm_setup():
    print("--- Testing LLM Setup ---")

    print("\n--- Getting Local Qwen Config ---")
    local_qwen_config = get_local_qwen_llm_instance()
    if local_qwen_config:
        print(f"Local Qwen Config: {local_qwen_config}")
        # 模拟调用
        try:
            messages = [{"role": "user", "content": "你好！"}]
            print(f"Simulating LiteLLM call with local Qwen config for: {messages}")
            # response = await litellm.acompletion(**local_qwen_config, messages=messages)
            # print(f"Simulated local Qwen response (first choice): {response.choices[0].message.content if response.choices else 'No response'}")
            print("Actual LiteLLM call commented out for setup test.")
        except Exception as e:
            print(f"Error simulating local Qwen call: {e}")
    else:
        print("Failed to get local Qwen config.")

    print("\n--- Getting Cloud Gemini Config ---")
    cloud_gemini_config = get_cloud_gemini_llm_instance()
    if cloud_gemini_config:
        print(f"Cloud Gemini Config: {cloud_gemini_config}")
        if cloud_gemini_config.get("api_base") == "YOUR_CLOUD_LITELLM_GATEWAY_URL_HERE/v1":
            print("Skipping simulated cloud Gemini call as gateway URL is a placeholder.")
        else:
            try:
                messages = [{"role": "user", "content": "Hello!"}]
                print(f"Simulating LiteLLM call with cloud Gemini config for: {messages}")
                # response = await litellm.acompletion(**cloud_gemini_config, messages=messages)
                # print(f"Simulated cloud Gemini response (first choice): {response.choices[0].message.content if response.choices else 'No response'}")
                print("Actual LiteLLM call commented out for setup test.")
            except Exception as e:
                print(f"Error simulating cloud Gemini call: {e}")
    else:
        print("Failed to get cloud Gemini config (likely due to placeholder URL).")

if __name__ == "__main__":
    # 为了运行异步的 main_test_llm_setup
    # import asyncio
    # asyncio.run(main_test_llm_setup())
    pass
    --- END OF FILE CONTENT ---

  File: local_llm_service.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/local_llm_service.py
import os
import time
import uuid
import traceback
from contextlib import asynccontextmanager
from typing import List, Dict, Any, Optional, Union
import json

import uvicorn
from fastapi import FastAPI, HTTPException, Request as FastAPIRequest
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from llama_cpp import Llama, LlamaGrammar
import re

# --- 配置 ---
MODEL_DIR = os.getenv("LOCAL_LLM_MODEL_DIR", "/home/zhz/models/Qwen3-1.7B-GGUF")
MODEL_FILENAME = os.getenv("LOCAL_LLM_MODEL_FILENAME")
N_GPU_LAYERS = int(os.getenv("LOCAL_LLM_N_GPU_LAYERS", 0))
N_CTX = int(os.getenv("LOCAL_LLM_N_CTX", 4096))
N_BATCH = int(os.getenv("LOCAL_LLM_N_BATCH", 512))
SERVICE_PORT = int(os.getenv("LOCAL_LLM_SERVICE_PORT", 8088))
SERVICE_HOST = "0.0.0.0"

GBNF_FILE_PATH = os.path.join(os.path.dirname(__file__), "core", "grammars", "cypher_or_unable_output.gbnf")

# --- 全局变量 ---
llama_model: Optional[Llama] = None
model_path_global: Optional[str] = None
# logit_bias 相关的全局变量 (在 lifespan 中初始化)
failure_phrase_token_ids: List[int] = []
logit_bias_for_failure_phrase: Optional[Dict[int, float]] = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    global llama_model, model_path_global, failure_phrase_token_ids, logit_bias_for_failure_phrase
    print("--- Local LLM Service: Lifespan startup (with GBNF and logit_bias prep) ---")
    
    model_file_to_load = MODEL_FILENAME
    if not model_file_to_load:
        print(f"MODEL_FILENAME environment variable not set. Attempting to auto-detect GGUF file in {MODEL_DIR}...")
        try:
            gguf_files = [f for f in os.listdir(MODEL_DIR) if f.endswith(".gguf")]
            if not gguf_files:
                error_msg = f"No GGUF models found in directory: {MODEL_DIR}"
                print(f"ERROR: {error_msg}")
                app.state.cypher_path_grammar = None # Ensure state variable exists even on error
                raise RuntimeError(error_msg)
            if len(gguf_files) > 1:
                print(f"Warning: Multiple GGUF models found in {MODEL_DIR}. Using the first one: {gguf_files[0]}")
            model_file_to_load = gguf_files[0]
            print(f"Auto-detected GGUF file: {model_file_to_load}")
        except FileNotFoundError:
            error_msg = f"Model directory not found: {MODEL_DIR}"
            print(f"ERROR: {error_msg}")
            app.state.cypher_path_grammar = None
            raise RuntimeError(error_msg)
        except Exception as e_find_model:
            error_msg = f"Error auto-detecting GGUF file: {e_find_model}"
            print(f"ERROR: {error_msg}")
            app.state.cypher_path_grammar = None
            raise RuntimeError(error_msg)

    model_path_global = os.path.join(MODEL_DIR, model_file_to_load)
    print(f"Attempting to load Llama model from: {model_path_global}")
    print(f"Parameters: n_gpu_layers={N_GPU_LAYERS}, n_ctx={N_CTX}, n_batch={N_BATCH}")

    try:
        llama_model = Llama(
            model_path=model_path_global,
            n_gpu_layers=N_GPU_LAYERS,
            n_ctx=N_CTX,
            n_batch=N_BATCH,
            verbose=True
        )
        print("Llama model loaded successfully.")

        # --- 初始化 logit_bias 相关 ---
        failure_phrase_str = "无法生成Cypher查询."
        if llama_model:
            try:
                # add_bos=False, special=False (通常用于非起始的、纯文本的词元化)
                failure_phrase_token_ids = llama_model.tokenize(failure_phrase_str.encode("utf-8"), add_bos=False, special=False)
                # 为这些 token ID 设置正向偏置，例如 10.0 (可以调整)
                # 避免偏置 EOS token (如果它意外地出现在短语的词元化结果中)
                eos_token_id = llama_model.token_eos()
                logit_bias_for_failure_phrase = {
                    token_id: 10.0 for token_id in failure_phrase_token_ids if token_id != eos_token_id
                }
                print(f"Successfully tokenized failure phrase '{failure_phrase_str}' to IDs: {failure_phrase_token_ids}")
                print(f"Logit bias for failure phrase: {logit_bias_for_failure_phrase}")
            except Exception as e_tokenize:
                print(f"ERROR: Failed to tokenize failure phrase for logit_bias: {e_tokenize}")
                failure_phrase_token_ids = []
                logit_bias_for_failure_phrase = None
        # --- 结束 logit_bias 初始化 ---

        gbnf_grammar_instance: Optional[LlamaGrammar] = None
        print(f"Attempting to load GBNF grammar from: {GBNF_FILE_PATH}")
        if os.path.exists(GBNF_FILE_PATH):
            try:
                gbnf_grammar_instance = LlamaGrammar.from_file(GBNF_FILE_PATH)
                print("GBNF grammar (success/failure paths) loaded successfully into lifespan.")
            except Exception as e_gbnf:
                print(f"ERROR: Failed to load or parse GBNF grammar file '{GBNF_FILE_PATH}': {e_gbnf}")
                traceback.print_exc()
        else:
            print(f"ERROR: GBNF grammar file not found at '{GBNF_FILE_PATH}'.")
        
        app.state.cypher_path_grammar = gbnf_grammar_instance

    except Exception as e:
        print(f"FATAL: Failed to load Llama model or prepare GBNF/logit_bias: {e}")
        app.state.cypher_path_grammar = None
        logit_bias_for_failure_phrase = None # Ensure this is also cleared
    
    yield
    print("--- Local LLM Service: Lifespan shutdown ---")

app = FastAPI(
    title="Local LLM Service (OpenAI Compatible)",
    description="Uses GBNF with logit_bias for conditional JSON output.",
    version="0.1.7", 
    lifespan=lifespan
)

class ChatMessage(BaseModel):
    role: str
    content: str

class ChatCompletionRequest(BaseModel):
    model: str
    messages: List[ChatMessage]
    temperature: float = 0.7
    max_tokens: Optional[int] = 1024 # Default to 1024 or higher
    stream: bool = False
    stop: Optional[Union[str, List[str]]] = None

class ChatCompletionMessage(BaseModel):
    role: str
    content: Optional[str] = None

class ChatCompletionChoice(BaseModel):
    index: int
    message: ChatCompletionMessage
    finish_reason: Optional[str] = "stop"

class UsageInfo(BaseModel):
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int

class ChatCompletionResponse(BaseModel):
    id: str = Field(default_factory=lambda: f"chatcmpl-{uuid.uuid4().hex}")
    object: str = "chat.completion"
    created: int = Field(default_factory=lambda: int(time.time()))
    model: str
    choices: List[ChatCompletionChoice]
    usage: UsageInfo
    system_fingerprint: Optional[str] = None

def post_process_llm_output(content: Optional[str], finish_reason: Optional[str]) -> Optional[str]:
    if content is None:
        return None
    processed_content = content
    # Remove <think>...</think> blocks
    think_block_pattern = re.compile(r"<think[^>]*>.*?</think>\s*", flags=re.DOTALL | re.IGNORECASE)
    processed_content = think_block_pattern.sub("", processed_content)

    # Handle potentially incomplete <think> tags if generation was cut short
    if finish_reason == "length" and \
       re.match(r"<think[^>]*>", processed_content.strip(), flags=re.IGNORECASE) and \
       not re.search(r"</think\s*>", processed_content, flags=re.IGNORECASE):
        print("DEBUG_POST_PROCESS: Incomplete think block due to length, attempting to remove partial tag.")
        # More aggressive removal of any leading <think...> tag if it's incomplete
        processed_content = re.sub(r"^<think[^>]*>","", processed_content.strip(), flags=re.IGNORECASE).strip()

    # Remove any remaining stray <think> or </think> tags
    stray_think_tag_pattern = re.compile(r"</?\s*think[^>]*?>\s*", flags=re.IGNORECASE)
    processed_content = stray_think_tag_pattern.sub("", processed_content)
    return processed_content.strip()

@app.post("/v1/chat/completions", response_model=ChatCompletionResponse)
async def create_chat_completion_endpoint(fastapi_req: FastAPIRequest, request: ChatCompletionRequest):
    global llama_model, model_path_global, logit_bias_for_failure_phrase
    
    loaded_cypher_path_grammar: Optional[LlamaGrammar] = getattr(fastapi_req.app.state, 'cypher_path_grammar', None)
    
    if llama_model is None:
        raise HTTPException(status_code=503, detail="Llama model is not loaded or failed to load.")
    if request.stream:
        raise HTTPException(status_code=400, detail="Streaming responses are not yet supported by this service.")

    response_content_raw: Optional[str] = None
    prompt_tokens = 0
    completion_tokens = 0
    total_tokens = 0
    finish_reason = "stop"
    request_model_name = request.model 
    final_json_output_str_for_client: Optional[str] = None

    if not hasattr(llama_model, "create_chat_completion"):
        print("CRITICAL_ERROR: llama_model.create_chat_completion method not found.")
        # Construct a proper JSON string for the error case directly
        error_response_obj = {"status": "unable_to_generate", "query": "LLM service misconfiguration."}
        final_json_output_str_for_client = json.dumps(error_response_obj, ensure_ascii=False)
        # Early return with the error structure if model is misconfigured
        return ChatCompletionResponse(
            id=f"chatcmpl-{uuid.uuid4().hex}",
            object="chat.completion",
            created=int(time.time()),
            model=request_model_name or (os.path.basename(model_path_global) if model_path_global else "local-llm-misconfigured"),
            choices=[ChatCompletionChoice(index=0, message=ChatCompletionMessage(role="assistant", content=final_json_output_str_for_client), finish_reason="error")],
            usage=UsageInfo(prompt_tokens=0, completion_tokens=0, total_tokens=0)
        )

    try:
        dict_messages = [msg.model_dump() for msg in request.messages]
        completion_params: Dict[str, Any] = {
            "messages": dict_messages,
            "temperature": request.temperature, 
            "max_tokens": request.max_tokens or 1024,
            "stop": request.stop,
        }

        is_cypher_gen_task = False
        if dict_messages and dict_messages[0]["role"] == "system":
            system_content_for_check = dict_messages[0]["content"]
            keyword_to_check = "知识图谱结构 (KuzuDB) 与 Cypher 查询生成规则" # Ensure this matches constants.py
            processed_system_content = system_content_for_check.lower()
            processed_keyword = keyword_to_check.lower()
            print(f"DEBUG_FastAPI_CypherTaskCheck: System prompt content (LOWERCASED, first 300 chars): '{processed_system_content[:300]}...'")
            print(f"DEBUG_FastAPI_CypherTaskCheck: Keyword to check (LOWERCASED): '{processed_keyword}'")
            if processed_keyword in processed_system_content:
                is_cypher_gen_task = True
                print("DEBUG_FastAPI_CypherTaskCheck: Cypher generation task DETECTED (after lowercasing and keyword adjustment).")
            else:
                print("DEBUG_FastAPI_CypherTaskCheck: Cypher generation task NOT DETECTED (keyword missing after lowercasing).")
        else:
            print("DEBUG_FastAPI_CypherTaskCheck: No system message found or messages empty, not a Cypher task.")
         
        if is_cypher_gen_task:
            print("DEBUG_FastAPI: Cypher generation task DETECTED.")
            if loaded_cypher_path_grammar is not None:
                print("DEBUG_FastAPI: Applying GBNF grammar (success/failure paths) FROM APP.STATE.")
                completion_params["grammar"] = loaded_cypher_path_grammar
                
                if logit_bias_for_failure_phrase:
                    print(f"DEBUG_FastAPI: Applying logit_bias for failure phrase: {logit_bias_for_failure_phrase}")
                    completion_params["logit_bias"] = logit_bias_for_failure_phrase
            else:
                print("DEBUG_FastAPI: GBNF grammar FROM APP.STATE IS NONE. Proceeding without grammar for Cypher task.")
        else:
            print("DEBUG_FastAPI: Not a Cypher task. No grammar or specific logit_bias applied.")
        
        print(f"DEBUG_FastAPI: Calling llama_model.create_chat_completion with params (excluding messages, grammar, logit_bias objects): "
              f"{ {k:v for k,v in completion_params.items() if k not in ['messages', 'grammar', 'logit_bias']} }")
        
        completion = llama_model.create_chat_completion(**completion_params)
        response_content_raw = completion['choices'][0]['message']['content']
        prompt_tokens = completion['usage']['prompt_tokens']
        completion_tokens = completion['usage']['completion_tokens']
        total_tokens = completion['usage']['total_tokens']
        finish_reason = completion['choices'][0].get('finish_reason', 'stop')
        print(f"DEBUG_FastAPI: Raw content from llama_model: '{response_content_raw}'")

    except Exception as e:
        print(f"Error during llama_model.create_chat_completion: {e}")
        traceback.print_exc()
        # Ensure this is a JSON string
        final_json_output_str_for_client = json.dumps({"status": "unable_to_generate", "query": "LLM call failed during generation."})
        finish_reason = "error" # Indicate an error finish

    if final_json_output_str_for_client is None: # Only process if no error above set this
        processed_llm_text = post_process_llm_output(response_content_raw, finish_reason)
        
        if is_cypher_gen_task:
            standard_success_template = {"status": "success", "query": ""} 
            standard_unable_json_obj = {"status": "unable_to_generate", "query": "无法生成Cypher查询."}
            final_json_to_return_obj = standard_unable_json_obj # Default to unable

            if processed_llm_text:
                cleaned_text_for_json_parse = processed_llm_text.strip()
                # Remove Markdown code block fences if present
                if cleaned_text_for_json_parse.startswith("```json"):
                    cleaned_text_for_json_parse = cleaned_text_for_json_parse[len("```json"):].strip()
                if cleaned_text_for_json_parse.endswith("```"):
                    cleaned_text_for_json_parse = cleaned_text_for_json_parse[:-len("```")].strip()
                
                try:
                    data = json.loads(cleaned_text_for_json_parse)
                    if isinstance(data, dict) and "status" in data and "query" in data:
                        if data.get("status") == "success" and isinstance(data.get("query"), str) and data.get("query").strip():
                            final_json_to_return_obj = data
                            print(f"DEBUG_FastAPI: LLM output is a valid 'success' JSON (GBNF success path likely): {json.dumps(final_json_to_return_obj, ensure_ascii=False)}")
                        elif data.get("status") == "unable_to_generate" and data.get("query") == "无法生成Cypher查询.":
                            final_json_to_return_obj = data # Already standard
                            print(f"DEBUG_FastAPI: LLM output is a valid 'unable_to_generate' JSON (GBNF failure path likely): {json.dumps(final_json_to_return_obj, ensure_ascii=False)}")
                        else: # JSON has status/query but not matching expected values
                            print(f"DEBUG_FastAPI: LLM JSON has unexpected status/query content. Status: '{data.get('status')}', Query: '{str(data.get('query'))[:100]}'. Defaulting to standard 'unable_to_generate'.")
                            # final_json_to_return_obj remains standard_unable_json_obj
                    else: 
                        print(f"DEBUG_FastAPI: LLM output parsed as JSON, but not the expected dict with status/query: '{cleaned_text_for_json_parse}'. Defaulting to standard 'unable_to_generate'.")
                except json.JSONDecodeError:
                    print(f"DEBUG_FastAPI: LLM output was not valid JSON. Raw (after post_process): '{processed_llm_text}'. Defaulting to standard 'unable_to_generate'.")
                except Exception as e_parse:
                     print(f"DEBUG_FastAPI: Unexpected error parsing LLM output: {e_parse}. Raw: '{processed_llm_text}'. Defaulting to 'unable_to_generate'.")
            else:
                print("DEBUG_FastAPI: LLM output was empty after post_processing. Defaulting to standard 'unable_to_generate' JSON.")
            
            final_json_output_str_for_client = json.dumps(final_json_to_return_obj, ensure_ascii=False)
        else: 
            # For non-Cypher tasks, return the processed text directly
            final_json_output_str_for_client = processed_llm_text if processed_llm_text is not None else ""


    print(f"DEBUG_FastAPI: Final content string to be returned to client: '{final_json_output_str_for_client}'")

    response_id = f"chatcmpl-{uuid.uuid4().hex}"
    created_time = int(time.time())
    effective_model_name = request_model_name
    if not effective_model_name and model_path_global:
        effective_model_name = os.path.basename(model_path_global)
    elif not effective_model_name:
        effective_model_name = "local-llm-unknown"
    
    # Ensure final_json_output_str_for_client is a string, even if empty (for non-Cypher tasks)
    if final_json_output_str_for_client is None:
        final_json_output_str_for_client = "" # Or some other default string

    return ChatCompletionResponse(
        id=response_id,
        object="chat.completion",
        created=created_time,
        model=effective_model_name,
        choices=[
            ChatCompletionChoice(
                index=0,
                message=ChatCompletionMessage(role="assistant", content=final_json_output_str_for_client),
                finish_reason=finish_reason 
            )
        ],
        usage=UsageInfo(
            prompt_tokens=prompt_tokens,
            completion_tokens=completion_tokens,
            total_tokens=total_tokens
        )
    )

@app.get("/v1/models", response_model=Dict[str, Any])
async def list_models():
    global model_path_global
    model_id_for_clients = "qwen3local_gguf_gbnf_logit_bias"
    model_name_to_display = "Qwen3-1.7B-GGUF (GBNF+LogitBias)"
    
    if model_path_global and os.path.exists(model_path_global):
        model_name_to_display = os.path.basename(model_path_global)
        created_timestamp = int(os.path.getctime(model_path_global))
    else:
        # Fallback if model_path_global is not set or file doesn't exist
        model_name_to_display = "Qwen3-1.7B-GGUF (Model path not resolved)"
        created_timestamp = int(time.time())

    return {
        "object": "list",
        "data": [
            {
                "id": model_id_for_clients,
                "object": "model",
                "created": created_timestamp,
                "owned_by": "user",
                "description": f"Locally hosted GGUF model: {model_name_to_display}. Uses GBNF and logit_bias for Cypher tasks."
            }
        ]
    }

if __name__ == "__main__":
    print(f"--- Starting Local LLM FastAPI Service on {SERVICE_HOST}:{SERVICE_PORT} ---")
    print(f"--- Model will be loaded from DIR: {MODEL_DIR}, FILE: {MODEL_FILENAME or 'Auto-detected GGUF'} ---")
    print(f"--- GBNF Grammar for Cypher/Unable output will be loaded from: {GBNF_FILE_PATH} ---")
    uvicorn.run("local_llm_service:app", host=SERVICE_HOST, port=SERVICE_PORT, reload=False)
    --- END OF FILE CONTENT ---

  File: mcp_servers.json
    --- START OF FILE CONTENT ---
{
  "mcpServers": {
    "zhz_rag_mcp_service": { 
      "command": "/home/zhz/zhz_agent/.venv/bin/python", 
      "args": [
        "-m", 
        "zhz_rag.api.rag_mcp_service" 
      ],
      "cwd": "/home/zhz/zhz_agent", 
      "env": {
        "PYTHONPATH": "/home/zhz/zhz_agent:/home/zhz/zhz_agent/zhz_rag",
        "SGLANG_API_URL": "http://localhost:8088/v1/chat/completions",
        "CHROMA_PERSIST_DIRECTORY": "/home/zhz/zhz_agent/zhz_rag/stored_data/chromadb_index",
        "BM25_INDEX_DIRECTORY": "/home/zhz/zhz_agent/zhz_rag/stored_data/bm25_index",
        "KUZU_DB_PATH": "/home/zhz/zhz_agent/zhz_rag/stored_data/kuzu_default_db"
      }
    },    
    "ddgsearch": {
      "command": "uvx",
      "args": ["duckduckgo-mcp-server"], 
      "disabled": false,
      "autoApprove": ["search", "fetch_content"] 
    },   
    "amap-maps": {
      "command": "node",
      "args": [
        "/home/zhz/mcp/amap-maps-mcp-server/build/index.js"
      ],
      "cwd": "/home/zhz/mcp/amap-maps-mcp-server",
      "env": {
        "AMAP_MAPS_API_KEY": "1ca72c66e1c964ce73eecd3822174442"
      },
      "disabled": false
    }
  }
}
    --- END OF FILE CONTENT ---

  File: project_snapshot.txt
    --- START OF FILE CONTENT ---
Project Root: /home/zhz/zhz_agent
--- Project Structure and File Contents ---

Directory: /home/zhz/zhz_agent/
  File (skipped, extension not included): .env
  File (skipped, extension not included): .envrc
  File (skipped, extension not included): .gitignore
  File: README.md
    --- START OF FILE CONTENT ---
# zhz_agent
后端：RAG增强+多agent+dagster

    --- END OF FILE CONTENT ---

  File: __init__.py
    --- File is empty ---

  File: agent_orchestrator_service.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/agent_orchestrator_service.py

import os
import asyncio
import traceback
import json
from typing import List, Dict, Any, Optional, Union
from contextlib import asynccontextmanager
import re
import uvicorn
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from datetime import datetime 

import logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(module)s:%(lineno)d - %(message)s')
service_logger = logging.getLogger(__name__)

from crewai import Agent, Task, Crew, Process
from crewai.tools import BaseTool

from core.llm_manager import get_llm_instance, CustomLiteLLMWrapper
from core.tools.enhanced_rag_tool import EnhancedRAGTool
from core.tools.excel_tool import ExcelOperationTool
from core.tools.search_tool import WebSearchTool
from core.tools.time_tool import GetCurrentTimeTool
from core.tools.calculator_tool import CalculateTool


AGENT_SERVICE_PORT = int(os.getenv("AGENT_SERVICE_PORT", 8090))
AGENT_SERVICE_HOST = "0.0.0.0"

class AgentTaskRequest(BaseModel):
    user_query: str = Field(description="用户的原始文本查询。")

class AgentTaskResponse(BaseModel):
    answer: str = Field(description="Agent 系统生成的最终答案或响应。")
    status: str = Field(default="success", description="执行状态: 'success', 'needs_clarification', 'error', 'processing_plan', 'task_created'.")
    intermediate_plan: Optional[Dict[str, Any]] = Field(None, description="如果status是'processing_plan', 这里包含Manager Agent的规划结果。")
    task_id: Optional[str] = Field(None, description="如果任务是异步的，返回任务ID。")
    error_message: Optional[str] = Field(None, description="如果发生错误，此字段包含错误信息。")
    debug_info: Optional[Dict[str, Any]] = Field(None, description="包含执行过程中的调试信息。")
    token_usage: Optional[Dict[str, Any]] = Field(None, description="LLM token 使用情况统计。")

class SubTaskDefinitionForManagerOutput(BaseModel):
    task_description: str = Field(description="用户的原始请求原文。")
    reasoning_for_plan: Optional[str] = Field(None, description="Manager Agent的决策思考过程。")
    selected_tool_names: List[str] = Field(description="选定的工具名称列表。如果直接回答，则为空列表。")
    direct_answer_content: Optional[str] = Field(None, description="如果选择直接回答，这里是答案内容。")
    tool_input_args: Optional[Dict[str, Any]] = Field(None, description="如果选择使用非Excel工具，这里是传递给该工具的参数。")
    excel_sqo_payload: Optional[List[Dict[str, Any]]] = Field(None, description="如果选择使用Excel工具，这里是SQO操作字典的列表。")

manager_llm: Optional[CustomLiteLLMWrapper] = None
worker_llm: Optional[CustomLiteLLMWrapper] = None
manager_agent_instance: Optional[Agent] = None
worker_agent_instance: Optional[Agent] = None
core_tools_instances: List[BaseTool] = []

# --- 覆盖开始 ---
CORE_TOOLS_ZHZ_AGENT = {
    "enhanced_rag_tool": "【核心RAG工具】用于从本地知识库查找信息、回答复杂问题，整合了向量、关键词和图谱检索。",
    "excel_operation_tool": "【Excel操作工具】通过结构化查询对象(SQO)对Excel文件执行复杂的数据查询、筛选、聚合等操作。此工具通过本地代理在Windows上运行。",
    "web_search_tool": "【网络搜索工具】使用DuckDuckGo搜索引擎在互联网上查找与用户查询相关的信息。此工具通过MCP调用。",
    "get_current_time_tool": "【时间工具】获取当前的日期和时间，可指定时区。此工具在Agent的Python环境中直接执行。",
    "calculate_tool": "【计算器工具】执行数学表达式的计算并返回数值结果。此工具在Agent的Python环境中直接执行。"
}
# --- 覆盖结束 ---
CORE_TOOL_NAMES_LIST = list(CORE_TOOLS_ZHZ_AGENT.keys()) # 这行会自动更新
TOOL_OPTIONS_STR_FOR_MANAGER = "\n".join( # 这行会自动更新
    [f"- '{name}': {desc}" for name, desc in CORE_TOOLS_ZHZ_AGENT.items()]
)

@asynccontextmanager
async def lifespan(app: FastAPI):
    global manager_llm, worker_llm, manager_agent_instance, worker_agent_instance, core_tools_instances
    # --- [修改] 为了让 Manager 的 prompt 也只看到 RAG 工具，我们需要在函数作用域内临时修改这些 ---
    global CORE_TOOL_NAMES_LIST, TOOL_OPTIONS_STR_FOR_MANAGER 
    
    print("--- Agent Orchestrator Service: Lifespan startup ---")

    # --- 步骤 1: 初始化核心工具实例 ---
    print("Initializing core tool instances...")
    enhanced_rag_tool_instance = None 
    excel_operation_tool_instance = None
    web_search_tool_instance = None
    get_current_time_tool_instance = None
    calculate_tool_instance = None
    try:
        enhanced_rag_tool_instance = EnhancedRAGTool()
        excel_operation_tool_instance = ExcelOperationTool() # 尝试实例化
        web_search_tool_instance = WebSearchTool()       # 尝试实例化
        get_current_time_tool_instance = GetCurrentTimeTool()
        calculate_tool_instance = CalculateTool()

        core_tools_instances = []
        if enhanced_rag_tool_instance:
            core_tools_instances.append(enhanced_rag_tool_instance)
        if get_current_time_tool_instance:
            core_tools_instances.append(get_current_time_tool_instance)
        if calculate_tool_instance:
            core_tools_instances.append(calculate_tool_instance)
        if excel_operation_tool_instance:
            core_tools_instances.append(excel_operation_tool_instance)
        if web_search_tool_instance:
            core_tools_instances.append(web_search_tool_instance)

        if not core_tools_instances:
            print("CRITICAL ERROR: No core tools could be initialized. Aborting LLM/Agent setup.")
            # 在这种情况下，后续的LLM和Agent初始化可能会失败或无意义
            # 可以考虑在这里抛出异常或设置一个全局状态阻止服务启动
        else:
            print(f"Successfully initialized tools: {[tool.name for tool in core_tools_instances]}")
        print(f"--- FULL TOOL CONFIGURATION WILL BE USED BY MANAGER (if successfully initialized) ---")
        print(f"Effective CORE_TOOL_NAMES_LIST for Manager (based on global def): {CORE_TOOL_NAMES_LIST}") # 这是全局的
        print(f"Effective TOOL_OPTIONS_STR_FOR_MANAGER for Manager (based on global def):\n{TOOL_OPTIONS_STR_FOR_MANAGER}") # 这是全局的
        print(f"Actually initialized tools for Worker: {[tool.name for tool in core_tools_instances]}")
        print(f"--- END OF TOOL CONFIGURATION ---")
        
    except Exception as e:
        print(f"ERROR during core tool initialization: {e}", exc_info=True)
        core_tools_instances = [] # 确保出错时为空列表
        # 后续LLM/Agent初始化时，如果 core_tools_instances 为空，它们可能需要特殊处理或报错

    # ... 后续的LLM和Agent初始化代码 ...
    # 确保 Manager Agent 的 tools=[] (它不直接调用工具)
    # 确保 Worker Agent 的 tools=core_tools_instances (它需要所有可用的工具实例)


    # --- 步骤 2: 初始化 LLM 实例 ---
    # LLM 初始化时，agent_tools 参数将使用上面步骤中已更新（且只包含RAG工具）的 core_tools_instances
    print("Initializing LLM instances...")
    try:
        gemini_tool_config = {"function_calling_config": {"mode": "AUTO"}}
        # Manager LLM 初始化
        manager_llm = get_llm_instance(
            llm_type="cloud_gemini", 
            temperature=0.1, 
            max_tokens=4096, 
            tool_config=gemini_tool_config,
            agent_tools=core_tools_instances # 使用已更新的 core_tools_instances
        )
        if not manager_llm:
            print("Failed to initialize Manager LLM (Cloud Gemini). Attempting fallback...")
            manager_llm = get_llm_instance(
                llm_type="local_qwen", 
                temperature=0.1, 
                max_tokens=3072, 
                tool_config=gemini_tool_config,
                agent_tools=core_tools_instances 
            )
        
        # Worker LLM 初始化
        print("Initializing Worker LLM (attempting Cloud Gemini first)...")
        worker_gemini_tool_config = {"function_calling_config": {"mode": "AUTO"}} 
        worker_llm = get_llm_instance(
            llm_type="cloud_gemini", 
            temperature=0.5, 
            max_tokens=3072,
            tool_config=worker_gemini_tool_config,
            agent_tools=core_tools_instances # 使用已更新的 core_tools_instances
        )
        if not worker_llm:
            print("Failed to initialize Worker LLM (Cloud Gemini). Attempting fallback to local_qwen...")
            worker_llm = get_llm_instance(
                llm_type="local_qwen", 
                temperature=0.6, 
                max_tokens=3072,
                agent_tools=core_tools_instances
            )

        if manager_llm: print(f"Manager LLM initialized: {manager_llm.model_name}")
        else: print("CRITICAL: Failed to initialize Manager LLM.")
        if worker_llm: print(f"Worker LLM initialized: {worker_llm.model_name}")
        else: print("CRITICAL: Failed to initialize Worker LLM.")
    except Exception as e:
        print(f"FATAL ERROR during LLM initialization: {e}") 
        traceback.print_exc() 
        manager_llm = None; worker_llm = None

    # --- 步骤 3: 初始化 Agent 实例 ---
    if manager_llm:
        manager_agent_instance = Agent(
            role='资深AI任务分解与Excel查询规划师 (Senior AI Task Decomposition and Excel Query Planner)',
            goal=f"""你的核心任务是分析用户的请求（该请求将在后续的任务描述中提供），并决定最佳的处理路径。你必须严格按照以下【工具选择规则和优先级】以及【示例】进行决策。

**【决策规则与优先级】**

1.  **【规则1：时间查询 - 强制使用工具】**
    *   如果用户查询明确是关于【获取当前日期或时间】。
    *   **行动**：【必须选择】`"get_current_time_tool"`。不要尝试自己回答“我没有实时时钟”。
    *   **参数**：为 `tool_input_args` 准备可选的 `timezone`。

2.  **【规则2：数学计算 - 强制使用工具】**
    *   如果用户查询明确是要求【执行一个数学表达式的计算】。
    *   **行动**：【必须选择】`"calculate_tool"`。不要尝试自己计算，即使很简单。
    *   **参数**：为 `tool_input_args` 准备 `expression`。

3.  **【规则3：实时/最新信息查询 - 网络搜索强制】**
    *   如果用户查询明显需要【实时、最新的、动态变化的、或广泛的外部互联网信息】（例如：今天的天气、最新的新闻、当前股价等）。
    *   **行动**：【必须且只能】选择 `"web_search_tool"`。
    *   **参数**：为 `tool_input_args` 准备 `query` 和可选的 `max_results`。
    *   **禁止**：不要使用 "enhanced_rag_tool" 查找此类信息。

4.  **【规则4：内部知识/文档深度查询 - RAG】**
    *   如果用户查询的是关于【已归档的、静态的公司内部信息、特定文档的详细内容、历史数据分析、或不需要实时更新的深度专业知识】。
    *   **行动**：选择 `"enhanced_rag_tool"`。
    *   **参数**：为 `tool_input_args` 准备 `query` 等。

5.  **【规则5：Excel文件操作 - 强制使用工具】**
    *   如果用户明确要求或其意图明显指向需要对【Excel文件进行复杂操作】。
    *   **行动**：【必须选择】`"excel_operation_tool"`。
    *   **任务**：为其构建【结构化查询对象 (SQO) 的JSON列表】到 `excel_sqo_payload`。SQO不含 "file_path" 或 "sheet_name"。

6.  **【规则6：LLM直接回答（在工具不适用后）】**
    *   **仅当**以上所有规则都不适用，并且你判断可以基于你【已有的知识和常识】直接、准确、完整地回答用户的全部请求时。
    *   **行动**：`selected_tool_names` 设为 `[]`，在 `direct_answer_content` 提供答案。

7.  **【规则7：无法处理/需要澄清（最终回退）】**
    *   如果所有规则都不适用，且你也无法直接回答。
    *   **行动**：`selected_tool_names` 设为 `[]`。在 `reasoning_for_plan` 中解释，如果合适，在 `direct_answer_content` 中礼貌回复。

**【可用工具的参考描述】：**
{TOOL_OPTIONS_STR_FOR_MANAGER}

**【决策示例 - 你必须学习并模仿这些示例的决策逻辑和输出格式】**

<example>
  <user_query>现在几点了？</user_query>
  <thought>用户明确询问当前时间。根据规则1，必须使用get_current_time_tool。</thought>
  <output_json>{{
    "task_description": "现在几点了？",
    "reasoning_for_plan": "用户询问当前时间，根据规则1，应使用时间工具。",
    "selected_tool_names": ["get_current_time_tool"],
    "direct_answer_content": null,
    "tool_input_args": {{"timezone": "Asia/Shanghai"}},
    "excel_sqo_payload": null
  }}</output_json>
</example>

<example>
  <user_query>计算 5 * (10 + 3)</user_query>
  <thought>用户要求进行数学计算。根据规则2，必须使用calculate_tool。</thought>
  <output_json>{{
    "task_description": "计算 5 * (10 + 3)",
    "reasoning_for_plan": "用户要求数学计算，根据规则2，应使用计算器工具。",
    "selected_tool_names": ["calculate_tool"],
    "direct_answer_content": null,
    "tool_input_args": {{"expression": "5 * (10 + 3)"}},
    "excel_sqo_payload": null
  }}</output_json>
</example>

<example>
  <user_query>今天上海的天气怎么样？</user_query>
  <thought>用户询问“今天”的天气，这是时效性信息。根据规则3，必须使用web_search_tool。</thought>
  <output_json>{{
    "task_description": "今天上海的天气怎么样？",
    "reasoning_for_plan": "查询今日天气，需要实时信息，根据规则3选择网络搜索。",
    "selected_tool_names": ["web_search_tool"],
    "direct_answer_content": null,
    "tool_input_args": {{"query": "今天上海天气"}},
    "excel_sqo_payload": null
  }}</output_json>
</example>

<example>
  <user_query>我们公司的报销政策是什么？</user_query>
  <thought>用户查询公司内部政策，属于内部知识库范畴。根据规则4，应使用enhanced_rag_tool。</thought>
  <output_json>{{
    "task_description": "我们公司的报销政策是什么？",
    "reasoning_for_plan": "查询公司内部政策，根据规则4选择RAG工具。",
    "selected_tool_names": ["enhanced_rag_tool"],
    "direct_answer_content": null,
    "tool_input_args": {{"query": "公司报销政策"}},
    "excel_sqo_payload": null
  }}</output_json>
</example>

<example>
  <user_query>中国的首都是哪里？</user_query>
  <thought>这是一个常见的常识性问题，我的内部知识足以回答，无需使用工具。根据规则6直接回答。</thought>
  <output_json>{{
    "task_description": "中国的首都是哪里？",
    "reasoning_for_plan": "常识性问题，可直接回答。",
    "selected_tool_names": [],
    "direct_answer_content": "中国的首都是北京。",
    "tool_input_args": null,
    "excel_sqo_payload": null
  }}</output_json>
</example>

<example>
  <user_query>请帮我分析 "sales_report_Q3.xlsx" 文件中 "产品类别" 列的销售额总和，并按 "区域" 进行分组。</user_query>
  <thought>用户明确要求对Excel文件进行分组聚合操作。根据规则5，必须使用excel_operation_tool，并生成SQO列表。</thought>
  <output_json>{{
    "task_description": "请帮我分析 \"sales_report_Q3.xlsx\" 文件中 \"产品类别\" 列的销售额总和，并按 \"区域\" 进行分组。",
    "reasoning_for_plan": "用户要求对Excel进行分组聚合，根据规则5选择Excel工具并生成SQO。",
    "selected_tool_names": ["excel_operation_tool"],
    "direct_answer_content": null,
    "tool_input_args": null,
    "excel_sqo_payload": [
      {{
        "operation_type": "group_by_aggregate",
        "group_by_columns": ["区域", "产品类别"],
        "aggregation_column": "销售额",
        "aggregation_function": "sum"
      }}
    ]
  }}</output_json>
</example>

**【输出格式要求 - 必须严格遵守！】**
你的唯一输出必须是一个JSON对象，符合 `SubTaskDefinitionForManagerOutput` Pydantic模型，包含：
*   `task_description`: (字符串) 用户的原始请求。
*   `reasoning_for_plan`: (字符串) 你做出决策的思考过程，清晰说明你遵循了上述哪条规则和哪个示例。
*   `selected_tool_names`: (字符串列表) 选定的工具名称列表。
*   `direct_answer_content`: (可选字符串) 仅在规则6适用时填充。
*   `tool_input_args`: (可选对象) 仅在规则1, 2, 3, 4适用时，为对应工具填充参数。
*   `excel_sqo_payload`: (可选SQO列表) 仅在规则5适用时填充。

我【不】自己执行任何工具操作。我的职责是精准规划并输出结构化的任务定义。
""",
            # --- 添加/恢复 backstory 参数 ---
            backstory="""我是一位经验丰富且高度智能的AI任务调度官和数据分析规划专家。我的核心使命是精确解读用户的每一个请求，并为其匹配最高效、最准确的处理路径。我的工作流程严谨细致：

1.  **【请求深度解析与意图识别】**：我会首先对用户的原始请求进行彻底的语义分析和意图识别。我会判断请求的性质：是简单问答？是需要内部知识检索？是需要实时外部信息？还是需要对特定数据文件（如Excel）进行操作？

2.  **【决策优先级：优先自主解决】**：在考虑动用任何外部工具之前，我会首先评估我的内部知识库和推理能力是否足以直接、完整且准确地回答用户的问题。只有当我确认无法自主解决时，我才会启动工具选择流程。

3.  **【工具选择的智慧：精准匹配，而非盲目调用】**：
    *   **时效性是关键**：对于新闻、天气、实时数据等具有强时效性的查询，我会毫不犹豫地选择【网络搜索工具】(`web_search_tool`)。
    *   **内部知识优先**：对于公司政策、历史项目资料、特定存档文档等内部信息查询，我会优先使用【增强型RAG工具】(`enhanced_rag_tool`)，因为它能从我们精心构建的本地知识库中提取精确信息。
    *   **Excel事务专家**：任何涉及Excel文件（.xlsx, .csv）的复杂数据操作——无论是读取、计算、筛选、聚合还是修改——我都会委派给【Excel操作工具】(`excel_operation_tool`)。此时，我的核心任务是为该工具生成一个或多个清晰、准确的【结构化查询对象 (SQO) 的JSON列表】，放入 `excel_sqo_payload` 字段。我深知SQO的质量直接影响执行结果，因此我会仔细构造每一个SQO的操作类型 (`operation_type`) 和所需参数，并且我【绝不会】在SQO中包含文件路径 (`file_path`) 或工作表名 (`sheet_name`)，这些将由后续流程处理。
    *   **基础计算与时间查询**：对于明确的数学表达式计算，我会选择【计算器工具】(`calculate_tool`)；对于获取当前日期时间的需求，我会选择【时间工具】(`get_current_time_tool`)。
    *   **审慎对待无法处理的请求**：如果用户请求过于宽泛、模糊，或者超出了当前所有可用工具和我的知识范围，我不会强行匹配工具或给出猜测性答复。我会选择不使用任何工具，并在我的思考过程（`reasoning_for_plan`）中解释原因，或者在 `direct_answer_content` 中礼貌地请求用户提供更多信息或说明无法处理。

4.  **【结构化输出：一切为了清晰执行】**：我的最终输出永远是一个严格符合 `SubTaskDefinitionForManagerOutput` Pydantic模型规范的JSON对象。这个JSON对象不仅包含了用户的原始请求 (`task_description`) 和我的决策理由 (`reasoning_for_plan`)，更重要的是，它清晰地指明了选定的工具 (`selected_tool_names`) 以及调用这些工具所需的一切参数（在 `tool_input_args` 或 `excel_sqo_payload` 中）。

我从不亲自执行任务的细节，我的价值在于运筹帷幄，确保每一个用户请求都能被分配到最合适的处理单元，从而实现高效、准确的问题解决。
""",
            # --- backstory 参数结束 ---
            llm=manager_llm,
            verbose=True,
            allow_delegation=False,
            tools=[] 
        )
        print(f"Manager Agent initialized with LLM: {manager_llm.model_name}")

    if worker_llm:
        worker_agent_instance = Agent(
            role='任务执行专家 (Task Execution Expert)',
            goal="根据Manager分配的具体任务描述和指定的工具，高效地执行任务并提供结果。",
            backstory="""我是一个AI执行者，专注于使用【Manager明确授权给我的工具】来解决问题。
                        我会严格遵循任务指令。如果任务是调用Excel工具并提供了SQO列表，我会按顺序迭代处理这些SQO，并整合结果。
                        对于像 'get_current_time_tool' 和 'calculate_tool' 这样的本地Python工具，我会直接在我的环境中执行它们。
                        对于其他工具，我会使用工具的名称（例如 'enhanced_rag_tool', 'excel_operation_tool', 'web_search_tool'）来调用它们。""",
            llm=worker_llm,
            verbose=True,
            allow_delegation=False,
            tools=core_tools_instances # Worker Agent 使用已更新（只含RAG工具）的 core_tools_instances
        )
        print(f"Worker Agent initialized with LLM: {worker_llm.model_name} and tools: {[t.name for t in worker_agent_instance.tools]}")

    if not manager_agent_instance or not worker_agent_instance:
        print("CRITICAL: One or more core agents failed to initialize. Service functionality will be severely limited.")
    elif not core_tools_instances and worker_agent_instance : 
        print("WARNING: Worker Agent initialized, but no core tools were successfully instantiated. Tool-based tasks will fail.")

    print("--- Agent Orchestrator Service: Lifespan startup complete ---")
    yield
    print("--- Agent Orchestrator Service: Lifespan shutdown ---")

app = FastAPI(
    title="Agent Orchestrator Service",
    description="接收用户请求，通过Manager/Worker Agent模型进行任务规划和执行。",
    version="0.1.1", # 版本更新
    lifespan=lifespan
)

@app.post("/v1/execute_task", response_model=AgentTaskResponse)
async def execute_task_endpoint(request: AgentTaskRequest):
    global manager_agent_instance, worker_agent_instance, core_tools_instances # 确保能访问全局 Agent 和工具实例
    
    print(f"Received agent task request: User Query='{request.user_query}'") # 使用 print 替代 logger

    if not manager_agent_instance or not worker_agent_instance:
        print("CRITICAL ERROR: Core agents are not initialized. Cannot process task.")
        raise HTTPException(status_code=503, detail="Service not ready: Core agents failed to initialize.")
    if not core_tools_instances:
        print("WARNING: Core tools are not initialized. Tool-based tasks may fail.")
    # --- 构建 Manager Task 的描述 ---
    # 我们将用户请求和可用的核心工具列表传递给 Manager Agent
    # Manager Agent 的 goal 和 backstory 已经包含了大部分指令
    # Task 的 description 主要用于传递动态信息，如当前用户查询
    manager_task_description_for_crewai = f"""
    请仔细分析以下用户请求：
    '{request.user_query}'

    你的目标是：
    1.  理解用户的核心意图。
    2.  **优先判断**：你能否基于你现有的知识直接、准确地回答这个问题？
        - 如果是，请在输出的JSON中填充 `direct_answer_content` 字段，并将 `selected_tool_names` 设为空列表。
    3.  **如果不能直接回答**：你当前只有一个核心工具可用：'enhanced_rag_tool'。请判断是否需要使用它。
    4.  如果选择了 'enhanced_rag_tool'，请准备好传递给该工具的参数，并将其放入 `tool_input_args` 字段。
    5.  严格按照 `SubTaskDefinitionForManagerOutput` 的JSON格式输出你的规划。`task_description` 字段必须是用户的原始请求原文: '{request.user_query}'。同时提供你的 `reasoning_for_plan`。
    """
    
    # --- Manager Task 的期望输出格式说明 ---
    manager_task_expected_output_description = f"""一个JSON对象，必须严格符合以下Pydantic模型的结构（你不需要输出 "SubTaskDefinitionForManagerOutput" 这个词本身）：
    {{
      "task_description": "string (固定为用户的原始请求: '{request.user_query}')",
      "reasoning_for_plan": "string (你的决策思考过程)",
      "selected_tool_names": ["list of strings (选定的工具名称列表。如果直接回答，则为空列表。如果使用工具，则为 ['enhanced_rag_tool'])"],
      "direct_answer_content": "string (可选, 仅当 selected_tool_names 为空列表时，这里是你的答案内容)",
      "tool_input_args": {{ "key": "value" }} (可选, 仅当 selected_tool_names 包含'enhanced_rag_tool'时，这里是给该工具的参数字典),
      "excel_sqo_payload": null # 当前Excel工具不可用，此字段应为null
    }}

    【重要输出规则】:
    - 如果你选择【直接回答】：`selected_tool_names` 必须是空列表 `[]`，`direct_answer_content` 必须包含你的答案，`tool_input_args` 和 `excel_sqo_payload` 应该为 `null` 或不存在。
    - 如果你选择使用【enhanced_rag_tool】：`selected_tool_names` 必须包含 `"enhanced_rag_tool"`，`direct_answer_content` 应该为 `null` 或不存在，`tool_input_args` 必须包含调用该工具所需的参数 (例如 `{{ "query": "{request.user_query}" }}` )，`excel_sqo_payload` 应该为 `null` 或不存在。

    示例输出 (直接回答):
    {{
      "task_description": "{request.user_query}",
      "reasoning_for_plan": "这是一个常识性问题，我可以根据我的内部知识直接回答。",
      "selected_tool_names": [],
      "direct_answer_content": "中国的首都是北京。",
      "tool_input_args": null,
      "excel_sqo_payload": null
    }}

    示例输出 (使用RAG工具):
    {{
      "task_description": "{request.user_query}",
      "reasoning_for_plan": "用户询问关于公司政策的问题，这需要从知识库中查找。",
      "selected_tool_names": ["enhanced_rag_tool"],
      "direct_answer_content": null,
      "tool_input_args": {{"query": "{request.user_query}", "top_k_vector": 5, "top_k_kg": 3, "top_k_bm25": 3}},
      "excel_sqo_payload": null
    }}
    请严格按照此JSON格式输出。
    """

    manager_task = Task(
        description=manager_task_description_for_crewai,
        expected_output=manager_task_expected_output_description,
        agent=manager_agent_instance,
        async_execution=False,
        output_pydantic=SubTaskDefinitionForManagerOutput
    )

    # --- 步骤 2: 执行 Manager Task ---
    print("Executing Manager Task...")
    manager_plan_object: Optional[SubTaskDefinitionForManagerOutput] = None
    manager_raw_output: Optional[str] = None

    try:
        manager_crew = Crew(
            agents=[manager_agent_instance],
            tasks=[manager_task],
            process=Process.sequential,
            verbose=True 
        )
        print(f"[{datetime.now()}] About to call manager_crew.kickoff()...") # <--- 添加日志
        manager_task_raw_result = manager_crew.kickoff(inputs={})
        print(f"[{datetime.now()}] manager_crew.kickoff() returned.") # <--- 添加日志

        # --- 新的解析逻辑，处理 CrewOutput ---
        actual_task_output: Optional[Any] = None # 用于存储实际的 TaskOutput 或字符串

        if hasattr(manager_task_raw_result, 'tasks_output') and isinstance(manager_task_raw_result.tasks_output, list) and manager_task_raw_result.tasks_output:
            # CrewOutput.tasks_output 是一个 TaskOutput 对象的列表
            actual_task_output = manager_task_raw_result.tasks_output[0] # 我们只有一个 manager_task
            print(f"Extracted TaskOutput from CrewOutput: {type(actual_task_output)}")
        elif hasattr(manager_task_raw_result, 'raw_output'): # 兼容直接返回 TaskOutput 的情况 (旧版或特定配置)
            actual_task_output = manager_task_raw_result
            print(f"kickoff returned a TaskOutput-like object directly: {type(actual_task_output)}")
        elif isinstance(manager_task_raw_result, str): # 直接返回字符串
            actual_task_output = manager_task_raw_result
            print(f"kickoff returned a raw string.")
        else:
            print(f"Warning: manager_crew.kickoff() returned an unexpected type: {type(manager_task_raw_result)}")
            actual_task_output = str(manager_task_raw_result)
        # --- 结束新的解析逻辑 ---


        # --- 后续的解析逻辑，现在基于 actual_task_output ---
        print(f"DEBUG: Type of actual_task_output: {type(actual_task_output)}")
        
        manager_raw_llm_output_str: Optional[str] = None
        if hasattr(actual_task_output, 'raw') and isinstance(actual_task_output.raw, str):
            manager_raw_llm_output_str = actual_task_output.raw.strip()
            print(f"DEBUG: actual_task_output.raw (LLM's original string output) IS:\n---\n{manager_raw_llm_output_str}\n---")
        else:
            print(f"DEBUG: actual_task_output.raw is not a string or does not exist. Value: {getattr(actual_task_output, 'raw', 'Attribute .raw not found')}")
            manager_raw_llm_output_str = str(actual_task_output) # Fallback

        # 优先尝试使用 CrewAI 已经解析好的 Pydantic 对象
        if hasattr(actual_task_output, 'pydantic_output') and \
           actual_task_output.pydantic_output is not None and \
           isinstance(actual_task_output.pydantic_output, SubTaskDefinitionForManagerOutput):
            print("DEBUG: Successfully using actual_task_output.pydantic_output.")
            manager_plan_object = actual_task_output.pydantic_output
        
        # 如果 Pydantic 对象不可用，但我们从 .raw 成功获取了字符串，则尝试解析它
        # 并且确保 manager_plan_object 之前没有被成功赋值
        elif manager_plan_object is None and manager_raw_llm_output_str: # <--- 添加 manager_plan_object is None 条件
            print(f"DEBUG: pydantic_output not available. Attempting to parse string from actual_task_output.raw:\n---\n{manager_raw_llm_output_str}\n---")
            json_to_parse_from_raw = manager_raw_llm_output_str # 使用我们从 .raw 获取的
            try:
                final_answer_marker = "## Final Answer:" # 虽然日志显示Qwen没输出这个，但保留以防万一
                if final_answer_marker in json_to_parse_from_raw:
                    json_to_parse_from_raw = json_to_parse_from_raw.split(final_answer_marker, 1)[-1].strip()
                
                cleaned_json_str = json_to_parse_from_raw.strip()
                if cleaned_json_str.startswith("```json"): cleaned_json_str = cleaned_json_str[len("```json"):].strip()
                if cleaned_json_str.endswith("```"): cleaned_json_str = cleaned_json_str[:-len("```")].strip()
                
                if not cleaned_json_str: raise ValueError("Cleaned JSON string (from .raw) is empty.")
                manager_plan_object = SubTaskDefinitionForManagerOutput(**json.loads(cleaned_json_str))
                print(f"Parsed Manager Plan (from actual_task_output.raw): {manager_plan_object.model_dump_json(indent=2)}")
            except (json.JSONDecodeError, ValueError, Exception) as e:
                print(f"Error parsing JSON from actual_task_output.raw: {e}. Raw content used: {json_to_parse_from_raw}")
        
        # 在所有尝试之后，如果 manager_plan_object 仍然是 None，才报告最终的解析失败
        if not manager_plan_object:
             # manager_raw_output 现在应该引用我们尝试过的 manager_raw_llm_output_str
             final_raw_output_for_debug = manager_raw_llm_output_str if manager_raw_llm_output_str else str(actual_task_output)
             print(f"Manager Task did not produce a valid Pydantic object after all attempts. Final Raw for debug: {final_raw_output_for_debug}")
             return AgentTaskResponse(answer="无法解析Manager Agent的规划结果。", status="error",
                                      error_message="Failed to parse manager plan after all attempts.",
                                      debug_info={"manager_raw_output_tried": final_raw_output_for_debug})
        
        # 如果 manager_plan_object 成功解析，则打印日志（这行已经在您的代码中）
        print(f"Parsed Manager Plan: {manager_plan_object.model_dump_json(indent=2)}")
        
    except Exception as e:
        print(f"Error executing Manager Task or its Crew: {e}") # <--- 修改后的第一行
        traceback.print_exc() # <--- 修改后的第二行
        return AgentTaskResponse(
            answer="执行Manager Agent任务时发生错误。", status="error", error_message=str(e),
            debug_info={"traceback": traceback.format_exc(), "manager_raw_output": manager_raw_output}
        )

    # --- 步骤 2: 根据 Manager 的规划执行后续操作 ---
    if not manager_plan_object.selected_tool_names and manager_plan_object.direct_answer_content:
        print(f"Manager decided to answer directly. Answer: {manager_plan_object.direct_answer_content}")
        return AgentTaskResponse(
            answer=manager_plan_object.direct_answer_content,
            status="success",
            debug_info={"manager_plan": manager_plan_object.model_dump()}
        )
    elif not manager_plan_object.selected_tool_names and not manager_plan_object.direct_answer_content:
        # 如果 Manager 既没有选择工具，也没有提供直接答案，这可能是一个规划错误
         print(f"Manager Warning: No tool selected and no direct answer provided. Reasoning: {manager_plan_object.reasoning_for_plan}")
         return AgentTaskResponse(
            answer=manager_plan_object.reasoning_for_plan or "Manager 未能提供明确的行动计划或答案。",
            status="success", # 或者 "error" 取决于您如何定义这种情况
            debug_info={"manager_plan": manager_plan_object.model_dump()}
        )

    # --- 步骤 3: 如果 Manager 规划使用工具，则创建并执行 Worker Task ---
    selected_tool_name = manager_plan_object.selected_tool_names[0] if manager_plan_object.selected_tool_names else None

    if not selected_tool_name:
        return AgentTaskResponse(
            answer="Manager规划使用工具但未在selected_tool_names中指定工具名称。", status="error",
            error_message="Tool name missing in manager plan's selected_tool_names.",
            debug_info={"manager_plan": manager_plan_object.model_dump()}
        )

    # 找到对应的工具实例
    tool_instance_for_worker = next((tool for tool in core_tools_instances if tool.name == selected_tool_name), None)

    if not tool_instance_for_worker:
        return AgentTaskResponse(
            answer=f"系统中未找到Manager规划使用的工具: {selected_tool_name}。", status="error",
            error_message=f"Tool '{selected_tool_name}' not found in core_tools_instances.",
            debug_info={"manager_plan": manager_plan_object.model_dump(), "available_tools": [t.name for t in core_tools_instances]}
        )

    print(f"Manager planned to use tool: {selected_tool_name}. Instance found: {tool_instance_for_worker is not None}")

    worker_task_description = ""
    worker_task_inputs = {} # 用于传递给 task.execute(inputs=...)

    if selected_tool_name == "enhanced_rag_tool":
        rag_query = manager_plan_object.tool_input_args.get("query", request.user_query) if manager_plan_object.tool_input_args else request.user_query
        top_k_v = manager_plan_object.tool_input_args.get("top_k_vector", 5) if manager_plan_object.tool_input_args else 5
        top_k_kg = manager_plan_object.tool_input_args.get("top_k_kg", 3) if manager_plan_object.tool_input_args else 3
        top_k_b = manager_plan_object.tool_input_args.get("top_k_bm25", 3) if manager_plan_object.tool_input_args else 3
        
        worker_task_description = f"请使用增强RAG工具回答以下问题：'{rag_query}'。\n使用以下参数进行检索：top_k_vector={top_k_v}, top_k_kg={top_k_kg}, top_k_bm25={top_k_b}。"
        worker_task_inputs = {
            "query": rag_query, 
            "top_k_vector": top_k_v, 
            "top_k_kg": top_k_kg, 
            "top_k_bm25": top_k_b
        }

    elif selected_tool_name == "excel_operation_tool":
        excel_sqo_list = manager_plan_object.excel_sqo_payload
        if not excel_sqo_list:
            return AgentTaskResponse(
                answer="Manager选择Excel工具但未提供SQO列表。", status="error",
                error_message="excel_sqo_payload is missing for excel_operation_tool.",
                debug_info={"manager_plan": manager_plan_object.model_dump()}
            )
        # --- 添加 elif 分支来处理 web_search_tool ---
    elif selected_tool_name == "web_search_tool": 
        search_query = manager_plan_object.tool_input_args.get("query", request.user_query) if manager_plan_object.tool_input_args else request.user_query
        max_results = manager_plan_object.tool_input_args.get("max_results", 5) if manager_plan_object.tool_input_args else 5
        worker_task_description = f"请使用网络搜索工具查找关于 '{search_query}' 的信息，返回最多 {max_results} 条结果。"
        worker_task_inputs = {"query": search_query, "max_results": max_results}
    # --- 添加结束 ---
    # --- 添加开始 ---
    elif selected_tool_name == "get_current_time_tool":
        timezone_str = manager_plan_object.tool_input_args.get("timezone", "Asia/Shanghai") if manager_plan_object.tool_input_args else "Asia/Shanghai"
        worker_task_description = f"请使用时间工具获取当前时间。时区参数为: '{timezone_str}'。"
        worker_task_inputs = {"timezone_str": timezone_str} # 注意参数名与工具类 _run 方法的参数名一致

    elif selected_tool_name == "calculate_tool":
        expression_str = manager_plan_object.tool_input_args.get("expression", "") if manager_plan_object.tool_input_args else ""
        if not expression_str:
            return AgentTaskResponse(
                answer="Manager选择计算器工具但未提供表达式。", status="error",
                error_message="expression is missing for calculate_tool.",
                debug_info={"manager_plan": manager_plan_object.model_dump()}
            )
        worker_task_description = f"请使用计算器工具计算以下表达式: '{expression_str}'。"
        worker_task_inputs = {"expression": expression_str} # 注意参数名与工具类 _run 方法的参数名一致
        # --- 添加结束 ---
        # TODO: 从用户原始请求 manager_plan_object.task_description 中提取 file_path 和 sheet_name

        excel_file_path = "/home/zhz/zhz_agent/data/test.xlsx" # 临时硬编码，后续需要动态获取
        excel_sheet_name: Union[str, int] = 0 # 临时硬编码

        worker_task_description = (
            f"你需要处理一个Excel文件相关的任务。文件路径是 '{excel_file_path}'，工作表是 '{excel_sheet_name}'。\n"
            f"请严格按照以下SQO操作列表，顺序执行每一个操作，并整合所有操作的结果形成最终答案。\n"
            f"SQO操作列表: {json.dumps(excel_sqo_list, ensure_ascii=False)}\n\n"
            f"对于列表中的每一个SQO字典，你需要调用 '{tool_instance_for_worker.name}' 工具一次，"
            f"将该SQO字典作为 'sqo_dict' 参数，同时传递 'file_path': '{excel_file_path}' 和 'sheet_name': '{excel_sheet_name}'。"
        )
        # Worker Task的inputs将是整个SQO列表以及文件和工作表信息
        # Worker Agent的Prompt需要指导它如何迭代处理这个列表
        worker_task_inputs = {
            "excel_sqo_list_to_execute": excel_sqo_list,
            "target_excel_file_path": excel_file_path,
            "target_excel_sheet_name": excel_sheet_name
        }
        
    elif selected_tool_name == "web_search_tool":
        search_query = manager_plan_object.tool_input_args.get("query", request.user_query) if manager_plan_object.tool_input_args else request.user_query
        max_results = manager_plan_object.tool_input_args.get("max_results", 5) if manager_plan_object.tool_input_args else 5

        worker_task_description = f"请使用网络搜索工具查找关于 '{search_query}' 的信息，返回最多 {max_results} 条结果。"
        worker_task_inputs = {"query": search_query, "max_results": max_results}
        
    else:
        return AgentTaskResponse(
            answer=f"未知的工具名称 '{selected_tool_name}' 被Manager规划。", status="error",
            error_message=f"Unknown tool '{selected_tool_name}' planned by manager.",
            debug_info={"manager_plan": manager_plan_object.model_dump()}
        )

    # 创建 Worker Task
    worker_task = Task(
        description=worker_task_description,
        expected_output="任务的执行结果，通常是一个字符串，其中包含答案或操作的状态。",
        agent=worker_agent_instance,
        tools=[tool_instance_for_worker], # 只给 Worker 当前任务需要的工具
        async_execution=False # Worker Task 通常也是同步的
    )

    print(f"Executing Worker Task with tool: {selected_tool_name}")
    print(f"Worker Task Description: {worker_task_description}")
    print(f"Worker Task Inputs: {worker_task_inputs}")
    
    worker_final_result: str = ""
    worker_crew_usage_metrics: Optional[Dict[str, Any]] = None # 用于存储 token usage
    token_usage_for_response: Optional[Dict[str, Any]] = None # <--- 在这里初始化
    try:
        # --- [关键修改] 为 Worker Task 创建并运行一个 Crew ---
        worker_crew = Crew(
            agents=[worker_agent_instance], # Worker Agent
            tasks=[worker_task],             # 它要执行的单个任务
            process=Process.sequential,
            verbose=True # 可以设为 True 或 2 来查看 Worker Crew 的详细日志
        )
        
        # 如果 worker_task_inputs 为空，则 inputs={}
        # CrewAI 的 kickoff 方法期望 inputs 是一个字典
        task_execution_inputs = worker_task_inputs if worker_task_inputs else {}
            
        # 执行 Worker Crew
        # kickoff 返回的是 CrewOutput 对象，或者在某些旧版本/配置下可能直接是结果字符串或 TaskOutput
        worker_crew_output = worker_crew.kickoff(inputs=task_execution_inputs)
        
        # 从 Worker Crew 的输出中提取结果
        # 这与我们处理 Manager Crew 输出的逻辑类似
        actual_worker_task_output: Optional[Any] = None
        if hasattr(worker_crew_output, 'tasks_output') and isinstance(worker_crew_output.tasks_output, list) and worker_crew_output.tasks_output:
            actual_worker_task_output = worker_crew_output.tasks_output[0] # 我们只有一个 worker_task
        elif hasattr(worker_crew_output, 'raw_output'): # 兼容直接返回 TaskOutput
            actual_worker_task_output = worker_crew_output
        elif isinstance(worker_crew_output, str): # 直接返回字符串
            actual_worker_task_output = worker_crew_output
        else: # 其他意外情况
            actual_worker_task_output = str(worker_crew_output)

        # 从 actual_worker_task_output 中提取最终的字符串结果
        if hasattr(actual_worker_task_output, 'raw') and isinstance(actual_worker_task_output.raw, str):
            worker_final_result = actual_worker_task_output.raw.strip()
        elif isinstance(actual_worker_task_output, str):
            worker_final_result = actual_worker_task_output.strip()
        else: # Fallback
            worker_final_result = str(actual_worker_task_output)
        
        print(f"Worker Task executed. Result: {worker_final_result}")
        
        # 获取 Worker Crew 的 token usage
        if hasattr(worker_crew, 'usage_metrics'):
            worker_crew_usage_metrics = worker_crew.usage_metrics
            print(f"DEBUG AGENT_ORCH: Raw worker_crew.usage_metrics object: {worker_crew_usage_metrics}")
            print(f"DEBUG AGENT_ORCH: Type of worker_crew.usage_metrics: {type(worker_crew_usage_metrics)}")
            if hasattr(worker_crew_usage_metrics, 'model_dump'):
                print(f"DEBUG AGENT_ORCH: worker_crew.usage_metrics.model_dump(): {worker_crew_usage_metrics.model_dump()}")
            elif isinstance(worker_crew_usage_metrics, dict):
                    print(f"DEBUG AGENT_ORCH: worker_crew.usage_metrics (is dict): {worker_crew_usage_metrics}")
            else:
                print(f"DEBUG AGENT_ORCH: worker_crew.usage_metrics is not a dict and has no model_dump.")
        else:
            print("DEBUG AGENT_ORCH: worker_crew does not have usage_metrics attribute.")
            worker_crew_usage_metrics = None # 确保它被定义
            # --- [结束关键修改] ---

            # 准备 token_usage_for_response
            token_usage_for_response = None
            if worker_crew_usage_metrics:
                if hasattr(worker_crew_usage_metrics, 'model_dump'):
                    token_usage_for_response = worker_crew_usage_metrics.model_dump()
                elif isinstance(worker_crew_usage_metrics, dict):
                    token_usage_for_response = worker_crew_usage_metrics
                else:
                    # 如果不是 Pydantic 模型或字典，尝试转换为字符串记录，但不作为结构化数据返回
                    service_logger.warning(f"Unexpected type for worker_crew_usage_metrics: {type(worker_crew_usage_metrics)}. Will not be included in structured token_usage.")

        return AgentTaskResponse(
                answer=worker_final_result,
                status="success",
                debug_info={
                    "manager_plan": manager_plan_object.model_dump(),
                    "worker_tool_used": selected_tool_name,
                    "worker_task_inputs": worker_task_inputs 
                },
                token_usage=token_usage_for_response # 使用处理后的 token_usage_for_response
            )

    except Exception as e:
    # 使用 traceback 打印详细错误
        print(f"Error executing Worker Task for tool {selected_tool_name}: {e}")
        traceback.print_exc() 
        return AgentTaskResponse(
            answer=f"执行工具 '{selected_tool_name}' 时发生错误。",
            status="error",
            error_message=str(e),
            debug_info={
                "manager_plan": manager_plan_object.model_dump(),
                "worker_tool_used": selected_tool_name,
                "worker_task_inputs": worker_task_inputs,
                "traceback": traceback.format_exc()
            }
        )

if __name__ == "__main__":
    print(f"--- Starting Agent Orchestrator FastAPI Service on {AGENT_SERVICE_HOST}:{AGENT_SERVICE_PORT} ---")
    uvicorn.run("agent_orchestrator_service:app", host=AGENT_SERVICE_HOST, port=AGENT_SERVICE_PORT, reload=True) # 确保模块名正确
    --- END OF FILE CONTENT ---

  File: config.yaml
    --- START OF FILE CONTENT ---
# config.yaml

model_list:
  - model_name: gemini/gemini-1.5-flash-latest # 您已有的 Gemini 配置 (如果存在)
    litellm_params:
      model: gemini/gemini-1.5-flash-latest
      api_key: os.environ/GEMINI_API_KEY
      # api_base: http://localhost:YOUR_CLASH_PORT # 如果需要

  # --- 修改我们本地的 Qwen3 1.7B 模型配置 ---
  - model_name: local/qwen3-1.7b-gguf
    litellm_params:
      model: openai/my-local-qwen # 使用 "openai/" 前缀，后面可以跟一个自定义的描述符
      api_base: http://localhost:8088/v1
      api_key: nokey

# ... (其他 LiteLLM 配置) ...
    --- END OF FILE CONTENT ---

  File: debug_kuzu_visibility.py
    --- START OF FILE CONTENT ---
# debug_kuzu_visibility.py
import kuzu
import os
import shutil
import time
import pandas as pd

DB_PATH = "./zhz_rag/stored_data/kuzu_test_visibility_db" # 使用一个新的测试路径

def setup_and_verify_schema():
    print(f"--- Phase 1: Setup and Initial Verification ---")
    if os.path.exists(DB_PATH):
        print(f"Removing existing database at {DB_PATH}")
        shutil.rmtree(DB_PATH)
    
    db_setup = None
    conn_setup = None
    try:
        print(f"Creating new database at {DB_PATH}")
        db_setup = kuzu.Database(DB_PATH, read_only=False)
        conn_setup = kuzu.Connection(db_setup)
        print("Database and connection created for setup.")

        ddl_queries = [
            "CREATE NODE TABLE IF NOT EXISTS ExtractedEntity (id_prop STRING, text STRING, label STRING, PRIMARY KEY (id_prop))",
            "CREATE REL TABLE IF NOT EXISTS WorksAt (FROM ExtractedEntity TO ExtractedEntity)",
            "CREATE REL TABLE IF NOT EXISTS AssignedTo (FROM ExtractedEntity TO ExtractedEntity)"
        ]
        for query in ddl_queries:
            print(f"Executing DDL: {query}")
            conn_setup.execute(query)
        print("DDL execution complete.")

        print("Executing CHECKPOINT...")
        conn_setup.execute("CHECKPOINT;")
        print("CHECKPOINT complete.")

        # Phase 1 Verification
        print("Verifying tables immediately after creation (same session)...")
        try:
            # 尝试 SHOW TABLES;
            print("Attempting 'SHOW TABLES;' in Phase 1...")
            result = conn_setup.execute("SHOW TABLES;") # <--- 修改这里
            df = pd.DataFrame(result.get_as_df())
            print(f"Tables found using 'SHOW TABLES;' (Phase 1):\n{df}")
            # 检查 'name' 列是否存在，并且 'ExtractedEntity' 是否在其中
            if not df.empty and 'name' in df.columns and "ExtractedEntity" in df["name"].tolist():
                print("Phase 1 Verification (SHOW TABLES;): ExtractedEntity table FOUND.")
            elif not df.empty and 'name' not in df.columns:
                print("Phase 1 Verification (SHOW TABLES;): 'name' column not found in SHOW TABLES result.")
                print(f"Columns available in SHOW TABLES result: {df.columns.tolist()}")
            else: # df is empty or ExtractedEntity not in name column
                print("Phase 1 Verification (SHOW TABLES;): ExtractedEntity table NOT FOUND or result was empty.")
        except Exception as e:
            print(f"Error during Phase 1 verification with 'SHOW TABLES;': {e}")
            print("Falling back to direct query for ExtractedEntity in Phase 1...")
            try:
                result_direct = conn_setup.execute("MATCH (e:ExtractedEntity) RETURN count(e) AS entity_count;")
                df_direct = pd.DataFrame(result_direct.get_as_df())
                print(f"Direct query result (Phase 1):\n{df_direct}")
                count_val = df_direct['entity_count'].iloc[0] if not df_direct.empty else -1
                print(f"Phase 1 Verification (Direct Query): ExtractedEntity table SEEMS TO EXIST (count: {count_val}).")
            except Exception as e_direct_phase1:
                print(f"Error during Phase 1 direct query verification: {e_direct_phase1}")

    except Exception as e:
        print(f"Error during Phase 1 setup: {e}")
    finally:
        if conn_setup:
            print("Closing setup connection.")
            # conn_setup.close() # Kuzu Connection 没有显式 close
        if db_setup:
            print("Deleting setup database instance reference (will trigger close).")
            del db_setup # 依赖 __del__
        print("--- Phase 1 Complete ---")

def verify_in_new_session():
    print(f"\n--- Phase 2: Verification in a New Session ---")
    if not os.path.exists(DB_PATH):
        print(f"Database at {DB_PATH} does not exist. Cannot perform Phase 2.")
        return

    db_verify = None
    conn_verify = None
    try:
        print(f"Opening existing database at {DB_PATH} for verification.")
        db_verify = kuzu.Database(DB_PATH, read_only=False) # 打开同一个数据库
        conn_verify = kuzu.Connection(db_verify)
        print("Database and connection created for verification.")

        # Phase 2 Verification
        print("Verifying tables in new session...")
        try:
            # 尝试 SHOW TABLES;
            print("Attempting 'SHOW TABLES;' in Phase 2...")
            result = conn_verify.execute("SHOW TABLES;") # <--- 修改这里
            df = pd.DataFrame(result.get_as_df())
            print(f"Tables found using 'SHOW TABLES;' (Phase 2):\n{df}")
            if not df.empty and 'name' in df.columns and "ExtractedEntity" in df["name"].tolist():
                print("Phase 2 Verification (SHOW TABLES;): ExtractedEntity table FOUND.")
            elif not df.empty and 'name' not in df.columns:
                print("Phase 2 Verification (SHOW TABLES;): 'name' column not found in SHOW TABLES result.")
                print(f"Columns available in SHOW TABLES result: {df.columns.tolist()}")
            else: # df is empty or ExtractedEntity not in name column
                print("Phase 2 Verification (SHOW TABLES;): ExtractedEntity table NOT FOUND or result was empty.")
        except Exception as e:
            print(f"Error during Phase 2 verification with 'SHOW TABLES;': {e}")
            print("Falling back to direct query for ExtractedEntity in Phase 2...")
            try:
                result_direct = conn_verify.execute("MATCH (e:ExtractedEntity) RETURN count(e) AS entity_count;")
                df_direct = pd.DataFrame(result_direct.get_as_df())
                print(f"Direct query result (Phase 2):\n{df_direct}")
                count_val = df_direct['entity_count'].iloc[0] if not df_direct.empty else -1
                print(f"Phase 2 Verification (Direct Query): ExtractedEntity table SEEMS TO EXIST (count: {count_val}).")
            except Exception as e_direct_phase2:
                print(f"Error during Phase 2 direct query verification: {e_direct_phase2}")

    except Exception as e:
        print(f"Error during Phase 2 setup: {e}")
    finally:
        if conn_verify:
            print("Closing verification connection.")
        if db_verify:
            print("Deleting verification database instance reference.")
            del db_verify
        print("--- Phase 2 Complete ---")

if __name__ == "__main__":
    print(f"Kuzu Python client version: {kuzu.__version__}")
    setup_and_verify_schema()
    print("\nWaiting a moment before trying to open in a new session (simulating process switch)...")
    time.sleep(2) # 短暂等待，模拟进程切换的间隙
    verify_in_new_session()
    --- END OF FILE CONTENT ---

  File: llm_setup.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/llm_setup.py

import os
from typing import Any, Optional, List, Dict

# LiteLLM 本身通常是直接调用其 API，例如 litellm.completion
# 如果要将其适配给 CrewAI 的 Agent，我们需要将其包装成一个符合 CrewAI BaseLLM 接口的类。
# CrewAI 自身也提供了通过 LiteLLM 使用各种模型的集成，我们也可以研究直接使用 CrewAI 的方式。
# 为简化起见，我们先创建一个函数，返回配置好的 LiteLLM 调用参数，或者一个简单的包装器。

# --- 从 agent_orchestrator_service.py 中获取的配置常量 ---
# 本地 LLM 服务地址 (Qwen3)
LOCAL_QWEN_API_BASE = os.getenv("LOCAL_LLM_API_BASE", "http://localhost:8088/v1") # 确保与 local_llm_service.py 的端口一致
LOCAL_QWEN_MODEL_NAME_FOR_LITELLM = os.getenv("LOCAL_LLM_MODEL_NAME", "local/qwen3-1.7b-gguf") # 这个是给 litellm 的 model 参数，需要与 LiteLLM 配置或调用方式对应

# 云端 LiteLLM 网关地址 (用于 Gemini 等)
CLOUD_LITELLM_GW_API_BASE = os.getenv("CLOUD_LITELLM_GW_API_BASE", "YOUR_CLOUD_LITELLM_GATEWAY_URL_HERE/v1") # 假设您的网关也提供 /v1 路径
GEMINI_MODEL_NAME_FOR_LITELLM = os.getenv("CLOUD_LLM_MODEL_NAME", "gemini/gemini-1.5-flash-latest") # 或您希望通过网关调用的模型

# API Keys (通常 LiteLLM 会在其自己的配置或环境变量中处理，但这里可以作为参考)
# GEMINI_API_KEY = os.getenv("GEMINI_API_KEY") # 如果直接调用 Gemini 而非通过网关
# 注意：本地模型的 API Key 通常是 "nokey" 或在服务中不强制

# --- CrewAI LLM 包装器 (如果需要直接传递给 CrewAI Agent) ---
# CrewAI 期望一个 LLM 对象，我们可以创建一个简单的包装器，或者使用 CrewAI 提供的 LiteLLM 集成
# from crewai.llms.base_llm import BaseLLM as CrewAIBaseLLM
# import litellm
#
# class LiteLLMWrapperForCrewAI(CrewAIBaseLLM):
#     model: str
#     api_base: Optional[str] = None
#     api_key: Optional[str] = None
#     custom_llm_provider: Optional[str] = None # 例如 "openai" for OpenAI-compatible endpoints
#     litellm_kwargs: Dict[str, Any] = {}
#
#     def __init__(self, model: str, api_base: Optional[str] = None, api_key: Optional[str] = None, custom_llm_provider: Optional[str] = None, **kwargs):
#         super().__init__(model=model) # CrewAI BaseLLM 需要 model 参数
#         self.model = model
#         self.api_base = api_base
#         self.api_key = api_key
#         self.custom_llm_provider = custom_llm_provider
#         self.litellm_kwargs = kwargs
#
#     def call(self, messages: List[Dict[str, str]], **kwargs: Any) -> str:
#         # CrewAI 的 BaseLLM.call 方法通常期望返回一个字符串
#         # LiteLLM 的 completion 返回一个 ModelResponse 对象
#         params = {
#             "model": self.model,
#             "messages": messages,
#             "api_base": self.api_base,
#             "api_key": self.api_key,
#             "custom_llm_provider": self.custom_llm_provider,
#             **self.litellm_kwargs, # 包含 temperature, max_tokens 等
#             **kwargs # 运行时可能传递的额外参数
#         }
#         # 移除值为 None 的参数，因为 litellm.completion 不喜欢 None 的 api_key 等
#         params_cleaned = {k: v for k, v in params.items() if v is not None}
#
#         response = litellm.completion(**params_cleaned)
#
#         # 从 LiteLLM 响应中提取内容
#         content = ""
#         if response.choices and response.choices[0].message and response.choices[0].message.content:
#             content = response.choices[0].message.content
#         return content
#
#     def get_token_ids(self, text: str) -> List[int]:
#         # LiteLLM 通常不直接暴露 tokenizer，但可以尝试通过 litellm.token_counter (如果它返回ids)
#         # 或者如果使用特定模型，可以尝试加载其 tokenizer
#         # 为简单起见，我们先返回空列表或引发 NotImplementedError
#         # return litellm.encode(model=self.model, text=text) # 如果 litellm.encode 可用
#         raise NotImplementedError("get_token_ids is not implemented for this LiteLLM wrapper.")


def get_local_qwen_llm_instance(
    temperature: float = 0.7,
    max_tokens: int = 2048,
    # crewai_wrapper: bool = False # 是否返回 CrewAI 兼容的包装器
) -> Any: # 返回 Any 以便后续决定是直接返回配置字典还是包装器实例
    """
    获取配置好的本地 Qwen3 LLM 实例（通过 LiteLLM 调用）。
    """
    print(f"LLM Setup: Configuring Local Qwen LLM via LiteLLM.")
    print(f"  Model: {LOCAL_QWEN_MODEL_NAME_FOR_LITELLM}")
    print(f"  API Base: {LOCAL_QWEN_API_BASE}")
    # if crewai_wrapper:
    #     return LiteLLMWrapperForCrewAI(
    #         model=LOCAL_QWEN_MODEL_NAME_FOR_LITELLM,
    #         api_base=LOCAL_QWEN_API_BASE,
    #         api_key="nokey", # 本地服务通常不需要 key
    #         custom_llm_provider="openai", # 因为我们的本地服务是 OpenAI 兼容的
    #         temperature=temperature,
    #         max_tokens=max_tokens
    #     )
    # else:
    # 返回一个配置字典，调用方可以使用 litellm.completion(**config, messages=...)
    return {
        "model": LOCAL_QWEN_MODEL_NAME_FOR_LITELLM,
        "api_base": LOCAL_QWEN_API_BASE,
        "api_key": "nokey",
        "custom_llm_provider": "openai",
        "temperature": temperature,
        "max_tokens": max_tokens
    }


def get_cloud_gemini_llm_instance(
    temperature: float = 0.5,
    max_tokens: int = 4096,
    # crewai_wrapper: bool = False
) -> Any:
    """
    获取配置好的云端 Gemini LLM 实例（通过云端 LiteLLM 网关调用）。
    """
    print(f"LLM Setup: Configuring Cloud Gemini LLM via LiteLLM Gateway.")
    print(f"  Model: {GEMINI_MODEL_NAME_FOR_LITELLM}")
    print(f"  API Base (Gateway): {CLOUD_LITELLM_GW_API_BASE}")

    if CLOUD_LITELLM_GW_API_BASE == "YOUR_CLOUD_LITELLM_GATEWAY_URL_HERE/v1":
        print("LLM Setup WARNING: CLOUD_LITELLM_GW_API_BASE is not configured. Cloud LLM calls will likely fail.")
        # 可以选择返回 None 或者一个无效的配置，让调用方处理
        return None

    # if crewai_wrapper:
    #     return LiteLLMWrapperForCrewAI(
    #         model=GEMINI_MODEL_NAME_FOR_LITELLM,
    #         api_base=CLOUD_LITELLM_GW_API_BASE,
    #         # API Key 通常由云端 LiteLLM 网关管理，客户端调用网关时可能不需要直接提供
    #         # 或者网关本身可能需要某种形式的认证 key
    #         api_key=os.getenv("CLOUD_LITELLM_GATEWAY_API_KEY"), # 假设网关可能需要一个 key
    #         # custom_llm_provider 可能不需要，因为网关会处理到具体云服务的转换
    #         temperature=temperature,
    #         max_tokens=max_tokens
    #     )
    # else:
    return {
        "model": GEMINI_MODEL_NAME_FOR_LITELLM,
        "api_base": CLOUD_LITELLM_GW_API_BASE,
        "api_key": os.getenv("CLOUD_LITELLM_GATEWAY_API_KEY"), # 网关本身的key
        "temperature": temperature,
        "max_tokens": max_tokens
    }

# --- 示例用法 (可选，用于测试此文件) ---
async def main_test_llm_setup():
    print("--- Testing LLM Setup ---")

    print("\n--- Getting Local Qwen Config ---")
    local_qwen_config = get_local_qwen_llm_instance()
    if local_qwen_config:
        print(f"Local Qwen Config: {local_qwen_config}")
        # 模拟调用
        try:
            messages = [{"role": "user", "content": "你好！"}]
            print(f"Simulating LiteLLM call with local Qwen config for: {messages}")
            # response = await litellm.acompletion(**local_qwen_config, messages=messages)
            # print(f"Simulated local Qwen response (first choice): {response.choices[0].message.content if response.choices else 'No response'}")
            print("Actual LiteLLM call commented out for setup test.")
        except Exception as e:
            print(f"Error simulating local Qwen call: {e}")
    else:
        print("Failed to get local Qwen config.")

    print("\n--- Getting Cloud Gemini Config ---")
    cloud_gemini_config = get_cloud_gemini_llm_instance()
    if cloud_gemini_config:
        print(f"Cloud Gemini Config: {cloud_gemini_config}")
        if cloud_gemini_config.get("api_base") == "YOUR_CLOUD_LITELLM_GATEWAY_URL_HERE/v1":
            print("Skipping simulated cloud Gemini call as gateway URL is a placeholder.")
        else:
            try:
                messages = [{"role": "user", "content": "Hello!"}]
                print(f"Simulating LiteLLM call with cloud Gemini config for: {messages}")
                # response = await litellm.acompletion(**cloud_gemini_config, messages=messages)
                # print(f"Simulated cloud Gemini response (first choice): {response.choices[0].message.content if response.choices else 'No response'}")
                print("Actual LiteLLM call commented out for setup test.")
            except Exception as e:
                print(f"Error simulating cloud Gemini call: {e}")
    else:
        print("Failed to get cloud Gemini config (likely due to placeholder URL).")

if __name__ == "__main__":
    # 为了运行异步的 main_test_llm_setup
    # import asyncio
    # asyncio.run(main_test_llm_setup())
    pass
    --- END OF FILE CONTENT ---

  File: local_llm_service.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/local_llm_service.py
import os
import time
import uuid
import traceback
from contextlib import asynccontextmanager
from typing import List, Dict, Any, Optional, Union
import json

import uvicorn
from fastapi import FastAPI, HTTPException, Request as FastAPIRequest
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from llama_cpp import Llama, LlamaGrammar
import re

# --- 配置 ---
MODEL_DIR = os.getenv("LOCAL_LLM_MODEL_DIR", "/home/zhz/models/Qwen3-1.7B-GGUF")
MODEL_FILENAME = os.getenv("LOCAL_LLM_MODEL_FILENAME")
N_GPU_LAYERS = int(os.getenv("LOCAL_LLM_N_GPU_LAYERS", 0))
N_CTX = int(os.getenv("LOCAL_LLM_N_CTX", 4096))
N_BATCH = int(os.getenv("LOCAL_LLM_N_BATCH", 512))
SERVICE_PORT = int(os.getenv("LOCAL_LLM_SERVICE_PORT", 8088))
SERVICE_HOST = "0.0.0.0"

GBNF_FILE_PATH = os.path.join(os.path.dirname(__file__), "core", "grammars", "cypher_or_unable_output.gbnf")

# --- 全局变量 ---
llama_model: Optional[Llama] = None
model_path_global: Optional[str] = None
# logit_bias 相关的全局变量 (在 lifespan 中初始化)
failure_phrase_token_ids: List[int] = []
logit_bias_for_failure_phrase: Optional[Dict[int, float]] = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    global llama_model, model_path_global, failure_phrase_token_ids, logit_bias_for_failure_phrase
    print("--- Local LLM Service: Lifespan startup (with GBNF and logit_bias prep) ---")
    
    model_file_to_load = MODEL_FILENAME
    if not model_file_to_load:
        print(f"MODEL_FILENAME environment variable not set. Attempting to auto-detect GGUF file in {MODEL_DIR}...")
        try:
            gguf_files = [f for f in os.listdir(MODEL_DIR) if f.endswith(".gguf")]
            if not gguf_files:
                error_msg = f"No GGUF models found in directory: {MODEL_DIR}"
                print(f"ERROR: {error_msg}")
                app.state.cypher_path_grammar = None # Ensure state variable exists even on error
                raise RuntimeError(error_msg)
            if len(gguf_files) > 1:
                print(f"Warning: Multiple GGUF models found in {MODEL_DIR}. Using the first one: {gguf_files[0]}")
            model_file_to_load = gguf_files[0]
            print(f"Auto-detected GGUF file: {model_file_to_load}")
        except FileNotFoundError:
            error_msg = f"Model directory not found: {MODEL_DIR}"
            print(f"ERROR: {error_msg}")
            app.state.cypher_path_grammar = None
            raise RuntimeError(error_msg)
        except Exception as e_find_model:
            error_msg = f"Error auto-detecting GGUF file: {e_find_model}"
            print(f"ERROR: {error_msg}")
            app.state.cypher_path_grammar = None
            raise RuntimeError(error_msg)

    model_path_global = os.path.join(MODEL_DIR, model_file_to_load)
    print(f"Attempting to load Llama model from: {model_path_global}")
    print(f"Parameters: n_gpu_layers={N_GPU_LAYERS}, n_ctx={N_CTX}, n_batch={N_BATCH}")

    try:
        llama_model = Llama(
            model_path=model_path_global,
            n_gpu_layers=N_GPU_LAYERS,
            n_ctx=N_CTX,
            n_batch=N_BATCH,
            verbose=True
        )
        print("Llama model loaded successfully.")

        # --- 初始化 logit_bias 相关 ---
        failure_phrase_str = "无法生成Cypher查询."
        if llama_model:
            try:
                # add_bos=False, special=False (通常用于非起始的、纯文本的词元化)
                failure_phrase_token_ids = llama_model.tokenize(failure_phrase_str.encode("utf-8"), add_bos=False, special=False)
                # 为这些 token ID 设置正向偏置，例如 10.0 (可以调整)
                # 避免偏置 EOS token (如果它意外地出现在短语的词元化结果中)
                eos_token_id = llama_model.token_eos()
                logit_bias_for_failure_phrase = {
                    token_id: 10.0 for token_id in failure_phrase_token_ids if token_id != eos_token_id
                }
                print(f"Successfully tokenized failure phrase '{failure_phrase_str}' to IDs: {failure_phrase_token_ids}")
                print(f"Logit bias for failure phrase: {logit_bias_for_failure_phrase}")
            except Exception as e_tokenize:
                print(f"ERROR: Failed to tokenize failure phrase for logit_bias: {e_tokenize}")
                failure_phrase_token_ids = []
                logit_bias_for_failure_phrase = None
        # --- 结束 logit_bias 初始化 ---

        gbnf_grammar_instance: Optional[LlamaGrammar] = None
        print(f"Attempting to load GBNF grammar from: {GBNF_FILE_PATH}")
        if os.path.exists(GBNF_FILE_PATH):
            try:
                gbnf_grammar_instance = LlamaGrammar.from_file(GBNF_FILE_PATH)
                print("GBNF grammar (success/failure paths) loaded successfully into lifespan.")
            except Exception as e_gbnf:
                print(f"ERROR: Failed to load or parse GBNF grammar file '{GBNF_FILE_PATH}': {e_gbnf}")
                traceback.print_exc()
        else:
            print(f"ERROR: GBNF grammar file not found at '{GBNF_FILE_PATH}'.")
        
        app.state.cypher_path_grammar = gbnf_grammar_instance

    except Exception as e:
        print(f"FATAL: Failed to load Llama model or prepare GBNF/logit_bias: {e}")
        app.state.cypher_path_grammar = None
        logit_bias_for_failure_phrase = None # Ensure this is also cleared
    
    yield
    print("--- Local LLM Service: Lifespan shutdown ---")

app = FastAPI(
    title="Local LLM Service (OpenAI Compatible)",
    description="Uses GBNF with logit_bias for conditional JSON output.",
    version="0.1.7", 
    lifespan=lifespan
)

class ChatMessage(BaseModel):
    role: str
    content: str

class ChatCompletionRequest(BaseModel):
    model: str
    messages: List[ChatMessage]
    temperature: float = 0.7
    max_tokens: Optional[int] = 1024 # Default to 1024 or higher
    stream: bool = False
    stop: Optional[Union[str, List[str]]] = None

class ChatCompletionMessage(BaseModel):
    role: str
    content: Optional[str] = None

class ChatCompletionChoice(BaseModel):
    index: int
    message: ChatCompletionMessage
    finish_reason: Optional[str] = "stop"

class UsageInfo(BaseModel):
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int

class ChatCompletionResponse(BaseModel):
    id: str = Field(default_factory=lambda: f"chatcmpl-{uuid.uuid4().hex}")
    object: str = "chat.completion"
    created: int = Field(default_factory=lambda: int(time.time()))
    model: str
    choices: List[ChatCompletionChoice]
    usage: UsageInfo
    system_fingerprint: Optional[str] = None

def post_process_llm_output(content: Optional[str], finish_reason: Optional[str]) -> Optional[str]:
    if content is None:
        return None
    processed_content = content
    # Remove <think>...</think> blocks
    think_block_pattern = re.compile(r"<think[^>]*>.*?</think>\s*", flags=re.DOTALL | re.IGNORECASE)
    processed_content = think_block_pattern.sub("", processed_content)

    # Handle potentially incomplete <think> tags if generation was cut short
    if finish_reason == "length" and \
       re.match(r"<think[^>]*>", processed_content.strip(), flags=re.IGNORECASE) and \
       not re.search(r"</think\s*>", processed_content, flags=re.IGNORECASE):
        print("DEBUG_POST_PROCESS: Incomplete think block due to length, attempting to remove partial tag.")
        # More aggressive removal of any leading <think...> tag if it's incomplete
        processed_content = re.sub(r"^<think[^>]*>","", processed_content.strip(), flags=re.IGNORECASE).strip()

    # Remove any remaining stray <think> or </think> tags
    stray_think_tag_pattern = re.compile(r"</?\s*think[^>]*?>\s*", flags=re.IGNORECASE)
    processed_content = stray_think_tag_pattern.sub("", processed_content)
    return processed_content.strip()

@app.post("/v1/chat/completions", response_model=ChatCompletionResponse)
async def create_chat_completion_endpoint(fastapi_req: FastAPIRequest, request: ChatCompletionRequest):
    global llama_model, model_path_global, logit_bias_for_failure_phrase
    
    loaded_cypher_path_grammar: Optional[LlamaGrammar] = getattr(fastapi_req.app.state, 'cypher_path_grammar', None)
    
    if llama_model is None:
        raise HTTPException(status_code=503, detail="Llama model is not loaded or failed to load.")
    if request.stream:
        raise HTTPException(status_code=400, detail="Streaming responses are not yet supported by this service.")

    response_content_raw: Optional[str] = None
    prompt_tokens = 0
    completion_tokens = 0
    total_tokens = 0
    finish_reason = "stop"
    request_model_name = request.model 
    final_json_output_str_for_client: Optional[str] = None

    if not hasattr(llama_model, "create_chat_completion"):
        print("CRITICAL_ERROR: llama_model.create_chat_completion method not found.")
        # Construct a proper JSON string for the error case directly
        error_response_obj = {"status": "unable_to_generate", "query": "LLM service misconfiguration."}
        final_json_output_str_for_client = json.dumps(error_response_obj, ensure_ascii=False)
        # Early return with the error structure if model is misconfigured
        return ChatCompletionResponse(
            id=f"chatcmpl-{uuid.uuid4().hex}",
            object="chat.completion",
            created=int(time.time()),
            model=request_model_name or (os.path.basename(model_path_global) if model_path_global else "local-llm-misconfigured"),
            choices=[ChatCompletionChoice(index=0, message=ChatCompletionMessage(role="assistant", content=final_json_output_str_for_client), finish_reason="error")],
            usage=UsageInfo(prompt_tokens=0, completion_tokens=0, total_tokens=0)
        )

    try:
        dict_messages = [msg.model_dump() for msg in request.messages]
        completion_params: Dict[str, Any] = {
            "messages": dict_messages,
            "temperature": request.temperature, 
            "max_tokens": request.max_tokens or 1024,
            "stop": request.stop,
        }

        is_cypher_gen_task = False
        if dict_messages and dict_messages[0]["role"] == "system":
            system_content_for_check = dict_messages[0]["content"]
            keyword_to_check = "知识图谱结构 (KuzuDB) 与 Cypher 查询生成规则" # Ensure this matches constants.py
            processed_system_content = system_content_for_check.lower()
            processed_keyword = keyword_to_check.lower()
            print(f"DEBUG_FastAPI_CypherTaskCheck: System prompt content (LOWERCASED, first 300 chars): '{processed_system_content[:300]}...'")
            print(f"DEBUG_FastAPI_CypherTaskCheck: Keyword to check (LOWERCASED): '{processed_keyword}'")
            if processed_keyword in processed_system_content:
                is_cypher_gen_task = True
                print("DEBUG_FastAPI_CypherTaskCheck: Cypher generation task DETECTED (after lowercasing and keyword adjustment).")
            else:
                print("DEBUG_FastAPI_CypherTaskCheck: Cypher generation task NOT DETECTED (keyword missing after lowercasing).")
        else:
            print("DEBUG_FastAPI_CypherTaskCheck: No system message found or messages empty, not a Cypher task.")
         
        if is_cypher_gen_task:
            print("DEBUG_FastAPI: Cypher generation task DETECTED.")
            if loaded_cypher_path_grammar is not None:
                print("DEBUG_FastAPI: Applying GBNF grammar (success/failure paths) FROM APP.STATE.")
                completion_params["grammar"] = loaded_cypher_path_grammar
                
                if logit_bias_for_failure_phrase:
                    print(f"DEBUG_FastAPI: Applying logit_bias for failure phrase: {logit_bias_for_failure_phrase}")
                    completion_params["logit_bias"] = logit_bias_for_failure_phrase
            else:
                print("DEBUG_FastAPI: GBNF grammar FROM APP.STATE IS NONE. Proceeding without grammar for Cypher task.")
        else:
            print("DEBUG_FastAPI: Not a Cypher task. No grammar or specific logit_bias applied.")
        
        print(f"DEBUG_FastAPI: Calling llama_model.create_chat_completion with params (excluding messages, grammar, logit_bias objects): "
              f"{ {k:v for k,v in completion_params.items() if k not in ['messages', 'grammar', 'logit_bias']} }")
        
        completion = llama_model.create_chat_completion(**completion_params)
        response_content_raw = completion['choices'][0]['message']['content']
        prompt_tokens = completion['usage']['prompt_tokens']
        completion_tokens = completion['usage']['completion_tokens']
        total_tokens = completion['usage']['total_tokens']
        finish_reason = completion['choices'][0].get('finish_reason', 'stop')
        print(f"DEBUG_FastAPI: Raw content from llama_model: '{response_content_raw}'")

    except Exception as e:
        print(f"Error during llama_model.create_chat_completion: {e}")
        traceback.print_exc()
        # Ensure this is a JSON string
        final_json_output_str_for_client = json.dumps({"status": "unable_to_generate", "query": "LLM call failed during generation."})
        finish_reason = "error" # Indicate an error finish

    if final_json_output_str_for_client is None: # Only process if no error above set this
        processed_llm_text = post_process_llm_output(response_content_raw, finish_reason)
        
        if is_cypher_gen_task:
            standard_success_template = {"status": "success", "query": ""} 
            standard_unable_json_obj = {"status": "unable_to_generate", "query": "无法生成Cypher查询."}
            final_json_to_return_obj = standard_unable_json_obj # Default to unable

            if processed_llm_text:
                cleaned_text_for_json_parse = processed_llm_text.strip()
                # Remove Markdown code block fences if present
                if cleaned_text_for_json_parse.startswith("```json"):
                    cleaned_text_for_json_parse = cleaned_text_for_json_parse[len("```json"):].strip()
                if cleaned_text_for_json_parse.endswith("```"):
                    cleaned_text_for_json_parse = cleaned_text_for_json_parse[:-len("```")].strip()
                
                try:
                    data = json.loads(cleaned_text_for_json_parse)
                    if isinstance(data, dict) and "status" in data and "query" in data:
                        if data.get("status") == "success" and isinstance(data.get("query"), str) and data.get("query").strip():
                            final_json_to_return_obj = data
                            print(f"DEBUG_FastAPI: LLM output is a valid 'success' JSON (GBNF success path likely): {json.dumps(final_json_to_return_obj, ensure_ascii=False)}")
                        elif data.get("status") == "unable_to_generate" and data.get("query") == "无法生成Cypher查询.":
                            final_json_to_return_obj = data # Already standard
                            print(f"DEBUG_FastAPI: LLM output is a valid 'unable_to_generate' JSON (GBNF failure path likely): {json.dumps(final_json_to_return_obj, ensure_ascii=False)}")
                        else: # JSON has status/query but not matching expected values
                            print(f"DEBUG_FastAPI: LLM JSON has unexpected status/query content. Status: '{data.get('status')}', Query: '{str(data.get('query'))[:100]}'. Defaulting to standard 'unable_to_generate'.")
                            # final_json_to_return_obj remains standard_unable_json_obj
                    else: 
                        print(f"DEBUG_FastAPI: LLM output parsed as JSON, but not the expected dict with status/query: '{cleaned_text_for_json_parse}'. Defaulting to standard 'unable_to_generate'.")
                except json.JSONDecodeError:
                    print(f"DEBUG_FastAPI: LLM output was not valid JSON. Raw (after post_process): '{processed_llm_text}'. Defaulting to standard 'unable_to_generate'.")
                except Exception as e_parse:
                     print(f"DEBUG_FastAPI: Unexpected error parsing LLM output: {e_parse}. Raw: '{processed_llm_text}'. Defaulting to 'unable_to_generate'.")
            else:
                print("DEBUG_FastAPI: LLM output was empty after post_processing. Defaulting to standard 'unable_to_generate' JSON.")
            
            final_json_output_str_for_client = json.dumps(final_json_to_return_obj, ensure_ascii=False)
        else: 
            # For non-Cypher tasks, return the processed text directly
            final_json_output_str_for_client = processed_llm_text if processed_llm_text is not None else ""


    print(f"DEBUG_FastAPI: Final content string to be returned to client: '{final_json_output_str_for_client}'")

    response_id = f"chatcmpl-{uuid.uuid4().hex}"
    created_time = int(time.time())
    effective_model_name = request_model_name
    if not effective_model_name and model_path_global:
        effective_model_name = os.path.basename(model_path_global)
    elif not effective_model_name:
        effective_model_name = "local-llm-unknown"
    
    # Ensure final_json_output_str_for_client is a string, even if empty (for non-Cypher tasks)
    if final_json_output_str_for_client is None:
        final_json_output_str_for_client = "" # Or some other default string

    return ChatCompletionResponse(
        id=response_id,
        object="chat.completion",
        created=created_time,
        model=effective_model_name,
        choices=[
            ChatCompletionChoice(
                index=0,
                message=ChatCompletionMessage(role="assistant", content=final_json_output_str_for_client),
                finish_reason=finish_reason 
            )
        ],
        usage=UsageInfo(
            prompt_tokens=prompt_tokens,
            completion_tokens=completion_tokens,
            total_tokens=total_tokens
        )
    )

@app.get("/v1/models", response_model=Dict[str, Any])
async def list_models():
    global model_path_global
    model_id_for_clients = "qwen3local_gguf_gbnf_logit_bias"
    model_name_to_display = "Qwen3-1.7B-GGUF (GBNF+LogitBias)"
    
    if model_path_global and os.path.exists(model_path_global):
        model_name_to_display = os.path.basename(model_path_global)
        created_timestamp = int(os.path.getctime(model_path_global))
    else:
        # Fallback if model_path_global is not set or file doesn't exist
        model_name_to_display = "Qwen3-1.7B-GGUF (Model path not resolved)"
        created_timestamp = int(time.time())

    return {
        "object": "list",
        "data": [
            {
                "id": model_id_for_clients,
                "object": "model",
                "created": created_timestamp,
                "owned_by": "user",
                "description": f"Locally hosted GGUF model: {model_name_to_display}. Uses GBNF and logit_bias for Cypher tasks."
            }
        ]
    }

if __name__ == "__main__":
    print(f"--- Starting Local LLM FastAPI Service on {SERVICE_HOST}:{SERVICE_PORT} ---")
    print(f"--- Model will be loaded from DIR: {MODEL_DIR}, FILE: {MODEL_FILENAME or 'Auto-detected GGUF'} ---")
    print(f"--- GBNF Grammar for Cypher/Unable output will be loaded from: {GBNF_FILE_PATH} ---")
    uvicorn.run("local_llm_service:app", host=SERVICE_HOST, port=SERVICE_PORT, reload=False)
    --- END OF FILE CONTENT ---

  File: requirements.txt
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/requirements.txt

# --- Core Application & Agent Framework ---
pydantic==2.11.5
crewai==0.121.0  # 您当前使用的版本，假设它兼容 Pydantic 2.11.5
crewai-tools==0.0.1 # 您当前使用的版本

# --- LLM Interaction & Management ---
litellm==1.68.0
openai==1.75.0 # 通常与 LiteLLM 一起使用

# --- Web Framework ---
fastapi==0.115.12
uvicorn[standard]==0.34.2 # [standard] 包含 websockets 和 httptools

# --- Data Storage & RAG ---
chromadb==0.6.3
sentence-transformers==4.1.0 # 您 freeze 输出中的版本，如果希望更新可以调整
transformers==4.52.3         # 您 freeze 输出中的版本，如果希望固定为 4.38.2 可以改回
torch==2.7.0                 # 您 freeze 输出中的版本
numpy==2.2.6                 # 您 freeze 输出中的版本 (注意：如果导致问题，可能需要回退到 <2.0)
kuzu # 让 uv 选择最新兼容版，或根据 Dagster 需求指定
portalocker # KuzuDB 可能需要

# --- Dagster Workflow ---
dagster==1.10.17             # 您当前使用的版本，假设兼容 Pydantic V2
dagster-webserver==1.10.17
dagster-graphql==1.10.17
dagster-pipes==1.10.17
dagster_shared==0.26.17

# --- Other Important Libraries from your freeze ---
aiohttp==3.12.0
aiosqlite==0.21.0
APScheduler==3.11.0
databases[aiosqlite]==0.9.0 # 确保包含 extras
httpx==0.28.1
jieba==0.42.1
langchain==0.3.25 # 这是一个元包，可能拉取很多 langchain-* 组件
langchain-core==0.3.61
langchain-community==0.3.24
langchain-text-splitters==0.3.8
neo4j==5.28.1
pandas==2.2.3
protobuf==5.29.4 # 确保这个版本与 opentelemetry 兼容
python-dotenv==1.1.0
pytz==2025.2
SQLAlchemy==2.0.41
bm25s==0.2.13

# --- OpenTelemetry (让 crewai 自动拉取，如果还有问题再显式指定) ---
# opentelemetry-api
# opentelemetry-sdk
# opentelemetry-exporter-otlp-proto-http
# opentelemetry-exporter-otlp-proto-grpc
# opentelemetry-proto
# opentelemetry-instrumentation
# opentelemetry-instrumentation-asgi
# opentelemetry-instrumentation-fastapi
# opentelemetry-semantic-conventions
# opentelemetry-util-http

# --- Potentially problematic or less critical (review if needed) ---
# 以下是一些在您 freeze 输出中但可能不需要严格固定，或者可以由其他包间接依赖的
# 如果遇到问题，可以考虑注释掉它们，让 uv 重新解析
# annotated-types==0.7.0 # 通常由 pydantic 依赖
# anyio==4.9.0 # 通常由 fastapi/starlette 依赖
# ... (您可以根据需要审查和删减 freeze 输出中的其他包) ...

# --- Editable local packages (uv pip install -e <path> 来安装) ---
# -e ./zhz_rag
# -e ./zhz_rag_pipeline_dagster
    --- END OF FILE CONTENT ---

  File: test_local_llm_cypher_gen.py
    --- START OF FILE CONTENT ---
# test_local_llm_cypher_gen.py
import httpx
import json
import os

# 从 zhz_rag.config.constants 导入 NEW_KG_SCHEMA_DESCRIPTION
# 这需要确保 PYTHONPATH 正确设置，或者脚本与 zhz_agent 在同一父目录下然后使用相对导入
# 为了简单，我们直接复制 Schema 描述到这里进行测试，或者确保能导入
try:
    from zhz_rag.config.constants import NEW_KG_SCHEMA_DESCRIPTION
except ImportError:
    print("WARNING: Could not import NEW_KG_SCHEMA_DESCRIPTION. Using a placeholder.")
    NEW_KG_SCHEMA_DESCRIPTION = "Node: :ExtractedEntity(text, label), Rel: :WorksAt, :AssignedTo"


LOCAL_LLM_URL = "http://localhost:8088/v1/chat/completions"

def construct_cypher_gen_messages(user_question: str, schema: str) -> list:
    # 使用您在 sglang_wrapper.py 中为 Cypher 生成构造的 messages 格式
    # 或者更直接地，使用 Qwen3 的标准聊天模板
    # 例如，一个简化的版本：
    # 注意：这里的 system prompt 应该与您 NEW_KG_SCHEMA_DESCRIPTION 的意图一致
    # 并且包含我们最新优化的指令
    system_prompt = NEW_KG_SCHEMA_DESCRIPTION # 使用我们优化后的完整 Schema 和指令

    return [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": f"用户问题: {user_question}"}
    ]

async def call_llm_for_cypher(question: str):
    print(f"\n--- Testing Cypher Generation for: {question} ---")
    messages = construct_cypher_gen_messages(question, NEW_KG_SCHEMA_DESCRIPTION)
    
    payload = {
        "model": "qwen3-1.7b-gguf", # 与 local_llm_service.py 中 ChatCompletionRequest.model 一致
        "messages": messages,
        "temperature": 0.0, # 对于代码生成，低temperature
        "max_tokens": 1024,   # 限制输出长度，避免因长度导致的其他问题
        "stop": ["<|im_end|>", "无法生成Cypher查询."] 
        # 注意：如果GBNF生效，stop可能不是主要控制因素了
    }
    
    # 打印将要发送的 messages 的 token 估算（如果可以的话）
    # from llama_cpp import Llama
    # temp_tokenizer_model = Llama(model_path="/path/to/your/qwen3-1.7b.gguf", verbose=False) # 需要实际模型路径
    # prompt_str_for_token_count = ""
    # for msg in messages:
    #     prompt_str_for_token_count += f"{msg['role']}\n{msg['content']}\n"
    # token_count = len(temp_tokenizer_model.tokenize(prompt_str_for_token_count.encode('utf-8')))
    # print(f"Estimated prompt tokens (approx): {token_count}")
    # del temp_tokenizer_model

    async with httpx.AsyncClient(timeout=60.0) as client:
        try:
            response = await client.post(LOCAL_LLM_URL, json=payload)
            print(f"Status Code: {response.status_code}")
            if response.status_code == 200:
                response_data = response.json()
                print("LLM Response JSON:")
                print(json.dumps(response_data, indent=2, ensure_ascii=False))
                if response_data.get("choices") and response_data["choices"][0].get("message"):
                    content = response_data["choices"][0]["message"].get("content")
                    print(f"Extracted Content:\n---\n{content}\n---")
            else:
                print(f"Error Response Text: {response.text}")
        except Exception as e:
            print(f"An error occurred: {e}")

async def main():
    # 确保 local_llm_service.py 正在运行
    test_questions = [
        "张三在哪里工作？",
        "项目Alpha的文档编写任务分配给了谁？",
        "法国的首都是哪里？" # 测试 "无法生成"
    ]
    for q in test_questions:
        await call_llm_for_cypher(q)

if __name__ == "__main__":
    import asyncio
    asyncio.run(main())
    --- END OF FILE CONTENT ---

  File: workspace.yaml
    --- START OF FILE CONTENT ---
load_from:
  - python_module:
      module_name: zhz_rag_pipeline_dagster.zhz_rag_pipeline.definitions
      working_directory: /home/zhz/zhz_agent
    --- END OF FILE CONTENT ---


Directory: zhz_rag_pipeline_dagster/
  File (skipped, extension not included): 25-5-28进度说明书.jpg
  File (skipped, extension not included): pyproject.toml
  File: setup.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/setup.py
from setuptools import find_packages, setup

setup(
    name="zhz_rag_pipeline_dagster_project", # 给一个包名
    version="0.0.1",
    packages=find_packages(), # 会找到 zhz_rag_pipeline 这个包
    install_requires=[
        "dagster",
        "dagster-webserver",
        # 添加其他 zhz_rag_pipeline_dagster 项目直接依赖的库
        # 例如 kuzu, dagster-pydantic (如果之后还要用) 等
        # 但核心的 zhz_rag 包的依赖不在这里列出，它应该是独立安装的
    ],
)
    --- END OF FILE CONTENT ---


Directory: zhz_rag_pipeline_dagster/zhz_rag_pipeline/
  File: __init__.py
    --- START OF FILE CONTENT ---
# zhz_rag_pipeline/__init__.py
# This file makes Python treat the directory as a package.
# You can also define a __version__ or import key modules here if needed.
    --- END OF FILE CONTENT ---

  File: custom_io_managers.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/zhz_rag_pipeline/custom_io_managers.py
import json
import os
from typing import List, Type, Union, get_args, get_origin, Any, Optional 
from dagster import UPathIOManager, InputContext, OutputContext, DagsterInvariantViolationError
from pydantic import BaseModel as PydanticBaseModel
from upath import UPath

class PydanticListJsonIOManager(UPathIOManager):
    extension: str = ".jsonl"

    def __init__(self, base_dir: Optional[str] = None): # Changed base_path to base_dir for clarity
        resolved_base_dir: UPath
        if base_dir:
            resolved_base_dir = UPath(base_dir).resolve() # Resolve to absolute path
        else:
            # Default to <DAGSTER_HOME>/storage/pydantic_jsonl_io
            # DAGSTER_HOME defaults to ~/.dagster, but can be overridden by env var
            dagster_home_str = os.getenv("DAGSTER_HOME", os.path.join(os.path.expanduser("~"), ".dagster"))
            resolved_base_dir = UPath(dagster_home_str) / "storage" / "pydantic_jsonl_io"
        
        # Ensure the directory exists
        try:
            resolved_base_dir.mkdir(parents=True, exist_ok=True)
        except Exception as e:
            # Log this error appropriately, perhaps using a direct print if logger isn't set up yet
            # or re-raise as a Dagster-specific error.
            print(f"[PydanticListJsonIOManager __init__] ERROR: Could not create bafef __init__(self, base_dir: Optse directory {resolved_base_dir}: {e}")
            # Depending on Dagster's init sequence, context.log might not be available here.
            # It's safer to let UPathIOManager handle its own base_path or ensure dir exists before.
            # For now, we proceed, UPathIOManager might handle it or fail later.

        super().__init__(base_path=resolved_base_dir)
        # Log the final base path used by the UPathIOManager instance
        # self.log available after super().__init__() in ConfigurableIOManager context
        # For direct instantiation, we might need to pass a logger or use a global one.
        # print(f"[PydanticListJsonIOManager __init__] Initialized with resolved base_path: {self.base_path}")


    def dump_to_path(self, context: OutputContext, obj: List[PydanticBaseModel], path: UPath):
        context.log.info(f"[PydanticListJsonIOManager dump_to_path] Attempting to dump to resolved path: {path.resolve()}")
        
        if not isinstance(obj, list):
            msg = f"Expected a list of Pydantic models, got {type(obj)}"
            context.log.error(msg)
            raise TypeError(msg)
        
        # Optional: More robust type checking for list items if needed, using context.dagster_type
        # For now, assume obj is List[PydanticBaseModel] based on upstream asset's type hint.

        try:
            with path.open("w", encoding="utf-8") as f:
                for model_instance in obj:
                    if not isinstance(model_instance, PydanticBaseModel):
                        context.log.warning(f"Item in list is not a Pydantic model: {type(model_instance)}. Skipping.")
                        continue
                    json_str = model_instance.json() # Pydantic V1
                    f.write(json_str + "\n")
            context.log.info(f"[PydanticListJsonIOManager dump_to_path] Successfully dumped {len(obj)} items to {path.resolve()}")
        except Exception as e:
            context.log.error(f"[PydanticListJsonIOManager dump_to_path] Failed to dump object to {path.resolve()}: {e}", exc_info=True)
            raise

    def load_from_path(self, context: InputContext, path: UPath) -> List[PydanticBaseModel]:
        context.log.info(f"[PydanticListJsonIOManager load_from_path] Attempting to load from resolved path: {path.resolve()}")
        
        list_typing_type = context.dagster_type.typing_type
        origin = get_origin(list_typing_type)
        args = get_args(list_typing_type)

        if not (origin is list and args and issubclass(args[0], PydanticBaseModel)):
            msg = (
                f"PydanticListJsonIOManager can only handle inputs of type List[PydanticModel], "
                f"but got {list_typing_type} for input '{context.name}'."
            )
            context.log.error(msg)
            raise DagsterInvariantViolationError(msg) # Use Dagster specific error
        
        model_type: Type[PydanticBaseModel] = args[0]
        context.log.info(f"[PydanticListJsonIOManager load_from_path] Target model type for list items: {model_type.__name__}")

        loaded_models: List[PydanticBaseModel] = []
        if not path.exists():
            context.log.warning(f"[PydanticListJsonIOManager load_from_path] File not found at {path.resolve()}, returning empty list for input '{context.name}'.")
            return loaded_models

        try:
            with path.open("r", encoding="utf-8") as f:
                for line_number, line in enumerate(f, 1):
                    line_content = line.strip()
                    if not line_content:
                        continue
                    try:
                        model_instance = model_type.parse_raw(line_content) # Pydantic V1
                        loaded_models.append(model_instance)
                    except Exception as e_parse:
                        context.log.error(
                            f"[PydanticListJsonIOManager load_from_path] Failed to parse JSON line {line_number} "
                            f"into {model_type.__name__} from {path.resolve()}: {e_parse}. "
                            f"Line content (first 100 chars): '{line_content[:100]}...'",
                            exc_info=True
                        )
                        # Optionally re-raise or decide to skip problematic lines
                        # For now, we'll skip
            context.log.info(f"[PydanticListJsonIOManager load_from_path] Successfully loaded {len(loaded_models)} instances of {model_type.__name__} from {path.resolve()}")
        except Exception as e_read:
            context.log.error(f"[PydanticListJsonIOManager load_from_path] Failed to read or process file {path.resolve()}: {e_read}", exc_info=True)
            raise # Re-raise if file reading itself fails catastrophically
            
        return loaded_models
    --- END OF FILE CONTENT ---

  File: definitions.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/zhz_rag_pipeline/definitions.py
import dagster as dg
import os

# 导入 in_process_executor
from dagster import define_asset_job, in_process_executor

from zhz_rag_pipeline_dagster.zhz_rag_pipeline.ingestion_assets import all_ingestion_assets
from zhz_rag_pipeline_dagster.zhz_rag_pipeline.processing_assets import (
    all_processing_assets,
    # 单独导入 Kuzu 相关的资产，以便放入特定作业
    kuzu_schema_initialized_asset,
    kuzu_entity_nodes_asset,
    kuzu_entity_relations_asset,
    # 也需要 kg_extractions 作为上游
    kg_extraction_asset,
    # 以及 kg_extractions 的上游
    clean_chunk_text_asset# 假设 kg_extraction_asset 的上游是 text_chunks
)
from zhz_rag_pipeline_dagster.zhz_rag_pipeline.evaluation_assets import all_evaluation_assets
from zhz_rag_pipeline_dagster.zhz_rag_pipeline.resources import (
    SentenceTransformerResource, SentenceTransformerResourceConfig,
    ChromaDBResource, ChromaDBResourceConfig,
    SGLangAPIResource, SGLangAPIResourceConfig,
    KuzuDBReadWriteResource, 
    KuzuDBReadOnlyResource,
    GeminiAPIResource, GeminiAPIResourceConfig
)
from zhz_rag_pipeline_dagster.zhz_rag_pipeline.custom_io_managers import PydanticListJsonIOManager

# 将所有非 Kuzu 写操作的资产组合起来
# 注意：我们需要从 all_processing_assets 中移除 Kuzu 相关的写资产
# 或者更简单的方式是，明确列出哪些资产属于哪个作业或全局资产组
non_kuzu_write_processing_assets = [
    asset for asset in all_processing_assets 
    if asset.key not in [
        kuzu_schema_initialized_asset.key, 
        kuzu_entity_nodes_asset.key, 
        kuzu_entity_relations_asset.key
    ]
]
# kg_extraction_asset 虽然是 kg_building 组，但它不直接写 KuzuDB，而是产生供下游写入的数据
# text_chunks_asset 是 kg_extraction_asset 的上游，也需要包含

# 所有资产（用于默认加载，或者如果某些资产不属于特定作业）
# all_project_assets = all_ingestion_assets + non_kuzu_write_processing_assets + all_evaluation_assets
# 实际上，定义了作业后，我们主要通过作业来执行。
# 我们可以将所有资产都列在 Definitions 的 assets 中，然后作业通过 selection 选择。

all_defined_assets = all_ingestion_assets + all_processing_assets + all_evaluation_assets


# 定义 KuzuDB 写入作业
kuzu_kg_write_job = define_asset_job(
    name="kuzu_kg_write_job",
    selection=[ # 选择所有参与 KuzuDB 知识图谱构建的资产
        clean_chunk_text_asset, # kg_extraction_asset 的上游
        kg_extraction_asset,
        kuzu_schema_initialized_asset,
        kuzu_entity_nodes_asset,
        kuzu_entity_relations_asset
    ],
    executor_def=in_process_executor # <--- 使用单进程执行器
)

# IO Manager
pydantic_io_manager_instance = PydanticListJsonIOManager()

defs = dg.Definitions(
    assets=all_defined_assets, # 列出所有资产
    jobs=[kuzu_kg_write_job],  # <--- 添加作业
    resources={
        "embedder": SentenceTransformerResource(
            model_name_or_path=SentenceTransformerResourceConfig().model_name_or_path
        ),
        "chroma_db": ChromaDBResource(
            collection_name=ChromaDBResourceConfig().collection_name,
            persist_directory=ChromaDBResourceConfig().persist_directory
        ),
        "sglang_api": SGLangAPIResource(
            api_url=SGLangAPIResourceConfig().api_url,
            default_temperature=SGLangAPIResourceConfig().default_temperature,
            default_max_new_tokens=SGLangAPIResourceConfig().default_max_new_tokens
        ),
        "kuzu_readwrite_db": KuzuDBReadWriteResource(
            # 使用资源类中定义的默认值
            # clear_on_startup_for_testing=True # 测试时可以设为True，确保从干净状态开始
        ),
        "kuzu_readonly_db": KuzuDBReadOnlyResource(
            # 使用资源类中定义的默认值
        ),
        "gemini_api": GeminiAPIResource(
            model_name=GeminiAPIResourceConfig().model_name,
            proxy_url=GeminiAPIResourceConfig().proxy_url,
            default_temperature=GeminiAPIResourceConfig().default_temperature,
            default_max_tokens=GeminiAPIResourceConfig().default_max_tokens
        ),
        "pydantic_json_io_manager": pydantic_io_manager_instance,
        # 如果 KuzuDB 资源被其他非此作业的资产使用，确保它们使用 KuzuDBReadOnlyResource
    }
)
    --- END OF FILE CONTENT ---

  File: evaluation_assets.py
    --- START OF FILE CONTENT ---
import dagster as dg
import os
from typing import Dict, List, Any # Optional 可能之后会用到

# 从项目中导入我们重构的批量评估函数和相关工具/常量
from zhz_rag.evaluation.batch_eval_cypher import run_cypher_batch_evaluation
from zhz_rag.evaluation.batch_eval_answer import run_answer_batch_evaluation
from zhz_rag.evaluation.analyze_cypher import perform_cypher_evaluation_analysis
from zhz_rag.evaluation.analyze_answer import perform_answer_evaluation_analysis
from zhz_rag.utils.common_utils import (
    find_latest_rag_interaction_log,
    RAG_INTERACTION_LOGS_DIR,
    EVALUATION_RESULTS_LOGS_DIR,
    get_evaluation_result_log_filepath
)
# 导入 GeminiAPIResource 以声明资源依赖
from zhz_rag_pipeline_dagster.zhz_rag_pipeline.resources import GeminiAPIResource

# --- 资产定义 ---

@dg.asset(
    name="latest_rag_interaction_log_for_evaluation",
    description="Provides the filepath of the latest RAG interaction log to be used for evaluation.",
    group_name="evaluation_pipeline",
    compute_kind="python" # 可选，指明计算类型
)
def latest_rag_interaction_log_for_evaluation_asset(context: dg.AssetExecutionContext) -> str:
    """
    Finds and returns the path to the latest RAG interaction log file.
    """
    log_filepath = find_latest_rag_interaction_log(RAG_INTERACTION_LOGS_DIR)
    if not log_filepath or not os.path.exists(log_filepath):
        error_msg = f"No RAG interaction log file found in directory: {RAG_INTERACTION_LOGS_DIR}"
        context.log.error(error_msg)
        raise dg.Failure(description=error_msg)
    
    context.log.info(f"Using RAG interaction log for evaluation: {log_filepath}")
    context.add_output_metadata({"log_filepath": log_filepath, "filename": os.path.basename(log_filepath)})
    return log_filepath

@dg.asset(
    name="batch_cypher_evaluations_log", # 资产名称最好能反映它产出的是日志文件
    description="Runs batch evaluation of Cypher queries and produces an evaluation log file.",
    group_name="evaluation_pipeline",
    compute_kind="python",
    # deps=[latest_rag_interaction_log_for_evaluation_asset] # 通过函数参数自动推断依赖
)
async def batch_cypher_evaluation_log_asset(
    context: dg.AssetExecutionContext,
    gemini_api: GeminiAPIResource,
    latest_rag_interaction_log_for_evaluation: str # <--- 修改参数名
) -> dg.Output[str]:
    context.log.info(f"Starting batch Cypher evaluation using log file: {latest_rag_interaction_log_for_evaluation}") # <--- 使用新参数名
    
    # 从 Dagster 配置中获取参数，或使用默认/环境变量
    # 这里我们先用之前脚本中的方式，未来可以转为 Dagster run_config
    app_version = os.getenv("APP_VERSION_TAG", "dagster_cypher_eval_0.2")
    # 对于 use_simulated_api，在 Dagster 中通常会通过资源配置或 op_config 来控制，
    # 而不是直接依赖环境变量，这样更灵活。但为了保持与脚本一致，暂时保留。
    use_simulated = os.getenv("USE_SIMULATED_GEMINI_CYPHER_EVAL", "false").lower() == "true"
    api_delay = float(os.getenv("GEMINI_API_CALL_DELAY_SECONDS", "4.1"))

    if use_simulated:
        context.log.warning("Cypher evaluation asset is using SIMULATED Gemini API calls.")

    # 调用我们重构的、现在接受 gemini_resource 的批量评估函数
    eval_stats = await run_cypher_batch_evaluation(
        gemini_resource_for_evaluator=gemini_api, # 传递注入的 Dagster 资源
        rag_interaction_log_filepath=latest_rag_interaction_log_for_evaluation,
        app_version=app_version,
        use_simulated_api=use_simulated, # 这个参数现在由 run_cypher_batch_evaluation 内部处理
        api_call_delay=api_delay
    )
    context.log.info(f"Batch Cypher evaluation completed. Statistics: {eval_stats}")

    # 确定输出的评估结果日志文件名 (与 evaluator.py 中一致)
    output_log_filepath = get_evaluation_result_log_filepath(evaluation_name="cypher_gemini_flash")
    
    # 确保目录存在 (get_evaluation_result_log_filepath 内部的 log_interaction_data 会处理)
    # 但这里我们也可以提前确保，或者依赖 log_interaction_data
    os.makedirs(os.path.dirname(output_log_filepath), exist_ok=True)
            
    metadata = {"evaluation_stats": eval_stats, "output_filepath": output_log_filepath}
    if eval_stats.get("cypher_queries_evaluated", 0) == 0:
        metadata["warning"] = "No Cypher queries were evaluated. Output log might be empty."
        context.log.warning(metadata["warning"])

    return dg.Output(output_log_filepath, metadata=metadata)


@dg.asset(
    name="batch_answer_evaluations_log", # 资产名称
    description="Runs batch evaluation of generated answers from RAG logs using Gemini.",
    group_name="evaluation_pipeline",
    compute_kind="python",
    # deps=[latest_rag_interaction_log_for_evaluation_asset] # 通过函数参数自动推断依赖
)
async def batch_answer_evaluation_log_asset(
    context: dg.AssetExecutionContext,
    gemini_api: GeminiAPIResource,
    latest_rag_interaction_log_for_evaluation: str # <--- 修改参数名
) -> dg.Output[str]:
    context.log.info(f"Starting batch Answer evaluation using log file: {latest_rag_interaction_log_for_evaluation}") # <--- 使用新参数名
    app_version = os.getenv("APP_VERSION_TAG", "dagster_answer_eval_0.2")
    use_simulated = os.getenv("USE_SIMULATED_GEMINI_ANSWER_EVAL", "false").lower() == "true"
    api_delay = float(os.getenv("GEMINI_API_CALL_DELAY_SECONDS", "4.1"))

    if use_simulated:
        context.log.warning("Answer evaluation asset is using SIMULATED Gemini API calls.")

    eval_stats = await run_answer_batch_evaluation(
        gemini_resource_for_evaluator=gemini_api, # 传递注入的 Dagster 资源
        rag_interaction_log_filepath=latest_rag_interaction_log_for_evaluation,
        app_version=app_version,
        use_simulated_api=use_simulated, # 这个参数现在由 run_answer_batch_evaluation 内部处理
        api_call_delay=api_delay
    )
    context.log.info(f"Batch Answer evaluation completed. Statistics: {eval_stats}")

    output_log_filepath = get_evaluation_result_log_filepath(evaluation_name="answer_gemini_flash")
    os.makedirs(os.path.dirname(output_log_filepath), exist_ok=True)

    metadata = {"evaluation_stats": eval_stats, "output_filepath": output_log_filepath}
    if eval_stats.get("answers_evaluated", 0) == 0:
        metadata["warning"] = "No answers were evaluated. Output log might be empty."
        context.log.warning(metadata["warning"])
        
    return dg.Output(output_log_filepath, metadata=metadata)

@dg.asset(
    name="cypher_evaluation_analysis_report", # 资产名称
    description="Generates a CSV analysis report from Cypher evaluation results.",
    group_name="evaluation_pipeline",
    compute_kind="python",
    # deps=[batch_cypher_evaluation_log_asset] # 通过函数参数自动推断依赖
)
def cypher_analysis_report_asset(
    context: dg.AssetExecutionContext,
    batch_cypher_evaluations_log: str # 上游资产的输出 (即 cypher 评估日志文件的路径)
) -> dg.Output[str]: # 输出 CSV 报告文件的路径
    """
    Analyzes Cypher evaluation logs and produces a CSV report.
    """
    context.log.info(f"Starting Cypher evaluation analysis using log file: {batch_cypher_evaluations_log}")

    if not os.path.exists(batch_cypher_evaluations_log):
        error_msg = f"Input Cypher evaluation log file not found: {batch_cypher_evaluations_log}"
        context.log.error(error_msg)
        raise dg.Failure(description=error_msg)

    # 构建输出CSV文件的路径
    # 我们希望CSV文件也存储在 EVALUATION_RESULTS_LOGS_DIR 目录下
    # 文件名可以基于输入日志名或固定一个模式
    base_input_log_name = os.path.basename(batch_cypher_evaluations_log)
    # 从 "eval_results_cypher_gemini_flash_YYYYMMDD.jsonl" 生成 "analysis_cypher_gemini_flash_YYYYMMDD.csv"
    if base_input_log_name.startswith("eval_results_") and base_input_log_name.endswith(".jsonl"):
        analysis_file_name = "analysis_" + base_input_log_name[len("eval_results_"):-len(".jsonl")] + ".csv"
    else: # Fallback naming
        analysis_file_name = f"analysis_cypher_report_{context.run_id[:8]}.csv"
    
    output_csv_filepath = os.path.join(EVALUATION_RESULTS_LOGS_DIR, analysis_file_name)
    
    success = perform_cypher_evaluation_analysis(
        evaluation_log_filepath=batch_cypher_evaluations_log,
        output_csv_filepath=output_csv_filepath
    )

    if success:
        context.log.info(f"Cypher evaluation analysis report generated: {output_csv_filepath}")
        return dg.Output(output_csv_filepath, metadata={"output_csv_filepath": output_csv_filepath, "source_log": base_input_log_name})
    else:
        error_msg = f"Cypher evaluation analysis failed for log file: {batch_cypher_evaluations_log}"
        context.log.error(error_msg)
        raise dg.Failure(description=error_msg)


@dg.asset(
    name="answer_evaluation_analysis_report", # 资产名称
    description="Generates a CSV analysis report from Answer evaluation results.",
    group_name="evaluation_pipeline",
    compute_kind="python",
    # deps=[batch_answer_evaluations_log_asset] # 通过函数参数自动推断依赖
)
def answer_analysis_report_asset(
    context: dg.AssetExecutionContext,
    batch_answer_evaluations_log: str # 上游资产的输出 (即 answer 评估日志文件的路径)
) -> dg.Output[str]: # 输出 CSV 报告文件的路径
    """
    Analyzes Answer evaluation logs and produces a CSV report.
    """
    context.log.info(f"Starting Answer evaluation analysis using log file: {batch_answer_evaluations_log}")

    if not os.path.exists(batch_answer_evaluations_log):
        error_msg = f"Input Answer evaluation log file not found: {batch_answer_evaluations_log}"
        context.log.error(error_msg)
        raise dg.Failure(description=error_msg)

    base_input_log_name = os.path.basename(batch_answer_evaluations_log)
    if base_input_log_name.startswith("eval_results_") and base_input_log_name.endswith(".jsonl"):
        analysis_file_name = "analysis_" + base_input_log_name[len("eval_results_"):-len(".jsonl")] + ".csv"
    else: # Fallback naming
        analysis_file_name = f"analysis_answer_report_{context.run_id[:8]}.csv"
        
    output_csv_filepath = os.path.join(EVALUATION_RESULTS_LOGS_DIR, analysis_file_name)

    success = perform_answer_evaluation_analysis(
        evaluation_log_filepath=batch_answer_evaluations_log,
        output_csv_filepath=output_csv_filepath
    )

    if success:
        context.log.info(f"Answer evaluation analysis report generated: {output_csv_filepath}")
        return dg.Output(output_csv_filepath, metadata={"output_csv_filepath": output_csv_filepath, "source_log": base_input_log_name})
    else:
        error_msg = f"Answer evaluation analysis failed for log file: {batch_answer_evaluations_log}"
        context.log.error(error_msg)
        raise dg.Failure(description=error_msg)

# 将所有评估相关的资产收集到一个列表中，方便在 definitions.py 中引用
all_evaluation_assets = [
    latest_rag_interaction_log_for_evaluation_asset,
    batch_cypher_evaluation_log_asset,
    batch_answer_evaluation_log_asset,
    cypher_analysis_report_asset, # <--- 新增
    answer_analysis_report_asset, # <--- 新增
]
    --- END OF FILE CONTENT ---

  File: ingestion_assets.py
    --- START OF FILE CONTENT ---
# zhz_rag_pipeline/ingestion_assets.py
import dagster as dg
import os
from typing import List, Dict, Any, Union

# 从我们新建的pydantic模型文件中导入
from .pydantic_models_dagster import LoadedDocumentOutput, ParsedDocumentOutput

class LoadDocumentsConfig(dg.Config):
    documents_directory: str = "/home/zhz/zhz_agent/data/raw_documents/" # 更新后的原始文档目录
    allowed_extensions: List[str] = [".txt"]

@dg.asset(
    name="raw_documents",
    description="Loads raw documents from a specified directory.",
    group_name="ingestion" # 给资产分组
)
def load_documents_asset(
    context: dg.AssetExecutionContext, 
    config: LoadDocumentsConfig
) -> List[LoadedDocumentOutput]:
    
    loaded_docs: List[LoadedDocumentOutput] = []
    target_directory = config.documents_directory
    allowed_exts = tuple(config.allowed_extensions) 

    context.log.info(f"Scanning directory: {target_directory} for files with extensions: {allowed_exts}")

    if not os.path.isdir(target_directory):
        context.log.error(f"Directory not found: {target_directory}")
        return loaded_docs

    for filename in os.listdir(target_directory):
        file_path = os.path.join(target_directory, filename)
        if os.path.isfile(file_path):
            file_name_lower = filename.lower()
            file_extension = os.path.splitext(file_name_lower)[1]

            if file_extension in allowed_exts:
                context.log.info(f"Found matching file: {file_path}")
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    doc_output = LoadedDocumentOutput(
                        document_path=file_path,
                        file_type=file_extension,
                        raw_content=content,
                        metadata={
                            "filename": filename,
                            "source_directory": target_directory,
                            "size_bytes": os.path.getsize(file_path)
                        }
                    )
                    loaded_docs.append(doc_output)
                    context.log.info(f"Successfully loaded and created output for: {file_path}")
                except Exception as e:
                    context.log.error(f"Failed to read or process file {file_path}: {e}")
            else:
                context.log.debug(f"Skipping file with non-allowed extension: {file_path}")
        else:
            context.log.debug(f"Skipping non-file item: {file_path}")
            
    if not loaded_docs:
        context.log.warning(f"No matching documents found in {target_directory}")

    if loaded_docs:
        context.add_output_metadata(
            metadata={
                "num_documents_loaded": len(loaded_docs),
                "first_document_path": loaded_docs[0].document_path if loaded_docs else "N/A"
            }
        )
    return loaded_docs


@dg.asset(
    name="parsed_documents",
    description="Parses loaded documents into text and extracts basic structure.",
    group_name="ingestion"
    # deps=[load_documents_asset] # <--- 删除这一行，因为依赖已通过函数参数 raw_documents 声明
)
def parse_document_asset(
    context: dg.AssetExecutionContext, 
    raw_documents: List[LoadedDocumentOutput] 
) -> List[ParsedDocumentOutput]:
    
    parsed_docs: List[ParsedDocumentOutput] = []
    context.log.info(f"Received {len(raw_documents)} documents to parse.")

    for doc_input in raw_documents:
        context.log.info(f"Parsing document: {doc_input.document_path} (Type: {doc_input.file_type})")
        parsed_text_content = ""
        
        try:
            if doc_input.file_type == ".txt":
                if isinstance(doc_input.raw_content, bytes):
                    parsed_text_content = doc_input.raw_content.decode('utf-8')
                elif isinstance(doc_input.raw_content, str):
                    parsed_text_content = doc_input.raw_content
                else:
                    # 抛出更具体的错误或记录并跳过
                    context.log.error(f"Unexpected raw_content type for .txt file: {type(doc_input.raw_content)} in {doc_input.document_path}")
                    parsed_text_content = f"[Error: Unexpected content type {type(doc_input.raw_content)}]"

            # TODO: Add parsers for other file types like .pdf, .docx here
            # elif doc_input.file_type == ".pdf":
            #     parsed_text_content = "[PDF parsing not yet implemented]"
            #     context.log.warning(f"PDF parsing not yet implemented for {doc_input.document_path}")
            else:
                parsed_text_content = f"[Unsupported file type: {doc_input.file_type}]"
                context.log.warning(f"Unsupported file type '{doc_input.file_type}' for parsing: {doc_input.document_path}")

            parsed_output = ParsedDocumentOutput(
                parsed_text=parsed_text_content,
                # document_structure is None by default
                original_metadata=doc_input.metadata 
            )
            parsed_docs.append(parsed_output)
            context.log.info(f"Successfully (or with placeholder) parsed: {doc_input.document_path}")

        except Exception as e:
            context.log.error(f"Failed to parse document {doc_input.document_path}: {e}")
            parsed_output = ParsedDocumentOutput(
                parsed_text=f"[Error parsing document: {str(e)}]",
                original_metadata=doc_input.metadata
            )
            parsed_docs.append(parsed_output)

    if parsed_docs:
        context.add_output_metadata(
            metadata={
                "num_documents_parsed": len(parsed_docs),
                "first_parsed_doc_filename": parsed_docs[0].original_metadata.get("filename", "N/A") if parsed_docs else "N/A"
            }
        )
    return parsed_docs

all_ingestion_assets = [load_documents_asset, parse_document_asset]
    --- END OF FILE CONTENT ---

  File: processing_assets.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/zhz_rag_pipeline/processing_assets.py
import dagster as dg
from typing import List, Dict, Any, Optional
import uuid
from langchain_text_splitters import RecursiveCharacterTextSplitter
import hashlib
import pandas as pd 
from zhz_rag.utils.common_utils import normalize_text_for_id
from zhz_rag_pipeline_dagster.zhz_rag_pipeline.pydantic_models_dagster import (
    ChunkOutput,
    ParsedDocumentOutput,
    EmbeddingOutput,
    KGTripleSetOutput, 
    ExtractedEntity,
    ExtractedRelation
)
from zhz_rag_pipeline_dagster.zhz_rag_pipeline.resources import (
    SentenceTransformerResource,
    ChromaDBResource,
    SGLangAPIResource,
    KuzuDBReadWriteResource,
    KuzuDBReadOnlyResource
)
import jieba
import bm25s
import pickle
import numpy as np
import os

class TextChunkerConfig(dg.Config):
    chunk_size: int = 500
    chunk_overlap: int = 50
    # separators: Optional[List[str]] = None # 可选的自定义分隔符

@dg.asset(
    name="text_chunks",
    description="Cleans and chunks parsed documents into smaller text segments.",
    group_name="processing", # 属于处理组
    # deps=["parsed_documents"] # <--- 删除或注释掉这一行
)
def clean_chunk_text_asset(
    context: dg.AssetExecutionContext,
    config: TextChunkerConfig,
    parsed_documents: List[ParsedDocumentOutput] 
) -> List[ChunkOutput]: 
    
    all_chunks: List[ChunkOutput] = []
    context.log.info(f"Received {len(parsed_documents)} parsed documents to clean and chunk.")

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=config.chunk_size,
        chunk_overlap=config.chunk_overlap,
        # length_function=len, # 默认
        # add_start_index=True, # 如果需要块的起始索引
        # separators=config.separators if config.separators else None # 使用配置的分隔符
    )

    for parsed_doc in parsed_documents:
        # 使用原始文件名作为文档ID，如果不存在则生成UUID
        doc_id_from_meta = parsed_doc.original_metadata.get("filename")
        if not doc_id_from_meta:
            doc_id_from_meta = parsed_doc.original_metadata.get("document_path", str(uuid.uuid4()))
            context.log.warning(f"Filename not found in metadata for a document, using path or UUID: {doc_id_from_meta}")
        
        source_dir = parsed_doc.original_metadata.get('source_directory', 'Unknown_Source_Dir')
        context.log.info(f"Processing document: {doc_id_from_meta} (from {source_dir})")

        cleaned_text = parsed_doc.parsed_text.strip()

        if not cleaned_text or cleaned_text.startswith("[Unsupported file type:") or cleaned_text.startswith("[Error parsing document:"):
            context.log.warning(f"Document {doc_id_from_meta} has no valid content or was unsupported/errored in parsing, skipping chunking.")
            continue
        
        try:
            chunks_text_list = text_splitter.split_text(cleaned_text)
            context.log.info(f"Document {doc_id_from_meta} split into {len(chunks_text_list)} chunks.")

            for i, chunk_text_content in enumerate(chunks_text_list):
                chunk_meta = parsed_doc.original_metadata.copy() 
                chunk_meta.update({
                    "chunk_number": i + 1,
                    "total_chunks_for_doc": len(chunks_text_list),
                    "chunk_char_length": len(chunk_text_content),
                })

                chunk_output = ChunkOutput( # chunk_id 会在 ChunkOutput 的 __init__ 中自动生成
                    chunk_text=chunk_text_content,
                    source_document_id=doc_id_from_meta, # 使用从元数据获取的文档ID
                    chunk_metadata=chunk_meta
                )
                all_chunks.append(chunk_output)
        
        except Exception as e:
            context.log.error(f"Failed to chunk document {doc_id_from_meta}: {e}")
            
    if all_chunks:
        context.add_output_metadata(
            metadata={
                "total_chunks_generated": len(all_chunks),
                "first_chunk_doc_id": all_chunks[0].source_document_id if all_chunks else "N/A"
            }
        )
    else:
        context.log.warning("No chunks were generated from the input documents.")
        
    return all_chunks

# --- 新增：EmbeddingGenerationAgent (实现为 Asset) ---
@dg.asset(
    name="text_embeddings",
    description="Generates vector embeddings for text chunks using a SentenceTransformer model.",
    group_name="processing",
    # deps=["text_chunks"] # <--- 删除或注释掉这一行
)
def generate_embeddings_asset(
    context: dg.AssetExecutionContext,
    text_chunks: List[ChunkOutput], # 输入是上游资产的输出列表
    embedder: SentenceTransformerResource # <--- 注入我们定义的Resource
) -> List[EmbeddingOutput]:
    
    all_embeddings: List[EmbeddingOutput] = []
    context.log.info(f"Received {len(text_chunks)} text chunks to generate embeddings for.")

    if not text_chunks:
        context.log.warning("No text chunks received, skipping embedding generation.")
        return all_embeddings

    # 提取所有块的文本内容进行批量编码
    chunk_texts_to_encode = [chunk.chunk_text for chunk in text_chunks]
    
    try:
        context.log.info(f"Starting batch embedding generation for {len(chunk_texts_to_encode)} texts...")
        # 使用Resource的encode方法
        vectors = embedder.encode(chunk_texts_to_encode) 
        context.log.info(f"Successfully generated {len(vectors)} embedding vectors.")

        if len(vectors) != len(text_chunks):
            # 这是一个预期外的情况，应该记录严重错误
            context.log.error(f"Mismatch in number of chunks ({len(text_chunks)}) and generated vectors ({len(vectors)}). Aborting.")
            # 可以在这里抛出异常来使资产失败
            raise ValueError("Embedding generation resulted in a mismatched number of vectors.")

        for i, chunk_input in enumerate(text_chunks):
            embedding_output = EmbeddingOutput(
                chunk_id=chunk_input.chunk_id,
                chunk_text=chunk_input.chunk_text,
                embedding_vector=vectors[i],
                embedding_model_name=embedder.model_name_or_path, # 从Resource获取模型名
                original_chunk_metadata=chunk_input.chunk_metadata
            )
            all_embeddings.append(embedding_output)
        
        context.log.info(f"All {len(all_embeddings)} embeddings prepared.")

    except Exception as e:
        context.log.error(f"Failed to generate embeddings: {e}")
        # 根据策略，可以选择让资产失败，或者返回空列表/部分结果
        # 这里我们选择抛出异常，让资产运行失败，以便调查
        raise

    if all_embeddings:
        context.add_output_metadata(
            metadata={
                "total_embeddings_generated": len(all_embeddings),
                "embedding_model_used": embedder.model_name_or_path,
                "first_chunk_id_embedded": all_embeddings[0].chunk_id if all_embeddings else "N/A"
            }
        )
    return all_embeddings

# --- 新增：VectorStorageAgent (实现为 Asset) ---
@dg.asset(
    name="vector_store_embeddings",
    description="Stores text embeddings into a ChromaDB vector store.",
    group_name="indexing", # 新的分组
    # deps=["text_embeddings"] # 依赖上一个资产的name
)
def vector_storage_asset(
    context: dg.AssetExecutionContext,
    text_embeddings: List[EmbeddingOutput], # 输入是上游资产的输出列表
    chroma_db: ChromaDBResource # <--- 注入ChromaDB Resource
) -> None: # 这个资产通常只执行操作，不产生新的可传递数据资产，所以返回None
    
    context.log.info(f"Received {len(text_embeddings)} embeddings to store in ChromaDB.")

    if not text_embeddings:
        context.log.warning("No embeddings received, nothing to store.")
        # 可以在这里添加一个 AssetMaterialization 来记录这个空操作
        context.add_output_metadata(metadata={"num_embeddings_stored": 0, "status": "No data to store"})
        return

    ids_to_store: List[str] = []
    embeddings_to_store: List[List[float]] = []
    metadatas_to_store: List[Dict[str, Any]] = []

    for emb_output in text_embeddings: # emb_output 是 EmbeddingOutput 类型
        ids_to_store.append(emb_output.chunk_id) 
        embeddings_to_store.append(emb_output.embedding_vector)
        
        simple_metadata = {}
        for key, value in emb_output.original_chunk_metadata.items(): # 从原始块元数据开始
            if isinstance(value, (str, int, float, bool)):
                simple_metadata[key] = value
            else:
                simple_metadata[key] = str(value) 
        
        simple_metadata["chunk_text"] = emb_output.chunk_text # <--- 直接从emb_output获取

        metadatas_to_store.append(simple_metadata)

    try:
        chroma_db.add_embeddings(
            ids=ids_to_store,
            embeddings=embeddings_to_store,
            metadatas=metadatas_to_store
        )
        context.log.info(f"Successfully stored/updated {len(ids_to_store)} embeddings in ChromaDB.")
        
        # 记录物化信息
        context.add_output_metadata(
            metadata={
                "num_embeddings_stored": len(ids_to_store),
                "collection_name": chroma_db.collection_name,
                "status": "Success"
            }
        )
    except Exception as e:
        context.log.error(f"Failed to store embeddings in ChromaDB: {e}")
        context.add_output_metadata(
            metadata={
                "num_embeddings_stored": 0,
                "collection_name": chroma_db.collection_name,
                "status": f"Failed: {str(e)}"
            }
        )
        raise # 让资产失败


# --- 新增：定义 KeywordIndexAgent 的配置 Pydantic 模型 ---
class BM25IndexConfig(dg.Config):
    index_file_path: str = "/home/zhz/zhz_agent/zhz_rag/stored_data/bm25_index/"

# --- 修改：KeywordIndexAgent (实现为 Asset) ---
@dg.asset(
    name="keyword_index",
    description="Builds and persists a BM25 keyword index from text chunks.",
    group_name="indexing",
    # deps=["text_chunks"] 
)
def keyword_index_asset(
    context: dg.AssetExecutionContext,
    config: BM25IndexConfig,
    text_chunks: List[ChunkOutput] 
) -> None:
    
    context.log.info(f"Received {len(text_chunks)} text chunks to build BM25 index.")

    if not text_chunks:
        # ... (无数据处理不变) ...
        context.log.warning("No text chunks received, skipping BM25 index building.")
        context.add_output_metadata(metadata={"num_documents_indexed": 0, "status": "No data"})
        return

    corpus_texts: List[str] = [chunk.chunk_text for chunk in text_chunks]
    document_ids: List[str] = [chunk.chunk_id for chunk in text_chunks] # 我们仍然需要保存这个映射

    context.log.info("Tokenizing corpus using jieba...")
    # 使用jieba分词，但bm25s有自己的tokenize函数，可以接受自定义分词器，或者直接处理分词后的列表
    # 为了与bm25s的tokenize函数配合，我们可以先用jieba分好，再传给bm25s的tokenizer
    # 或者，如果bm25s的默认分词或其Tokenizer类能满足中文需求，可以直接用。
    # 为了简单且利用jieba，我们先分词
    corpus_tokenized_jieba = [list(jieba.cut_for_search(text)) for text in corpus_texts]
    context.log.info(f"Jieba tokenization complete. Example: {corpus_tokenized_jieba[0][:10] if corpus_tokenized_jieba else 'N/A'}")

    context.log.info("Initializing BM25s model and indexing corpus...")
    try:
        # 根据bm25s文档，先创建BM25对象，然后调用index方法
        bm25_model = bm25s.BM25() # 可以传入k1, b等参数
        # .index() 方法接受已经分词的语料库 (list of list of str)
        bm25_model.index(corpus_tokenized_jieba)
        context.log.info("BM25s model indexed successfully.")
    except Exception as e:
        context.log.error(f"Failed to initialize or index with BM25s model: {e}")
        raise

    index_directory = config.index_file_path # 这应该是一个目录路径

    if not os.path.exists(index_directory):
        os.makedirs(index_directory, exist_ok=True)
        context.log.info(f"Created directory for BM25 index: {index_directory}")

    try:
        context.log.info(f"Saving BM25 model to directory: {index_directory}")
        # 使用bm25s的save方法，它会将多个文件保存到该目录下
        # 它会自动保存词汇表 (vocab.index.json) 和其他必要文件
        bm25_model.save(
            index_directory,
            # 文件名参数是可选的，bm25s有默认文件名，例如：
            # data_name="data.csc.index.npy",
            # indices_name="indices.csc.index.npy",
            # indptr_name="indptr.csc.index.npy",
            # vocab_name="vocab.index.json", 
            # params_name="params.index.json"
        )
        context.log.info(f"BM25 model saved successfully to {index_directory}")

        # 单独保存我们的 document_ids 列表，因为bm25s内部索引是基于0,1,2...
        # 而我们需要映射回原始的chunk_id
        doc_ids_path = os.path.join(index_directory, "doc_ids.pkl") # 保持这个文件名
        with open(doc_ids_path, 'wb') as f_out:
            pickle.dump(document_ids, f_out)
        context.log.info(f"Document IDs saved successfully to {doc_ids_path}")
        
        context.add_output_metadata(
            metadata={
                "num_documents_indexed": len(corpus_texts),
                "index_directory_path": index_directory, # <--- 修改：现在是目录路径
                "status": "Success"
            }
        )
    except Exception as e:
        # ... (错误处理不变) ...
        context.log.error(f"Failed to save BM25 model or document IDs: {e}")
        context.add_output_metadata(
            metadata={
                "num_documents_indexed": 0,
                "index_directory_path": index_directory,
                "status": f"Failed to save index: {str(e)}"
            }
        )
        raise

# 知识图谱
DEFAULT_KG_EXTRACTION_SCHEMA = { # <--- 覆盖这里的整个字典
    "type": "object",
    "properties": {
        "entities": {
            "type": "array",
            "items": {
                "type": "object",
                "properties": {
                    "text": {"type": "string", "description": "提取到的实体原文"},
                    "label": {"type": "string", "description": "实体类型 (例如: PERSON, ORGANIZATION, TASK)"}
                },
                "required": ["text", "label"]
            },
            "description": "从文本中提取出的实体列表。"
        },
        "relations": { # <--- 新增/确保这部分存在且正确
            "type": "array",
            "items": {
                "type": "object",
                "properties": {
                    "head_entity_text": {"type": "string", "description": "头实体的文本"},
                    "head_entity_label": {"type": "string", "description": "头实体的类型 (例如: PERSON, TASK)"},
                    "relation_type": {"type": "string", "description": "关系类型 (例如: WORKS_AT, ASSIGNED_TO)"},
                    "tail_entity_text": {"type": "string", "description": "尾实体的文本"},
                    "tail_entity_label": {"type": "string", "description": "尾实体的类型 (例如: ORGANIZATION, PERSON)"}
                },
                "required": ["head_entity_text", "head_entity_label", "relation_type", "tail_entity_text", "tail_entity_label"]
            },
            "description": "从文本中提取出的关系三元组列表。"
        }
    },
    "required": ["entities", "relations"] # <--- 确保 "relations" 也在这里
}

class KGExtractionConfig(dg.Config):
    extraction_prompt_template: str = ( # <--- 覆盖这里的整个多行字符串
        "你是一个信息抽取助手。请从以下提供的文本中抽取出所有的人名(PERSON)、组织机构名(ORGANIZATION)和任务(TASK)实体。\n"
        "同时，请抽取出以下两种关系：\n"
        "1. WORKS_AT (当一个人在一个组织工作时，例如：PERSON WORKS_AT ORGANIZATION)\n"
        "2. ASSIGNED_TO (当一个任务分配给一个人时，例如：TASK ASSIGNED_TO PERSON)\n\n"
        "请严格按照以下JSON格式进行输出，不要包含任何额外的解释或Markdown标记：\n"
        "{{\n"
        "  \"entities\": [\n"
        "    {{\"text\": \"实体1原文\", \"label\": \"实体1类型\"}},\n"
        "    ...\n"
        "  ],\n"
        "  \"relations\": [\n"
        "    {{\"head_entity_text\": \"头实体文本\", \"head_entity_label\": \"头实体类型\", \"relation_type\": \"关系类型\", \"tail_entity_text\": \"尾实体文本\", \"tail_entity_label\": \"尾实体类型\"}},\n"
        "    ...\n"
        "  ]\n"
        "}}\n"
        "如果文本中没有可抽取的实体或关系，请返回一个空的对应列表 (例如 {{\"entities\": [], \"relations\": []}})。\n\n"
        "示例文本1：'项目Alpha的文档编写任务分配给了张三。张三在谷歌工作。'\n"
        "期望JSON输出1：\n"
        "{{\n"
        "  \"entities\": [\n"
        "    {{\"text\": \"项目Alpha的文档编写任务\", \"label\": \"TASK\"}},\n"
        "    {{\"text\": \"张三\", \"label\": \"PERSON\"}},\n"
        "    {{\"text\": \"谷歌\", \"label\": \"ORGANIZATION\"}}\n"
        "  ],\n"
        "  \"relations\": [\n"
        "    {{\"head_entity_text\": \"项目Alpha的文档编写任务\", \"head_entity_label\": \"TASK\", \"relation_type\": \"ASSIGNED_TO\", \"tail_entity_text\": \"张三\", \"tail_entity_label\": \"PERSON\"}},\n"
        "    {{\"head_entity_text\": \"张三\", \"head_entity_label\": \"PERSON\", \"relation_type\": \"WORKS_AT\", \"tail_entity_text\": \"谷歌\", \"tail_entity_label\": \"ORGANIZATION\"}}\n"
        "  ]\n"
        "}}\n\n"
        "文本：\n"
        "\"{text_to_extract}\"\n\n"
        "JSON输出："
    )
    sglang_model_name: str = "Qwen2.5-3B-Instruct_via_SGLang" # 保持不变

# --- 新增：KGExtractionAgent (实现为 Asset) ---
@dg.asset(
    name="kg_extractions",
    description="Extracts entities (and potentially relations) from text chunks for knowledge graph construction.",
    group_name="kg_building",
    io_manager_key="pydantic_json_io_manager" 
)
async def kg_extraction_asset(
    context: dg.AssetExecutionContext,
    text_chunks: List[ChunkOutput], 
    config: KGExtractionConfig,     
    sglang_api: SGLangAPIResource
) -> List[KGTripleSetOutput]: # 确保返回类型注解正确
    
    all_kg_outputs: List[KGTripleSetOutput] = []
    context.log.info(f"Received {len(text_chunks)} text chunks for KG extraction.")

    if not text_chunks:
        context.log.warning("No text chunks received, skipping KG extraction.")
        context.add_output_metadata(metadata={"num_chunks_processed": 0, "total_entities_extracted": 0, "total_relations_extracted": 0})
        return all_kg_outputs

    total_entities_count = 0
    total_relations_count = 0
    for i, chunk in enumerate(text_chunks):
        context.log.info(f"Extracting KG from chunk {i+1}/{len(text_chunks)} (ID: {chunk.chunk_id})")
        
        prompt = config.extraction_prompt_template.format(text_to_extract=chunk.chunk_text)
        
        try:
            structured_response = await sglang_api.generate_structured_output(
                prompt=prompt,
                json_schema=DEFAULT_KG_EXTRACTION_SCHEMA 
            )
            context.log.debug(f"SGLang structured response for chunk {chunk.chunk_id}: {structured_response}")

            entities_data = structured_response.get("entities", [])
            extracted_entities_list: List[ExtractedEntity] = []
            if isinstance(entities_data, list):
                for entity_dict in entities_data:
                    if isinstance(entity_dict, dict) and "text" in entity_dict and "label" in entity_dict:
                        # --- 规范化实体文本和标签 ---
                        normalized_entity_text = normalize_text_for_id(entity_dict["text"])
                        # 标签通常已经是大写或特定枚举值，但如果LLM可能返回不同大小写，也应规范化
                        normalized_entity_label = entity_dict["label"].upper() # 保持标签大写
                        extracted_entities_list.append(ExtractedEntity(
                            text=normalized_entity_text, # <--- 使用规范化后的文本
                            label=normalized_entity_label  # <--- 使用规范化/统一后的标签
                        ))
                    else:
                        context.log.warning(f"Skipping malformed entity data in chunk {chunk.chunk_id}: {entity_dict}")
            else:
                context.log.warning(f"'entities' field in SGLang response for chunk {chunk.chunk_id} is not a list: {entities_data}")
            
            total_entities_count += len(extracted_entities_list)

            relations_data = structured_response.get("relations", [])
            extracted_relations_list: List[ExtractedRelation] = []
            if isinstance(relations_data, list):
                for rel_dict in relations_data:
                    if (isinstance(rel_dict, dict) and
                        all(key in rel_dict for key in ["head_entity_text", "head_entity_label", 
                                                        "relation_type", "tail_entity_text", "tail_entity_label"])):
                        # --- 规范化关系中的实体文本和标签 ---
                        normalized_head_text = normalize_text_for_id(rel_dict["head_entity_text"])
                        normalized_head_label = rel_dict["head_entity_label"].upper()
                        normalized_tail_text = normalize_text_for_id(rel_dict["tail_entity_text"])
                        normalized_tail_label = rel_dict["tail_entity_label"].upper()
                        # 关系类型通常也应该是规范的
                        normalized_relation_type = rel_dict["relation_type"].upper()
                        extracted_relations_list.append(ExtractedRelation(
                            head_entity_text=normalized_head_text,
                            head_entity_label=normalized_head_label,
                            relation_type=normalized_relation_type,
                            tail_entity_text=normalized_tail_text,
                            tail_entity_label=normalized_tail_label
                        ))
                    else:
                        context.log.warning(f"Skipping malformed relation data in chunk {chunk.chunk_id}: {rel_dict}")
            else:
                context.log.warning(f"'relations' field in SGLang response for chunk {chunk.chunk_id} is not a list: {relations_data}")
            
            total_relations_count += len(extracted_relations_list)
            
            kg_output = KGTripleSetOutput(
                chunk_id=chunk.chunk_id,
                extracted_entities=extracted_entities_list,
                extracted_relations=extracted_relations_list,
                extraction_model_name=config.sglang_model_name,
                original_chunk_metadata=chunk.chunk_metadata
            )
            all_kg_outputs.append(kg_output)

        except Exception as e:
            context.log.error(f"Failed KG extraction for chunk {chunk.chunk_id}: {e}", exc_info=True) # 添加 exc_info=True
            all_kg_outputs.append(KGTripleSetOutput(
                chunk_id=chunk.chunk_id,
                extraction_model_name=config.sglang_model_name,
                original_chunk_metadata={"error": str(e), **chunk.chunk_metadata}
            ))
            
    context.add_output_metadata(
        metadata={
            "num_chunks_processed": len(text_chunks),
            "total_entities_extracted": total_entities_count,
            "total_relations_extracted": total_relations_count,
            "status": "Success" if len(all_kg_outputs) == len(text_chunks) else "Partial Success"
        }
    )
    return all_kg_outputs

# KuzuDB Concurrency Key (as suggested in the new strategy text file for write assets)
KUZU_WRITE_CONCURRENCY_KEY = "kuzu_write_access"

@dg.asset(
    name="kuzu_schema_initialized",
    description="Ensures KuzuDB is initialized and schema (tables) are ready, and performs DDL/Checkpoint.",
    group_name="kg_building",
    # deps=[], # 依赖通过参数自动推断，或者如果资源不作为参数传入，则不需要显式 deps
    tags={dg.MAX_RUNTIME_SECONDS_TAG: "300"} # KUZU_WRITE_CONCURRENCY_KEY 在 in_process 作业中意义不大
)
def kuzu_schema_initialized_asset(
    context: dg.AssetExecutionContext,
    kuzu_readwrite_db: KuzuDBReadWriteResource # 注入资源
) -> dg.Output[str]:
    db_path_used = kuzu_readwrite_db._resolved_db_path # 可以从资源获取路径信息
    context.log.info(f"Using KuzuDB at: {db_path_used}.")
    
    conn = kuzu_readwrite_db.get_connection() # <--- 从资源获取连接

    try:
        context.log.info("Executing DDL statements for schema creation...")
        schema_ddl_queries = [
            "CREATE NODE TABLE IF NOT EXISTS ExtractedEntity (id_prop STRING, text STRING, label STRING, PRIMARY KEY (id_prop))",
            "CREATE REL TABLE IF NOT EXISTS WorksAt (FROM ExtractedEntity TO ExtractedEntity)",
            "CREATE REL TABLE IF NOT EXISTS AssignedTo (FROM ExtractedEntity TO ExtractedEntity)"
        ]
        for ddl_query in schema_ddl_queries:
            context.log.debug(f"Executing DDL: {ddl_query}")
            conn.execute(ddl_query)
        context.log.info("DDL statements execution completed.")
        
        context.log.info("Executing manual CHECKPOINT.")
        conn.execute("CHECKPOINT;")
        context.log.info("Manual CHECKPOINT completed.")

        context.log.info("Verifying table existence after schema initialization...")
        node_table_names = conn._get_node_table_names()
        rel_tables_info = conn._get_rel_table_names()
        rel_table_names = [info['name'] for info in rel_tables_info]
        all_defined_tables = node_table_names + rel_table_names
        context.log.info(f"All defined tables in KuzuDB: {all_defined_tables}")
        
        required_tables = ["ExtractedEntity", "WorksAt", "AssignedTo"]
        all_found = True
        missing_tables = []
        for tbl in required_tables:
            if tbl not in all_defined_tables:
                all_found = False
                missing_tables.append(tbl)
        
        if all_found:
            context.log.info("Verification SUCCESS: All required tables found in KuzuDB.")
        else:
            raise dg.Failure(f"Schema verification failed. Missing tables: {', '.join(missing_tables)}")

    except Exception as e:
        context.log.error(f"Failed during KuzuDB schema DDL/CHECKPOINT/Verification: {e}", exc_info=True)
        raise dg.Failure(f"KuzuDB schema initialization/verification failed: {e}")
        
    return dg.Output("KuzuDB schema ensured, checkpointed, and verified.", metadata={"db_path": db_path_used, "defined_tables": all_defined_tables})

@dg.asset(
    name="kuzu_entity_nodes",
    description="Stores extracted entities as nodes in KuzuDB knowledge graph.",
    group_name="kg_building",
    # deps=[kuzu_schema_initialized_asset], # 通过参数推断
    tags={dg.MAX_RUNTIME_SECONDS_TAG: "600"}
)
def kuzu_entity_nodes_asset(
    context: dg.AssetExecutionContext,
    kg_extractions: List[KGTripleSetOutput],
    kuzu_schema_initialized: str, # 依赖上游资产的输出
    kuzu_readwrite_db: KuzuDBReadWriteResource # 注入资源
) -> None:
    context.log.info(f"Received {len(kg_extractions)} KG extraction sets to store entities in KuzuDB.")
    context.log.info(f"Upstream kuzu_schema_initialized_asset reported: {kuzu_schema_initialized}")

    if not kg_extractions:
        context.log.warning("No KG extractions received, nothing to store in KuzuDB.")
        context.add_output_metadata(metadata={"nodes_created_or_merged": 0, "status": "No data"})
        return
        
    dml_statements: List[tuple[str, Dict[str, Any]]] = []
    total_nodes_processed = 0

    for kg_output_set in kg_extractions:
        for entity in kg_output_set.extracted_entities:
            # 假设 entity.text 和 entity.label 已经是规范化过的
            text_for_id = entity.text 
            label_for_id = entity.label # 假设已经是大写
            id_prop_input_string = text_for_id + label_for_id # <--- 用于生成哈希的字符串
            entity_id_prop = hashlib.md5(id_prop_input_string.encode('utf-8')).hexdigest()  
            context.log.info(f"NODE CREATION: text='{entity.text}', label='{entity.label}', "
                             f"id_prop_input='{id_prop_input_string}', generated_id_prop='{entity_id_prop}'")
            query = """
                MERGE (e:ExtractedEntity {id_prop: $id_prop})
                ON CREATE SET e.text = $text, e.label = $label 
            """
            params = {
                "id_prop": entity_id_prop,
                "text": entity.text, 
                "label": entity.label
            }
            dml_statements.append((query, params))

    if not dml_statements:
        context.log.info("No valid entities found to store in KuzuDB after processing extractions.")
        context.add_output_metadata(metadata={"nodes_created_or_merged": 0, "status": "No entities to store"})
        return
    
    conn = kuzu_readwrite_db.get_connection() # <--- 从资源获取连接
    try:
        context.log.info(f"Executing {len(dml_statements)} MERGE operations for entities in KuzuDB...")
        executed_count = 0
        for query, params in dml_statements:
            context.log.debug(f"Executing DML: {query} with params: {params}")
            conn.execute(query, parameters=params)
            executed_count +=1
        context.log.info(f"Executed {executed_count} DML statements successfully for entities.")
        
        context.add_output_metadata(
            metadata={
                "nodes_created_or_merged": executed_count, # 使用实际执行数量
                "status": "Success"
            }
        )
    except Exception as e:
        context.log.error(f"Failed to store entities in KuzuDB: {e}", exc_info=True)
        raise dg.Failure(description=f"Failed to store entities in KuzuDB: {str(e)}")

@dg.asset(
    name="kuzu_entity_relations",
    description="Creates relationships in KuzuDB based on extracted KG data.",
    group_name="kg_building",
    deps=[kuzu_entity_nodes_asset.key], # 显式声明对 kuzu_entity_nodes_asset 的依赖
    tags={dg.MAX_RUNTIME_SECONDS_TAG: "600"}
)

def kuzu_entity_relations_asset(
    context: dg.AssetExecutionContext,
    kg_extractions: List[KGTripleSetOutput],
    kuzu_schema_initialized: str, # 依赖 kuzu_schema_initialized_asset 的输出
    kuzu_readwrite_db: KuzuDBReadWriteResource
) -> None:
    context.log.info(f"Received {len(kg_extractions)} KG extraction sets to create relations in KuzuDB.")
    context.log.info(f"Upstream kuzu_schema_initialized_asset reported: {kuzu_schema_initialized}")

    conn = kuzu_readwrite_db.get_connection()

    # --- Schema验证日志 ---
    try:
        context.log.info("Verifying table existence at the START of kuzu_entity_relations_asset...")
        node_tables_at_start = conn._get_node_table_names()
        rel_tables_info_at_start = conn._get_rel_table_names()
        rel_tables_at_start = [info['name'] for info in rel_tables_info_at_start]
        all_tables_at_start_actual_case = node_tables_at_start + rel_tables_at_start
        
        # 我们期望的表名（在DDL中定义的大小写）
        expected_relation_tables_in_schema = ["AssignedTo", "WorksAt"] 

        context.log.info(f"Node tables at start of relations asset: {node_tables_at_start}")
        context.log.info(f"Rel tables at start of relations asset (actual case): {rel_tables_at_start}")
        context.log.info(f"All tables at start of relations asset (actual case): {all_tables_at_start_actual_case}")

        for expected_table in expected_relation_tables_in_schema:
            # 检查时，我们将从数据库获取的表名与期望的表名进行比较
            # KuzuDB 返回的表名是区分大小写的，与 DDL 一致
            if expected_table not in all_tables_at_start_actual_case:
                context.log.error(f"CRITICAL: Expected table '{expected_table}' NOT FOUND at the very start of kuzu_entity_relations_asset on the shared connection!")
            else:
                context.log.info(f"Expected table '{expected_table}' IS PRESENT at the start of kuzu_entity_relations_asset.")
    except Exception as e_verify_start:
        context.log.error(f"Error verifying tables at start of kuzu_entity_relations_asset: {e_verify_start}")
    # --- Schema验证日志结束 ---

    if not kg_extractions:
        context.log.warning("No KG extractions received, nothing to store for relations.")
        context.add_output_metadata(metadata={"relations_created_or_merged": 0, "status": "No data"})
        return

    relations_to_create_params: List[Dict[str, Any]] = []
    relation_type_for_cypher_map: List[str] = [] # 用于存储 Cypher 查询中实际使用的关系表名

    for kg_output_set in kg_extractions:
        for rel_idx, rel in enumerate(kg_output_set.extracted_relations):
            context.log.info(f"Processing relation {rel_idx+1} from chunk {kg_output_set.chunk_id}: "
                             f"{rel.head_entity_text} -[{rel.relation_type}]-> {rel.tail_entity_text}")

            head_text_for_id = rel.head_entity_text
            head_label_for_id = rel.head_entity_label 
            tail_text_for_id = rel.tail_entity_text
            tail_label_for_id = rel.tail_entity_label

            head_id_prop_input_string = head_text_for_id + head_label_for_id
            head_entity_id_prop = hashlib.md5(head_id_prop_input_string.encode('utf-8')).hexdigest()
            
            tail_id_prop_input_string = tail_text_for_id + tail_label_for_id
            tail_entity_id_prop = hashlib.md5(tail_id_prop_input_string.encode('utf-8')).hexdigest()
            
            context.log.info(f"  Attempting to match Head: text='{rel.head_entity_text}', label='{rel.head_entity_label}', "
                             f"id_prop_input='{head_id_prop_input_string}', generated_head_id_prop='{head_entity_id_prop}'")
            context.log.info(f"  Attempting to match Tail: text='{rel.tail_entity_text}', label='{rel.tail_entity_label}', "
                             f"id_prop_input='{tail_id_prop_input_string}', generated_tail_id_prop='{tail_entity_id_prop}'")

            head_found = False
            tail_found = False
            try:
                head_check_result = conn.execute("MATCH (n:ExtractedEntity {id_prop: $id}) RETURN n.text, n.label", {"id": head_entity_id_prop})
                if head_check_result.has_next():
                    head_data = head_check_result.get_next()
                    context.log.info(f"    VERIFIED Head Node: id_prop='{head_entity_id_prop}', text='{head_data[0]}', label='{head_data[1]}'")
                    head_found = True
                else:
                    context.log.error(f"    VERIFICATION FAILED: Head Node with id_prop='{head_entity_id_prop}' (from text='{rel.head_entity_text}') NOT FOUND.")
                head_check_result.close()

                tail_check_result = conn.execute("MATCH (n:ExtractedEntity {id_prop: $id}) RETURN n.text, n.label", {"id": tail_entity_id_prop})
                if tail_check_result.has_next():
                    tail_data = tail_check_result.get_next()
                    context.log.info(f"    VERIFIED Tail Node: id_prop='{tail_entity_id_prop}', text='{tail_data[0]}', label='{tail_data[1]}'")
                    tail_found = True
                else:
                    context.log.error(f"    VERIFICATION FAILED: Tail Node with id_prop='{tail_entity_id_prop}' (from text='{rel.tail_entity_text}') NOT FOUND.")
                tail_check_result.close()
            except Exception as e_verify:
                context.log.error(f"    Error during node verification for relation: {e_verify}")

            if head_found and tail_found:
                # rel.relation_type 应该是从 kg_extraction_asset 传来的全大写形式，例如 "ASSIGNED_TO"
                relation_type_from_extraction = rel.relation_type 
                
                # 将其映射到 DDL 中定义的实际表名（首字母大写）
                actual_cypher_table_name = ""
                if relation_type_from_extraction == "ASSIGNED_TO":
                    actual_cypher_table_name = "AssignedTo"
                elif relation_type_from_extraction == "WORKS_AT":
                    actual_cypher_table_name = "WorksAt"
                
                if actual_cypher_table_name: # 如果是我们支持的关系类型
                    relations_to_create_params.append({
                        "head_id_prop": head_entity_id_prop,
                        "tail_id_prop": tail_entity_id_prop,
                    })
                    relation_type_for_cypher_map.append(actual_cypher_table_name)
                else:
                    context.log.warning(f"  Skipping relation due to unmapped original type: '{relation_type_from_extraction}'")
            else:
                context.log.warning(f"  Skipping relation creation for '{rel.head_entity_text} -[{rel.relation_type}]-> {rel.tail_entity_text}' because one or both nodes were not found with generated id_props.")
            
    if not relations_to_create_params:
        context.log.info("No valid relations to create after node verification and type mapping.")
        context.add_output_metadata(metadata={"relations_created_or_merged": 0, "status": "No valid relations after node/type check"})
        return
            
    executed_count = 0
    try:
        context.log.info(f"Attempting to execute {len(relations_to_create_params)} CREATE operations for relations in KuzuDB...")
        for i, params in enumerate(relations_to_create_params):
            rel_table_for_cypher = relation_type_for_cypher_map[i]
            query = f"""
                MATCH (h:ExtractedEntity {{id_prop: $head_id_prop}}), (t:ExtractedEntity {{id_prop: $tail_id_prop}})
                CREATE (h)-[r:{rel_table_for_cypher}]->(t)
            """
            context.log.debug(f"Executing DML for relation type '{rel_table_for_cypher}': {query} with params: {params}")
            conn.execute(query, parameters=params)
            executed_count += 1
        context.log.info(f"Successfully submitted {executed_count} DML statements for relations.")
        
        context.log.info("Final verification of relation counts after DML execution...")
        final_rel_counts = {}
        unique_relation_types_attempted = set(relation_type_for_cypher_map)
        for rel_type_to_check_in_cypher in unique_relation_types_attempted:
            try:
                count_query_result = conn.execute(f"MATCH ()-[r:{rel_type_to_check_in_cypher}]->() RETURN count(r) AS count")
                count_df = pd.DataFrame(count_query_result.get_as_df())
                actual_count_in_db_raw = count_df['count'].iloc[0] if not count_df.empty else 0
                actual_count_in_db = int(actual_count_in_db_raw) # <--- 修改这里：转换为Python int
                final_rel_counts[rel_type_to_check_in_cypher] = actual_count_in_db
                context.log.info(f"  Relation table '{rel_type_to_check_in_cypher}' final count in DB: {actual_count_in_db}")
            except Exception as e_count_final:
                context.log.error(f"  Error during final count for relation table '{rel_type_to_check_in_cypher}': {e_count_final}")
        context.add_output_metadata(
            metadata={
                "relations_attempted_creation": len(relations_to_create_params),
                "relations_dml_submitted_and_executed_by_kuzu": executed_count, # 更准确的描述
                "final_relation_counts_in_db": final_rel_counts,
                "status": "Success" if executed_count == len(relations_to_create_params) and executed_count > 0 else "Partial Success or Issues" if executed_count > 0 else "No relations created"
            }
        )
    except Exception as e:
        context.log.error(f"Failed during DML execution for relations: {e}", exc_info=True)
        raise dg.Failure(description=f"Failed to create relations in KuzuDB: {str(e)}")
    
# 确保 all_processing_assets 列表正确
all_processing_assets = [
    clean_chunk_text_asset,
    generate_embeddings_asset,
    vector_storage_asset,
    keyword_index_asset,
    kg_extraction_asset,
    kuzu_schema_initialized_asset,
    kuzu_entity_nodes_asset,
    kuzu_entity_relations_asset
]
    --- END OF FILE CONTENT ---

  File: pydantic_models_dagster.py
    --- START OF FILE CONTENT ---
# zhz_rag_pipeline/pydantic_models_dagster.py
from typing import List, Dict, Any, Union, Optional
from pydantic import BaseModel
import uuid
from typing import List # 确保 List 已导入


class LoadedDocumentOutput(BaseModel):
    document_path: str
    file_type: str
    raw_content: Union[str, bytes]
    metadata: Dict[str, Any]

class ParsedDocumentOutput(BaseModel):
    parsed_text: str
    document_structure: Optional[Dict[str, Any]] = None
    original_metadata: Dict[str, Any]

class ChunkOutput(BaseModel):
    chunk_id: str = "" 
    chunk_text: str
    source_document_id: str 
    chunk_metadata: Dict[str, Any]

    def __init__(self, **data: Any):
        if 'chunk_id' not in data or not data['chunk_id']:
            data['chunk_id'] = str(uuid.uuid4())
        super().__init__(**data)

class EmbeddingOutput(BaseModel):
    chunk_id: str 
    chunk_text: str 
    embedding_vector: List[float]
    embedding_model_name: str 
    original_chunk_metadata: Dict[str, Any]

class ExtractedEntity(BaseModel):
    text: str 
    label: str 

class ExtractedRelation(BaseModel):
    head_entity_text: str
    head_entity_label: str # 例如 "PERSON"
    relation_type: str    # 例如 "WORKS_AT"
    tail_entity_text: str
    tail_entity_label: str # 例如 "ORGANIZATION"

class KGTripleSetOutput(BaseModel):
    chunk_id: str 
    extracted_entities: List[ExtractedEntity] = []
    extracted_relations: List[ExtractedRelation] = [] 
    extraction_model_name: str 
    original_chunk_metadata: Dict[str, Any]

    --- END OF FILE CONTENT ---

  File: resources.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/zhz_rag_pipeline/resources.py
import dagster as dg
from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.config import Settings
from typing import List, Dict, Any, Union, Optional, ContextManager, Iterator
import logging
import httpx
import asyncio
import json
# from neo4j import GraphDatabase, Driver, Result # Neo4j不再直接用于此资源
import litellm
import os
import kuzu # 确保导入 kuzu
import shutil
from pydantic import PrivateAttr, Field as PydanticField
from contextlib import contextmanager
import portalocker # <--- 重新导入 portalocker
import time # <--- 导入 time，可能用于短暂等待

# --- SentenceTransformerResource ---
class SentenceTransformerResourceConfig(dg.Config):
    model_name_or_path: str = "/home/zhz/models/bge-small-zh-v1.5"

class SentenceTransformerResource(dg.ConfigurableResource):
    model_name_or_path: str
    _model: SentenceTransformer = PrivateAttr(default=None)
    _logger: Optional[dg.DagsterLogManager] = PrivateAttr(default=None)

    def setup_for_execution(self, context: dg.InitResourceContext) -> None:
        self._logger = context.log
        self._logger.info(f"Initializing SentenceTransformer model from: {self.model_name_or_path}")
        try:
            self._model = SentenceTransformer(self.model_name_or_path)
            self._logger.info("SentenceTransformer model initialized successfully.")
        except Exception as e:
            self._logger.error(f"Failed to initialize SentenceTransformer model: {e}", exc_info=True)
            raise

    def encode(self, texts: List[str], batch_size: int = 32, normalize_embeddings: bool = True) -> List[List[float]]:
        if self._model is None:
            if self._logger:
                self._logger.error("SentenceTransformer model is not initialized in encode method.")
            raise RuntimeError("SentenceTransformer model is not initialized.")
        
        logger_instance = self._logger if self._logger else dg.get_dagster_logger()
        logger_instance.debug(f"Encoding {len(texts)} texts. Normalize embeddings: {normalize_embeddings}")
        
        embeddings_np = self._model.encode(
            texts, 
            batch_size=batch_size, 
            convert_to_tensor=False, 
            normalize_embeddings=normalize_embeddings
        )
        return [emb.tolist() for emb in embeddings_np]

# --- ChromaDBResource ---
class ChromaDBResourceConfig(dg.Config):
    collection_name: str = "rag_documents"
    persist_directory: str = "/home/zhz/zhz_agent/zhz_rag/stored_data/chromadb_index/"

class ChromaDBResource(dg.ConfigurableResource):
    collection_name: str
    persist_directory: str

    _client: chromadb.Client = PrivateAttr(default=None)
    _collection: chromadb.Collection = PrivateAttr(default=None)
    _logger: Optional[dg.DagsterLogManager] = PrivateAttr(default=None)

    def setup_for_execution(self, context: dg.InitResourceContext) -> None:
        self._logger = context.log
        self._logger.info(f"Initializing ChromaDB client and collection '{self.collection_name}'...")
        self._logger.info(f"ChromaDB data will be persisted to: {self.persist_directory}")
        try:
            os.makedirs(self.persist_directory, exist_ok=True)
            self._client = chromadb.PersistentClient(path=self.persist_directory)
            self._collection = self._client.get_or_create_collection(
                name=self.collection_name,
                metadata={"hnsw:space": "cosine"}
            )
            self._logger.info(f"ChromaDB collection '{self.collection_name}' initialized/loaded. Count: {self._collection.count()}")
        except Exception as e:
            self._logger.error(f"Failed to initialize ChromaDB: {e}", exc_info=True)
            raise

    def add_embeddings(self, ids: List[str], embeddings: List[List[float]], metadatas: List[Dict[str, Any]] = None):
        logger_instance = self._logger if self._logger else dg.get_dagster_logger()
        if self._collection is None:
            logger_instance.error("ChromaDB collection is not initialized. Cannot add embeddings.")
            raise RuntimeError("ChromaDB collection is not initialized.")
        
        if not (len(ids) == len(embeddings) and (metadatas is None or len(ids) == len(metadatas))):
            logger_instance.error("Length mismatch for ids, embeddings, or metadatas.")
            raise ValueError("Length of ids, embeddings, and metadatas (if provided) must be the same.")

        if not ids:
            logger_instance.info("No ids provided to add_embeddings, skipping.")
            return

        logger_instance.info(f"Adding/updating {len(ids)} embeddings to ChromaDB collection '{self.collection_name}'...")
        self._collection.add(ids=ids, embeddings=embeddings, metadatas=metadatas)
        logger_instance.info(f"Embeddings added/updated. Collection count now: {self._collection.count()}")

    def query_embeddings(self, query_embeddings: List[List[float]], n_results: int = 5) -> chromadb.QueryResult:
        logger_instance = self._logger if self._logger else dg.get_dagster_logger()
        if self._collection is None:
            logger_instance.error("ChromaDB collection is not initialized. Cannot query embeddings.")
            raise RuntimeError("ChromaDB collection is not initialized.")
        logger_instance.debug(f"Querying ChromaDB collection '{self.collection_name}' with {len(query_embeddings)} vectors, n_results={n_results}.")
        return self._collection.query(query_embeddings=query_embeddings, n_results=n_results)

# --- SGLangAPIResource ---
class SGLangAPIResourceConfig(dg.Config):
    api_url: str = "http://127.0.0.1:30000/generate"
    default_temperature: float = 0.1
    default_max_new_tokens: int = 512

class SGLangAPIResource(dg.ConfigurableResource):
    api_url: str
    default_temperature: float
    default_max_new_tokens: int
    _logger: Optional[dg.DagsterLogManager] = PrivateAttr(default=None)

    def setup_for_execution(self, context: dg.InitResourceContext) -> None:
        self._logger = context.log
        self._logger.info(f"SGLangAPIResource configured with API URL: {self.api_url}")

    async def generate_structured_output(
        self, prompt: str, json_schema: Dict[str, Any],
        temperature: Optional[float] = None, max_new_tokens: Optional[int] = None
    ) -> Dict[str, Any]:
        logger_instance = self._logger if self._logger else dg.get_dagster_logger()
        temp_to_use = temperature if temperature is not None else self.default_temperature
        tokens_to_use = max_new_tokens if max_new_tokens is not None else self.default_max_new_tokens
        payload = {
            "text": prompt,
            "sampling_params": {
                "temperature": temp_to_use,
                "max_new_tokens": tokens_to_use,
                "stop": ["<|im_end|>"],
                "json_schema": json.dumps(json_schema)
            }
        }
        logger_instance.debug(f"Sending request to SGLang. Prompt (start): {prompt[:100]}... Schema: {json.dumps(json_schema)}")
        try:
            async with httpx.AsyncClient(timeout=120.0) as client:
                response = await client.post(self.api_url, json=payload)
                response.raise_for_status()
                response_json = response.json()
                generated_text = response_json.get("text", "").strip()
                logger_instance.debug(f"SGLang raw response text: {generated_text}")
                try:
                    parsed_output = json.loads(generated_text)
                    return parsed_output
                except json.JSONDecodeError as e:
                    logger_instance.error(f"Failed to decode SGLang JSON output: {generated_text}. Error: {e}", exc_info=True)
                    raise ValueError(f"SGLang output was not valid JSON: {generated_text}") from e
        except httpx.HTTPStatusError as e:
            logger_instance.error(f"SGLang API HTTP error: {e.response.status_code} - {e.response.text}", exc_info=True)
            raise
        except httpx.RequestError as e:
            logger_instance.error(f"SGLang API request error: {e}", exc_info=True)
            raise
        except Exception as e:
            logger_instance.error(f"Unexpected error during SGLang call: {e}", exc_info=True)
            raise

# --- GeminiAPIResource ---
class GeminiAPIResourceConfig(dg.Config):
    model_name: str = PydanticField(default="gemini/gemini-1.5-flash-latest", description="Name of the Gemini model.")
    proxy_url: Optional[str] = PydanticField(default_factory=lambda: os.getenv("LITELLM_PROXY_URL"), description="Optional proxy URL for LiteLLM.")
    default_temperature: float = 0.1
    default_max_tokens: int = 2048
    
class GeminiAPIResource(dg.ConfigurableResource):
    model_name: str
    proxy_url: Optional[str]
    default_temperature: float
    default_max_tokens: int
    _api_key: Optional[str] = PrivateAttr(default=None)
    _logger: Optional[dg.DagsterLogManager] = PrivateAttr(default=None)

    def setup_for_execution(self, context: dg.InitResourceContext) -> None:
        self._logger = context.log
        self._api_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
        if self.model_name and not self.model_name.startswith("gemini/"):
            if "gemini" in self.model_name.lower():
                self._logger.info(f"Model name '{self.model_name}' auto-prefixed to 'gemini/'.")
                self.model_name = f"gemini/{self.model_name.split('/')[-1]}"
            else:
                self._logger.warning(f"Model name '{self.model_name}' does not start with 'gemini/'.")
        if not self._api_key:
            self._logger.warning("Gemini API key not found. API calls will likely fail.")
        else:
            self._logger.info(f"GeminiAPIResource initialized. Model: {self.model_name}, Proxy: {self.proxy_url or 'Not set'}")

    async def call_completion(
        self, messages: List[Dict[str, str]],
        temperature: Optional[float] = None, max_tokens: Optional[int] = None,
    ) -> Optional[str]:
        logger_instance = self._logger if self._logger else dg.get_dagster_logger()
        if not self._api_key:
            logger_instance.error("Gemini API key is not configured.")
            return None
        temp_to_use = temperature if temperature is not None else self.default_temperature
        tokens_to_use = max_tokens if max_tokens is not None else self.default_max_tokens
        litellm_params = {
            "model": self.model_name, "messages": messages, "api_key": self._api_key,
            "temperature": temp_to_use, "max_tokens": tokens_to_use,
        }
        if self.proxy_url:
            litellm_params["proxy"] = {"http": self.proxy_url, "https": self.proxy_url} # type: ignore
        logger_instance.debug(f"Calling LiteLLM (Gemini) with params (excluding messages): { {k:v for k,v in litellm_params.items() if k != 'messages'} }")
        raw_output_text: Optional[str] = None
        try:
            response = await litellm.acompletion(**litellm_params) # type: ignore
            if response and response.choices and response.choices[0].message and response.choices[0].message.content:
                raw_output_text = response.choices[0].message.content
                logger_instance.debug(f"LiteLLM (Gemini) raw response (first 300 chars): {raw_output_text[:300]}...")
            else:
                logger_instance.warning(f"LiteLLM (Gemini) returned empty/malformed response: {response}")
        except Exception as e_generic:
            logger_instance.error(f"Error calling Gemini via LiteLLM: {e_generic}", exc_info=True)
        return raw_output_text

# --- KuzuDB Resources (New Strategy Applied) ---
class KuzuDBReadWriteResource(dg.ConfigurableResource):
    db_path_str: str = PydanticField(
        default=os.path.join("zhz_rag", "stored_data", "kuzu_default_db"),
        description=(
            "Path to the KuzuDB database directory. "
            "Can be relative to the project root (if not starting with '/') or absolute."
        )
    )
    clear_on_startup_for_testing: bool = PydanticField(
        default=False, # 生产中通常为 False，测试时可设为 True
        description="If true, delete and re-initialize the DB when the resource is first set up. USE WITH CAUTION."
    )

    _logger: Optional[dg.DagsterLogManager] = PrivateAttr(default=None)
    _resolved_db_path: str = PrivateAttr()
    _db: Optional[kuzu.Database] = PrivateAttr(default=None) # <--- 持有 Database 实例
    _conn: Optional[kuzu.Connection] = PrivateAttr(default=None) # <--- 持有 Connection 实例

    def setup_for_execution(self, context: dg.InitResourceContext) -> None:
        self._logger = context.log
        if os.path.isabs(self.db_path_str):
            self._resolved_db_path = self.db_path_str
        else:
            self._resolved_db_path = os.path.abspath(self.db_path_str)
        
        self._logger.info(
            f"KuzuDBReadWriteResource setup: resolved_path='{self._resolved_db_path}', "
            f"clear_on_startup_for_testing={self.clear_on_startup_for_testing}"
        )

        if self.clear_on_startup_for_testing:
            if os.path.exists(self._resolved_db_path):
                self._logger.warning(f"Clearing KuzuDB directory: {self._resolved_db_path}")
                try:
                    shutil.rmtree(self._resolved_db_path)
                    self._logger.info(f"Successfully removed KuzuDB directory.")
                except OSError as e:
                    self._logger.error(f"Failed to remove KuzuDB directory {self._resolved_db_path}: {e}", exc_info=True)
                    raise
        
        try:
            os.makedirs(os.path.dirname(self._resolved_db_path), exist_ok=True)
            self._db = kuzu.Database(self._resolved_db_path, read_only=False)
            self._conn = kuzu.Connection(self._db)
            self._logger.info(f"KuzuDB Database and Connection initialized successfully at {self._resolved_db_path}.")
        except Exception as e:
            self._logger.error(f"Failed to initialize KuzuDB: {e}", exc_info=True)
            raise

    def teardown_for_execution(self, context: dg.InitResourceContext) -> None:
        self._logger.info(f"Tearing down KuzuDBReadWriteResource for {self._resolved_db_path}...")
        if self._conn is not None:
            # 可以在这里选择性地执行最后的 CHECKPOINT，但通常 KuzuDB 关闭时会处理
            # try:
            #     self._logger.info("Executing final CHECKPOINT on KuzuDB connection before teardown.")
            #     self._conn.execute("CHECKPOINT;")
            #     self._logger.info("Final CHECKPOINT successful.")
            # except Exception as e_chk:
            #     self._logger.error(f"Error during final CHECKPOINT: {e_chk}")
            del self._conn # Kuzu Connection 没有 close()
            self._conn = None
        if self._db is not None:
            del self._db # 依赖 KuzuDB Database 的 __del__ 方法进行清理和锁释放
            self._db = None
        self._logger.info("KuzuDBReadWriteResource teardown complete.")

    def get_connection(self) -> kuzu.Connection:
        """Returns the managed KuzuDB connection."""
        if self._conn is None:
            # 这种错误不应该在 in_process_executor 下发生，因为 setup_for_execution 会先运行
            self._logger.error("KuzuDB connection not available. Resource might not have been set up correctly.")
            raise Exception("KuzuDB connection not available. Resource might not have been set up correctly.")
        return self._conn
            
# KuzuDBReadOnlyResource 定义保持不变
class KuzuDBReadOnlyResource(dg.ConfigurableResource):
    db_path_str: str = PydanticField(
        default=os.path.join("zhz_rag", "stored_data", "kuzu_default_db"),
        description=(
            "Path to the KuzuDB database directory for read-only access. "
            "Can be relative to the project root (if not starting with '/') or absolute."
        )
    )
    _logger: Optional[dg.DagsterLogManager] = PrivateAttr(default=None)
    _resolved_db_path: str = PrivateAttr()

    def setup_for_execution(self, context: dg.InitResourceContext) -> None:
        self._logger = context.log
        if os.path.isabs(self.db_path_str):
            self._resolved_db_path = self.db_path_str
        else:
            self._resolved_db_path = os.path.abspath(self.db_path_str)

        self._logger.info(f"KuzuDBReadOnlyResource setup: resolved_path='{self._resolved_db_path}'")
        if not os.path.exists(self._resolved_db_path):
            self._logger.error(f"KuzuDB path {self._resolved_db_path} does not exist for ReadOnly access. Operations will likely fail.")

    def teardown_for_execution(self, context: dg.InitResourceContext) -> None:
        logger_instance = self._logger if self._logger else dg.get_dagster_logger()
        logger_instance.info("KuzuDBReadOnlyResource teardown complete.")

    @contextmanager
    def get_readonly_connection(self) -> Iterator[kuzu.Connection]:
        logger_instance = self._logger if self._logger else dg.get_dagster_logger()
        db_instance: Optional[kuzu.Database] = None
        logger_instance.info(f"Attempting to open KuzuDB(RO) at {self._resolved_db_path} for readonly session.")
        
        if not os.path.exists(self._resolved_db_path):
            logger_instance.error(f"KuzuDB directory {self._resolved_db_path} not found for read-only access.")
            raise FileNotFoundError(f"KuzuDB directory {self._resolved_db_path} not found for read-only access.")

        try:
            db_instance = kuzu.Database(self._resolved_db_path, read_only=True)
            logger_instance.info(f"KuzuDB(RO) session opened at {self._resolved_db_path}")
            conn = kuzu.Connection(db_instance)
            yield conn
        except Exception as e:
            logger_instance.error(f"Error during KuzuDB(RO) session: {e}", exc_info=True)
            raise
        finally:
            if db_instance:
                del db_instance
                logger_instance.info(f"KuzuDB(RO) Database object for session at {self._resolved_db_path} dereferenced (closed).")
    --- END OF FILE CONTENT ---

  File: 获取当前目录下的所有py格式文件.py
    --- START OF FILE CONTENT ---
import os
import sys
import fnmatch

# --- 配置 ---
# 1. 输出文件的名称
OUTPUT_FILENAME = '现阶段核心代码.txt' # <--- 输出文件名

# 2. 需要包含的文件扩展名 (小写) 或完整文件名
TARGET_ITEMS = (
    '.py',      # Python 脚本
)

# 3. 需要排除的目录名称 (主要用于递归扫描，此处作用较小，但保留以防万一)
EXCLUDE_DIRS = {
    '.git',
    '__pycache__',
    'venv', '.venv', # 虚拟环境
    'tests', 'test',
    '.vscode',
    'node_modules',
    'dist', 'build',
}

# 4. 需要排除的文件名或模式 (使用 fnmatch，不区分大小写)
EXCLUDE_FILES = {
    '.DS_Store',
    '*.log',
    # 脚本自身和输出文件会自动排除
}
# ---

script_dir = os.path.dirname(os.path.abspath(__file__))
# 项目路径现在就是脚本所在的目录
project_path = script_dir
output_file_path = os.path.join(script_dir, OUTPUT_FILENAME)

combined_content = []
processed_files_count = 0

print(f"正在读取 '{project_path}' 目录下的核心 Python 文件...")

# 自动排除脚本自身和输出文件
exclude_files_lower_patterns = {f.lower() for f in EXCLUDE_FILES}
exclude_files_lower_patterns.add(os.path.basename(__file__).lower())
exclude_files_lower_patterns.add(OUTPUT_FILENAME.lower())

exclude_dirs_lower = {d.lower() for d in EXCLUDE_DIRS} # 保留，但在此版本中作用不大

def should_exclude_dir(dir_name_full_path):
    # 此函数在此版本中基本不会被积极使用，因为我们不递归进入子目录
    dir_name = os.path.basename(dir_name_full_path)
    return dir_name.lower() in exclude_dirs_lower

def should_include_file(file_name_full_path):
    file_name = os.path.basename(file_name_full_path)
    file_name_lower = file_name.lower()

    # 检查是否在排除文件列表
    for pattern in exclude_files_lower_patterns:
        if fnmatch.fnmatchcase(file_name_lower, pattern):
            return False # 排除

    # 检查是否匹配目标扩展名或完整文件名
    for target in TARGET_ITEMS:
        if target.startswith('.'): # 是扩展名
            if file_name_lower.endswith(target):
                return True
        else: # 是完整文件名 (虽然我们现在只用扩展名)
            if file_name_lower == target.lower():
                return True
    return False # 不包含

def collect_files_in_current_dir(current_path_abs):
    global processed_files_count
    try:
        items = sorted(os.listdir(current_path_abs))
    except Exception as e:
        print(f"错误：无法读取目录 '{current_path_abs}' 的内容: {e}")
        return

    for item_name in items:
        item_path_abs = os.path.join(current_path_abs, item_name)
        # 对于当前目录的文件，其相对于 project_path (即 current_path_abs) 的路径就是文件名本身
        relative_path_to_project_root = item_name

        if os.path.isfile(item_path_abs): # 只处理文件
            if should_include_file(item_path_abs):
                print(f"  正在添加: {relative_path_to_project_root}")
                processed_files_count += 1
                combined_content.append(f"--- START OF FILE {relative_path_to_project_root.replace(os.sep, '/')} ---")
                try:
                    with open(item_path_abs, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read()
                        combined_content.append(content)
                except Exception as e:
                    print(f"  *** 警告：读取文件 {relative_path_to_project_root} 时出错: {e} ***")
                    combined_content.append(f"*** ERROR READING FILE {relative_path_to_project_root}: {e} ***")
                combined_content.append(f"--- END OF FILE {relative_path_to_project_root.replace(os.sep, '/')} ---")
                combined_content.append("\n\n")
        # 我们不再递归进入子目录，所以移除了 os.path.isdir 的检查和递归调用

# 执行收集
collect_files_in_current_dir(project_path) # project_path 就是 script_dir

# --- 保存合并后的内容到文件 ---
final_output = "".join(combined_content)

if processed_files_count > 0:
    try:
        with open(output_file_path, 'w', encoding='utf-8') as outfile:
            outfile.write(final_output)
        print(f"\n成功！已将 {processed_files_count} 个 Python 文件的合并内容保存到文件: {output_file_path}")
    except Exception as e:
        print(f"\n错误：无法将内容写入文件 '{output_file_path}': {e}")
        sys.exit(1)
else:
    print(f"\n在当前目录 '{project_path}' 中未找到任何符合条件的 Python 文件。请检查 TARGET_ITEMS 设置。")
    --- END OF FILE CONTENT ---


Directory: zhz_rag_pipeline_dagster/zhz_rag_pipeline_dagster_project.egg-info/
  File (skipped, extension not included): PKG-INFO
  File: SOURCES.txt
    --- START OF FILE CONTENT ---
pyproject.toml
setup.py
zhz_rag_pipeline/__init__.py
zhz_rag_pipeline/custom_io_managers.py
zhz_rag_pipeline/definitions.py
zhz_rag_pipeline/evaluation_assets.py
zhz_rag_pipeline/ingestion_assets.py
zhz_rag_pipeline/processing_assets.py
zhz_rag_pipeline/pydantic_models_dagster.py
zhz_rag_pipeline/resources.py
zhz_rag_pipeline/获取当前目录下的所有py格式文件.py
zhz_rag_pipeline_dagster_project.egg-info/PKG-INFO
zhz_rag_pipeline_dagster_project.egg-info/SOURCES.txt
zhz_rag_pipeline_dagster_project.egg-info/dependency_links.txt
zhz_rag_pipeline_dagster_project.egg-info/requires.txt
zhz_rag_pipeline_dagster_project.egg-info/top_level.txt
    --- END OF FILE CONTENT ---

  File: dependency_links.txt
    --- START OF FILE CONTENT ---


    --- END OF FILE CONTENT ---

  File: requires.txt
    --- START OF FILE CONTENT ---
dagster
dagster-webserver

    --- END OF FILE CONTENT ---

  File: top_level.txt
    --- START OF FILE CONTENT ---
zhz_rag_pipeline

    --- END OF FILE CONTENT ---


Directory: zhz_rag/
  File: __init__.py
    --- File is empty ---

  File (skipped, extension not included): setup - 副本.py:Zone.Identifier
  File: setup.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/zhz_rag/setup.py
from setuptools import find_packages, setup

setup(
    name="zhz_rag_core",
    version="0.1.0",
    packages=find_packages(),
    install_requires=[
        # Pydantic 版本由主 requirements.txt 控制
        # LiteLLM 版本由主 requirements.txt 控制
        # ChromaDB 版本由主 requirements.txt 控制

        "protobuf>=4.25.0,<5.30.0", # 放宽 protobuf 上限，因为 pydantic 2.11.5 可能需要较新的
        "packaging>=23.2,<25.0",
        "rich>=13.7.0,<14.0.0",
        
        "fastapi>=0.110.0,<0.116.0", # 保持较新
        "starlette>=0.35.0,<0.47.0", # 保持较新

        "langchain-core>=0.1.50,<0.4.0", # 较新 langchain 可能更好兼容
        "langchain-text-splitters>=0.0.1,<0.3.0",

        "httpx>=0.27.0", # 使用较新 httpx
        "python-dotenv>=1.0.0",
        "neo4j>=5.0.0", # neo4j 驱动
        "sentence-transformers>=2.2.0", # sentence-transformers
        "transformers>=4.38.0,<4.39.0", # 固定您之前的版本或小幅更新
        "torch>=2.0.0",
        "numpy<2.0", # 保持 Numpy < 2.0
        "bm25s",
        "jieba",
        "uvicorn[standard]", # 添加 standard extras
        "pandas>=2.0.0",
        "sqlalchemy>=2.0.0",
        "databases[aiosqlite]>=0.9.0", # for async sqlite
        "apscheduler>=3.10.0",
        "pytz",
    ],
)
    --- END OF FILE CONTENT ---


Directory: zhz_rag/crewai_integration/
  File: __init__.py
    --- File is empty ---

  File: run_crew.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/run_agent.py

import os
import json
import datetime

from crewai import Agent, Task, Crew, Process

# --- 导入我们自己的项目模块 (使用绝对导入) ---
from zhz_rag.crewai_integration.tools import HybridRAGTool, BaseMCPTool
from zhz_rag.config.pydantic_models import QueryRequest # 用于 RAG 工具的输入
from zhz_rag.utils.common_utils import call_mcpo_tool
from zhz_rag.llm.custom_crewai_llms import CustomGeminiLLM

# --- 环境配置 ---
from dotenv import load_dotenv
load_dotenv()

# --- CrewAI 基类和事件系统 ---
from crewai.tools import BaseTool
from pydantic import BaseModel, Field
from typing import Any, Dict, List, Optional, Type
from crewai.llms.base_llm import BaseLLM as CrewAIBaseLLM
try:
    from crewai.utilities.events.base_event_listener import BaseEventListener as CrewAIBaseCallbackHandler
    from crewai.utilities.events import LLMCallStartedEvent, LLMCallCompletedEvent
    print("Successfully imported BaseEventListener and Event Types")
except ImportError:
    print("Failed to import BaseEventListener or Event Types, using dummy classes.")
    class CrewAIBaseCallbackHandler: pass
    class LLMCallStartedEvent: pass
    class LLMCallCompletedEvent: pass

# --- LiteLLM ---
import litellm

# --- 定义简单工具以供测试 ---
class SimpleToolInput(BaseModel):
    message: str = Field(description="A simple message string for the tool.")

class MySimpleTestTool(BaseTool):
    name: str = "MySimpleTestTool"
    description: str = "A very simple test tool that takes a message and returns it."
    args_schema: Type[BaseModel] = SimpleToolInput

    def _run(self, message: str) -> str:
        print(f"MySimpleTestTool received: {message}")
        return f"MySimpleTestTool processed: {message}"

# --- 配置 Agent 使用的 LLM 实例 ---
GEMINI_MODEL_NAME = "gemini/gemini-1.5-flash-latest"
GEMINI_API_KEY = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")

if not GEMINI_API_KEY:
    print("CRITICAL ERROR: GOOGLE_API_KEY or GEMINI_API_KEY not set.")
    exit(1)

# --- 定义详细的事件监听器 ---
class MyDetailedLogger(CrewAIBaseCallbackHandler):
    def __init__(self):
        super().__init__()
        print(f"[{datetime.datetime.now()}] MyDetailedLogger: 已初始化。")

    def setup_listeners(self, crewai_event_bus):
        print(f"[{datetime.datetime.now()}] MyDetailedLogger: 正在设置监听器...")

        @crewai_event_bus.on(LLMCallStartedEvent)
        def handle_llm_start(source, event: LLMCallStartedEvent):
            self.on_llm_start_logic(source, event)

        @crewai_event_bus.on(LLMCallCompletedEvent)
        def handle_llm_completed(source, event: LLMCallCompletedEvent):
            self.on_llm_end_logic(source, event)

        print(f"[{datetime.datetime.now()}] MyDetailedLogger: 监听器设置完成。")

    def on_llm_start_logic(self, source, event: LLMCallStartedEvent):
        print(f"\n>>>> LLM 调用开始 (Event Logic) <<<<")
        llm_inputs = getattr(event, 'llm_inputs', {})
        messages = llm_inputs.get('messages')
        tools = llm_inputs.get('tools')
        print(f"来源 (Source): {source}")
        if messages:
            print("消息 (来自 event.llm_inputs):")
            if isinstance(messages, list) and len(messages) > 0:
                first_message = messages[0]
                if isinstance(first_message, dict) and 'content' in first_message:
                    content_snippet = str(first_message.get('content', ''))[:300]
                    print(f"   Role: {first_message.get('role')}, Content Snippet: {content_snippet}...")
                else:
                     print(f"  First message (raw): {first_message}")
            else:
                 print(f"  Messages (raw): {messages}")
        else:
            print("消息 (来自 event.llm_inputs): 无")
        if tools:
            print("工具 (来自 event.llm_inputs):")
            try:
                print(f"  {json.dumps(tools, indent=2, ensure_ascii=False)}")
            except Exception as e:
                print(f"  无法序列化工具为 JSON: {e}. 工具: {tools}")
        else:
            print("工具 (来自 event.llm_inputs): 无")
        print("----------------------------------")

    def on_llm_end_logic(self, source, event: LLMCallCompletedEvent):
        print(f"\n>>>> LLM 调用结束 (Event Logic) <<<<")
        response = getattr(event, 'llm_output', None)
        print(f"来源 (Source): {source}")
        if response:
            if hasattr(response, 'choices') and response.choices:
                choice = response.choices[0]
                if hasattr(choice, 'message') and choice.message:
                    print(f"  消息内容: {choice.message.content}")
                    if hasattr(choice.message, 'tool_calls') and choice.message.tool_calls:
                        print(f"  工具调用: {choice.message.tool_calls}")
                    else:
                        print(f"  工具调用: 无")
            elif hasattr(response, 'content'):
                print(f"  响应内容: {response.content}")
            else:
                print(f"  LLM 响应 (来自 event.llm_output): {str(response)[:500]}...")
        else:
            print("  在 event.llm_output 中未找到响应对象。")
        print("----------------------------------")

# --- 实例化 CustomGeminiLLM ---
custom_llm_tool_config = {"function_calling_config": {"mode": "AUTO"}}
zhz_agent_tool = HybridRAGTool()
researcher_tools = [zhz_agent_tool]

llm_for_agent = CustomGeminiLLM(
    model=GEMINI_MODEL_NAME,
    api_key=GEMINI_API_KEY,
    temperature=0.1,
    max_tokens=2048,
    tool_config=custom_llm_tool_config,
    agent_tools=researcher_tools # 传递工具列表以供缓存
)
print(f"Custom Agent LLM configured: {GEMINI_MODEL_NAME} with custom tool_config")

# --- 设置 BaseMCPTool 的调用器 ---
BaseMCPTool.set_mcpo_caller(call_mcpo_tool)

# --- 定义 Agents ---
researcher_agent = Agent(
    role='信息检索专家',
    goal='准确地回答用户查询，并且只使用提供的工具。',
    backstory=(
        "你是一位高级AI助手，专注于信息检索。"
        "你的专长在于高效地利用工具来查找最相关和最精确的答案来回应用户的查询。"
    ),
    llm=llm_for_agent,
    tools=researcher_tools,
    verbose=True,
    allow_delegation=False,
)

writer_agent = Agent(
    role='报告撰写专家',
    goal='根据提供的信息，撰写清晰、结构良好且富有洞察力的报告。',
    backstory=(
        "您是一位资深的报告撰写专家，拥有出色的分析和写作能力。"
        "您擅长将复杂的信息提炼成易于理解的报告，并能根据不同的输出状态（答案、澄清、错误）"
        "灵活调整报告内容和格式。"
    ),
    llm=llm_for_agent,
    verbose=True,
    allow_delegation=False,
)

# --- 定义 Tasks (包含上下文传递修复) ---
research_task_description = """你收到了来自用户的以下查询：

'{query}'

你应该使用提供的 `HybridRAGQueryTool` 工具来处理这个查询。
如果这个工具需要 `top_k_vector`, `top_k_kg`, 或 `top_k_bm25` 这些参数，请使用以下建议值：
top_k_vector: 5, top_k_kg: 3, top_k_bm25: 5。
在使用完必要的工具后，你的最终输出应该是（使用中文）：'我的最终答案是：' 
后跟你使用的工具返回的精确、完整的JSON字符串输出内容。"""

research_task_expected_output = "短语 '我的最终答案是：' 后跟你使用的工具返回的精确、完整的JSON字符串输出内容。"

research_task = Task(
    description=research_task_description,
    expected_output=research_task_expected_output,
    agent=researcher_agent,
)

report_writing_task = Task(
    description="""根据【前一个任务】（信息检索专家）提供的RAG工具输出（它是一个JSON字符串），生成一份报告或响应。
请仔细分析这个JSON字符串输出，它应该包含一个 'status' 字段。
1. 如果 'status' 是 'success'，则提取 'final_answer' 字段的内容，并基于此答案撰写一份简洁的报告。
2. 如果 'status' 是 'clarification_needed'，则提取 'clarification_question' 字段的内容，并向用户明确指出需要澄清的问题，例如：'系统需要澄清：[澄清问题]'。
3. 如果 'status' 是 'error'，则提取 'error_message' (或 'error') 字段的内容，并向用户报告错误，例如：'RAG服务发生错误：[错误信息]'。
你的最终输出必须是清晰、专业且符合上述情况的报告或响应。""",
    expected_output="一份清晰的报告，或者一个明确的澄清请求，或者一个错误报告。",
    agent=writer_agent,
    context=[research_task],
)

# --- 实例化监听器 ---
my_event_logger = MyDetailedLogger()

# --- 定义 Crew (添加 event_listeners) ---
office_brain_crew = Crew(
    agents=[researcher_agent, writer_agent],
    tasks=[research_task, report_writing_task],
    process=Process.sequential,
    verbose=True,
    event_listeners=[my_event_logger] # <<< --- 激活事件监听器 ---
)

# --- 启动 Crew ---
if __name__ == "__main__":
    print("--- 启动智能助手终端大脑 Crew (使用 CustomGeminiLLM 和事件监听器) ---")
    user_query_input = "公司2024年第一季度在华东和华北的总销售额一共是多少？"
    # --- 修复：kickoff inputs 只包含 query ---
    inputs = {'query': user_query_input}
    result = office_brain_crew.kickoff(inputs=inputs)
    print("\n\n=== 最终报告 ===\n")
    if hasattr(result, 'raw'):
        print(result.raw)
    else:
        print(result)
    print("\n--- Crew 任务完成 ---")
    --- END OF FILE CONTENT ---

  File: tools.py
    --- START OF FILE CONTENT ---
# zhz_agent/custom_crewai_tools.py

import os
import json
import asyncio
import traceback
from typing import Type, List, Dict, Any, Optional, ClassVar
from pydantic import BaseModel, Field
from crewai.tools import BaseTool
import httpx

# 从 zhz_agent.pydantic_models 导入 QueryRequest
from zhz_rag.config.pydantic_models import QueryRequest # 用于 RAG 工具的输入

# MCPO 代理的基地址
MCPO_BASE_URL = os.getenv("MCPO_BASE_URL", "http://localhost:8006")

class BaseMCPTool(BaseTool):
    mcpo_base_url: str = MCPO_BASE_URL
    _call_mcpo_func: ClassVar[callable] = None

    @classmethod
    def set_mcpo_caller(cls, caller: callable):
        cls._call_mcpo_func = caller

    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    async def _call_mcpo_endpoint(self, service_and_tool_path: str, payload: dict) -> dict | str:
        api_url = f"{self.mcpo_base_url}/{service_and_tool_path}"
        cleaned_payload = {k: v for k, v in payload.items() if v is not None}
        print(f"BaseMCPTool: Calling mcpo endpoint: POST {api_url} with payload: {json.dumps(cleaned_payload, ensure_ascii=False)}")
        
        # --- [修改] 移除 proxies=None 参数 ---
        async with httpx.AsyncClient(trust_env=False) as client:
            response = None
            try:
                headers = {"Content-Type": "application/json"}
                response = await client.post(api_url, json=cleaned_payload, headers=headers, timeout=300.0)
                print(f"BaseMCPTool: mcpo status code for {service_and_tool_path}: {response.status_code}")
                print(f"BaseMCPTool: mcpo response headers for {service_and_tool_path}: {response.headers}") # <--- 新增日志
                # 尝试分块读取响应或提前获取少量内容进行日志记录，以防响应过大卡住 .text 或 .json()
                try:
                    response_text_snippet = await response.aread(num_bytes=1024) # 读取前1KB
                    print(f"BaseMCPTool: mcpo response text snippet (first 1KB) for {service_and_tool_path}: {response_text_snippet.decode(errors='ignore')}")
                except Exception as e_read:
                    print(f"BaseMCPTool: Error reading response snippet: {e_read}")

                if response.status_code == 200:
                    try:
                        # print(f"BaseMCPTool: mcpo raw response text for {service_and_tool_path}: {response.text}") # 如果怀疑内容问题，可以取消注释，但小心大响应
                        return response.json()
                    except json.JSONDecodeError:
                        print(f"BaseMCPTool Warning: mcpo returned status 200 but response is not JSON for '{service_and_tool_path}'. Returning raw text.")
                        return response.text
                else:
                    error_text = f"mcpo call to '{service_and_tool_path}' failed with status {response.status_code}. Response: {response.text[:500]}..."
                    print(f"BaseMCPTool Error: {error_text}")
                    return {"error": error_text, "status_code": response.status_code}
            except httpx.RequestError as exc:
                error_msg = f"BaseMCPTool HTTP RequestError calling mcpo tool '{service_and_tool_path}': {type(exc).__name__} - {exc}"
                print(f"BaseMCPTool Error: {error_msg}"); traceback.print_exc()
                return {"error": error_msg, "exception_type": type(exc).__name__}
            except Exception as exc:
                error_msg = f"BaseMCPTool Unexpected error calling mcpo tool '{service_and_tool_path}': {type(exc).__name__} - {exc}"
                response_text_snippet = response.text[:500] if response and hasattr(response, 'text') else "Response object not available or no text."
                print(f"BaseMCPTool Error: {error_msg}. Response snippet: {response_text_snippet}"); traceback.print_exc()
                return {"error": error_msg, "exception_type": type(exc).__name__}

    def _handle_tool_result(self, result: dict | str, tool_name_for_log: str) -> str:
        print(f"BaseMCPTool DEBUG: {tool_name_for_log} result from mcpo: {str(result)[:500]}...")
        parsed_result = result
        if isinstance(result, str):
            try:
                parsed_result = json.loads(result)
            except json.JSONDecodeError:
                if "error" in result.lower() or "failed" in result.lower() or "traceback" in result.lower():
                    return f"调用 {tool_name_for_log} 失败，返回非JSON错误文本: {result}"
                print(f"BaseMCPTool Info: Result for {tool_name_for_log} is a non-JSON string, returning as is.")
                return result
        if isinstance(parsed_result, dict):
            if "error" in parsed_result and "status_code" in parsed_result:
                return f"调用 {tool_name_for_log} 时发生HTTP错误：{parsed_result.get('error')}"
            if parsed_result.get("status") == "error":
                error_msg = parsed_result.get("error_message", "未知错误")
                error_code = parsed_result.get("error_code", "NO_CODE")
                return f"工具 {tool_name_for_log} 执行失败 (错误码: {error_code})：{error_msg}"
            try:
                return json.dumps(parsed_result, ensure_ascii=False, indent=2)
            except Exception as e:
                print(f"BaseMCPTool Error formatting successful dict result for {tool_name_for_log}: {e}")
                return str(parsed_result)
        print(f"BaseMCPTool Warning: Unexpected result format from {tool_name_for_log} mcpo call: {type(result)}, content: {str(result)[:200]}")
        return f"从 {tool_name_for_log} 服务收到的结果格式不正确或无法处理: {str(result)[:500]}"

    def _run_default_sync_wrapper(self, **kwargs) -> str:
        tool_name = getattr(self, 'name', self.__class__.__name__)
        print(f"BaseMCPTool INFO: Synchronous _run called for {tool_name} with args: {kwargs}.")
        result_str = ""
        try:
            # --- 改进的 asyncio.run 处理 ---
            try:
                loop = asyncio.get_running_loop()
            except RuntimeError:
                loop = None

            async def async_runner():
                return await self._arun(**kwargs)

            if loop and loop.is_running():
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                    future = executor.submit(asyncio.run, async_runner())
                    result = future.result(timeout=120)
            else:
                result = asyncio.run(async_runner())
            result_str = str(result)
        except asyncio.TimeoutError:
            error_message = f"Tool {tool_name} execution timed out after 120 seconds."
            print(f"BaseMCPTool ERROR_TRACE: {error_message}"); result_str = error_message
        except RuntimeError as e:
            if "cannot run event loop while another loop is running" in str(e).lower() or "event loop is already running" in str(e).lower():
                error_message = (f"BaseMCPTool Error in {tool_name} _run: Nested asyncio event loop conflict. Original error: {e}")
            else:
                error_message = f"BaseMCPTool RuntimeError in {tool_name} _run: {type(e).__name__} - {e}"
            print(f"BaseMCPTool ERROR_TRACE: {error_message}"); traceback.print_exc();
            result_str = error_message
        except Exception as e:
            error_message = f"BaseMCPTool General Exception in {tool_name} _run: {type(e).__name__} - {e}"
            print(f"BaseMCPTool ERROR_TRACE: {error_message}"); traceback.print_exc(); result_str = error_message
        return result_str

class HybridRAGTool(BaseMCPTool):
    name: str = "HybridRAGQueryTool"
    description: str = (
        "【核心RAG工具】用于通过执行混合检索增强生成 (RAG) 搜索来回答用户问题。 "
        "该工具整合了向量检索、知识图谱检索和关键词检索，并进行智能融合和重排序。 "
        "当用户需要从知识库中获取信息、回答复杂问题或生成报告时，应调用此工具。"
    )
    args_schema: Type[BaseModel] = QueryRequest
    target_mcp_service_path: str = "zhz_agent_rag_service/query_rag_v2"

    async def _arun(self, query: str, top_k_vector: int, top_k_kg: int, top_k_bm25: int, **kwargs: Any) -> str: # --- 确认有 **kwargs ---
        tool_name_for_log = getattr(self, 'name', "HybridRAGTool")
        print(f"CrewAI Tool DEBUG: {tool_name_for_log}._arun called with query='{query}', top_k_vector={top_k_vector}, top_k_kg={top_k_kg}, top_k_bm25={top_k_bm25}, additional_kwargs={kwargs}")

        security_context = kwargs.get('security_context')
        if security_context:
            print(f"CrewAI Tool INFO: Received security_context (in HybridRAGTool): {str(security_context)[:200]}...")

        payload = {
            "query": query,
            "top_k_vector": top_k_vector,
            "top_k_kg": top_k_kg,
            "top_k_bm25": top_k_bm25
        }
        result = await self._call_mcpo_endpoint(self.target_mcp_service_path, payload)
        return self._handle_tool_result(result, self.name)

    def _run(self, query: str, top_k_vector: int, top_k_kg: int, top_k_bm25: int, **kwargs: Any) -> str: # --- 确认有 **kwargs ---
        return self._run_default_sync_wrapper(query=query, top_k_vector=top_k_vector, top_k_kg=top_k_kg, top_k_bm25=top_k_bm25, **kwargs)
    --- END OF FILE CONTENT ---


Directory: zhz_rag/config/
  File: constants.py
    --- START OF FILE CONTENT ---
# zhz_agent/zhz_rag/config/constants.py

NEW_KG_SCHEMA_DESCRIPTION = """
# 知识图谱结构 (KuzuDB) 与 Cypher 查询生成规则

## 1. 节点定义:
- 节点标签: `:ExtractedEntity` (你必须且只能使用此节点标签)
- 节点属性:
    - `id_prop`: STRING (主键，实体的唯一标识)
    - `text`: STRING (实体的名称或文本内容)
    - `label`: STRING (实体类型。允许的值: "PERSON", "ORGANIZATION", "TASK")

## 2. 关系定义:
- 关系类型: `:WorksAt`
    - 结构: `(:ExtractedEntity {label:"PERSON"}) -[:WorksAt]-> (:ExtractedEntity {label:"ORGANIZATION"})`
    - 含义: 一个人 (PERSON) 在一个组织 (ORGANIZATION) 工作。
- 关系类型: `:AssignedTo`
    - 结构: `(:ExtractedEntity {label:"TASK"}) -[:AssignedTo]-> (:ExtractedEntity {label:"PERSON"})`
    - 含义: 一个任务 (TASK) 被分配给一个人 (PERSON)。

## 3. Cypher 查询生成 - 输出为 JSON 对象:

    你的【完整且唯一】的回答，必须是一个包含 "status" 和 "query" 字段的JSON对象。
    - 如果你能根据用户问题和Schema生成一个有效的Cypher查询：
        - "status" 字段应为 "success"。
        - "query" 字段应为该Cypher查询字符串。
    - 如果你无法生成有效的Cypher查询：
        - "status" 字段应为 "unable_to_generate"。
        - "query" 字段应为 "无法生成Cypher查询."。
    【不要在JSON之外或query字段内（当status为success时）包含任何解释或额外文本。】

## 4. JSON 输出格式示例:

### 示例 1 (能够生成查询):
用户问题: "任务'FixBug123'分配给了谁？"
你的【完整且唯一】的 JSON 回答:
```json
{
  "status": "success",
  "query": "MATCH (t:ExtractedEntity {text: 'FixBug123', label: 'TASK'})-[:AssignedTo]->(p:ExtractedEntity {label: 'PERSON'}) RETURN p.text AS Assignee"
}
示例 2 (无法根据Schema回答):
用户问题: "法国的首都是哪里？"
你的【完整且唯一】的 JSON 回答:
{
  "status": "unable_to_generate",
  "query": "无法生成Cypher查询."
}
"""
    --- END OF FILE CONTENT ---

  File: pydantic_models.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/zhz_rag/config/pydantic_models.py
from pydantic import BaseModel, Field, root_validator # 使用 root_validator 替代 model_validator
from typing import List, Dict, Any, Optional
from enum import Enum
from datetime import datetime
import uuid

# --- RAG Models ---
class QueryRequest(BaseModel):
    query: str = Field(description="用户提出的原始查询文本。", json_schema_extra=lambda schema: schema.pop('default', None) if schema.get('default') is None else None)
    top_k_vector: int = Field(description="期望检索的向量搜索结果数量。", json_schema_extra=lambda schema: schema.pop('default', None) if schema.get('default') is None else None)
    top_k_kg: int = Field(description="期望检索的知识图谱结果数量。", json_schema_extra=lambda schema: schema.pop('default', None) if schema.get('default') is None else None)
    top_k_bm25: int = Field(description="期望检索的 BM25 关键词搜索结果数量。", json_schema_extra=lambda schema: schema.pop('default', None) if schema.get('default') is None else None)

    @root_validator(pre=True)
    @classmethod
    def remove_internal_params(cls, values: Dict[str, Any]) -> Dict[str, Any]:
        if isinstance(values, dict):
            # print(f"Pydantic DEBUG (QueryRequest before validation): Received data for validation: {str(values)[:500]}") # 可以取消注释以进行调试
            removed_security_context = values.pop('security_context', None)
            if removed_security_context:
                # print(f"Pydantic INFO (QueryRequest before validation): Removed 'security_context': {str(removed_security_context)[:100]}")
                pass
            removed_agent_fingerprint = values.pop('agent_fingerprint', None)
            if removed_agent_fingerprint:
                # print(f"Pydantic INFO (QueryRequest before validation): Removed 'agent_fingerprint': {str(removed_agent_fingerprint)[:100]}")
                pass
        return values

    class Config:
        extra = 'forbid'

class RetrievedDocument(BaseModel):
    source_type: str
    content: str
    score: Optional[float] = Field(default=None, json_schema_extra=lambda schema: schema.pop('default', None) if schema.get('default') is None else None)
    metadata: Optional[Dict[str, Any]] = Field(default=None, json_schema_extra=lambda schema: schema.pop('default', None) if schema.get('default') is None else None)

class HybridRAGResponse(BaseModel):
    original_query: str
    answer: str
    retrieved_sources: List[RetrievedDocument]
    debug_info: Optional[Dict[str, Any]] = Field(default=None, json_schema_extra=lambda schema: schema.pop('default', None) if schema.get('default') is None else None)


# --- Task Management Models ---
class TaskStatus(str, Enum):
    PENDING = "pending"
    ACTIVE = "active"
    COMPLETED = "completed"
    CANCELLED = "cancelled"
    FAILED = "failed"
    REMINDING = "reminding"

class ReminderMethod(str, Enum):
    NOTIFICATION = "notification"

class TaskModel(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid.uuid4()), description="任务的唯一ID (自动生成)")
    title: str = Field(description="任务标题")
    description: Optional[str] = Field(None, description="任务的详细描述")
    status: TaskStatus = Field(default=TaskStatus.PENDING, description="任务当前状态")
    created_at: datetime = Field(default_factory=datetime.utcnow, description="任务创建时间 (UTC)")
    updated_at: datetime = Field(default_factory=datetime.utcnow, description="任务最后更新时间 (UTC)")
    due_date: Optional[datetime] = Field(None, description="任务截止日期或计划执行时间 (UTC)")
    reminder_time: Optional[datetime] = Field(None, description="任务提醒时间 (UTC)")
    reminder_offset_minutes: Optional[int] = Field(None, description="提醒时间相对于due_date的提前分钟数 (例如10分钟前)")
    reminder_methods: List[ReminderMethod] = Field(default=[ReminderMethod.NOTIFICATION], description="提醒方式列表")
    priority: int = Field(default=0, description="任务优先级 (例如 0:普通, 1:重要, 2:紧急)")
    tags: List[str] = Field(default_factory=list, description="任务标签")
    action_type: Optional[str] = Field(None, description="任务到期时需要执行的动作类型 (例如 'navigate', 'send_message', 'run_report')")
    action_payload: Dict[str, Any] = Field(default_factory=dict, description="执行动作时需要的参数 (例如导航的目的地)")
    execution_result: Optional[str] = Field(None, description="任务执行后的结果或错误信息")
    last_executed_at: Optional[datetime] = Field(None, description="上次执行时间 (UTC)")

    class Config:
        use_enum_values = True
        orm_mode = True # Pydantic V1 中使用 orm_mode = True 替代 from_attributes

class CreateTaskRequest(BaseModel):
    title: str
    description: Optional[str] = None
    due_date: Optional[datetime] = None
    reminder_offset_minutes: Optional[int] = None
    reminder_methods: Optional[List[ReminderMethod]] = [ReminderMethod.NOTIFICATION]
    priority: Optional[int] = 0
    tags: Optional[List[str]] = None
    action_type: Optional[str] = None
    action_payload: Optional[Dict[str, Any]] = None
    
    class Config:
        extra = 'forbid'

class UpdateTaskRequest(BaseModel):
    title: Optional[str] = None
    description: Optional[str] = None
    status: Optional[TaskStatus] = None
    due_date: Optional[datetime] = None
    reminder_offset_minutes: Optional[int] = None
    reminder_methods: Optional[List[ReminderMethod]] = None
    priority: Optional[int] = None
    tags: Optional[List[str]] = None
    action_type: Optional[str] = None
    action_payload: Optional[Dict[str, Any]] = None

    class Config:
        extra = 'forbid'
    --- END OF FILE CONTENT ---


Directory: zhz_rag/zhz_rag_core.egg-info/
  File (skipped, extension not included): PKG-INFO
  File: SOURCES.txt
    --- START OF FILE CONTENT ---
setup.py
api/__init__.py
api/main_api.py
api/rag_mcp_service.py
api/task_manager_api.py
core_rag/__init__.py
core_rag/fusion_engine.py
core_rag/kg_retriever.py
core_rag/retrievers/__init__.py
core_rag/retrievers/chromadb_retriever.py
core_rag/retrievers/file_bm25_retriever.py
crewai_integration/__init__.py
crewai_integration/run_crew.py
crewai_integration/tools.py
evaluation/__init__.py
evaluation/analyze_answer.py
evaluation/analyze_cypher.py
evaluation/batch_eval_answer.py
evaluation/batch_eval_cypher.py
evaluation/evaluator.py
finetuning/__init__.py
finetuning/refine_answer_data.py
finetuning/refine_cypher_data.py
llm/__init__.py
llm/custom_crewai_llms.py
llm/sglang_wrapper.py
stored_data/__init__.py
task_management/__init__.py
task_management/db_models.py
task_management/jobs.py
zhz_rag_core.egg-info/PKG-INFO
zhz_rag_core.egg-info/SOURCES.txt
zhz_rag_core.egg-info/dependency_links.txt
zhz_rag_core.egg-info/requires.txt
zhz_rag_core.egg-info/top_level.txt
    --- END OF FILE CONTENT ---

  File: dependency_links.txt
    --- START OF FILE CONTENT ---


    --- END OF FILE CONTENT ---

  File: requires.txt
    --- START OF FILE CONTENT ---
protobuf<5.30.0,>=4.25.0
packaging<25.0,>=23.2
rich<14.0.0,>=13.7.0
fastapi<0.116.0,>=0.110.0
starlette<0.47.0,>=0.35.0
langchain-core<0.4.0,>=0.1.50
langchain-text-splitters<0.3.0,>=0.0.1
httpx>=0.27.0
python-dotenv>=1.0.0
neo4j>=5.0.0
sentence-transformers>=2.2.0
transformers<4.39.0,>=4.38.0
torch>=2.0.0
numpy<2.0
bm25s
jieba
uvicorn[standard]
pandas>=2.0.0
sqlalchemy>=2.0.0
databases[aiosqlite]>=0.9.0
apscheduler>=3.10.0
pytz

    --- END OF FILE CONTENT ---

  File: top_level.txt
    --- START OF FILE CONTENT ---
api
core_rag
crewai_integration
evaluation
finetuning
llm
stored_data
task_management

    --- END OF FILE CONTENT ---


Directory: zhz_rag/utils/
  File (skipped, extension not included): ZHZ_AGENT_tasks.db
  File: common_utils.py
    --- START OF FILE CONTENT ---
# zhz_rag/utils/common_utils.py

import httpx
import json
import traceback
import os
import glob
from dotenv import load_dotenv
from datetime import datetime, timezone
import uuid
import logging
import asyncio #确保 asyncio 被导入
from typing import List, Dict, Any, Optional
import re
import unicodedata
import logging

load_dotenv()

# --- Logger Configuration ---
utils_logger = logging.getLogger("UtilsLogger")
utils_logger.setLevel(logging.INFO)
if not utils_logger.hasHandlers():
    _utils_console_handler = logging.StreamHandler()
    _utils_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
    _utils_console_handler.setFormatter(_utils_formatter)
    utils_logger.addHandler(_utils_console_handler)
    utils_logger.propagate = False
utils_logger.info("--- UtilsLogger configured ---")

# --- MCP Configuration ---
MCPO_BASE_URL = os.getenv("MCPO_BASE_URL", "http://localhost:8006")

_CURRENT_FILE_DIR = os.path.dirname(os.path.abspath(__file__))
_ZHZ_RAG_PACKAGE_DIR = os.path.dirname(_CURRENT_FILE_DIR)

STORED_DATA_ROOT_DIR = os.path.join(_ZHZ_RAG_PACKAGE_DIR, 'stored_data')

RAG_INTERACTION_LOGS_DIR = os.path.join(STORED_DATA_ROOT_DIR, 'rag_interaction_logs')
EVALUATION_RESULTS_LOGS_DIR = os.path.join(STORED_DATA_ROOT_DIR, 'evaluation_results_logs')

FINETUNING_GENERATED_DATA_DIR = os.path.join(_ZHZ_RAG_PACKAGE_DIR, 'finetuning', 'generated_data')

# Ensure these directories exist
_DIRECTORIES_TO_CREATE = [
    STORED_DATA_ROOT_DIR,
    RAG_INTERACTION_LOGS_DIR,
    EVALUATION_RESULTS_LOGS_DIR,
    FINETUNING_GENERATED_DATA_DIR
]
for dir_path in _DIRECTORIES_TO_CREATE:
    if not os.path.exists(dir_path):
        try:
            os.makedirs(dir_path, exist_ok=True)
            utils_logger.info(f"Successfully created directory: {dir_path}")
        except Exception as e:
            utils_logger.error(f"Error creating directory {dir_path}: {e}. Consider creating it manually.")

# --- Log File Path Getters ---

def get_interaction_log_filepath() -> str:
    """Gets the full path for the current RAG interaction log file (daily rotation)."""
    today_str = datetime.now(timezone.utc).strftime("%Y%m%d")
    return os.path.join(RAG_INTERACTION_LOGS_DIR, f"rag_interactions_{today_str}.jsonl")

def get_evaluation_result_log_filepath(evaluation_name: str) -> str:
    """Gets the full path for an evaluation result log file (daily rotation, by evaluation name)."""
    today_str = datetime.now(timezone.utc).strftime("%Y%m%d")
    return os.path.join(EVALUATION_RESULTS_LOGS_DIR, f"eval_results_{evaluation_name}_{today_str}.jsonl")

def find_latest_rag_interaction_log(log_dir: str = RAG_INTERACTION_LOGS_DIR) -> Optional[str]:
    """
    Finds the latest RAG interaction log file (rag_interactions_*.jsonl) in the specified directory.
    Defaults to RAG_INTERACTION_LOGS_DIR.
    """
    utils_logger.debug(f"Searching for RAG interaction logs in: {log_dir}")
    rag_log_pattern = os.path.join(log_dir, "rag_interactions_*.jsonl")
    candidate_rag_logs = glob.glob(rag_log_pattern)

    if candidate_rag_logs:
        candidate_rag_logs.sort(key=os.path.getmtime, reverse=True)
        utils_logger.info(f"Automatically selected RAG interaction log: {candidate_rag_logs[0]}")
        return candidate_rag_logs[0]
    else:
        utils_logger.warning(f"No RAG interaction log files found matching pattern: {rag_log_pattern} in directory {log_dir}")
        return None

# --- Logging Function ---

async def log_interaction_data(
    interaction_data: Dict[str, Any],
    is_evaluation_result: bool = False,
    evaluation_name_for_file: Optional[str] = None
):
    """
    Asynchronously appends a single interaction data or evaluation result to a JSONL file.
    """
    if is_evaluation_result:
        if not evaluation_name_for_file:
            evaluation_name_for_file = interaction_data.get("task_type", "general_eval_result") # More specific default
        filepath = get_evaluation_result_log_filepath(evaluation_name=evaluation_name_for_file)
    else:
        filepath = get_interaction_log_filepath()

    if "timestamp_utc" not in interaction_data:
        interaction_data["timestamp_utc"] = datetime.now(timezone.utc).isoformat()
    if "interaction_id" not in interaction_data and not is_evaluation_result: # Eval results use original_interaction_id_ref
        interaction_data["interaction_id"] = str(uuid.uuid4())
    elif "interaction_id" not in interaction_data and is_evaluation_result and "original_interaction_id_ref" in interaction_data:
        # For eval results, ensure there's an ID, can be a new one for the eval log entry itself
         interaction_data["interaction_id"] = str(uuid.uuid4())

    try:
        # --- 新增 DEBUG 日志 ---
        print(f"COMMON_UTILS_LOG_DATA: Preparing to dump JSON. Keys in interaction_data: {list(interaction_data.keys())}")
        if "final_context_docs_full" in interaction_data:
            print(f"COMMON_UTILS_LOG_DATA: 'final_context_docs_full' IS PRESENT before dumps.")
            if interaction_data["final_context_docs_full"]:
                 print(f"COMMON_UTILS_LOG_DATA: 'final_context_docs_full' is NOT EMPTY before dumps. Length: {len(interaction_data['final_context_docs_full'])}")
                 try:
                    first_content = interaction_data["final_context_docs_full"][0].get("content", "CONTENT_KEY_MISSING")
                    print(f"COMMON_UTILS_LOG_DATA: First content in final_context_docs_full: {str(first_content)[:50]}...")
                 except:
                    pass # 简单忽略打印错误
            else:
                print(f"COMMON_UTILS_LOG_DATA: 'final_context_docs_full' IS EMPTY LIST before dumps.")
        else:
            print(f"COMMON_UTILS_LOG_DATA: 'final_context_docs_full' KEY IS MISSING before dumps!")
        # --- 结束新增 DEBUG 日志 ---

        def _write_sync():
            log_file_dir = os.path.dirname(filepath)
            if not os.path.exists(log_file_dir):
                try:
                    os.makedirs(log_file_dir, exist_ok=True)
                    # utils_logger.info(f"Created directory for log file: {log_file_dir}") # 使用print替代，避免日志级别问题
                    print(f"COMMON_UTILS_LOG_DATA: Created directory for log file: {log_file_dir}")
                except Exception as e_mkdir:
                    # utils_logger.error(f"Error creating directory {log_file_dir} for log file: {e_mkdir}")
                    print(f"COMMON_UTILS_LOG_DATA: Error creating directory {log_file_dir} for log file: {e_mkdir}")
                    return 

            with open(filepath, 'a', encoding='utf-8') as f:
                json_string_to_write = json.dumps(interaction_data, ensure_ascii=False, default=str)

                # --- 新增 DEBUG 日志 ---
                print(f"COMMON_UTILS_LOG_DATA: JSON string to write (first 300 chars): {json_string_to_write[:300]}...")
                if "\"final_context_docs_full\"" not in json_string_to_write: # 检查序列化后的字符串
                    print(f"COMMON_UTILS_LOG_DATA: CRITICAL! 'final_context_docs_full' NOT IN JSON string after dumps!")
                # --- 结束新增 DEBUG 日志 ---
                
                f.write(json_string_to_write + "\n")
        
        await asyncio.to_thread(_write_sync)
        # utils_logger.debug(f"Successfully logged data (type: {interaction_data.get('task_type', 'N/A')}) to {filepath}")
    except Exception as e:
        utils_logger.error(f"Failed to log interaction data to {filepath}: {e}", exc_info=True)

# --- MCP Tool Calling Utility ---

async def call_mcpo_tool(tool_name_with_prefix: str, payload: Dict[str, Any]) -> Dict[str, Any]:
    """
    异步调用MCP工具服务，并返回结构化的成功或错误响应。
    """
    api_url = f"{MCPO_BASE_URL}/{tool_name_with_prefix}"
    cleaned_payload = {k: v for k, v in (payload or {}).items() if v is not None}

    utils_logger.info(f"CALL_MCPO_TOOL: Attempting to call {api_url}") # 使用 utils_logger
    utils_logger.debug(f"CALL_MCPO_TOOL: Payload: {json.dumps(cleaned_payload, ensure_ascii=False)}") # 使用 utils_logger

    timeout_config = httpx.Timeout(120.0, connect=10.0, read=120.0, write=10.0) 
    
    async with httpx.AsyncClient(timeout=timeout_config) as client:
        response: Optional[httpx.Response] = None 
        try:
            headers = {
                "Content-Type": "application/json",
                "Accept": "application/json",
                "User-Agent": "ZhzAgent/1.0 (call_mcpo_tool)"
            }
            utils_logger.debug(f"CALL_MCPO_TOOL: Sending POST request to {api_url} with headers: {headers}") # 使用 utils_logger
            response = await client.post(api_url, json=cleaned_payload, headers=headers)
            
            utils_logger.info(f"CALL_MCPO_TOOL: Response from {api_url} - Status: {response.status_code}") # 使用 utils_logger
            utils_logger.debug(f"CALL_MCPO_TOOL: Response Headers: {response.headers}") # 使用 utils_logger
            
            try:
                response_text_snippet = response.text[:500] 
                utils_logger.debug(f"CALL_MCPO_TOOL: Response Text Snippet (first 500 chars): {response_text_snippet}") # 使用 utils_logger
            except Exception as e_read_snippet:
                utils_logger.warning(f"CALL_MCPO_TOOL: Could not read response text snippet: {e_read_snippet}") # 使用 utils_logger

            response.raise_for_status() 

            try:
                result_data = response.json()
                utils_logger.info(f"CALL_MCPO_TOOL: Successfully received and parsed JSON response from {api_url}.") # 使用 utils_logger
                if isinstance(result_data, dict) and result_data.get("isError"):
                    error_content_list = result_data.get("content", [{"type": "text", "text": "Unknown error from MCP tool"}])
                    error_text_from_mcp_payload = "Unknown error from MCP tool"
                    for item in error_content_list:
                        if item.get("type") == "text":
                            error_text_from_mcp_payload = item.get("text", error_text_from_mcp_payload)
                            break
                    utils_logger.error(f"CALL_MCPO_TOOL: MCP Tool '{tool_name_with_prefix}' reported an application-level error (isError=true): {error_text_from_mcp_payload}") # 使用 utils_logger
                    return {
                        "success": False,
                        "error": f"MCP tool '{tool_name_with_prefix}' reported failure: {error_text_from_mcp_payload}",
                        "error_type": "MCP_APPLICATION_ERROR",
                        "status_code": response.status_code
                    }
                return {
                        "success": True, 
                        "data": result_data 
                }
            except json.JSONDecodeError as e_json_decode:
                utils_logger.error(f"CALL_MCPO_TOOL: Response from {api_url} was 2xx but not valid JSON. Error: {e_json_decode}", exc_info=True) # 使用 utils_logger
                return {
                    "success": False,
                    "error": "MCP service returned a 2xx status but the response was not valid JSON.",
                    "error_type": "JSON_DECODE_ERROR",
                    "status_code": response.status_code,
                    "raw_response_snippet": response.text[:500] if response else "N/A"
                }

        except httpx.HTTPStatusError as exc_http_status:
            error_message = f"HTTP Error {exc_http_status.response.status_code} when calling {api_url}."
            utils_logger.error(f"CALL_MCPO_TOOL: {error_message} Response: {exc_http_status.response.text[:500]}", exc_info=True) # 使用 utils_logger
            error_detail_from_response = exc_http_status.response.text
            try:
                parsed_error_json = exc_http_status.response.json()
                if isinstance(parsed_error_json, dict) and "detail" in parsed_error_json:
                    error_detail_from_response = parsed_error_json["detail"]
                elif isinstance(parsed_error_json, dict) and "error" in parsed_error_json: 
                    error_detail_from_response = parsed_error_json["error"]
            except json.JSONDecodeError:
                pass 
            return {
                "success": False,
                "error": f"HTTP error from MCP service: {error_detail_from_response}",
                "error_type": "HTTP_STATUS_ERROR",
                "status_code": exc_http_status.response.status_code,
                "raw_response_snippet": exc_http_status.response.text[:500] if exc_http_status.response else "N/A"
            }
        except httpx.TimeoutException as exc_timeout:
            utils_logger.error(f"CALL_MCPO_TOOL: Timeout when calling {api_url}. Error: {exc_timeout}", exc_info=True) # 使用 utils_logger
            return {
                "success": False,
                "error": f"Request to MCP service timed out after {timeout_config.read if timeout_config else 'default'}s.",
                "error_type": "TIMEOUT_ERROR",
                "status_code": None 
            }
        except httpx.ConnectError as exc_connect:
            utils_logger.error(f"CALL_MCPO_TOOL: Connection error when calling {api_url}. Is the MCP service running at {MCPO_BASE_URL}? Error: {exc_connect}", exc_info=True) # 使用 utils_logger
            return {
                "success": False,
                "error": f"Could not connect to MCP service at {MCPO_BASE_URL}.",
                "error_type": "CONNECTION_ERROR",
                "status_code": None
            }
        except httpx.RequestError as exc_request_other: 
            utils_logger.error(f"CALL_MCPO_TOOL: Network request error when calling {api_url}. Error: {exc_request_other}", exc_info=True) # 使用 utils_logger
            return {
                "success": False,
                "error": f"A network request error occurred: {str(exc_request_other)}",
                "error_type": type(exc_request_other).__name__,
                "status_code": None
            }
        except Exception as exc_unexpected:
            utils_logger.error(f"CALL_MCPO_TOOL: Unexpected error when calling {api_url}. Error: {exc_unexpected}", exc_info=True) # 使用 utils_logger
            return {
                "success": False,
                "error": f"An unexpected error occurred during MCP call: {str(exc_unexpected)}",
                "error_type": type(exc_unexpected).__name__,
                "status_code": response.status_code if response else None, 
                "traceback": traceback.format_exc() 
            }

def load_jsonl_file(filepath: str, encoding: str = 'utf-8') -> List[Dict[str, Any]]:
    """
    从 JSONL 文件加载数据。

    Args:
        filepath (str): JSONL 文件的路径。
        encoding (str): 文件编码，默认为 'utf-8'。

    Returns:
        List[Dict[str, Any]]: 从文件中加载的字典列表。如果文件不存在或解析出错，
                              会记录错误并返回空列表。
    """
    data_list: List[Dict[str, Any]] = []
    if not os.path.exists(filepath):
        utils_logger.error(f"File not found: {filepath}") # 使用已有的 utils_logger
        return data_list

    try:
        with open(filepath, 'r', encoding=encoding) as f:
            for line_number, line in enumerate(f, 1):
                try:
                    if line.strip(): # 确保行不是空的
                        data_list.append(json.loads(line.strip()))
                except json.JSONDecodeError as e_json:
                    utils_logger.warning(f"Skipping malformed JSON line {line_number} in {filepath}: {e_json}")
                except Exception as e_line:
                    utils_logger.warning(f"Error processing line {line_number} in {filepath}: {e_line}")
    except FileNotFoundError: # 再次捕获以防万一，虽然上面已经检查了
        utils_logger.error(f"File not found during open: {filepath}")
    except Exception as e_file:
        utils_logger.error(f"Error reading or processing file {filepath}: {e_file}", exc_info=True)
        return [] # 如果文件读取层面发生严重错误，返回空列表

    utils_logger.info(f"Successfully loaded {len(data_list)} entries from {filepath}")
    return data_list


def normalize_text_for_id(text: str) -> str:
    if not isinstance(text, str):
        return str(text) 
    
    try:
        normalized_text = unicodedata.normalize('NFKD', text)
        normalized_text = normalized_text.lower()
        normalized_text = normalized_text.strip()
        normalized_text = re.sub(r'\s+', ' ', normalized_text)
        return normalized_text
    except Exception as e:
        return text
    --- END OF FILE CONTENT ---

  File: db_utils.py
    --- START OF FILE CONTENT ---
# ZHZ_AGENT/database.py
import os
from databases import Database
from sqlalchemy import create_engine
from sqlalchemy.orm import declarative_base
from typing import Optional

# --- APScheduler 相关导入 ---
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.jobstores.sqlalchemy import SQLAlchemyJobStore
# --- [修改] 明确导入并使用 pytz ---
import pytz #

# --- 数据库配置 ---
ZHZ_AGENT_DIR = os.path.dirname(os.path.abspath(__file__))
DB_NAME = "ZHZ_AGENT_tasks.db"
DATABASE_FILE_PATH = os.path.join(ZHZ_AGENT_DIR, DB_NAME)
DATABASE_URL = f"sqlite+aiosqlite:///{DATABASE_FILE_PATH}"

database = Database(DATABASE_URL)
sqlalchemy_engine = create_engine(DATABASE_URL.replace("+aiosqlite", ""))
Base = declarative_base() #

# --- 全局调度器实例定义 ---
scheduler: Optional[AsyncIOScheduler] = None

def get_scheduler() -> AsyncIOScheduler:
    """获取或创建调度器实例，并配置作业存储和 UTC 时区。"""
    global scheduler
    if scheduler is None:
        jobstore_url = f"sqlite:///{DATABASE_FILE_PATH}"
        jobstores = {
            'default': SQLAlchemyJobStore(url=jobstore_url, tablename='apscheduler_jobs_v2') #
        }
        # --- [修复] 明确使用 pytz.utc 设置时区 ---
        scheduler = AsyncIOScheduler(
            jobstores=jobstores,
            timezone=pytz.utc # <--- 强制使用 pytz.utc #
        )
        import logging
        logging.getLogger('apscheduler').setLevel(logging.DEBUG)
        print(f"APScheduler initialized with timezone: {pytz.utc}") # 确认使用 pytz.utc #
    return scheduler #
    --- END OF FILE CONTENT ---

  File: load_neo4j_data.py
    --- START OF FILE CONTENT ---
# zhz_agent/load_neo4j_data.py
import json
import os
from neo4j import GraphDatabase, basic_auth
from dotenv import load_dotenv
import traceback

load_dotenv() # 确保加载 .env 文件中的NEO4J凭证

NEO4J_URI = os.getenv("NEO4J_URI", "bolt://localhost:7687")
NEO4J_USER = os.getenv("NEO4J_USER", "neo4j")
NEO4J_PASSWORD = os.getenv("NEO4J_PASSWORD") # 您需要确保这个密码是正确的

DATA_PATH = os.path.join(os.path.dirname(__file__), "data")
SAMPLE_KG_PATH = os.path.join(DATA_PATH, "sample_kg.json")

def clear_database(driver):
    """清除数据库中的所有节点和关系"""
    with driver.session(database="neo4j") as session:
        session.run("MATCH (n) DETACH DELETE n")
        print("Cleared all nodes and relationships from the database.")

def create_constraints(driver):
    """创建一些基本约束，确保节点属性的唯一性（如果适用）"""
    with driver.session(database="neo4j") as session:
        try:
            session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (p:Person) REQUIRE p.name IS UNIQUE")
            session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (pr:Project) REQUIRE pr.name IS UNIQUE")
            session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (r:Region) REQUIRE r.name IS UNIQUE")
            session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (prod:Product) REQUIRE prod.name IS UNIQUE")
            session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (d:Document) REQUIRE d.id IS UNIQUE")
            # SalesAmount 通常不需要唯一约束，因为它可能重复（例如不同区域同一时期的销售）
            print("Ensured constraints are created (or already exist).")
        except Exception as e:
            print(f"Error creating constraints: {e}")


def load_data(driver, kg_data):
    """根据kg_data中的facts加载数据到Neo4j"""
    facts = kg_data.get("facts", [])
    
    with driver.session(database="neo4j") as session:
        entities_to_create = set()
        node_types_from_schema = { # 定义了主要实体的标签和它们的主要标识属性
            "Person": "name", "Project": "name", "Region": "name", 
            "Product": "name", "Document": "id", "Idea": "name" # 新增Idea类型
        }

        for fact in facts:
            subject_name = fact.get("subject")
            object_name = fact.get("object")
            fact_type = fact.get("type", "")

            subject_label = None
            # 基于fact_type或其他逻辑推断subject_label
            if "person_" in fact_type: subject_label = "Person"
            elif "region_" in fact_type: subject_label = "Region"
            elif "product_" in fact_type: subject_label = "Product"
            # ... 其他类型的映射 ...
            
            if subject_label and subject_name:
                prop_name = node_types_from_schema.get(subject_label, "name")
                entities_to_create.add((subject_label, prop_name, subject_name))

            object_label = None
            if not fact_type.endswith("_amount"): # 不是直接的销售额事实
                if "_project" in fact_type: object_label = "Project"
                elif "_product" in fact_type: object_label = "Product"
                elif "_document" in fact_type: object_label = "Document"
                elif "_idea" in fact_type: object_label = "Idea" # 新增对Idea类型的处理
                # ... 其他类型的映射 ...

                if object_label and object_name:
                    prop_name = node_types_from_schema.get(object_label, "name") # Document会用id, Idea会用name
                    entities_to_create.add((object_label, prop_name, object_name))
        
        for label, prop, value in entities_to_create:
            if value is not None:
                query = f"MERGE (n:{label} {{{prop}: $value}})"
                session.run(query, value=value)
                print(f"Merged node: ({label} {{{prop}: '{value}'}})")

        for fact in facts:
            s_name = fact.get("subject")
            rel = fact.get("relation")
            o_name = fact.get("object")
            fact_type = fact.get("type", "")
            period = fact.get("period")

            if fact_type == "region_sales_amount" and period:
                session.run("MERGE (r:Region {name: $s_name})", s_name=s_name)
                try:
                    # ... (销售额解析逻辑不变) ...
                    if isinstance(o_name, str) and '万元' in o_name:
                        numeric_val_str = o_name.replace('万元', '').strip()
                        numeric_val = float(numeric_val_str)
                        unit_val = '万元'
                    # ... (其他单位解析) ...
                    else:
                        numeric_val = float(o_name) # 尝试直接转换
                        unit_val = None 
                    
                    query = """
                    MATCH (r:Region {name: $s_name})
                    CREATE (sa:SalesAmount {numeric_amount: $num_val, period: $period, unit: $unit_val})
                    CREATE (r)-[:HAS_SALES_AMOUNT]->(sa)
                    """
                    session.run(query, s_name=s_name, num_val=numeric_val, period=period, unit_val=unit_val)
                    print(f"Created SalesAmount for {s_name}, {period}: {numeric_val} {unit_val or ''}")
                except ValueError:
                    print(f"Could not parse sales amount: {o_name} for {s_name}, {period}. Skipping this SalesAmount fact.")
                
            elif s_name and rel and o_name: 
                s_label, o_label = None, None
                s_prop, o_prop = "name", "name" 

                # --- 更精确的标签和属性推断 ---
                if fact_type == "person_project" and rel == "WORKS_ON":
                    s_label, o_label = "Person", "Project"
                elif fact_type == "person_idea" and rel == "PROPOSED_IDEA": # 新增
                    s_label, o_label = "Person", "Idea"
                elif fact_type == "region_product" and rel == "HAS_SALES_PRODUCT": # 假设type是 region_product
                    s_label, o_label = "Region", "Product"
                elif fact_type == "product_document" and rel == "RELATED_TO":
                    s_label, o_label = "Product", "Document"
                    o_prop = "id" # Document用id匹配
                # 您可以根据您的fact_type添加更多精确的映射规则

                if s_label and o_label:
                    query = f"""
                    MATCH (s:{s_label} {{{s_prop}: $s_name}})
                    MATCH (o:{o_label} {{{o_prop}: $o_name}})
                    MERGE (s)-[:{rel}]->(o)
                    """
                    session.run(query, s_name=s_name, o_name=o_name)
                    print(f"Merged relationship: ({s_label} {{{s_prop}:'{s_name}'}})-[:{rel}]->({o_label} {{{o_prop}:'{o_name}'}})")
                else:
                    print(f"Could not determine labels for fact: {fact} (s_label: {s_label}, o_label: {o_label}). Relationship not created.")
            else:
                print(f"Skipping incomplete fact: {fact}")


if __name__ == "__main__":
    driver = None
    try:
        driver = GraphDatabase.driver(NEO4J_URI, auth=basic_auth(NEO4J_USER, NEO4J_PASSWORD))
        driver.verify_connectivity()
        print("Successfully connected to Neo4j.")
        
        clear_database(driver) # 清空数据库
        create_constraints(driver) # 创建约束

        with open(SAMPLE_KG_PATH, 'r', encoding='utf-8') as f:
            kg_data_to_load = json.load(f)
        
        load_data(driver, kg_data_to_load)
        
        print("\nData loading process completed.")
        print("You can now verify the data in Neo4j Browser (http://localhost:7474).")
        print("Example query to check SalesAmount:")
        print("MATCH (r:Region)-[:HAS_SALES_AMOUNT]->(sa:SalesAmount) RETURN r.name, sa.numeric_amount, sa.unit, sa.period")
        print("Example query to check Person-Project:")
        print("MATCH (p:Person)-[:WORKS_ON]->(proj:Project) RETURN p.name, proj.name")

    except Exception as e:
        print(f"An error occurred: {e}")
        traceback.print_exc()
    finally:
        if driver:
            driver.close()
            print("Neo4j connection closed.")
    --- END OF FILE CONTENT ---


Directory: zhz_rag/core_rag/
  File: __init__.py
    --- START OF FILE CONTENT ---
# zhz_agent/core_rag/__init__.py
from .kg_retriever import KGRetriever
from .fusion_engine import FusionEngine
# 如果上面 retrievers/__init__.py 也做了导出，这里也可以考虑是否进一步导出
    --- END OF FILE CONTENT ---

  File: fusion_engine.py
    --- START OF FILE CONTENT ---
# zhz_agent/fusion.py
import hashlib
import jieba
import torch
import asyncio
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from typing import List, Dict, Any, Optional
import logging 
import os

# 从项目内部导入pydantic_models
from zhz_rag.config.pydantic_models import RetrievedDocument

class FusionEngine:
    _current_script_path = os.path.abspath(__file__)
    _script_directory = os.path.dirname(_current_script_path)

    # 确保您的模型路径指向正确的位置，如果不在 local_models/bge-reranker-base
    LOCAL_RERANKER_MODEL_PATH = os.getenv(
        "RERANKER_MODEL_PATH", 
        "/home/zhz/models/bge-reranker-base" # <--- 直接指定新的、统一管理后的模型路径
    )

    def __init__(self, logger: Optional[logging.Logger] = None):
        if logger:
            self.logger = logger
        else:
            self.logger = logging.getLogger("FusionEngineLogger")
            if not self.logger.hasHandlers():
                self.logger.setLevel(logging.INFO)
                self.logger.info("FusionEngine initialized with its own basic logger (no handlers configured by default).")
            else:
                self.logger.info("FusionEngine initialized, re-using existing logger configuration for FusionEngineLogger.")

        self.reranker_tokenizer: Optional[AutoTokenizer] = None
        self.reranker_model: Optional[AutoModelForSequenceClassification] = None
        self.reranker_device = "cuda" if torch.cuda.is_available() else "cpu"
        self._load_reranker_model()

    def _load_reranker_model(self):
        self.logger.info(f"FusionEngine: Loading reranker model from: {self.LOCAL_RERANKER_MODEL_PATH} to {self.reranker_device}...")
        
        if not os.path.isdir(self.LOCAL_RERANKER_MODEL_PATH): # 检查是否是目录
            _error_msg_model_path = f"Error: Reranker model local path does not exist or is not a directory: {self.LOCAL_RERANKER_MODEL_PATH}."
            self.logger.error(_error_msg_model_path)
            # 在实际应用中，这里可能应该抛出异常，或者让服务无法启动
            # 为了测试，我们先允许模型为空，后续调用会检查
            self.reranker_model = None
            self.reranker_tokenizer = None
            return # 提前返回

        try:
            self.reranker_tokenizer = AutoTokenizer.from_pretrained(self.LOCAL_RERANKER_MODEL_PATH)
            self.reranker_model = AutoModelForSequenceClassification.from_pretrained(self.LOCAL_RERANKER_MODEL_PATH)
            self.reranker_model.to(self.reranker_device)

            if self.reranker_device == 'cuda' and hasattr(self.reranker_model, 'half'): # 检查是否有half方法
                self.reranker_model.half()
                self.logger.info("FusionEngine: Reranker model loaded to GPU and using FP16.")
            else:
                self.logger.info(f"FusionEngine: Reranker model loaded to {self.reranker_device}.")
            
            self.reranker_model.eval()
            self.logger.info("FusionEngine: Reranker model loading successful!")
        except Exception as e:
            self.logger.error(f"Error: Reranker model loading failed: {e}", exc_info=True)
            self.reranker_tokenizer = None
            self.reranker_model = None

    def _rerank_documents_sync(self, query: str, documents: List[RetrievedDocument]) -> List[RetrievedDocument]:
        if not self.reranker_model or not self.reranker_tokenizer:
            self.logger.warning("FusionEngine: Reranker model not loaded. Cannot perform fine-grained reranking. Returning documents as is (or after basic sort if any).")
            # 可以选择返回原始顺序，或者按原始分数（如果可比）排序
            return sorted(documents, key=lambda doc: doc.score if doc.score is not None else -float('inf'), reverse=True)


        if not documents:
            self.logger.info("FusionEngine: No documents to rerank.")
            return []

        # 确保文档内容是字符串
        pairs = []
        valid_documents_for_reranking = []
        for doc in documents:
            if isinstance(doc.content, str):
                pairs.append([query, doc.content])
                valid_documents_for_reranking.append(doc)
            else:
                self.logger.warning(f"FusionEngine: Document with non-string content skipped for reranking. ID: {doc.metadata.get('chunk_id', 'N/A')}, Type: {type(doc.content)}")
        
        if not pairs:
            self.logger.info("FusionEngine: No valid document pairs for reranking after content check.")
            return []

        self.logger.info(f"FusionEngine: Reranking {len(valid_documents_for_reranking)} documents with CrossEncoder...")
        
        try:
            with torch.no_grad():
                inputs = self.reranker_tokenizer(
                    pairs, 
                    padding=True, 
                    truncation=True, 
                    return_tensors='pt', 
                    max_length=512 # BGE Reranker通常是512
                ).to(self.reranker_device)
                
                logits = self.reranker_model(**inputs).logits
                # CrossEncoder通常直接输出一个分数，而不是需要sigmoid/softmax
                # BGE-Reranker输出的是logit，可以直接用作分数，或者通过sigmoid转为概率（但不必要）
                scores = logits.view(-1).float().cpu().numpy()

            for i, doc in enumerate(valid_documents_for_reranking):
                doc.score = float(scores[i]) # 更新文档的score为reranker的打分

            reranked_docs = sorted(valid_documents_for_reranking, key=lambda doc: doc.score, reverse=True)
            self.logger.info(f"FusionEngine: Reranking complete. {len(reranked_docs)} documents sorted.")
            return reranked_docs
        except Exception as e_rerank_detail:
            self.logger.error(f"FusionEngine: Detailed error during reranking with CrossEncoder: {e_rerank_detail}", exc_info=True)
            # 如果重排序失败，返回按原始分数（如果可比）排序的文档，或者简单返回valid_documents_for_reranking
            return sorted(valid_documents_for_reranking, key=lambda d: d.score if d.score is not None else -float('inf'), reverse=True)


    def _tokenize_text(self, text: str) -> set[str]:
        if not isinstance(text, str): # 添加类型检查
            self.logger.warning(f"FusionEngine: _tokenize_text received non-string input: {type(text)}. Returning empty set.")
            return set()
        return set(jieba.cut(text))

    def _calculate_jaccard_similarity(self, query_tokens: set[str], doc_tokens: set[str]) -> float:
        if not query_tokens or not doc_tokens: # 处理空集合的情况
            return 0.0
        intersection = len(query_tokens.intersection(doc_tokens))
        union = len(query_tokens.union(doc_tokens))
        return intersection / union if union > 0 else 0.0
        
    async def fuse_results(
        self,
        all_raw_retrievals: List[RetrievedDocument],
        user_query: str,
        top_n_final: int = 3
    ) -> List[RetrievedDocument]:

        self.logger.info(f"FusionEngine: Fusing {len(all_raw_retrievals)} raw retrieved documents for query: '{user_query}'. Target top_n_final: {top_n_final}")

        if not all_raw_retrievals:
            self.logger.info("FusionEngine: No documents to fuse.")
            return []

        # 1. 去重 (基于内容的哈希值)
        unique_docs_map: Dict[str, RetrievedDocument] = {}
        for doc in all_raw_retrievals:
            if not isinstance(doc.content, str) or not doc.content.strip():
                self.logger.debug(f"FusionEngine: Skipping doc with invalid content for hashing: {doc.metadata.get('chunk_id', 'N/A') if doc.metadata else 'N/A'}")
                continue
            content_hash = hashlib.md5(doc.content.encode('utf-8')).hexdigest()
            if content_hash not in unique_docs_map:
                unique_docs_map[content_hash] = doc
            else:
                # 如果内容重复，可以保留分数较高的一个（如果分数可比且来自不同召回源）
                # 这里简化处理，保留第一个遇到的，或者可以根据source_type和score进行更复杂的选择
                if doc.score is not None and unique_docs_map[content_hash].score is not None:
                    if doc.score > unique_docs_map[content_hash].score: # type: ignore
                        unique_docs_map[content_hash] = doc # 保留分数更高的
                elif doc.score is not None: # 当前文档有分数，已存的没有
                     unique_docs_map[content_hash] = doc
                self.logger.debug(f"FusionEngine: Duplicate content hash found. Doc with score {doc.score} vs existing {unique_docs_map[content_hash].score}. Content: {doc.content[:50]}...")
        
        unique_docs = list(unique_docs_map.values())
        self.logger.info(f"FusionEngine: After deduplication (content hash): {len(unique_docs)} documents.")

        if not unique_docs:
            return []

        # 2. 初步筛选 (基于长度和Jaccard相似度)
        # 定义阈值
        JACCARD_THRESHOLD = 0.05  # Jaccard相似度阈值，低于此则可能被过滤
        MIN_DOC_LENGTH_CHARS_KG = 10    # 知识图谱结果的最小字符长度
        MIN_DOC_LENGTH_CHARS_OTHER = 10 # 其他来源（向量、BM25）的最小字符长度
        # Reranker (如BGE-Reranker) 通常处理的token上限是512。
        # 一个中文字符大致对应1-3个token，英文单词大致对应1个token。
        # 为安全起见，可以设置一个字符上限，例如 1000-1500 字符，避免超长输入给Reranker。
        # 如果Reranker的tokenizer有max_length参数，它会自动截断，但预先过滤可以减少不必要的处理。
        MAX_DOC_LENGTH_CHARS = 1500 # 文档的最大字符长度，防止过长输入给reranker

        query_tokens_set = self._tokenize_text(user_query)
        screened_results: List[RetrievedDocument] = []
        
        self.logger.info(f"FusionEngine: Starting light screening for {len(unique_docs)} unique documents.")
        for doc_idx, doc in enumerate(unique_docs):
            doc_content_str = str(doc.content) # 确保是字符串
            doc_length = len(doc_content_str)
            
            # 长度筛选
            min_len_chars = MIN_DOC_LENGTH_CHARS_KG if doc.source_type == "knowledge_graph" else MIN_DOC_LENGTH_CHARS_OTHER
            if not (min_len_chars <= doc_length <= MAX_DOC_LENGTH_CHARS):
                self.logger.debug(f"  Screening: Doc {doc_idx} (ID: {doc.metadata.get('chunk_id', 'N/A') if doc.metadata else 'N/A'}) failed length check. Length: {doc_length}, Expected: [{min_len_chars}-{MAX_DOC_LENGTH_CHARS}], Type: {doc.source_type}. Content: '{doc_content_str[:50]}...'")
                continue

            # Jaccard相似度筛选 (可选，如果query_tokens_set为空则跳过)
            if query_tokens_set: # 只有当查询分词结果非空时才进行Jaccard计算
                doc_tokens_set = self._tokenize_text(doc_content_str)
                if not doc_tokens_set: # 如果文档分词结果为空，Jaccard为0
                    jaccard_sim = 0.0
                else:
                    jaccard_sim = self._calculate_jaccard_similarity(query_tokens_set, doc_tokens_set)
                
                if jaccard_sim < JACCARD_THRESHOLD:
                    self.logger.debug(f"  Screening: Doc {doc_idx} (ID: {doc.metadata.get('chunk_id', 'N/A') if doc.metadata else 'N/A'}) failed Jaccard check. Similarity: {jaccard_sim:.4f} < {JACCARD_THRESHOLD}. Content: '{doc_content_str[:50]}...'")
                    continue
            else:
                self.logger.debug(f"  Screening: Doc {doc_idx} (ID: {doc.metadata.get('chunk_id', 'N/A') if doc.metadata else 'N/A'}) - Query tokens empty, skipping Jaccard check.")


            screened_results.append(doc)
            self.logger.debug(f"  Screening: Doc {doc_idx} (ID: {doc.metadata.get('chunk_id', 'N/A') if doc.metadata else 'N/A'}) passed light screening.")
        
        self.logger.info(f"FusionEngine: After light screening: {len(screened_results)} documents remain.")
        
        if not screened_results:
            self.logger.info("FusionEngine: No documents remain after light screening. Returning empty list.")
            return []
        
        # 如果初筛后文档数量仍然很多，可以考虑再根据原始分数进行一次粗排和截断
        # 例如，如果 screened_results 数量 > top_n_final * 10，则取分数最高的前 top_n_final * 10 个
        # 这需要确保原始分数具有一定的可比性，或者对不同来源的分数进行大致的归一化
        # 当前我们先不加这一步，假设上游召回和初步筛选已将数量控制在合理范围
        docs_for_reranking = screened_results

        # 3. 使用Cross-Encoder进行精细重排序
        # _rerank_documents_sync 是同步函数，在异步函数中调用需要用 asyncio.to_thread
        final_fused_and_reranked_results = await asyncio.to_thread(
            self._rerank_documents_sync,
            query=user_query,
            documents=docs_for_reranking # 使用筛选后的文档
        )

        self.logger.info(f"FusionEngine: After reranking: {len(final_fused_and_reranked_results)} documents.")
        for i_doc, doc_reranked in enumerate(final_fused_and_reranked_results[:top_n_final+5]): # 日志多打几条看看分数
            self.logger.debug(f"  Reranked Doc {i_doc}: type={doc_reranked.source_type}, new_score={doc_reranked.score:.4f}, content='{str(doc_reranked.content)[:100]}...'")

        # 4. 根据 top_n_final 截取最终结果
        final_output_documents = final_fused_and_reranked_results[:top_n_final]

        self.logger.info(f"FusionEngine: Returning final top {len(final_output_documents)} documents.")
        return final_output_documents
    --- END OF FILE CONTENT ---

  File: kg_retriever.py
    --- START OF FILE CONTENT ---
# zhz_rag/core_rag/kg_retriever.py
import os
import json
import kuzu
import pandas as pd
from typing import List, Dict, Any, Optional, Callable, Iterator # 确保 Iterator 已导入
import logging
from contextlib import contextmanager

# 导入您的Cypher生成函数和Schema描述
from zhz_rag.llm.sglang_wrapper import generate_cypher_query # 确保路径正确
from zhz_rag.config.constants import NEW_KG_SCHEMA_DESCRIPTION # 确保路径正确

# 日志配置
kg_logger = logging.getLogger(__name__) 
# 确保 kg_logger 的级别和处理器已在 zhz_rag_mcp_service 或其他主入口配置，
# 或者在这里为它单独配置 handler 和 formatter，例如：
if not kg_logger.hasHandlers():
    kg_logger.setLevel(logging.DEBUG) # 开发时可以设为 DEBUG
    ch = logging.StreamHandler()
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s')
    ch.setFormatter(formatter)
    kg_logger.addHandler(ch)
    kg_logger.propagate = False # 避免重复日志（如果根logger也配置了handler）
kg_logger.info("KGRetriever (KuzuDB) logger initialized/reconfirmed.")


class KGRetriever:
    KUZU_DB_PATH_ENV = os.getenv("KUZU_DB_PATH", "/home/zhz/zhz_agent/zhz_rag/stored_data/kuzu_default_db")

    def __init__(self, db_path: Optional[str] = None, llm_cypher_generator_func: Callable = generate_cypher_query):
        self.db_path = db_path if db_path else self.KUZU_DB_PATH_ENV
        self.llm_cypher_generator_func = llm_cypher_generator_func
        self._db: Optional[kuzu.Database] = None
        kg_logger.info(f"KGRetriever (KuzuDB) __init__ called. Attempting to connect to DB path: {self.db_path}")
        self._connect_to_kuzu()
        # 这条日志现在移到 _connect_to_kuzu 成功之后打印

    def _connect_to_kuzu(self):
        kg_logger.info(f"Attempting to load KuzuDB from path: {self.db_path}")
        try:
            if not os.path.exists(self.db_path):
                kg_logger.error(f"KuzuDB path does not exist: {self.db_path}. KGRetriever cannot connect.")
                self._db = None
                return # 明确返回
            
            # 对于检索，通常只读即可，除非有特殊写需求
            # 如果 mcpo 服务可能并发访问，需要考虑 KuzuDB 的并发处理能力和锁机制
            self._db = kuzu.Database(self.db_path, read_only=True) 
            kg_logger.info(f"Successfully loaded KuzuDB from {self.db_path}. KGRetriever (KuzuDB) initialized and connected.")
        except Exception as e:
            kg_logger.error(f"Failed to connect to KuzuDB at {self.db_path}: {e}", exc_info=True)
            self._db = None

    @contextmanager
    def _get_connection(self) -> Iterator[kuzu.Connection]:
        if not self._db:
            kg_logger.warning("KuzuDB database object is None in _get_connection. Attempting to reconnect...")
            self._connect_to_kuzu() # 尝试重新连接
            if not self._db: # 再次检查
                kg_logger.error("KuzuDB reconnection failed. Cannot get a connection.")
                raise ConnectionError("KuzuDB is not connected or failed to reconnect. Cannot get a connection.")
        
        conn = None # 初始化 conn
        try:
            conn = kuzu.Connection(self._db)
            kg_logger.debug("KuzuDB connection obtained.")
            yield conn
        except Exception as e_conn: # 捕获 kuzu.Connection() 可能的异常
            kg_logger.error(f"Failed to create KuzuDB connection object: {e_conn}", exc_info=True)
            raise ConnectionError(f"Failed to create KuzuDB connection: {e_conn}")
        finally:
            # Kuzu Connection 对象没有显式的 close() 方法。
            # 它通常在其关联的 Database 对象被销毁时或垃圾回收时关闭。
            kg_logger.debug("KuzuDB connection context manager exiting.")
            pass 

    def close(self):
        kg_logger.info(f"Closing KuzuDB for retriever using path: {self.db_path}")
        if self._db:
            # KuzuDB Database 对象在其 __del__ 方法中处理关闭和资源释放。
            # 显式删除引用有助于触发垃圾回收，但不保证立即关闭。
            # KuzuDB 没有显式的 db.close() 方法。
            del self._db
            self._db = None
            kg_logger.info("KuzuDB Database object dereferenced (closed).")

    def execute_cypher_query_sync(self, query: str, parameters: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        if not self._db:
            kg_logger.error("KuzuDB not initialized in execute_cypher_query_sync. Cannot execute query.")
            return []
        
        kg_logger.info(f"--- Executing KuzuDB Cypher ---")
        kg_logger.info(f"Query: {query}")
        kg_logger.info(f"Params: {parameters if parameters else 'No parameters'}")
        
        results_list: List[Dict[str, Any]] = []
        try:
            with self._get_connection() as conn: # 使用上下文管理器获取连接
                prepared_statement = conn.prepare(query)
                # 注意：KuzuDB 的 execute 方法对参数的处理方式。
                # 如果 parameters 为 None 或空字典，应传递 None 或 {}。
                # 如果查询本身不包含参数占位符，传递参数字典可能会导致错误。
                # 我们需要确保Cypher查询中的参数占位符（如 $param）与parameters字典中的键匹配。
                
                actual_params = parameters if parameters else {} # 确保是字典
                query_result = conn.execute(prepared_statement, **actual_params) # 使用 ** 解包参数
                
                df = query_result.get_as_df()
                results_list = df.to_dict(orient='records')
                
                kg_logger.info(f"KuzuDB Cypher executed. Records count: {len(results_list)}")
                if results_list:
                    kg_logger.debug(f"First KuzuDB record (sample): {json.dumps(results_list[0], ensure_ascii=False, indent=2, default=str)}")
                else:
                    kg_logger.debug("KuzuDB query returned no records.")
        except RuntimeError as kuzu_runtime_error: # KuzuDB Python API 通常抛出 RuntimeError
             kg_logger.error(f"KuzuDB RuntimeError during Cypher execution: '{query}' with params: {parameters}. Error: {kuzu_runtime_error}", exc_info=True)
             # 可以考虑将 KuzuDB 的错误信息包装后向上抛出或返回
             # return [{"error": f"KuzuDB execution error: {kuzu_runtime_error}"}] 
        except ConnectionError as conn_err: # 如果 _get_connection 内部抛出
             kg_logger.error(f"KuzuDB ConnectionError during Cypher execution: {conn_err}", exc_info=True)
        except Exception as e:
            kg_logger.error(f"Unexpected error executing KuzuDB Cypher query: '{query}' with params: {parameters}. Error: {e}", exc_info=True)
        return results_list

    # ... ( _format_kuzu_record_for_retrieval 和 retrieve_with_llm_cypher 保持不变，
    # 但 retrieve_with_llm_cypher 内部对 execute_cypher_query_sync 的调用现在会经过新的错误处理) ...
    def _format_kuzu_record_for_retrieval(self, record_data: Dict[str, Any]) -> str:
        # ... (保持不变)
        parts = []
        for key, value in record_data.items():
            if isinstance(value, dict): 
                if 'label' in value and 'text' in value: 
                    parts.append(f"{key}({value['text']}:{value['label']})")
                elif '_label' in value and '_src' in value and '_dst' in value: 
                     parts.append(f"{key}(TYPE={value['_label']})")
                else:
                    value_str = json.dumps(value, ensure_ascii=False, default=str) 
                    if len(value_str) > 100: value_str = value_str[:100] + "..."
                    parts.append(f"{key}: {value_str}")
            elif value is not None:
                parts.append(f"{key}: {str(value)}")
        return " | ".join(parts) if parts else "No specific details found in this KuzuDB record."


    async def retrieve_with_llm_cypher(self, query: str, top_k: int = 3) -> List[Dict[str, Any]]:
        kg_logger.info(f"Starting KG retrieval (KuzuDB) with LLM-generated Cypher for query: '{query}', top_k: {top_k}")
        
        kg_logger.info(f"Calling LLM to generate Cypher query for KuzuDB...")
        # 将接收结果的变量名统一为 cypher_query_or_unable_msg
        cypher_query_or_unable_msg = await self.llm_cypher_generator_func(
            user_question=query,
            kg_schema_description=NEW_KG_SCHEMA_DESCRIPTION 
        )
        kg_logger.info(f"LLM generated Cypher query/message for KuzuDB:\n---\n{cypher_query_or_unable_msg}\n---")

        # --- [使用正确的变量名进行检查] ---
        if not cypher_query_or_unable_msg or cypher_query_or_unable_msg == "无法生成Cypher查询.":
            kg_logger.warning("LLM could not generate a valid Cypher query for KuzuDB or returned 'unable to generate' message.")
            return [] # 直接返回空列表

        # 如果是有效的 Cypher 查询，则继续
        cypher_to_execute = cypher_query_or_unable_msg # <--- 使用正确的变量名
        
        cypher_query_with_limit: str # 明确类型
        if "LIMIT" not in cypher_to_execute.upper(): # 保持大小写不敏感的检查
            cypher_query_with_limit = f"{cypher_to_execute} LIMIT {top_k}"
        else: 
            cypher_query_with_limit = cypher_to_execute
            kg_logger.info(f"Query already contains LIMIT, using as is: {cypher_query_with_limit}")

        results_from_kuzu = self.execute_cypher_query_sync(cypher_query_with_limit)
        
        retrieved_docs_for_rag: List[Dict[str, Any]] = []
        for record_dict in results_from_kuzu:
            content_str = self._format_kuzu_record_for_retrieval(record_dict)
            doc_data = {
                "source_type": "knowledge_graph_kuzu",
                "content": content_str,
                "score": 1.0, 
                # 使用 cypher_to_execute (即原始的、未加 LIMIT 的 Cypher) 进行记录
                "metadata": {"cypher_query": cypher_to_execute, "original_query": query, "raw_kuzu_record": record_dict}
            }
            retrieved_docs_for_rag.append(doc_data)

        kg_logger.info(f"Retrieved {len(retrieved_docs_for_rag)} documents from KuzuDB using LLM-generated Cypher.")
        return retrieved_docs_for_rag
    --- END OF FILE CONTENT ---


Directory: zhz_rag/core_rag/retrievers/
  File: __init__.py
    --- START OF FILE CONTENT ---
# zhz_agent/core_rag/retrievers/__init__.py
from .chromadb_retriever import ChromaDBRetriever
from .file_bm25_retriever import FileBM25Retriever
    --- END OF FILE CONTENT ---

  File: chromadb_retriever.py
    --- START OF FILE CONTENT ---
from typing import List, Dict, Any, Optional
from sentence_transformers import SentenceTransformer
import chromadb
import logging


# 配置ChromaDBRetriever的日志记录器
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

class ChromaDBRetriever:
    def __init__(
        self,
        collection_name: str = "rag_documents",
        persist_directory: str = "/home/zhz/dagster_home/chroma_data",
        embedding_model_name_or_path: str = "/home/zhz/models/bge-small-zh-v1.5",
    ):
        """
        初始化ChromaDBRetriever。

        Args:
            collection_name (str): 要查询的ChromaDB集合名称。
            persist_directory (str): ChromaDB数据持久化的目录。
            embedding_model_name_or_path (str): 用于查询向量化的SentenceTransformer模型名称或路径。
            device (str): 模型运行的设备 (例如 "cpu", "cuda")。
        """
        self.collection_name = collection_name
        self.persist_directory = persist_directory
        self.embedding_model_name_or_path = embedding_model_name_or_path
        self._client: Optional[chromadb.Client] = None
        self._collection: Optional[chromadb.Collection] = None
        self._embedding_model: Optional[SentenceTransformer] = None

        self._initialize_retriever()

    def _initialize_retriever(self):
        """
        初始化ChromaDB客户端、集合和嵌入模型。
        """
        try:
            logger.info(f"Initializing ChromaDB client from path: {self.persist_directory}")
            self._client = chromadb.PersistentClient(path=self.persist_directory)
            logger.info(f"Getting ChromaDB collection: {self.collection_name}")
            self._collection = self._client.get_collection(name=self.collection_name)
            if self._collection.count() == 0:
                logger.warning(f"ChromaDB collection '{self.collection_name}' is empty!")
            else:
                logger.info(f"ChromaDB collection '{self.collection_name}' loaded. Item count: {self._collection.count()}")
        except Exception as e:
            logger.error(f"Failed to initialize ChromaDB client or collection: {e}")
            raise

        try:
            logger.info(f"Loading SentenceTransformer model: {self.embedding_model_name_or_path}")
            self._embedding_model = SentenceTransformer(self.embedding_model_name_or_path)
            logger.info("SentenceTransformer model loaded successfully.")
        except Exception as e:
            logger.error(f"Failed to load SentenceTransformer model: {e}")
            raise
            
    def retrieve(self, query_text: str, n_results: int = 5, include_fields: Optional[List[str]] = None) -> List[Dict[str, Any]]:
        """
        根据查询文本从ChromaDB检索相似的文档块。

        Args:
            query_text (str): 用户查询的文本。
            n_results (int): 希望返回的最大结果数量。
            include_fields (Optional[List[str]]): 希望从ChromaDB返回的字段列表，
                                               例如 ["metadatas", "documents", "distances"]。
                                               如果为None，则ChromaDB通常会返回默认字段。
                                               我们存储时，chunk_text在metadatas里。

        Returns:
            List[Dict[str, Any]]: 检索到的文档块列表。每个字典通常包含：
                                   'id' (chunk_id), 
                                   'text' (chunk_text from metadata), 
                                   'metadata' (原始元数据),
                                   'distance' (相似度距离，越小越相似)
                                   具体结构取决于ChromaDB的返回和我们的处理。
        """
        if self._collection is None or self._embedding_model is None:
            logger.error("Retriever is not properly initialized.")
            # 尝试重新初始化，或者直接返回错误/空列表
            try:
                self._initialize_retriever()
                if self._collection is None or self._embedding_model is None: # 再次检查
                    return []
            except Exception as e:
                logger.error(f"Failed to re-initialize retriever during retrieve call: {e}")
                return []

        logger.info(f"Retrieving documents for query: '{query_text[:100]}...' with n_results={n_results}")
        
        try:
            # 1. 将查询文本向量化并归一化
            query_embedding_np = self._embedding_model.encode(
                query_text, 
                convert_to_tensor=False, 
                normalize_embeddings=True # <--- 关键：归一化查询嵌入
            )
            query_embedding = query_embedding_np.tolist()
            
            # 2. 在ChromaDB中查询 (include_fields_query 的逻辑不变)
            if include_fields is None:
                include_fields_query = ["metadatas", "distances"] 
            else:
                include_fields_query = include_fields
            results = self._collection.query(
                query_embeddings=[query_embedding], 
                n_results=n_results,
                include=include_fields_query 
            )
            # 3. 处理并格式化结果
            retrieved_docs = []
            if results and results.get("ids") and results.get("ids")[0]:
                ids_list = results["ids"][0]
                metadatas_list = results.get("metadatas", [[]])[0] 
                distances_list = results.get("distances", [[]])[0] 

                for i in range(len(ids_list)):
                    # ... (提取 chunk_id, metadata, distance 的代码不变) ...
                    chunk_id = ids_list[i]
                    metadata = metadatas_list[i] if metadatas_list and i < len(metadatas_list) else {}
                    distance = distances_list[i] if distances_list and i < len(distances_list) else float('inf')
                    chunk_text_content = metadata.get("chunk_text", "[Chunk text not found in metadata]")

                    # 计算相似度分数
                    # 如果ChromaDB使用cosine距离 (范围0-2, 0表示最相似)
                    # 相似度 = 1 - (distance / 2)  => 范围 0-1, 1表示最相似
                    # 或者直接用 cosine_similarity = 1 - distance (如果distance是 1-cos_sim)
                    # ChromaDB的cosine距离是 sqrt(2-2*cos_sim) 的平方，即 2-2*cos_sim (如果向量已归一化)
                    # 所以 cos_sim = 1 - distance / 2
                    # 我们希望分数越大越好
                    score = (1 - distance / 2.0) if distance != float('inf') and distance <=2.0 else 0.0 
                    # 确保分数在合理范围，如果distance意外地大于2，则score为0

                    retrieved_docs.append({
                        "id": chunk_id,
                        "text": chunk_text_content,
                        "score": score, # <--- 更新了分数计算
                        "distance": distance, 
                        "metadata": metadata 
                    })
                
                logger.info(f"Retrieved {len(retrieved_docs)} documents from ChromaDB.")
            else:
                logger.info("No documents retrieved from ChromaDB for the query.")

            return retrieved_docs

        except Exception as e:
            logger.error(f"Error during ChromaDB retrieval: {e}")
            return []

if __name__ == '__main__':
    # 简单的测试代码
    logger.info("--- ChromaDBRetriever Test ---")
    
    # 确保您的ChromaDB数据库中已经有通过Dagster流水线存入的数据
    # 并且模型路径正确
    try:
        retriever = ChromaDBRetriever()
        
        # 测试查询
        test_query = "人工智能的应用有哪些？" 
        # 根据您doc1.txt "这是第一个测试文档，关于人工智能和机器学习。" 应该能召回一些
        
        retrieved_results = retriever.retrieve(test_query, n_results=3)
        
        if retrieved_results:
            print(f"\n--- Retrieved Results for query: '{test_query}' ---")
            for i, doc in enumerate(retrieved_results):
                print(f"Result {i+1}:")
                print(f"  ID: {doc['id']}")
                print(f"  Text (first 100 chars): {doc['text'][:100]}...")
                print(f"  Score: {doc['score']:.4f} (Distance: {doc['distance']:.4f})")
                print(f"  Metadata: {doc['metadata']}")
                print("-" * 20)
        else:
            print(f"\nNo results retrieved for query: '{test_query}'")
            
        # 测试一个可能没有结果的查询
        # test_query_no_results = "恐龙为什么会灭绝？"
        # retrieved_no_results = retriever.retrieve(test_query_no_results, n_results=3)
        # if not retrieved_no_results:
        #     print(f"\nCorrectly retrieved no results for query: '{test_query_no_results}'")

    except Exception as e:
        print(f"An error occurred during the test: {e}")

    --- END OF FILE CONTENT ---

  File: file_bm25_retriever.py
    --- START OF FILE CONTENT ---
from typing import List, Dict, Any, Optional, Tuple
import jieba
import bm25s # 我们确认使用 bm25s
import pickle
import os
import logging
import numpy as np

# 配置日志记录器
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

class FileBM25Retriever:
    def __init__(
        self,
        index_directory_path: str = "/home/zhz/dagster_home/bm25_index_data/", # 与Dagster中配置一致
        # user_dict_path: Optional[str] = None 
    ):
        self.index_directory_path = index_directory_path
        # self.user_dict_path = user_dict_path

        self._bm25_model: Optional[bm25s.BM25] = None
        self._doc_ids: Optional[List[str]] = None 
        # self._tokenizer: Optional[bm25s.Tokenizer] = None # bm25s.tokenize 是一个函数，或者可以用Tokenizer类

        self._initialize_retriever()

    def _initialize_retriever(self):
        # if self.user_dict_path and os.path.exists(self.user_dict_path):
        #     # ... (加载jieba用户词典的逻辑不变) ...
        
        if not os.path.isdir(self.index_directory_path): # <--- 修改：检查目录是否存在
            logger.error(f"BM25 index directory not found at: {self.index_directory_path}")
            raise FileNotFoundError(f"BM25 index directory not found: {self.index_directory_path}")

        try:
            logger.info(f"Loading BM25 model from directory: {self.index_directory_path}")
            # 使用bm25s的load类方法
            # load_corpus=False 因为我们不期望在模型文件中包含原始语料库文本
            # mmap=False (默认) 先不使用内存映射，除非索引非常大
            self._bm25_model = bm25s.BM25.load(
                self.index_directory_path,
                load_corpus=False, # 通常我们不需要在这里加载原始语料库
                # mmap=False 
            )
            
            if self._bm25_model is None: # load 失败通常会抛异常，但以防万一
                logger.error("Failed to load BM25 model (returned None).")
                raise ValueError("Failed to load BM25 model.")
            logger.info("BM25 model loaded successfully.")

            # 单独加载 document_ids 列表
            doc_ids_path = os.path.join(self.index_directory_path, "doc_ids.pkl")
            if not os.path.exists(doc_ids_path):
                logger.error(f"doc_ids.pkl not found in {self.index_directory_path}")
                raise FileNotFoundError(f"doc_ids.pkl not found in {self.index_directory_path}")
            
            with open(doc_ids_path, 'rb') as f_in:
                self._doc_ids = pickle.load(f_in)
            
            if self._doc_ids is None: # pickle 加载空文件可能返回None
                 logger.warning(f"doc_ids.pkl loaded, but it was empty or invalid.")
                 self._doc_ids = [] # 设为空列表以避免后续错误
            logger.info(f"Document IDs loaded successfully. Number of indexed documents: {len(self._doc_ids)}")

        except Exception as e:
            logger.error(f"Failed to load BM25 index or document IDs: {e}")
            raise
            
    def retrieve(self, query_text: str, n_results: int = 5) -> List[Dict[str, Any]]:
        # ... (初始化检查和空索引检查不变) ...
        if self._bm25_model is None or self._doc_ids is None:
            logger.error("BM25 Retriever is not properly initialized.")
            # ... (尝试重新初始化或返回空的逻辑不变) ...
            return []
        
        if not self._doc_ids: 
            logger.info("BM25 index is empty, no results to retrieve.")
            return []

        logger.info(f"Retrieving documents with BM25 for query: '{query_text[:100]}...' with n_results={n_results}")

        try:
            # 1. 将查询文本分词
            # bm25s 有自己的 tokenize 函数，它会处理停用词、词干提取（如果配置了）并返回模型期望的格式
            # 我们需要确保查询时的分词方式与索引时一致。
            # 如果索引时用了jieba，查询时也应该用jieba。
            # bm25s.BM25 对象在加载后，其内部应该已经有了词汇表 (vocab_dict)，
            # 它的 get_scores 方法期望的是与词汇表ID对应的输入，或者它能自己处理分词后的文本列表。
            
            query_tokenized_jieba = list(jieba.cut_for_search(query_text))
            logger.debug(f"Tokenized query (jieba): {query_tokenized_jieba}")

            # 2. 使用BM25模型进行查询
            # bm25s 的 retrieve 方法可以直接返回文档索引和分数
            # 它接受分词后的查询 (list of str)
            # results 是文档索引 (numpy array), scores 是对应的分数 (numpy array)
            # 它们都是 (n_queries, k) 的形状，我们只有一个查询，所以是 (1, k)
            
            # 确保 k 不超过实际文档数
            actual_k = min(n_results, len(self._doc_ids))
            if actual_k == 0: # 如果索引中没有文档
                return []

            results_indices, results_scores = self._bm25_model.get_top_n(
                query_tokenized_jieba, 
                corpus=None, # 我们不需要在这里提供原始语料库，它返回的是索引
                n=actual_k
            )
            # get_top_n 返回的是一个列表（每个查询一个结果列表），我们只有一个查询
            # 每个结果列表中的元素是 (doc_index, score) 吗？还是直接是doc_index?
            # 查阅 bm25s 文档：get_top_n(query_tokens, documents, n=5)
            #   - query_tokens: list of tokens for the query.
            #   - documents: list of documents (list of tokens).
            #   - n: number of top documents to retrieve.
            # Returns: list of top n documents.
            # 这看起来是返回文档本身，不是我们想要的。

            # 让我们回到使用 get_scores 然后自己排序的方式，这更可控
            all_scores = self._bm25_model.get_scores(query_tokenized_jieba)
            
            top_n_indices = np.argsort(all_scores)[-actual_k:][::-1] # 降序取前N

            retrieved_docs = []
            for doc_corpus_index in top_n_indices:
                doc_id = self._doc_ids[doc_corpus_index] # 从0-based索引映射到我们的chunk_id
                score = float(all_scores[doc_corpus_index]) 
                retrieved_docs.append({
                    "id": doc_id,
                    "score": score
                })
            
            logger.info(f"Retrieved {len(retrieved_docs)} documents using BM25.")
            return retrieved_docs

        except Exception as e:
            logger.error(f"Error during BM25 retrieval: {e}")
            return []
            
    def retrieve(self, query_text: str, n_results: int = 5) -> List[Dict[str, Any]]:
        """
        根据查询文本使用BM25检索相关的文档块ID和分数。

        Args:
            query_text (str): 用户查询的文本。
            n_results (int): 希望返回的最大结果数量。

        Returns:
            List[Dict[str, Any]]: 检索到的文档块列表。每个字典包含：
                                   'id' (chunk_id), 
                                   'score' (BM25分数)
        """
        if self._bm25_model is None or self._doc_ids is None:
            logger.error("BM25 Retriever is not properly initialized.")
            try:
                self._initialize_retriever()
                if self._bm25_model is None or self._doc_ids is None:
                    return []
            except Exception as e:
                logger.error(f"Failed to re-initialize BM25 retriever during retrieve call: {e}")
                return []
        
        if not self._doc_ids: # 如果索引为空
            logger.info("BM25 index is empty, no results to retrieve.")
            return []

        logger.info(f"Retrieving documents with BM25 for query: '{query_text[:100]}...' with n_results={n_results}")

        try:
            # 1. 将查询文本分词
            query_tokenized = list(jieba.cut_for_search(query_text))
            logger.debug(f"Tokenized query: {query_tokenized}")

            # 2. 使用BM25模型进行查询
            # bm25s 的 get_scores 方法返回所有文档的分数
            # bm25s 的 get_batch_results (或类似名称) 可能更适合获取top-N
            # 我们需要查阅 bm25s 的API来获取top-N的文档索引和分数
            # 假设它有一个类似 get_top_n 的方法，或者我们需要自己处理 get_scores 的结果

            # 查阅 bm25s 文档，它通常使用 `bm25_model.get_scores(query_tokenized)` 得到所有分数
            # 然后我们需要自己排序并取top N
            # 或者，bm25s.BM25 可能有更直接的方法，例如 `search` 或 `query`
            # 经过快速查阅，bm25s 似乎没有直接的 top_n 方法，但其设计是为了快速计算所有分数。
            # `bm25_model.get_scores(query_tokenized)` 返回一个numpy数组，包含每个文档的分数。

            all_scores = self._bm25_model.get_scores(query_tokenized)
            
            # 获取分数最高的n_results个文档的索引
            # 注意：如果实际文档数少于n_results，则取实际数量
            actual_n_results = min(n_results, len(self._doc_ids))
            
            # 使用numpy的argsort来获取排序后的索引，然后取最后N个（因为argsort默认升序）
            # 或者取负数再取前N个
            top_n_indices = np.argsort(all_scores)[-actual_n_results:][::-1] # 降序取前N

            retrieved_docs = []
            for index in top_n_indices:
                doc_id = self._doc_ids[index]
                score = float(all_scores[index]) # 转换为Python float
                # 我们只返回ID和分数，文本内容由上层逻辑获取
                retrieved_docs.append({
                    "id": doc_id,
                    "score": score
                })
            
            logger.info(f"Retrieved {len(retrieved_docs)} documents using BM25.")
            return retrieved_docs

        except Exception as e:
            logger.error(f"Error during BM25 retrieval: {e}")
            return []

if __name__ == '__main__':
    logger.info("--- FileBM25Retriever Test ---")
    
    # 确保您的BM25索引文件已通过Dagster流水线创建
    # (例如 /home/zhz/dagster_home/bm25_index/rag_bm25_index.pkl)
    try:
        retriever = FileBM25Retriever()
        
        # 测试查询 (与ChromaDBRetriever使用相同的查询，以便后续比较和融合)
        test_query = "人工智能的应用有哪些？" 
        
        retrieved_results = retriever.retrieve(test_query, n_results=3)
        
        if retrieved_results:
            print(f"\n--- BM25 Retrieved Results for query: '{test_query}' ---")
            for i, doc in enumerate(retrieved_results):
                print(f"Result {i+1}:")
                print(f"  ID: {doc['id']}")
                print(f"  Score: {doc['score']:.4f}")
                # 我们这里不获取文本，由rag_service负责
                print("-" * 20)
        else:
            print(f"\nNo results retrieved with BM25 for query: '{test_query}'")

    except Exception as e:
        print(f"An error occurred during the BM25 test: {e}")
    --- END OF FILE CONTENT ---


Directory: zhz_rag/llm/
  File: __init__.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/zhz_rag/llm/__init__.py

from .sglang_wrapper import call_llm_via_openai_api_local_only # 导入新函数

call_sglang_llm = call_llm_via_openai_api_local_only # 别名指向新函数

from .sglang_wrapper import (
    generate_answer_from_context,
    generate_cypher_query,
    generate_expanded_queries,
    generate_intent_classification, # 这个现在直接用 litellm 调用 Gemini
    generate_clarification_question,
    generate_clarification_options,
    NO_ANSWER_PHRASE_ANSWER_CLEAN,
    NO_ANSWER_PHRASE_KG_CLEAN
)
    --- END OF FILE CONTENT ---

  File: custom_crewai_llms.py
    --- START OF FILE CONTENT ---
#/home/zhz/zhz_agent/custom_llm.py
import os
import json
import httpx
import asyncio
import traceback
from typing import List, Dict, Any, Optional, Union, Sequence, Type 
# --- CrewAI & LiteLLM Imports ---
from crewai.tools import BaseTool
from crewai.llms.base_llm import BaseLLM as CrewAIBaseLLM
import litellm

# --- [修改] Local Imports -> 改为绝对导入 ---
from zhz_rag.llm.sglang_wrapper import call_sglang_llm # For SGLang LLM
from dotenv import load_dotenv

load_dotenv()

# --- SGLang Config ---
SGLANG_API_URL_FOR_LLM = os.getenv("SGLANG_API_URL", "http://localhost:30000/generate")

# --- CustomGeminiLLM (from ceshi/run_agent.py with fixes) ---
class CustomGeminiLLM(CrewAIBaseLLM):
    model_name: str
    api_key: str
    max_tokens: Optional[int] = 2048
    tool_config: Optional[Dict[str, Any]] = None
    stop: Optional[List[str]] = None
    _gemini_tools_cache: Optional[List[Dict[str, Any]]] = None

    def __init__(self, model: str, api_key: str, temperature: float = 0.1, max_tokens: Optional[int] = 2048, tool_config: Optional[Dict[str, Any]] = None, stop: Optional[List[str]] = None, agent_tools: Optional[List[BaseTool]] = None, **kwargs):
        super().__init__(model=model, temperature=temperature)
        self.model_name = model
        self.api_key = api_key
        self.max_tokens = max_tokens
        self.tool_config = tool_config or {"function_calling_config": {"mode": "AUTO"}}
        self.stop = stop
        if agent_tools:
            self._gemini_tools_cache = self._convert_crewai_tools_to_gemini_format(agent_tools)
            print(f"CustomGeminiLLM __init__: Cached {len(self._gemini_tools_cache)} tools.")
        else:
            print("CustomGeminiLLM __init__: No agent_tools provided for caching.")

    def _remove_unwanted_fields(self, schema: Dict[str, Any]) -> Dict[str, Any]:
        if not isinstance(schema, dict):
            return schema

        schema.pop('title', None)

        if "properties" in schema:
            if "type" not in schema:
                schema["type"] = "object"
            for prop_name, prop_def in list(schema["properties"].items()):
                if isinstance(prop_def, dict):
                    prop_def.pop('default', None)
                    prop_def.pop('title', None)
                    self._remove_unwanted_fields(prop_def)
        elif schema.get("type") == "object" and "properties" not in schema:
            schema["properties"] = {}

        keys_to_delete = [k for k, v in schema.items() if k == 'default']
        for k in keys_to_delete:
            del schema[k]

        for k, v in schema.items():
            if isinstance(v, dict):
                self._remove_unwanted_fields(v)
            elif isinstance(v, list):
                for i, item in enumerate(v):
                    if isinstance(item, dict):
                        v[i] = self._remove_unwanted_fields(item)
        return schema

    def _convert_crewai_tools_to_gemini_format(self, tools: Optional[List[BaseTool]]) -> Optional[List[Dict[str, Any]]]:
        if not tools:
            return None
        gemini_tool_declarations = []
        for tool_instance in tools:
            tool_name = tool_instance.name
            tool_description = tool_instance.description
            if not hasattr(tool_instance, 'args_schema') or not tool_instance.args_schema:
                parameters_schema = {"type": "object", "properties": {}}
            else:
                try:
                    if hasattr(tool_instance.args_schema, 'model_json_schema'):
                        pydantic_schema = tool_instance.args_schema.model_json_schema()
                    else:
                        pydantic_schema = tool_instance.args_schema.schema()
                    cleaned_schema = self._remove_unwanted_fields(pydantic_schema.copy())
                    parameters_schema = cleaned_schema
                except Exception as e:
                    print(f"Error processing schema for tool {tool_name}: {e}")
                    parameters_schema = {"type": "object", "properties": {}}
            gemini_tool_declarations.append({
                "name": tool_name,
                "description": tool_description,
                "parameters": parameters_schema
            })
        final_tools_for_litellm = []
        for declaration in gemini_tool_declarations:
            final_tools_for_litellm.append({
                "type": "function",
                "function": declaration
            })
        return final_tools_for_litellm

    def call(self, messages: Union[str, List[Dict[str, str]]], tools: Optional[List[dict]] = None, callbacks: Optional[List[Any]] = None, **kwargs: Any) -> Union[str, Any]:
        print(f"CustomGeminiLLM CALL method invoked.")
        print(f"  CALL - Tools received by CustomLLM.call: {'Yes' if tools else 'No'}")
        print(f"  CALL - Callbacks received by CustomLLM.call: {'Yes' if callbacks else 'No'}")

        if isinstance(messages, str):
            processed_messages = [{"role": "user", "content": messages}]
        else:
            processed_messages = messages

        litellm_params = {
            "model": self.model_name,
            "messages": processed_messages,
            "api_key": self.api_key,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "stop": self.stop
        }

        # --- Proxy Addition ---
        proxy_url = os.getenv("LITELLM_PROXY_URL")
        if proxy_url:
            litellm_params["proxy"] = {
                "http": proxy_url,
                "https": proxy_url,
            }
            print(f"CustomGeminiLLM.call - Using proxy: {proxy_url}")
        else:
            print("CustomGeminiLLM.call - No proxy configured (LITELLM_PROXY_URL not set).")

        # --- Tool Handling (tools: null fix) ---
        final_tools_for_litellm = None
        received_tools_to_process = tools
        if not received_tools_to_process and self._gemini_tools_cache:
            print("  CALL - INFO: Tools argument was None, using cached tools.")
            received_tools_to_process = self._gemini_tools_cache

        if received_tools_to_process:
            cleaned_tools_for_litellm = []
            for tool_dict in received_tools_to_process:
                current_tool_def = tool_dict.copy()
                if current_tool_def.get("type") == "function" and "function" in current_tool_def:
                    func_def = current_tool_def["function"].copy()
                    if "parameters" in func_def:
                        func_def["parameters"] = self._remove_unwanted_fields(func_def["parameters"].copy())
                    current_tool_def["function"] = func_def
                    cleaned_tools_for_litellm.append(current_tool_def)
                else:
                    cleaned_tools_for_litellm.append(tool_dict)
            final_tools_for_litellm = cleaned_tools_for_litellm

        if final_tools_for_litellm:
            litellm_params["tools"] = final_tools_for_litellm
            fc_config = self.tool_config.get("function_calling_config", {})
            mode = fc_config.get("mode", "AUTO").upper()
            allowed_names = fc_config.get("allowed_function_names")

            if mode == "ANY" and allowed_names:
                litellm_params["tool_choice"] = {
                    "type": "function",
                    "function": {"name": allowed_names[0]}
                }
            elif mode in ["AUTO", "ANY", "NONE"]:
                litellm_params["tool_choice"] = mode.lower()
            else:
                litellm_params["tool_choice"] = "auto"
            print(f"CustomGeminiLLM DEBUG: Setting tool_choice to: {litellm_params['tool_choice']}")

        if callbacks:
            litellm_params["callbacks"] = callbacks

        try:
            print(f"CustomGeminiLLM.call - LiteLLM PARAMS (Preview): model={litellm_params['model']}, msgs_count={len(litellm_params['messages'])}, tools={'Yes' if 'tools' in litellm_params else 'No'}, tool_choice={litellm_params.get('tool_choice')}, proxy={'Yes' if 'proxy' in litellm_params else 'No'}")
            response = litellm.completion(**litellm_params)
        except Exception as e:
            print(f"CRITICAL ERROR: LiteLLM completion call failed: {e}")
            if callbacks:
                for handler in callbacks:
                    if hasattr(handler, 'on_llm_error'):
                        try:
                            handler.on_llm_error(error=e, llm=self, **kwargs)
                        except Exception as cb_err:
                            print(f"Error in callback on_llm_error: {cb_err}")
            raise

        llm_message_response = response.choices[0].message
        if hasattr(llm_message_response, 'tool_calls') and llm_message_response.tool_calls:
            print(f"CustomGeminiLLM.call - Detected tool_calls: {llm_message_response.tool_calls}")
            # --- ReAct Format Workaround (AttributeError fix) ---
            tool_call = llm_message_response.tool_calls[0]
            action = tool_call.function.name
            action_input = tool_call.function.arguments
            react_string = f"Action: {action}\nAction Input: {action_input}"
            print(f"CustomGeminiLLM.call - Returning ReAct string: {react_string}")
            return react_string
        else:
            print(f"CustomGeminiLLM.call - Returning text content.")
            return llm_message_response.content or ""

    def get_token_counter_instance(self):
        class GeminiTokenCounter:
            def __init__(self, model_name):
                self.model_name = model_name

            def count_tokens(self, text: Union[str, List[Dict[str,str]]]) -> int:
                try:
                    if isinstance(text, list):
                        return litellm.token_counter(model=self.model_name, messages=text)
                    return litellm.token_counter(model=self.model_name, text=str(text))
                except Exception as e:
                    print(f"Warning: Token counting failed ({e}), falling back to rough estimate.")
                    if isinstance(text, list):
                        return sum(len(str(m.get("content","")).split()) for m in text)
                    return len(str(text).split())
        return GeminiTokenCounter(model_name=self.model_name)


# --- CustomSGLangLLM (from hybrid_rag/custom_llm.py) ---
class CustomSGLangLLM(CrewAIBaseLLM):
    endpoint_url: str = SGLANG_API_URL_FOR_LLM
    model_name: str = "qwen2-3b-instruct"
    temperature: float = 0.1
    max_new_tokens_val: int = 1024

    def __init__(self, endpoint: Optional[str] = None, model: Optional[str] = None, temperature: Optional[float] = None, max_new_tokens: Optional[int] = None, **kwargs: Any):
        super().__init__(**kwargs)
        if endpoint: self.endpoint_url = endpoint
        if model: self.model_name = model
        if temperature is not None: self.temperature = temperature
        if max_new_tokens is not None: self.max_new_tokens_val = max_new_tokens
        print(f"CustomSGLangLLM initialized. Endpoint: {self.endpoint_url}, Model: {self.model_name}, Temp: {self.temperature}, MaxTokens: {self.max_new_tokens_val}")

    def _prepare_sglang_prompt(self, messages: Sequence[Dict[str, str]]) -> str:
        prompt_str = ""
        for message in messages:
            role = message.get("role")
            content = message.get("content")
            if role and content:
                prompt_str += f"<|im_start|>{role}\n{content}<|im_end|>\n"
        prompt_str += "<|im_start|>assistant\n"
        return prompt_str

    def call(self, messages: Sequence[Dict[str, str]], **kwargs: Any) -> str:
        print(f"CustomSGLangLLM.call received messages: {messages}")
        sglang_prompt = self._prepare_sglang_prompt(messages)
        print(f"CustomSGLangLLM.call prepared sglang_prompt (first 200 chars): {sglang_prompt[:200]}...")
        stop_sequences_for_sglang = kwargs.get("stop", ["<|im_end|>", "<|endoftext|>"])

        try:
            try:
                loop = asyncio.get_running_loop()
            except RuntimeError:
                loop = None

            async def async_runner():
                return await call_sglang_llm(
                    prompt=sglang_prompt,
                    temperature=self.temperature,
                    max_new_tokens=self.max_new_tokens_val,
                    stop_sequences=stop_sequences_for_sglang
                )

            if loop and loop.is_running():
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                    future = executor.submit(asyncio.run, async_runner())
                    response_text = future.result(timeout=120)
            else:
                response_text = asyncio.run(async_runner())

        except Exception as e:
            print(f"CustomSGLangLLM.call: Error during SGLang call: {type(e).__name__} - {e}")
            traceback.print_exc()
            return f"LLM_CALL_ERROR: 调用SGLang服务失败 - {str(e)}"

        if response_text is None:
            print("CustomSGLangLLM.call: SGLang returned None.")
            return "LLM_CALL_ERROR: SGLang服务未返回任何文本。"

        print(f"CustomSGLangLLM.call: SGLang returned text (first 200 chars): {response_text[:200]}...")
        return response_text

    def get_token_ids(self, text: str) -> List[int]:
        print("CustomSGLangLLM.get_token_ids: Not implemented, returning empty list.")
        return []

    @property
    def support_function_calling(self) -> bool:
        return False

    @property
    def support_stop_words(self) -> bool:
        return True

    @property
    def available_models(self) -> List[str]:
        return [self.model_name]

    @property
    def context_window(self) -> int:
        return 32768

    @property
    def identifying_params(self) -> Dict[str, Any]:
        return {
            "model": self.model_name,
            "endpoint_url": self.endpoint_url,
            "temperature": self.temperature,
            "max_new_tokens": self.max_new_tokens_val,
        }
    --- END OF FILE CONTENT ---

  File: sglang_wrapper.py
    --- START OF FILE CONTENT ---
# zhz_agent/llm.py (renamed to sglang_wrapper.py as per typical module naming)
# or more accurately, this is the content for sglang_wrapper.py based on the inputs

import os
import httpx  # 用于异步HTTP请求
import json  # 用于处理JSON数据
import asyncio  # 用于 asyncio.to_thread
from typing import List, Dict, Any, Optional, Union # Added Union
from dotenv import load_dotenv
import traceback  # Ensure traceback is imported
from zhz_rag.utils.common_utils import log_interaction_data # 导入通用日志函数
from zhz_rag.config.constants import NEW_KG_SCHEMA_DESCRIPTION # <--- 确保导入这个常量
from zhz_rag.utils.common_utils import log_interaction_data # 导入通用日志函数
import logging
import re
import uuid  # 用于生成 interaction_id
from datetime import datetime, timezone  # 用于生成时间戳
import litellm # <--- 确保这个导入存在

load_dotenv()  # 确保加载.env文件

_LLM_DIR = os.path.dirname(os.path.abspath(__file__))
RAG_INTERACTION_LOGS_DIR = os.path.join(_LLM_DIR, '..', '..', 'stored_data', 'rag_interaction_logs')


if not os.path.exists(RAG_INTERACTION_LOGS_DIR):
    try:
        os.makedirs(RAG_INTERACTION_LOGS_DIR)
    except Exception:
        pass

def get_llm_log_filepath() -> str:
    """获取当前LLM交互日志文件的完整路径，按天分割。"""
    today_str = datetime.now(timezone.utc).strftime("%Y%m%d")
    return os.path.join(RAG_INTERACTION_LOGS_DIR, f"rag_interactions_{today_str}.jsonl")

async def log_llm_interaction_to_jsonl(interaction_data: Dict[str, Any]):
    """
    将单条LLM交互数据异步追加到JSONL文件中。
    (This function might be part of what log_interaction_data uses, or an alternative logger. Keeping for completeness from original llm.py)
    """
    filepath = get_llm_log_filepath()
    try:
        def _write_sync():
            with open(filepath, 'a', encoding='utf-8') as f:
                f.write(json.dumps(interaction_data, ensure_ascii=False) + "\n")
        await asyncio.to_thread(_write_sync)
        llm_py_logger.debug(f"Successfully logged LLM interaction to {filepath}")
    except Exception as e:
        llm_py_logger.error(f"Failed to log LLM interaction to {filepath}: {e}", exc_info=True)

llm_py_logger = logging.getLogger("LLMUtilsLogger")
llm_py_logger.setLevel(os.getenv("LLM_LOG_LEVEL", "INFO").upper())

if not llm_py_logger.hasHandlers():
    _llm_console_handler = logging.StreamHandler()
    _llm_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
    _llm_console_handler.setFormatter(_llm_formatter)
    llm_py_logger.addHandler(_llm_console_handler)
    llm_py_logger.propagate = False

llm_py_logger.info("--- LLMUtilsLogger configured ---")

NO_ANSWER_PHRASE_ANSWER_CLEAN = "根据目前提供的资料，我无法找到关于您问题的明确信息。"
NO_ANSWER_PHRASE_KG_CLEAN = "从知识图谱中未找到直接相关信息。"
UNIQUE_STOP_TOKEN = "<|im_endofunable|>"
NO_ANSWER_PHRASE_ANSWER_WITH_STOP_TOKEN = f"{NO_ANSWER_PHRASE_ANSWER_CLEAN}{UNIQUE_STOP_TOKEN}"
NO_ANSWER_PHRASE_KG_WITH_STOP_TOKEN = f"{NO_ANSWER_PHRASE_KG_CLEAN}{UNIQUE_STOP_TOKEN}"

# Placeholder for the schema description. Replace with actual schema.
NEW_KG_SCHEMA_DESCRIPTION = """
{
  "node_labels": ["Person", "Project", "Task", "Document", "Region", "SalesAmount", "Product"],
  "relationship_types": ["WORKS_ON", "ASSIGNED_TO", "HAS_DOCUMENT", "HAS_SALES_AMOUNT", "RELATED_TO"],
  "node_properties": {
    "Person": [{"property": "name", "type": "STRING"}, {"property": "role", "type": "STRING"}],
    "Project": [{"property": "name", "type": "STRING"}, {"property": "status", "type": "STRING"}],
    "Task": [{"property": "name", "type": "STRING"}, {"property": "status", "type": "STRING"}, {"property": "priority", "type": "STRING"}],
    "Document": [{"property": "id", "type": "STRING"}, {"property": "title", "type": "STRING"}, {"property": "type", "type": "STRING"}],
    "Region": [{"property": "name", "type": "STRING"}],
    "SalesAmount": [{"property": "period", "type": "STRING"}, {"property": "numeric_amount", "type": "FLOAT"}, {"property": "unit", "type": "STRING"}],
    "Product": [{"property": "name", "type": "STRING"}, {"property": "category", "type": "STRING"}]
  },
  "relationship_properties": {},
  "output_format_guidance": {
    "description": "Your response MUST be a JSON object with two fields: 'status' and 'query'.",
    "status_field": {
      "description": "The 'status' field can be one of two values: 'success' or 'unable_to_generate'.",
      "success": "If you can generate a Cypher query, status should be 'success'.",
      "unable_to_generate": "If you cannot generate a Cypher query based on the question and schema, status should be 'unable_to_generate'."
    },
    "query_field": {
      "description": "The 'query' field contains the Cypher query as a string if status is 'success'.",
      "success_example": "MATCH (n) RETURN n LIMIT 1",
      "unable_to_generate_example": "无法生成Cypher查询."
    }
  },
  "examples": [
    {
      "User Question": "Who is task 'FixBug123' assigned to?",
      "Your EXACT Response": {
        "status": "success",
        "query": "MATCH (t:Task {name: 'FixBug123'})<-[:ASSIGNED_TO]-(p:Person) RETURN p.name AS assignedTo"
      }
    },
    {
      "User Question": "What is the color of the sky?",
      "Your EXACT Response": {
        "status": "unable_to_generate",
        "query": "无法生成Cypher查询."
      }
    }
  ]
}
"""

LLM_API_URL = os.getenv("SGLANG_API_URL", "http://localhost:8088/v1/chat/completions")

# 添加回这个函数，但它现在只被 generate_answer_from_context, generate_cypher_query 等调用
# 并且目标是本地的 OpenAI 兼容服务
async def call_llm_via_openai_api_local_only( # 改个名字以示区分
    prompt: Union[str, List[Dict[str, str]]], # prompt 可以是字符串或消息列表
    temperature: float = 0.2,
    max_new_tokens: Optional[int] = 1024,
    stop_sequences: Optional[List[str]] = None,
    task_type: str = "unknown_local_llm_call",
    user_query_for_log: Optional[str] = None,
    model_name_for_log: str = "local_qwen_via_openai_api_compat",
    application_version_for_log: str = "0.1.0_local_compat"
) -> Optional[str]:
    llm_py_logger.info(f"Calling LOCAL LLM ({model_name_for_log}) for task: {task_type}, Target API: {LLM_API_URL}")

    current_messages: List[Dict[str, str]]
    original_prompt_for_log: str

    if isinstance(prompt, str): # 假设旧的SGLang风格的prompt字符串
        original_prompt_for_log = prompt
        # 尝试从SGLang格式转换为OpenAI messages格式
        # 这个转换逻辑需要根据您SGLang prompt的具体格式来定
        # 一个简化的例子，可能需要调整：
        current_messages = []
        # 简单的假设：如果prompt以<|im_start|>system开头，则提取system和user部分
        if prompt.startswith("<|im_start|>system"):
            parts = prompt.split("<|im_start|>")
            for part in parts:
                if not part.strip(): continue
                role_content = part.split("<|im_end|>")[0].strip()
                if "\n" in role_content:
                    role, content = role_content.split("\n", 1)
                    current_messages.append({"role": role.strip().lower(), "content": content.strip()})
        if not current_messages: # 如果转换失败或不是SGLang格式，则认为是单个user消息
            current_messages = [{"role": "user", "content": prompt}]
    elif isinstance(prompt, list):
        current_messages = prompt
        original_prompt_for_log = "Messages list provided directly."
    else:
        llm_py_logger.error(f"Invalid 'prompt' argument type: {type(prompt)}")
        return None

    payload = {
        "model": model_name_for_log, # 这个model名会被本地服务忽略，但符合OpenAI格式
        "messages": current_messages,
        "temperature": temperature,
        "max_tokens": max_new_tokens,
    }
    if stop_sequences:
        payload["stop"] = stop_sequences

    headers = {"Content-Type": "application/json"}
    llm_parameters_for_log = {k:v for k,v in payload.items() if k not in ['messages', 'model']} # model已在顶层记录
    raw_llm_output_text = None
    error_info = None

    try:
        async with httpx.AsyncClient(timeout=120.0) as client:
            response = await client.post(LLM_API_URL, json=payload, headers=headers) # LLM_API_URL 指向本地服务
            response.raise_for_status()
            response_json = response.json()
            if response_json.get("choices") and response_json["choices"][0].get("message"):
                raw_llm_output_text = response_json["choices"][0]["message"].get("content", "")
            else:
                raw_llm_output_text = "[[LLM_RESPONSE_MALFORMED_CHOICES_OR_MESSAGE_LOCAL]]"
            llm_py_logger.info(f"Local LLM Raw Output: {str(raw_llm_output_text)[:200]}...")
    # ... (省略错误处理和日志记录，与您之前的 call_llm_via_openai_api 类似) ...
    except Exception as e:
        llm_py_logger.error(f"Error calling local LLM service: {e}", exc_info=True)
        error_info = str(e)
        # 确保记录错误
        log_error_data = {
            "interaction_id": str(uuid.uuid4()), "timestamp_utc": datetime.now(timezone.utc).isoformat(),
            "task_type": task_type + "_local_error", "user_query_for_task": user_query_for_log,
            "llm_input_messages": current_messages,
            "llm_input_original_prompt_if_string": original_prompt_for_log if isinstance(prompt, str) else None,
            "llm_parameters": llm_parameters_for_log,
            "raw_llm_output": f"Error: {error_info}. Partial raw output: {str(raw_llm_output_text)[:200] if raw_llm_output_text else 'N/A'}",
            "error_details": traceback.format_exc(), "application_version": application_version_for_log
        }
        await log_interaction_data(log_error_data)
        return None # 出错时返回None

    # 记录成功的调用
    log_success_data = {
        "interaction_id": str(uuid.uuid4()), "timestamp_utc": datetime.now(timezone.utc).isoformat(),
        "task_type": task_type, "user_query_for_task": user_query_for_log,
        "llm_input_messages": current_messages,
        "llm_input_original_prompt_if_string": original_prompt_for_log if isinstance(prompt, str) else None,
        "llm_parameters": llm_parameters_for_log,
        "raw_llm_output": raw_llm_output_text, "application_version": application_version_for_log
    }
    await log_interaction_data(log_success_data)
    return raw_llm_output_text

async def generate_cypher_query(user_question: str, kg_schema_description: str = NEW_KG_SCHEMA_DESCRIPTION) -> Optional[str]:
    llm_py_logger.info(f"Attempting to generate Cypher query for: '{user_question}' via local service with GBNF + post-processing.")
    
    system_prompt_for_json_cypher = kg_schema_description 
    messages_for_llm = [
        {"role": "system", "content": system_prompt_for_json_cypher},
        {"role": "user", "content": f"用户问题: {user_question}"} 
    ]
    cypher_stop_sequences = ['<|im_end|>', '无法生成Cypher查询.', '```'] # 添加 '```' 以防模型生成 Markdown 后想继续

    llm_response_json_str = await call_llm_via_openai_api_local_only( 
        prompt=messages_for_llm,
        temperature=0.0, 
        max_new_tokens=1024, 
        stop_sequences=cypher_stop_sequences, # <--- 使用定义的 stop_sequences
        task_type="cypher_generation_final_attempt_local_service",
        user_query_for_log=user_question,
        model_name_for_log="qwen3_gguf_cypher_final_local"
    )

    if not llm_response_json_str:
        llm_py_logger.warning(f"LLM call for Cypher (local_final) returned None or empty. User question: '{user_question}'")
        return "无法生成Cypher查询."

    try:
        parsed_response = json.loads(llm_response_json_str)
        
        status = parsed_response.get("status")
        query_content = parsed_response.get("query")

        # Log the received JSON from local_llm_service
        log_data_received = {
            "interaction_id": str(uuid.uuid4()),
            "timestamp_utc": datetime.now(timezone.utc).isoformat(),
            "task_type": "cypher_json_received_from_local_service",
            "user_query_for_task": user_question,
            "raw_json_from_local_service": llm_response_json_str,
            "parsed_status_from_local_service": status,
            "parsed_query_from_local_service": query_content
        }
        await log_interaction_data(log_data_received)

        if status == "success" and isinstance(query_content, str) and query_content.strip():
            llm_py_logger.info(f"Successfully extracted Cypher from local service JSON: {query_content}")
            return query_content.strip()
        elif status == "unable_to_generate" and query_content == "无法生成Cypher查询.":
            llm_py_logger.info(f"Local service indicated 'unable_to_generate' for: '{user_question}'")
            return "无法生成Cypher查询."
        else: # Should not happen if local_llm_service.py works as designed
            llm_py_logger.warning(f"Unexpected JSON structure from local_llm_service. Status: {status}, Query: {query_content}. Defaulting to 'unable'.")
            return "无法生成Cypher查询."
            
    except json.JSONDecodeError:
        llm_py_logger.error(f"Failed to parse JSON response from local_llm_service: '{llm_response_json_str}'", exc_info=True)
        return "无法生成Cypher查询."
    except Exception as e:
        llm_py_logger.error(f"Error processing response from local_llm_service: {e}", exc_info=True)
        return "无法生成Cypher查询."

async def generate_answer_from_context(user_query: str, context_str: str) -> Optional[str]: # context 参数名改为 context_str
    llm_py_logger.info(f"Generating answer for query: '{user_query[:100]}...' using provided context.")
    
    system_prompt_for_answer = f"""
你是一个非常严谨的AI问答助手。你的任务是根据【上下文信息】回答【用户问题】。

**核心指令：**

1.  **严格基于上下文：** 你的回答【必须只能】使用【上下文信息】中明确提供的文字。禁止进行任何形式的推断、联想、猜测或引入外部知识。
2.  **逐句核对：** 对于用户问题中的每一个信息点或子问题，你都必须在上下文中找到【直接对应的证据】才能回答。
3.  **明确关联性：** 如果用户问题试图关联上下文中的不同信息片段（例如，A是否与B有关），你必须在上下文中找到【明确陈述这种关联性的直接证据】。如果上下文中分别提到了A和B，但没有明确说明它们之间的关系，则视为无法关联。
4.  **处理无法回答的部分：**
    *   如果【上下文信息】完全不包含回答【用户问题】的任何相关信息，或者无法找到任何直接证据，请【只回答】：“{NO_ANSWER_PHRASE_ANSWER_CLEAN}”
    *   如果【用户问题】包含多个子问题，而【上下文信息】只能回答其中的一部分：
        *   请只回答你能找到直接证据的部分。
        *   对于上下文中没有直接证据支持的其他子问题，请明确指出：“关于[某子问题]，上下文中未提供明确信息。”
        *   不要对未提供信息的部分进行猜测或尝试回答。
5.  **简洁明了：** 回答要直接、简洁。

请严格遵守以上指令。
"""
    # 构造 messages 列表
    messages_for_llm = [
        {"role": "system", "content": system_prompt_for_answer},
        {"role": "user", "content": f"用户问题: {user_query}\n\n上下文信息:\n{context_str}"}
    ]

    raw_answer = await call_llm_via_openai_api_local_only(
        prompt=messages_for_llm, # <--- 传递 messages 列表
        temperature=0.05,
        max_new_tokens=1024, 
        stop_sequences=['<|im_end|>', UNIQUE_STOP_TOKEN], # 可以保留，以防万一
        task_type="answer_generation_from_context",
        user_query_for_log=user_query,
        model_name_for_log="qwen3_gguf_answer_gen"
    )
    
    if raw_answer and raw_answer.strip() and \
       raw_answer.strip() != "[[LLM_RESPONSE_MALFORMED_CHOICES_OR_MESSAGE]]" and \
       raw_answer.strip() != "[[CONTENT_NOT_FOUND]]":
        
        # local_llm_service.py 中的 post_process_llm_output 应该已经处理了 <think>
        # 但如果模型仍然可能输出 "根据目前提供的资料..." 之外的内容，
        # 而我们期望严格匹配，这里可以再加一层检查。
        # 对于答案生成，通常不需要像Cypher那样严格的后处理。
        final_answer = raw_answer.strip()
        if final_answer == NO_ANSWER_PHRASE_ANSWER_CLEAN:
            llm_py_logger.info("LLM indicated unable to answer from context.")
        return final_answer
    else:
        llm_py_logger.warning(f"Answer generation returned None, empty, or placeholder. Query: {user_query}")
        return NO_ANSWER_PHRASE_ANSWER_CLEAN # Fallback

async def generate_simulated_kg_query_response(user_query: str, kg_schema_description: str, kg_data_summary_for_prompt: str) -> Optional[str]:
    prompt_str = f"""<|im_start|>system
你是一个知识图谱查询助手。你的任务是根据用户提出的问题、知识图谱Schema描述和图谱中的数据摘要，直接抽取出与问题最相关的1-2个事实片段作为答案。
只输出事实片段，不要解释，不要生成Cypher语句，不要包含任何额外对话或标记。
如果找不到直接相关的事实，请**直接且完整地**回答：“{NO_ANSWER_PHRASE_KG_WITH_STOP_TOKEN}”<|im_end|>
<|im_start|>user
知识图谱Schema描述:
{kg_schema_description}

知识图谱数据摘要: 
{kg_data_summary_for_prompt}

用户问题: {user_query}<|im_end|>
<|im_start|>assistant
"""
    stop_sequences = ["<|im_end|>", UNIQUE_STOP_TOKEN]
    return await call_llm_via_openai_api_local_only(
        prompt=prompt_str,
        temperature=0.5,
        max_new_tokens=256,
        stop_sequences=stop_sequences,
        task_type="simulated_kg_query_response",
        user_query_for_log=user_query
    )

async def generate_expanded_queries(original_query: str) -> List[str]:
    prompt_str = f"""<|im_start|>system
你是一个专家查询分析师。根据用户提供的查询，生成3个不同但相关的子问题，以探索原始查询的不同方面。这些子问题将用于检索更全面的信息。
你的回答必须是一个JSON数组（列表），其中每个元素是一个字符串（子问题）。
只输出JSON数组，不要包含任何其他解释、对话标记或代码块。

示例:
用户查询: "公司年度财务报告和未来一年的预算规划"
助手:
[
  "公司最近的年度财务报告总结是什么？",
  "未来一年的详细预算规划有哪些主要构成？",
  "对比往年，公司财务状况有何显著变化？"
]<|im_end|>
<|im_start|>user
原始查询: {original_query}<|im_end|>
<|im_start|>assistant
"""
    stop_sequences = ["<|im_end|>"]
    
    llm_py_logger.info(f"调用LLM API进行查询扩展 (Prompt长度: {len(prompt_str)} 字符)...")
    llm_output = await call_llm_via_openai_api_local_only(
        prompt=prompt_str,
        temperature=0.7,
        max_new_tokens=512,
        stop_sequences=stop_sequences,
        task_type="query_expansion",
        user_query_for_log=original_query
    )
    expanded_queries = []
    if llm_output:
        try:
            json_str = llm_output.strip()
            if json_str.startswith("```json"):
                json_str = json_str[len("```json"):].strip()
            if json_str.endswith("```"):
                json_str = json_str[:-len("```")].strip()
            
            parsed_queries = json.loads(json_str)
            if isinstance(parsed_queries, list) and all(isinstance(q, str) for q in parsed_queries):
                expanded_queries = parsed_queries
                llm_py_logger.info(f"LLM成功生成 {len(expanded_queries)} 个扩展查询。")
            else:
                llm_py_logger.warning(f"LLM生成的扩展查询JSON格式不符合预期 (不是字符串列表): {llm_output[:200]}...")
        except json.JSONDecodeError as e:
            llm_py_logger.error(f"解析LLM扩展查询JSON失败: {e}. 原始输出: {llm_output[:200]}...", exc_info=True)
        except Exception as e:
            llm_py_logger.error(f"处理LLM扩展查询时发生未知错误: {e}. 原始输出: {llm_output[:200]}...", exc_info=True)
    else:
        llm_py_logger.warning("LLM未能生成扩展查询。")

    # Always include the original query
    if original_query not in expanded_queries:
        expanded_queries.append(original_query)
    return expanded_queries


async def generate_clarification_question(original_query: str, uncertainty_reason: str) -> Optional[str]:
    prompt_str = f"""<|im_start|>system
你是一个智能助手，擅长在理解用户查询时识别歧义并请求澄清。
你的任务是根据用户原始查询和系统检测到的不确定性原因，生成一个简洁、明确的澄清问题。
澄清问题应该帮助用户选择正确的意图，或者提供更多必要的信息。
只输出澄清问题，不要包含任何额外解释、对话标记或代码块。<|im_end|>
<|im_start|>user
用户原始查询: {original_query}
不确定性原因: {uncertainty_reason}

请生成一个澄清问题:<|im_end|>
<|im_start|>assistant
"""
    stop_sequences = ["<|im_end|>"]
    llm_py_logger.info(f"调用LLM API生成澄清问题 (Prompt长度: {len(prompt_str)} 字符)...")
    clarification_question = await call_llm_via_openai_api_local_only(
        prompt=prompt_str,
        temperature=0.5,
        max_new_tokens=128,
        stop_sequences=stop_sequences,
        task_type="clarification_question_generation",
        user_query_for_log=original_query
    )
    if not clarification_question or clarification_question.strip() == "":
        llm_py_logger.warning("LLM未能生成澄清问题，返回默认提示。")
        return "抱歉，我不太理解您的意思，请您再具体说明一下。"
    llm_py_logger.info(f"LLM成功生成澄清问题: {clarification_question.strip()}")
    return clarification_question.strip()

async def generate_clarification_options(original_query: str, uncertainty_reason: str) -> List[str]:
    prompt_str = f"""<|im_start|>system
你是一个智能助手，擅长根据用户查询的模糊性提供具体的澄清选项。
你的任务是根据用户原始查询和系统检测到的不确定性原因，生成3-5个具体的、可供用户选择的澄清选项。
每个选项都应该是一个简洁的短语或问题，帮助用户明确其意图。
你的回答必须是一个JSON数组（列表），其中每个元素是一个字符串（澄清选项）。
只输出JSON数组，不要包含任何其他解释、对话标记或代码块。

示例:
用户查询: "帮我预定机票。"
不确定性原因: "缺少出发城市、目的地、日期等信息。"
助手:
[
  "请问您想从哪个城市出发？",
  "请问您的目的地是哪里？",
  "请问您希望在哪一天出行？",
  "您有偏好的航空公司或舱位等级吗？"
]<|im_end|>
<|im_start|>user
用户原始查询: {original_query}
不确定性原因: {uncertainty_reason}

请生成澄清选项:<|im_end|>
<|im_start|>assistant
"""
    stop_sequences = ["<|im_end|>"]
    llm_py_logger.info(f"调用LLM API生成澄清选项 (Prompt长度: {len(prompt_str)} 字符)...")
    llm_output = await call_llm_via_openai_api_local_only(
        prompt=prompt_str,
        temperature=0.7,
        max_new_tokens=256,
        stop_sequences=stop_sequences,
        task_type="clarification_options_generation",
        user_query_for_log=original_query
    )

    options = []
    if llm_output:
        try:
            json_str = llm_output.strip()
            if json_str.startswith("```json"):
                json_str = json_str[len("```json"):].strip()
            if json_str.endswith("```"):
                json_str = json_str[:-len("```")].strip()
            
            parsed_options = json.loads(json_str)
            if isinstance(parsed_options, list) and all(isinstance(o, str) for o in parsed_options):
                options = parsed_options
                llm_py_logger.info(f"LLM成功生成 {len(options)} 个澄清选项。")
            else:
                llm_py_logger.warning(f"LLM生成的澄清选项JSON格式不符合预期 (不是字符串列表): {llm_output[:200]}...")
        except json.JSONDecodeError as e:
            llm_py_logger.error(f"解析LLM澄清选项JSON失败: {e}. 原始输出: {llm_output[:200]}...", exc_info=True)
        except Exception as e:
            llm_py_logger.error(f"处理LLM澄清选项时发生未知错误: {e}. 原始输出: {llm_output[:200]}...", exc_info=True)
    else:
        llm_py_logger.warning("LLM未能生成澄清选项。")
    
    if not options:
        options.append("请提供更多详细信息。")
    
    return options


INTENT_CLASSIFICATION_JSON_SCHEMA = {
    "type": "object",
    "properties": {
        "clarification_needed": {"type": "boolean"},
        "reason": {"type": "string"}
    },
    "required": ["clarification_needed", "reason"]
}

async def generate_intent_classification(user_query: str) -> Dict[str, Any]:
    llm_py_logger.info(f"Generating intent classification for query: '{user_query[:100]}...' using Gemini.")
    
    # 针对Gemini优化的Prompt，强调直接输出JSON
    system_prompt_for_intent = f"""你是一个智能意图分类器。你的任务是分析用户查询，判断该查询是否清晰明确，或者是否存在歧义、信息不足导致需要进一步澄清。
如果查询包含具体的命名实体（如人名“张三”、项目名“项目X”、产品名“新产品A”等），并且问题是关于这些实体的特定信息（例如“张三的职位是什么？”、“项目X的截止日期是哪天？”、“新产品A的功能有哪些？”），则通常认为查询是清晰的，不需要澄清。
只有当查询缺少定位关键信息所必需的核心实体，或者询问的范围过于宽泛无法直接操作时，才需要澄清。

如果查询需要澄清，请说明原因。
你的【唯一输出】必须是一个严格符合以下结构的JSON对象，不要包含任何其他文本、解释或markdown标记:
{{
  "clarification_needed": true/false,
  "reason": "如果需要澄清，请简要说明原因；如果不需要，则为空字符串。"
}}

示例1 (需要澄清 - 信息不足):
用户查询: "帮我预定明天去上海的机票。"
助手 JSON 输出:
{{
  "clarification_needed": true,
  "reason": "缺少出发城市、具体时间（上午/下午/晚上）、舱位等级等信息。"
}}

示例2 (不需要澄清 - 清晰):
用户查询: "公司最新的销售额报告在哪里可以找到？"
助手 JSON 输出:
{{
  "clarification_needed": false,
  "reason": ""
}}
"""
    
    messages_for_gemini = [
        {"role": "system", "content": system_prompt_for_intent},
        {"role": "user", "content": f"用户查询: {user_query}"}
    ]

    # 从环境变量获取Gemini配置
    gemini_model_name = os.getenv("CLOUD_LLM_MODEL_NAME_FOR_LITELLM", "gemini/gemini-1.5-flash-latest")
    gemini_api_key = os.getenv("GEMINI_API_KEY") # 或者 GOOGLE_API_KEY
    proxy_url = os.getenv("LITELLM_PROXY_URL")

    litellm_params: Dict[str, Any] = {
        "model": gemini_model_name,
        "messages": messages_for_gemini,
        "api_key": gemini_api_key,
        "temperature": 0.1, 
        "max_tokens": 256,  # 意图分类的JSON输出通常较短
        # "response_format": {"type": "json_object"} # LiteLLM的Gemini集成可能尚不支持此参数，暂时注释
    }
    if proxy_url:
        # LiteLLM 的 proxy 参数期望一个字典，或者直接是一个字符串URL (取决于LiteLLM版本和具体实现)
        # 为保险起见，我们按文档常见的字典格式提供
        litellm_params["proxy"] = {
            "http": proxy_url,
            "https": proxy_url,
        }
        # 或者，如果您的LiteLLM版本支持直接传递字符串URL作为代理：
        # litellm_params["api_base"] = proxy_url # 这会将代理用于所有请求，如果Gemini也通过此代理
        # litellm_params["base_url"] = proxy_url # 有些版本用 base_url
        # 更通用的方式是设置环境变量 HTTP_PROXY 和 HTTPS_PROXY，LiteLLM通常会读取它们
        # 但为了显式，我们这里尝试通过参数传递给litellm.acompletion

    llm_py_logger.info(f"Calling Gemini (via LiteLLM) for intent classification. Model: {gemini_model_name}")
    debug_params = {k:v for k,v in litellm_params.items() if k not in ['messages', 'api_key']}
    llm_py_logger.debug(f"LiteLLM params for intent (excluding messages & api_key): {debug_params}")
    
    raw_gemini_output_text = None
    error_info_intent = None
    parsed_result_dict: Optional[Dict[str, Any]] = None # 用于存储最终解析结果

    try:
        response = await litellm.acompletion(**litellm_params)
        if response and response.choices and response.choices[0].message and response.choices[0].message.content:
            raw_gemini_output_text = response.choices[0].message.content.strip()
            llm_py_logger.info(f"Gemini intent classification raw output: {raw_gemini_output_text[:300]}...")
            
            # 尝试解析JSON (与之前的提取逻辑类似)
            json_str_candidate = raw_gemini_output_text
            # 1. 尝试从 markdown block 中提取
            markdown_match = re.search(r"```json\s*(\{[\s\S]*?\})\s*```", json_str_candidate, re.DOTALL)
            if markdown_match:
                json_str_candidate = markdown_match.group(1)
                llm_py_logger.debug(f"Extracted JSON candidate from markdown: {json_str_candidate[:200]}...")
            
            # 2. 如果没有markdown，或者提取后仍然不是纯JSON，尝试直接解析或查找第一个 '{' 和最后一个 '}'
            try:
                parsed_result_dict = json.loads(json_str_candidate)
            except json.JSONDecodeError: # 如果直接解析失败
                first_brace = json_str_candidate.find('{')
                last_brace = json_str_candidate.rfind('}')
                if first_brace != -1 and last_brace != -1 and last_brace > first_brace:
                    json_str_candidate = json_str_candidate[first_brace : last_brace+1]
                    llm_py_logger.debug(f"Extracted JSON candidate by braces: {json_str_candidate[:200]}...")
                    try:
                        parsed_result_dict = json.loads(json_str_candidate)
                    except json.JSONDecodeError as e_json_brace:
                        error_info_intent = f"Failed to decode JSON from Gemini intent (braces): {e_json_brace}"
                        llm_py_logger.error(error_info_intent, exc_info=True)
                else: # 没有找到有效的花括号对
                    error_info_intent = "No valid JSON object found in Gemini intent output."
                    llm_py_logger.error(error_info_intent + f" Raw: {raw_gemini_output_text[:200]}")
            
            # 验证解析后的JSON结构
            if parsed_result_dict and isinstance(parsed_result_dict, dict) and \
               "clarification_needed" in parsed_result_dict and \
               "reason" in parsed_result_dict:
                llm_py_logger.info(f"Gemini successfully classified intent: {parsed_result_dict}")
                # 记录成功的调用
                log_data_intent = {
                    "interaction_id": str(uuid.uuid4()), "timestamp_utc": datetime.now(timezone.utc).isoformat(),
                    "task_type": "intent_classification_gemini", "user_query_for_task": user_query,
                    "llm_input_messages": messages_for_gemini, "llm_parameters": {k:v for k,v in litellm_params.items() if k not in ['messages', 'api_key', 'proxy']},
                    "raw_llm_output": raw_gemini_output_text, "application_version": "0.1.0_intent_gemini"
                }
                await log_interaction_data(log_data_intent)
                return parsed_result_dict
            else: # 解析成功但结构不对
                if parsed_result_dict: # 避免对None调用get
                    error_info_intent = f"Gemini intent output JSON structure mismatch. Parsed: {parsed_result_dict}"
                else: # parsed_result_dict 为 None (例如，花括号提取失败后)
                    error_info_intent = "Gemini intent output JSON structure mismatch (parsed_result_dict is None)."
                llm_py_logger.warning(error_info_intent)
        else: # response.choices[0].message.content 为空或不存在
            error_info_intent = "Gemini intent call returned empty or malformed response content."
            llm_py_logger.error(f"{error_info_intent} Full response object: {response}")

    except Exception as e_gemini_call:
        error_info_intent = f"Error calling Gemini for intent: {e_gemini_call}"
        llm_py_logger.error(error_info_intent, exc_info=True)

    # 如果执行到这里，说明出错了或者没有得到期望的JSON
    log_error_data_intent = {
        "interaction_id": str(uuid.uuid4()), "timestamp_utc": datetime.now(timezone.utc).isoformat(),
        "task_type": "intent_classification_gemini_error", "user_query_for_task": user_query,
        "llm_input_messages": messages_for_gemini, "llm_parameters": {k:v for k,v in litellm_params.items() if k not in ['messages', 'api_key', 'proxy']},
        "raw_llm_output": raw_gemini_output_text or "N/A", "error_details": error_info_intent,
        "application_version": "0.1.0_intent_gemini"
    }
    await log_interaction_data(log_error_data_intent)
    
    llm_py_logger.warning(f"Gemini failed to generate valid intent classification, defaulting to no clarification needed. Error: {error_info_intent or 'Unknown reason'}")
    return {"clarification_needed": False, "reason": f"Intent classification by Gemini failed: {error_info_intent or 'Unknown reason'}"}
    --- END OF FILE CONTENT ---


Directory: zhz_rag/llm/rag_eval_data/

Directory: zhz_rag/task_management/
  File: __init__.py
    --- File is empty ---

  File: db_models.py
    --- START OF FILE CONTENT ---
# zhz_agent/database_models.py
from sqlalchemy import Column, String, DateTime, Integer, Text, Enum as SQLAlchemyEnum, ForeignKey, Boolean, JSON
from sqlalchemy.sql import func
import uuid

# --- [修改] 从 pydantic_models 导入枚举 -> 改为绝对导入 ---
from zhz_rag.config.pydantic_models import TaskStatus, ReminderMethod

# --- [修改] 从新的 database.py 导入 Base -> 改为绝对导入 ---
from zhz_rag.utils.db_utils import Base # <--- 确保只从这里导入 Base #

class TaskDB(Base): # 命名为 TaskDB 以区分 Pydantic 的 TaskModel
    __tablename__ = "tasks"

    id = Column(String, primary_key=True, index=True, default=lambda: str(uuid.uuid4()))
    title = Column(String, index=True, nullable=False)
    description = Column(Text, nullable=True) #
    status = Column(SQLAlchemyEnum(TaskStatus), default=TaskStatus.PENDING, nullable=False) #
    created_at = Column(DateTime(timezone=True), server_default=func.now(), nullable=False) #
    updated_at = Column(DateTime(timezone=True), server_default=func.now(), onupdate=func.now(), nullable=False) #
    due_date = Column(DateTime(timezone=True), nullable=True) #
    reminder_time = Column(DateTime(timezone=True), nullable=True) #
    reminder_offset_minutes = Column(Integer, nullable=True) #
    reminder_methods = Column(JSON, default=[ReminderMethod.NOTIFICATION.value], nullable=False) #
    priority = Column(Integer, default=0, nullable=False) #
    tags = Column(JSON, default=[], nullable=False) #
    action_type = Column(String, nullable=True) #
    action_payload = Column(JSON, default={}, nullable=True) #
    execution_result = Column(Text, nullable=True) #
    last_executed_at = Column(DateTime(timezone=True), nullable=True) #

    def __repr__(self):
        return f"<TaskDB(id={self.id}, title='{self.title}', status='{self.status.value}')>"
    --- END OF FILE CONTENT ---

  File: jobs.py
    --- START OF FILE CONTENT ---
# zhz_agent/task_jobs.py
from datetime import datetime
from typing import Dict, Any
import os
import traceback
import httpx # <--- 确保 httpx 已导入
import json # <--- 确保 json 已导入

# 从 .database 导入 database 对象以便查询任务详情
# 从 .pydantic_models 导入 TaskModel 以便类型转换
# 从 .main 导入 scheduler 以便在需要时重新调度（虽然通常作业函数不直接操作调度器）
# 更好的做法是通过参数传递必要的信息，而不是依赖全局导入
WINDOWS_HOST_IP = os.getenv("WINDOWS_HOST_IP_FOR_WSL", "192.168.3.11") # <--- 请务必替换为您真实的Windows IP
LOCAL_AGENT_PORT = os.getenv("LOCAL_AGENT_PORT", "8003") # 与 local_agent_app.py 中的端口一致

# 如果 WINDOWS_HOST_IP 仍然是占位符，给出提示
if WINDOWS_HOST_IP == "在此处填写您上一步找到的Windows主机IP":
    print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
    print("REMINDER_JOB WARNING: WINDOWS_HOST_IP 未正确设置在 task_jobs.py 中!")
    print("请编辑 task_jobs.py 文件，将 '在此处填写您上一步找到的Windows主机IP' 替换为实际IP地址。")
    print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")

LOCAL_AGENT_NOTIFY_URL = f"http://{WINDOWS_HOST_IP}:{LOCAL_AGENT_PORT}/notify"

async def send_task_reminder(task_id: str, task_title: str, reminder_methods: list):
    """
    实际发送任务提醒的函数。
    """
    print(f"REMINDER_JOB: [{datetime.utcnow()}] 正在为任务 '{task_id}' - '{task_title}' 发送提醒。")
    for method in reminder_methods:
        if method == "notification": # 假设 ReminderMethod.NOTIFICATION.value 是 "notification"
            print(f"  REMINDER_JOB: 尝试通过 Local Agent 发送桌面通知: '{task_title}'")
            try:
                async with httpx.AsyncClient(timeout=10.0) as client:
                    response = await client.post(
                        LOCAL_AGENT_NOTIFY_URL,
                        json={"title": f"任务提醒: {task_title}", "message": f"任务 '{task_title}' 即将到期或需要关注。"}
                    )
                    response.raise_for_status() # Raise an exception for bad status codes
                    print(f"  REMINDER_JOB: 本地代理通知请求发送成功. 状态: {response.status_code}")
            except httpx.RequestError as e:
                print(f"  REMINDER_JOB: 通过本地代理发送通知失败 (RequestError): {e}")
                traceback.print_exc()
            except Exception as e:
                print(f"  REMINDER_JOB: 通过本地代理发送通知失败 (General Error): {e}")
                traceback.print_exc()
        # elif method == "email": #
        #     print(f"  REMINDER_JOB: 模拟发送邮件提醒...")

async def execute_task_action(task_id: str, action_type: str, action_payload: Dict[str, Any]):
    """
    实际执行任务动作的函数。
    """
    print(f"EXECUTION_JOB: [{datetime.utcnow()}] 正在为任务 '{task_id}' 执行动作 '{action_type}'。")
    print(f"  EXECUTION_JOB: 动作参数: {action_payload}")

    final_result = f"动作 '{action_type}' 已模拟执行。"
    success = True

    if action_type == "navigate":
        destination = action_payload.get("destination")
        if destination:
            print(f"  EXECUTION_JOB: 模拟导航到 '{destination}'...")
            final_result = f"已模拟为导航到 '{destination}' 准备好路线。"
        else:
            final_result = "导航动作失败：缺少目的地。"
            success = False
    elif action_type == "log_event":
        event_details = action_payload.get("event_details", "无详情")
        print(f"  EXECUTION_JOB: 记录事件: '{event_details}'")
        final_result = f"事件 '{event_details}' 已记录。"
    else:
        final_result = f"未知的动作类型: {action_type}"
        success = False

    # 更新数据库中的任务状态和结果 (需要访问数据库)
    # 这部分逻辑最好通过API调用或服务层来完成，以避免循环导入和分散DB操作
    # 这里我们只打印信息，实际应用中需要实现DB更新
    print(f"  EXECUTION_JOB: 任务 '{task_id}' 执行完毕。结果: {final_result}, 状态: {'COMPLETED' if success else 'FAILED'}")

    --- END OF FILE CONTENT ---


Directory: zhz_rag/finetuning/
  File: __init__.py
    --- File is empty ---

  File: refine_answer_data.py
    --- START OF FILE CONTENT ---
# zhz_agent/refine_answer_finetune_data.py
import json
import os
import pandas as pd
from typing import List, Dict, Any, Optional
import glob
from datetime import datetime

# 假设 utils.py 和 constants.py 在同一个 zhz_agent 包内
try:
    from zhz_rag.utils.common_utils import (
    RAG_INTERACTION_LOGS_DIR,
    EVALUATION_RESULTS_LOGS_DIR,
    FINETUNING_GENERATED_DATA_DIR,
    find_latest_rag_interaction_log # 确保这个也被导入
)
    from zhz_rag.config.pydantic_models import RetrievedDocument
    # NO_ANSWER_PHRASE_ANSWER_CLEAN 将从 llm.py 导入，或者在constants.py中定义
    # 我们需要与 llm.py -> generate_answer_from_context 一致的 "无法回答" 短语
    from zhz_rag.llm.sglang_wrapper import NO_ANSWER_PHRASE_ANSWER_CLEAN 
except ImportError as e:
    print(f"ERROR: Could not import necessary modules for refine_answer_finetune_data: {e}")
    exit(1)

import logging

# 配置此脚本的logger
refine_answer_logger = logging.getLogger("RefineAnswerFinetuneDataLogger")
refine_answer_logger.setLevel(logging.INFO)
if not refine_answer_logger.hasHandlers():
    _console_handler = logging.StreamHandler()
    _formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
    _console_handler.setFormatter(_formatter)
    refine_answer_logger.addHandler(_console_handler)
    refine_answer_logger.info("--- RefineAnswerFinetuneDataLogger configured ---")

# --- 配置 ---
RAG_LOG_DIR = "zhz_rag/stored_data/evaluation_results_logs/"
EVAL_LOG_DIR = "zhz_rag/stored_data/evaluation_results_logs/"
FINETUNE_DATA_DIR = "zhz_rag/finetuning/generated_data/"
os.makedirs(FINETUNE_DATA_DIR, exist_ok=True)

# --- 与 run_batch_answer_evaluation.py 中类似的上下文格式化函数 ---
def format_contexts_for_prompt(context_docs_raw: List[Dict[str, Any]]) -> str:
    """
    将从日志中解析出的上下文文档列表格式化为单一字符串，用于构建LLM的输入Prompt。
    这个格式应该与 llm.py -> generate_answer_from_context 中构建上下文的方式一致。
    """
    context_strings_for_llm = []
    if not context_docs_raw:
        return "No context provided."
        
    for i, doc_data in enumerate(context_docs_raw):
        try:
            # 尝试使用RetrievedDocument模型解析，如果原始日志中已经是这个结构
            # 但通常日志中可能是字典列表
            doc_content = doc_data.get("content", "[Content not available]")
            doc_source = doc_data.get("source_type", "unknown_source")
            doc_score = doc_data.get("score")
            
            # 与 rag_service.py 中准备上下文给LLM的格式保持一致
            # 在 rag_service.py 中是:
            # f"Source Type: {doc.source_type}, Score: {doc.score:.4f}\nContent: {doc.content}"
            # 我们这里也尽量模拟，但日志中的score可能不存在或格式不同
            header = f"Source Type: {doc_source}"
            if doc_score is not None:
                try:
                    header += f", Score: {float(doc_score):.4f}"
                except ValueError:
                    header += f", Score: {doc_score}" # 如果分数不是数字，直接用原始值
            
            context_strings_for_llm.append(f"{header}\nContent: {doc_content}")

        except Exception as e:
            refine_answer_logger.warning(f"Could not parse a context document fully for prompt: {doc_data}. Error: {e}")
            content = doc_data.get("content", "[Content not available]")
            context_strings_for_llm.append(f"Content: {content}") # 简化版

    return "\n\n---\n\n".join(context_strings_for_llm) if context_strings_for_llm else "No context provided."


def construct_qwen_answer_input_prompt(user_question: str, formatted_context: str) -> str:
    """
    根据用户问题和格式化的上下文构建Qwen生成答案时的完整输入Prompt。
    这个函数必须与 llm.py 中 generate_answer_from_context 内部构建Prompt的逻辑完全一致。
    """
    # --- 从 llm.py 的 generate_answer_from_context 函数复制并粘贴完整的 prompt 模板 ---
    # 注意：这里需要确保模板与 llm.py 中的完全一致
    prompt = f"""
<|im_start|>system
你是一个AI问答助手。你的任务是根据【上下文信息】回答【用户问题】。

**核心指令：**

1.  **尝试直接回答：** 请首先仔细阅读【上下文信息】，如果其中包含能直接回答【用户问题】的内容，请用上下文中的信息直接、简洁地回答。
2.  **忠实原文：** 你的回答必须严格基于【上下文信息】，禁止加入任何外部知识或个人观点。
3.  **如果无法回答：** 如果你分析了【上下文信息】后，确认其中确实没有能回答【用户问题】的明确信息，那么请只回答以下这句话：
    "根据目前提供的资料，我无法找到关于您问题的明确信息。"
    **不要添加任何其他解释、建议或反问。**

**请直接给出答案，或者只给出上述那句固定的“无法找到信息”的回复。**
<|im_start|>user
用户问题: {user_question}

上下文信息:
{formatted_context}
<|im_end|>
<|im_start|>assistant
"""
    return prompt

def load_logs_to_dict(filepath: str, key_field: str = "interaction_id") -> Dict[str, Dict[str, Any]]:
    """将JSONL文件加载到一个以指定字段为键的字典中。"""
    data_dict = {}
    if not os.path.exists(filepath):
        refine_answer_logger.error(f"Log file not found: {filepath}")
        return data_dict
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                log_entry = json.loads(line.strip())
                if key_field in log_entry and log_entry[key_field]: # 确保key_field的值不是None或空
                    data_dict[log_entry[key_field]] = log_entry
                elif key_field == "original_interaction_id_ref" and log_entry.get("original_interaction_id_ref"):
                    data_dict[log_entry["original_interaction_id_ref"]] = log_entry
            except json.JSONDecodeError:
                refine_answer_logger.warning(f"Skipping malformed JSON line in {filepath}")
    return data_dict

def generate_finetune_samples_for_answer(
    rag_interaction_logs: Dict[str, Dict[str, Any]],
    answer_evaluation_logs: Dict[str, Dict[str, Any]]
) -> List[Dict[str, str]]:
    finetune_samples = []
    processed_ids = set()

    refine_answer_logger.info(f"Processing {len(rag_interaction_logs)} RAG interaction logs and {len(answer_evaluation_logs)} Answer evaluation logs.")

    for interaction_id, rag_log in rag_interaction_logs.items():
        if rag_log.get("task_type") != "rag_query_processing_success":
            continue

        if interaction_id in processed_ids:
            continue
        processed_ids.add(interaction_id)

        user_question = rag_log.get("user_query")
        qwen_generated_answer_raw = rag_log.get("processed_llm_output") # Qwen的原始答案
        # retrieved_context_docs 在 rag_log 中可能是 "retrieved_context_docs" 或 "retrieved_documents_summary"
        # 我们需要原始的、完整的上下文文档
        retrieved_context_docs_raw = rag_log.get("retrieved_context_docs") 
        
        if not retrieved_context_docs_raw and rag_log.get("debug_info"): # 尝试从debug_info获取
             retrieved_context_docs_raw = rag_log.get("debug_info",{}).get("retrieved_context_docs")


        if qwen_generated_answer_raw is None or not qwen_generated_answer_raw.strip():
            qwen_generated_answer = NO_ANSWER_PHRASE_ANSWER_CLEAN # 空答案视为无法回答
        else:
            qwen_generated_answer = qwen_generated_answer_raw.strip()

        if not user_question or not retrieved_context_docs_raw:
            refine_answer_logger.warning(f"Skipping RAG log {interaction_id} due to missing user_question or retrieved_context_docs.")
            continue
        
        # 构建Prompt
        formatted_contexts_for_prompt = format_contexts_for_prompt(retrieved_context_docs_raw)
        qwen_answer_input_prompt = construct_qwen_answer_input_prompt(user_question, formatted_contexts_for_prompt)

        ideal_answer_output = None
        source_of_ideal = "unknown"
        gemini_scores_for_log = {}

        eval_log = answer_evaluation_logs.get(interaction_id)

        if eval_log and eval_log.get("eval_llm_processed_output_json"):
            eval_json = eval_log["eval_llm_processed_output_json"]
            summary_eval = eval_json.get("evaluation_summary", {})
            dimensions_eval = eval_json.get("dimensions", {})
            
            overall_score_str = summary_eval.get("overall_answer_quality_score")
            faithfulness_score_str = dimensions_eval.get("faithfulness", {}).get("score")
            relevance_score_str = dimensions_eval.get("relevance", {}).get("score")
            completeness_score_str = dimensions_eval.get("completeness", {}).get("score")
            context_sufficiency = dimensions_eval.get("completeness", {}).get("context_sufficiency_assessment", "Unknown")
            gemini_suggestion_answer = eval_json.get("suggestion_for_answer_improvement", "").strip()

            try:
                overall_score = int(overall_score_str) if overall_score_str is not None else 0
                faithfulness_score = int(faithfulness_score_str) if faithfulness_score_str is not None else 0
                relevance_score = int(relevance_score_str) if relevance_score_str is not None else 0
                completeness_score = int(completeness_score_str) if completeness_score_str is not None else 0
                gemini_scores_for_log = {
                    "overall": overall_score,
                    "faithfulness": faithfulness_score,
                    "relevance": relevance_score,
                    "completeness": completeness_score,
                    "context_sufficiency": context_sufficiency
                }
            except (ValueError, TypeError) as e:
                refine_answer_logger.warning(f"Could not parse one or more scores for {interaction_id}: {e}")
                overall_score = faithfulness_score = relevance_score = completeness_score = 0
                gemini_scores_for_log = { # 记录解析失败
                    "overall": "parse_error", "faithfulness": "parse_error", 
                    "relevance": "parse_error", "completeness": "parse_error",
                    "context_sufficiency": context_sufficiency
                }
                
            # --- Completion选择逻辑 (改进版) ---
            ideal_answer_output = None # 重新初始化
            source_of_ideal = "unknown" # 重新初始化

            # 规则 0: Qwen的原始答案就是标准的“无法回答”短语
            is_qwen_standard_no_answer = (qwen_generated_answer == NO_ANSWER_PHRASE_ANSWER_CLEAN)

            # 规则 1: 上下文不足，且Qwen正确地给出了标准的“无法回答”
            if is_qwen_standard_no_answer and \
               context_sufficiency == "Insufficient" and \
               overall_score >= 4: # Gemini认为Qwen的这个“无法回答”是高质量的
                ideal_answer_output = NO_ANSWER_PHRASE_ANSWER_CLEAN
                source_of_ideal = "qwen_standard_no_answer_confirmed_by_gemini_context_insufficient"
            
            # 规则 2: 上下文不足，Qwen可能没有给出标准“无法回答”，但Gemini建议应指出信息不足
            elif not is_qwen_standard_no_answer and \
                 context_sufficiency == "Insufficient" and \
                 completeness_score <=2 and \
                 ("information is not available" in gemini_suggestion_answer.lower() or \
                  "context does not contain" in gemini_suggestion_answer.lower() or \
                  "cannot be answered from the context" in gemini_suggestion_answer.lower() or \
                  "should state that the information is not found" in gemini_suggestion_answer.lower()):
                ideal_answer_output = NO_ANSWER_PHRASE_ANSWER_CLEAN
                source_of_ideal = "gemini_suggests_no_answer_due_to_insufficient_context"

            # 规则 3: Gemini 整体评分很高 (例如 overall, faithfulness, relevance 都 >= 4)
            # 并且 Qwen 的答案不是标准的“无法回答”（如果已经是，则由规则1处理）
            elif not is_qwen_standard_no_answer and \
                 overall_score >= 4 and faithfulness_score >= 4 and relevance_score >= 4:
                ideal_answer_output = qwen_generated_answer
                source_of_ideal = "qwen_high_score_by_gemini"
                # 如果此时 Gemini 仍有改进建议，可以额外标记
                if gemini_suggestion_answer and \
                   gemini_suggestion_answer != "No improvement needed." and \
                   "suggestion" not in source_of_ideal: # 避免重复标记
                    source_of_ideal += "_with_minor_gemini_suggestion"


            # 规则 4: Qwen的答案评分不高，但Gemini给出了具体的改进建议
            # 我们将这类样本标记出来，completion暂时使用Qwen的答案，供人工审核和优化
            elif overall_score < 4 and \
                 gemini_suggestion_answer and \
                 gemini_suggestion_answer != "No improvement needed." and \
                 len(gemini_suggestion_answer) > 10: # 假设太短的建议可能不具体
                ideal_answer_output = qwen_generated_answer # 保留Qwen答案作为基础
                source_of_ideal = "qwen_low_score_with_gemini_suggestion_for_review"
                refine_answer_logger.info(f"Answer log {interaction_id} (Qwen: '{qwen_generated_answer[:100]}...') marked for review due to low score but has Gemini suggestion: '{gemini_suggestion_answer[:100]}...'")
            
            # 规则 5: 如果Qwen的答案是标准“无法回答”，但上下文其实是充分的，或者Gemini认为可以回答
            # 这通常意味着Qwen可能错误地判断无法回答，或者Gemini的评估与Qwen的判断不一致
            elif is_qwen_standard_no_answer and \
                 (context_sufficiency == "Sufficient" or (context_sufficiency == "Partially Sufficient" and completeness_score >=3)) and \
                 overall_score < 4 : # Gemini不认可这个“无法回答”
                ideal_answer_output = qwen_generated_answer # 保留Qwen的“无法回答”
                source_of_ideal = "qwen_no_answer_but_gemini_disagrees_or_context_sufficient_for_review"
                refine_answer_logger.info(f"Answer log {interaction_id}: Qwen said 'no answer', but Gemini scores/context sufficiency suggest it might be answerable. Marked for review. Gemini scores: {gemini_scores_for_log}, Suggestion: '{gemini_suggestion_answer[:100]}...'")

            # 规则 6: 其他所有情况，暂时跳过，等待更明确的规则或人工审核
            else:
                refine_answer_logger.info(f"Answer log {interaction_id} (Qwen: '{qwen_generated_answer[:100]}...') did not meet current finetune criteria. Needs manual review or rule adjustment. Gemini scores: {gemini_scores_for_log}, Suggestion: '{gemini_suggestion_answer[:100]}...'")
                continue

        else: # 没有有效的Gemini评估日志
            refine_answer_logger.warning(f"No valid Gemini evaluation found for Answer log {interaction_id}. Qwen's output: '{qwen_generated_answer[:100]}...'. Skipping for finetune data.")
            continue
            
        if ideal_answer_output is not None:
            finetune_samples.append({
                "prompt": qwen_answer_input_prompt,
                "completion": ideal_answer_output.strip(),
                "original_qwen_answer": qwen_generated_answer_raw.strip() if qwen_generated_answer_raw else NO_ANSWER_PHRASE_ANSWER_CLEAN, # 记录Qwen最原始的输出
                "gemini_scores": gemini_scores_for_log,
                "gemini_suggestion": gemini_suggestion_answer if eval_log and eval_log.get("eval_llm_processed_output_json") else None,
                "source_of_ideal": source_of_ideal,
                "interaction_id": interaction_id
            })

    refine_answer_logger.info(f"Generated {len(finetune_samples)} Answer finetuning samples.")
    return finetune_samples


if __name__ == "__main__":
    rag_log_file = find_latest_rag_interaction_log(RAG_INTERACTION_LOGS_DIR)
    
    eval_log_file = None
    if rag_log_file:
        rag_log_basename = os.path.basename(rag_log_file)
        date_str_match = "".join(filter(str.isdigit, rag_log_basename))
        if len(date_str_match) >= 8:
            date_str = date_str_match[:8]
            evaluation_name = "answer_gemini_flash" # 与 evaluation.py 中一致
            eval_file_name = f"eval_results_{evaluation_name}_{date_str}.jsonl"
            eval_log_file = os.path.join(EVAL_LOG_DIR, eval_file_name)
            refine_answer_logger.info(f"Attempting to load Answer evaluation results from: {eval_log_file}")
        else:
            refine_answer_logger.error(f"Could not reliably extract date from RAG log filename: {rag_log_basename}")

    if not rag_log_file or not eval_log_file or not os.path.exists(eval_log_file):
        refine_answer_logger.error("Required log files for answer finetune data generation not found. Exiting.")
    else:
        rag_interactions = load_logs_to_dict(rag_log_file, key_field="interaction_id")
        answer_evaluations = load_logs_to_dict(eval_log_file, key_field="original_interaction_id_ref")

        if rag_interactions and answer_evaluations:
            finetune_data = generate_finetune_samples_for_answer(rag_interactions, answer_evaluations)
            
            if finetune_data:
                today_for_filename = datetime.now().strftime("%Y%m%d")
                output_filepath = os.path.join(FINETUNING_GENERATED_DATA_DIR, f"answer_finetune_samples_{today_for_filename}.jsonl")
                
                with open(output_filepath, 'w', encoding='utf-8') as f_out:
                    for sample in finetune_data:
                        f_out.write(json.dumps(sample, ensure_ascii=False) + "\n")
                refine_answer_logger.info(f"Successfully saved {len(finetune_data)} Answer finetuning samples to: {output_filepath}")
                
                try:
                    df = pd.DataFrame(finetune_data)
                    csv_output_filepath = os.path.join(FINETUNING_GENERATED_DATA_DIR, f"answer_finetune_samples_review_{today_for_filename}.csv")
                    df.to_csv(csv_output_filepath, index=False, encoding='utf-8-sig')
                    refine_answer_logger.info(f"Reviewable CSV for answers saved to: {csv_output_filepath}")
                except Exception as e_csv:
                    refine_answer_logger.error(f"Failed to save answer review CSV: {e_csv}")
            else:
                refine_answer_logger.info("No answer finetuning samples were generated.")
        else:
            refine_answer_logger.error("Failed to load data from log files for answer finetuning.")
    --- END OF FILE CONTENT ---

  File: refine_cypher_data.py
    --- START OF FILE CONTENT ---
# zhz_agent/refine_cypher_finetune_data.py
import json
import os
import pandas as pd
from typing import List, Dict, Any, Optional, Tuple
import glob
from datetime import datetime

# 假设 utils.py 和 constants.py 在同一个 zhz_agent 包内
try:
    from zhz_rag.utils.common_utils import get_interaction_log_filepath, get_evaluation_result_log_filepath, find_latest_rag_interaction_log # <--- 修改这里
    from zhz_rag.config.constants import NEW_KG_SCHEMA_DESCRIPTION
except ImportError as e:
    print(f"ERROR: Could not import necessary modules: {e}")
    # ... (错误处理)
    exit(1)

import logging

# 配置此脚本的logger
refine_logger = logging.getLogger("RefineFinetuneDataLogger")
refine_logger.setLevel(logging.INFO)
if not refine_logger.hasHandlers():
    _console_handler = logging.StreamHandler()
    _formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
    _console_handler.setFormatter(_formatter)
    refine_logger.addHandler(_console_handler)
    refine_logger.info("--- RefineFinetuneDataLogger configured ---")

# --- 配置 ---
# 原始RAG交互日志的目录 (包含cypher_generation类型)
RAG_LOG_DIR = "zhz_rag/stored_data/rag_interaction_logs/"
# Gemini评估结果日志的目录
EVAL_LOG_DIR = "zhz_rag/stored_data/evaluation_results_logs/"
# 输出微调数据文件的目录
FINETUNE_DATA_DIR = "zhz_rag/finetuning/generated_data/"
os.makedirs(FINETUNE_DATA_DIR, exist_ok=True)


def load_logs_to_dict(filepath: str, key_field: str = "interaction_id") -> Dict[str, Dict[str, Any]]:
    """将JSONL文件加载到一个以指定字段为键的字典中。"""
    data_dict = {}
    if not os.path.exists(filepath):
        refine_logger.error(f"Log file not found: {filepath}")
        return data_dict
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                log_entry = json.loads(line.strip())
                if key_field in log_entry:
                    data_dict[log_entry[key_field]] = log_entry
                # 对于评估日志，我们可能需要用 original_interaction_id_ref 作为键
                elif key_field == "original_interaction_id_ref" and log_entry.get("original_interaction_id_ref"):
                    data_dict[log_entry["original_interaction_id_ref"]] = log_entry
            except json.JSONDecodeError:
                refine_logger.warning(f"Skipping malformed JSON line in {filepath}")
    return data_dict

def construct_qwen_input_prompt(user_question: str, schema_description: str) -> str:
    """
    根据用户问题和Schema描述构建Qwen生成Cypher时的完整输入Prompt。
    这个函数应该与 llm.py 中 generate_cypher_query 内部构建Prompt的逻辑一致。
    """
    # 这是我们在 llm.py -> generate_cypher_query 中使用的Prompt模板
    # 我们需要确保这里的模板与Qwen实际接收到的一致
    # 注意：这里使用了最新的V7版本（或您当前使用的版本）的Schema描述作为基础
    # 如果您的 generate_cypher_query 中的模板不同，请相应调整
    prompt = f"""<|im_start|>system
你是顶级Neo4j Cypher查询生成专家。你的任务是根据用户问题和严格提供的【知识图谱Schema】，生成一个【语法正确】、【逻辑合理】且【高效】的Cypher查询。

**【核心指令与约束 - 必须严格遵守！】**

1.  **【Schema绝对绑定 - 最高优先级】**:
    *   你生成的Cypher查询中所有用到的【节点标签】、【关系类型】、【属性名称】及其对应的【数据类型】，都**必须严格存在于**下面提供的 "知识图谱Schema描述" 中。
    *   在构建查询的每一步，都要反复与Schema核对。**严禁臆断、猜测或使用任何Schema中未明确定义的元素。**
    *   **属性名称的大小写和确切拼写必须与Schema完全一致。**
    *   **关系类型的名称和方向必须与Schema完全一致。** 例如，如果Schema定义为 `(Person)-[:WORKS_ON]->(Project)`，则查询中不能是 `(Project)-[:WORKS_ON]->(Person)`，除非Schema中也定义了反向关系。

2.  **【纯净输出格式 - 严格要求】**:
    *   如果能生成有效查询，你的回答**必须只包含纯粹的Cypher查询语句本身**。
    *   如果根据问题和Schema无法生成有效的Cypher查询（例如，问题超出了Schema表达能力，或问题本身逻辑不通），则**必须只输出固定的短语：“无法生成Cypher查询。”**
    *   **绝对禁止**在有效的Cypher语句前后添加任何前缀（如“Cypher查询: ”）、后缀、解释、注释、markdown标记（如 ```cypher ```）或任何其他多余的文本。

3.  **【属性与值的使用】**:
    *   当在`WHERE`子句中对属性进行匹配时，确保值的类型与Schema中定义的属性类型一致。例如，如果`name`是字符串，则匹配 `name: '张三'`；如果`year`是数字，则匹配 `year: 2023`。
    *   对于数值计算（如`SUM`, `AVG`），**必须**使用Schema中明确指定的数字类型属性（例如，`SalesAmount`节点的 `numeric_amount`）。

4.  **【查询构建逻辑指引】**:
    *   **实体识别**: 准确识别用户问题中的核心实体及其在Schema中对应的节点标签和属性。
    *   **关系路径**: 基于问题和Schema构建清晰的`MATCH`路径。
    *   **条件过滤**: 使用`WHERE`子句添加必要的过滤条件。
    *   **结果返回**: 使用`RETURN`子句指定需要返回的信息，并用`AS`为返回的列指定清晰、合法的别名（字母或下划线开头）。
    *   **多步查询**: 对于需要关联多个信息点的问题，合理使用`WITH`传递中间结果。
    *   **聚合**: 如需统计或汇总，正确使用`COUNT()`, `SUM()`, `COLLECT()`等聚合函数。

**【知识图谱Schema描述】**:
{schema_description}

**【查询示例 - 严格基于上述Schema】**:

*   用户问题: "张三参与了哪个项目？"
    Cypher查询: MATCH (p:Person {{name: '张三'}})-[:WORKS_ON]->(proj:Project) RETURN proj.name AS projectName

*   用户问题: "华东区域2024年第一季度的销售额是多少？"
    Cypher查询: MATCH (r:Region {{name: '华东'}})-[:HAS_SALES_AMOUNT]->(sa:SalesAmount {{period: '2024年第一季度'}}) RETURN sa.numeric_amount AS salesAmount, sa.unit AS salesUnit

*   用户问题: "查询所有产品的名称。"
    Cypher查询: MATCH (prod:Product) RETURN prod.name AS productName

*   用户问题: "项目X有哪些人参与？"
    Cypher查询: MATCH (p:Person)-[:WORKS_ON]->(proj:Project {{name: '项目X'}}) RETURN p.name AS participantName

*   用户问题: "2024年第一季度所有区域的总销售额是多少？"
    Cypher查询: MATCH (r:Region)-[:HAS_SALES_AMOUNT]->(sa:SalesAmount {{period: '2024年第一季度'}}) RETURN sum(sa.numeric_amount) AS totalSales, sa.unit AS commonUnit LIMIT 1 
    (此查询假设所有相关销售额的单位是相同的，并取第一个出现的单位作为代表)

*   用户问题: "与新产品A相关的文档ID是什么？"
    Cypher查询: MATCH (p:Product {{name: '新产品A'}})-[:RELATED_TO]->(d:Document) RETURN d.id AS documentId

*   用户问题: "公司CEO是谁？" (假设Schema中没有CEO信息)
    Cypher查询: 无法生成Cypher查询。

现在，请根据以下用户问题和上述Schema及规则生成Cypher查询。
<|im_end|>
<|im_start|>user
用户问题: {user_question}
<|im_end|>
<|im_start|>assistant
"""
    return prompt

def generate_finetune_samples_for_cypher(
    rag_interaction_logs: Dict[str, Dict[str, Any]],
    cypher_evaluation_logs: Dict[str, Dict[str, Any]]
) -> List[Dict[str, str]]:
    """
    根据原始交互日志和Gemini评估日志，生成用于Cypher微调的样本。
    返回一个列表，每个元素是 {"prompt": "...", "completion": "..."}
    """
    finetune_samples = []
    processed_ids = set()

    refine_logger.info(f"Processing {len(rag_interaction_logs)} RAG interaction logs and {len(cypher_evaluation_logs)} Cypher evaluation logs.")

    for interaction_id, rag_log in rag_interaction_logs.items():
        if rag_log.get("task_type") != "cypher_generation":
            continue

        if interaction_id in processed_ids:
            continue
        processed_ids.add(interaction_id)

        user_question = rag_log.get("user_query")
        qwen_generated_cypher_raw = rag_log.get("processed_llm_output") # 这是Qwen原始输出

        # --- 改进点: 处理Qwen输出为空或仅包含空白的情况 ---
        if qwen_generated_cypher_raw is None or not qwen_generated_cypher_raw.strip():
            qwen_generated_cypher = "无法生成Cypher查询." # 将空输出也视为无法生成
            refine_logger.info(f"Interaction {interaction_id}: Qwen output was empty/None, treating as '无法生成Cypher查询.'.")
        else:
            qwen_generated_cypher = qwen_generated_cypher_raw.strip()


        qwen_input_prompt = rag_log.get("llm_input_prompt")
        if not qwen_input_prompt:
            if user_question:
                qwen_input_prompt = construct_qwen_input_prompt(user_question, NEW_KG_SCHEMA_DESCRIPTION)
            else:
                refine_logger.warning(f"Skipping Cypher log {interaction_id} due to missing user_question for prompt reconstruction.")
                continue
        
        if not user_question: # qwen_generated_cypher 已确保非None
            refine_logger.warning(f"Skipping Cypher log {interaction_id} due to missing user_question.")
            continue

        ideal_cypher_output = None
        source_of_ideal = "unknown"
        gemini_score_for_log = None # 用于记录

        eval_log = cypher_evaluation_logs.get(interaction_id)

        if eval_log and eval_log.get("eval_llm_processed_output_json"):
            eval_json = eval_log["eval_llm_processed_output_json"]
            overall_score_str = eval_json.get("evaluation_summary", {}).get("overall_quality_score_cypher")
            gemini_suggestion_raw = eval_json.get("suggestion_for_improvement_cypher", "").strip()
            
            try:
                overall_score = int(overall_score_str)
                gemini_score_for_log = overall_score
            except (ValueError, TypeError):
                refine_logger.warning(f"Could not parse overall_quality_score_cypher for {interaction_id}: {overall_score_str}")
                overall_score = 0 # 默认给个低分
                gemini_score_for_log = 0

            # --- 规则1: Qwen自己就说无法生成 ---
            if qwen_generated_cypher == "无法生成Cypher查询.":
                # 如果Gemini也认为无法生成或评分低，那么采纳
                if "无法生成Cypher查询" in gemini_suggestion_raw or overall_score <= 2:
                    ideal_cypher_output = "无法生成Cypher查询."
                    source_of_ideal = "qwen_and_gemini_cannot_generate"
                # 如果Qwen说无法生成，但Gemini给出了高分建议，这很奇怪，需要人工看
                elif overall_score >=4 and "MATCH" in gemini_suggestion_raw.upper():
                     refine_logger.info(f"Cypher log {interaction_id}: Qwen said '无法生成', but Gemini suggested a high-score query '{gemini_suggestion_raw}'. Needs manual review.")
                     continue
                else: # Qwen说无法生成，Gemini建议不明确或中低分，也采纳Qwen的
                    ideal_cypher_output = "无法生成Cypher查询."
                    source_of_ideal = "qwen_cannot_generate_gemini_unclear"


            # --- 规则2: Qwen生成了查询，看Gemini评估 ---
            elif overall_score >= 4: # Gemini认为Qwen的输出质量高
                ideal_cypher_output = qwen_generated_cypher
                source_of_ideal = "qwen_high_score_by_gemini"

            
            # --- 规则3: Qwen的查询质量不高，但Gemini给出了具体的、看起来像Cypher的建议 ---
            elif gemini_suggestion_raw and \
                "无法生成Cypher查询" not in gemini_suggestion_raw and \
                "cannot be improved" not in gemini_suggestion_raw.lower() and \
                "needs to be extended" not in gemini_suggestion_raw.lower() and \
                ("MATCH " in gemini_suggestion_raw.upper() or \
                    "RETURN " in gemini_suggestion_raw.upper() or \
                    "CREATE " in gemini_suggestion_raw.upper() or \
                    "MERGE " in gemini_suggestion_raw.upper() or \
                    "WITH " in gemini_suggestion_raw.upper() or \
                    "OPTIONAL MATCH " in gemini_suggestion_raw.upper()
                ):

                # 简化处理：直接将 Gemini 的原始建议作为 completion 的候选
                # 清洗工作主要交给人工审核阶段
                # 我们仍然可以做非常基础的清理，比如首尾空格和常见的 markdown
                
                temp_completion = gemini_suggestion_raw.strip()
                if temp_completion.startswith("```") and temp_completion.endswith("```"):
                    temp_completion = temp_completion[3:-3].strip()
                    if temp_completion.lower().startswith("cypher"):
                        temp_completion = temp_completion[len("cypher"):].strip()
                elif temp_completion.startswith("`") and temp_completion.endswith("`"):
                    temp_completion = temp_completion[1:-1].strip()

                # 只要建议中包含核心Cypher关键字，我们就认为它有价值被审核
                core_cypher_keywords_check = ["MATCH", "RETURN", "CREATE", "MERGE", "WITH", "OPTIONAL MATCH"]
                suggestion_contains_cypher_keyword = False
                if temp_completion:
                    for core_keyword in core_cypher_keywords_check:
                        if core_keyword in temp_completion.upper():
                            suggestion_contains_cypher_keyword = True
                            break
                
                if suggestion_contains_cypher_keyword:
                    ideal_cypher_output = temp_completion # 使用初步清理后的建议
                    source_of_ideal = "gemini_suggestion_for_review" # 明确标记为需要审核
                    refine_logger.info(f"Interaction {interaction_id}: Gemini suggestion adopted for review. Raw: '{gemini_suggestion_raw[:150]}...', Processed for completion: '{ideal_cypher_output[:150]}...'")
                else:
                    refine_logger.warning(f"Interaction {interaction_id}: Gemini suggestion '{gemini_suggestion_raw[:150]}...' did not appear to contain core Cypher keywords after basic cleaning. Skipping.")
                    continue

            
            # --- 规则4: Gemini明确建议“无法生成” 或 Qwen的查询质量低且有严重问题 ---
            elif "无法生成Cypher查询" in gemini_suggestion_raw or \
                 (overall_score <= 2 and ("hallucinated" in eval_log.get("eval_llm_raw_output", "").lower() or \
                                         "schema violation" in eval_log.get("eval_llm_raw_output", "").lower() or \
                                         "syntax error" in eval_log.get("eval_llm_raw_output", "").lower())):
                ideal_cypher_output = "无法生成Cypher查询."
                source_of_ideal = "gemini_explicitly_cannot_generate_or_qwen_low_quality"
            
            # --- 规则5: 其他情况，需要人工审核 ---
            else:
                refine_logger.info(f"Cypher log {interaction_id} (Qwen: '{qwen_generated_cypher[:100]}...') needs manual review. Gemini score: {overall_score}, Suggestion: '{gemini_suggestion_raw[:100]}...'")
                continue 
        
        # --- 如果没有Gemini评估日志 ---
        else:
            refine_logger.warning(f"No valid Gemini evaluation found for Cypher log {interaction_id}. Qwen's output: '{qwen_generated_cypher[:100]}...'. Skipping for finetune data.")
            continue
            
        if ideal_cypher_output is not None:
            finetune_samples.append({
                "prompt": qwen_input_prompt,
                "completion": ideal_cypher_output.strip(), # 确保completion也strip
                "original_qwen_cypher": qwen_generated_cypher,
                "gemini_score": gemini_score_for_log,
                "source_of_ideal": source_of_ideal,
                "interaction_id": interaction_id
            })

    refine_logger.info(f"Generated {len(finetune_samples)} Cypher finetuning samples.")
    return finetune_samples


if __name__ == "__main__":
    # 1. 确定要处理的原始RAG交互日志文件 (包含cypher_generation)
    #    和对应的Gemini评估结果日志文件 (包含cypher_evaluation_result)
    
    # 自动查找最新的原始RAG交互日志
    rag_log_file = find_latest_rag_interaction_log(RAG_LOG_DIR) # utils.py中的函数
    
    # 构造对应的Gemini Cypher评估结果文件名
    # 假设评估文件名与原始日志文件名日期部分相同，且评估类型固定
    eval_log_file = None
    if rag_log_file:
        rag_log_basename = os.path.basename(rag_log_file)
        date_str_match = "".join(filter(str.isdigit, rag_log_basename)) # 提取文件名中的日期部分
        if len(date_str_match) >= 8: # 确保提取到至少YYYYMMDD
            date_str = date_str_match[:8]

            # 根据 evaluation.py 中 log_interaction_data 的 evaluation_name_for_file 参数构造
            evaluation_name = "cypher_gemini_flash" 
            eval_file_name = f"eval_results_{evaluation_name}_{date_str}.jsonl"
            
            eval_log_file = os.path.join(EVAL_LOG_DIR, eval_file_name)
            refine_logger.info(f"Attempting to load Cypher evaluation results from: {eval_log_file}")
        else:
            refine_logger.error(f"Could not reliably extract date from RAG log filename: {rag_log_basename}")
    
    if not rag_log_file or not eval_log_file or not os.path.exists(eval_log_file):
        refine_logger.error("Required log files not found. Exiting.")
        if not rag_log_file: refine_logger.error(f"RAG interaction log missing (expected pattern rag_interactions_*.jsonl in {RAG_LOG_DIR})")
        if rag_log_file and (not eval_log_file or not os.path.exists(eval_log_file)): refine_logger.error(f"Cypher evaluation result log missing (expected: {eval_log_file})")
    else:
        rag_interactions = load_logs_to_dict(rag_log_file, key_field="interaction_id")
        cypher_evaluations = load_logs_to_dict(eval_log_file, key_field="original_interaction_id_ref")

        if rag_interactions and cypher_evaluations:
            finetune_data = generate_finetune_samples_for_cypher(rag_interactions, cypher_evaluations)
            
            if finetune_data:
                # 获取当前日期用于文件名
                today_for_filename = datetime.now().strftime("%Y%m%d")
                output_filepath = os.path.join(FINETUNE_DATA_DIR, f"cypher_finetune_samples_{today_for_filename}.jsonl")
                
                with open(output_filepath, 'w', encoding='utf-8') as f_out:
                    for sample in finetune_data:
                        f_out.write(json.dumps(sample, ensure_ascii=False) + "\n")
                refine_logger.info(f"Successfully saved {len(finetune_data)} Cypher finetuning samples to: {output_filepath}")
                
                # 也可以同时保存一个CSV版本供人工审查
                try:
                    df = pd.DataFrame(finetune_data)
                    csv_output_filepath = os.path.join(FINETUNE_DATA_DIR, f"cypher_finetune_samples_review_{today_for_filename}.csv")
                    df.to_csv(csv_output_filepath, index=False, encoding='utf-8-sig')
                    refine_logger.info(f"Reviewable CSV saved to: {csv_output_filepath}")
                except Exception as e_csv:
                    refine_logger.error(f"Failed to save review CSV: {e_csv}")
            else:
                refine_logger.info("No finetuning samples were generated.")
        else:
            refine_logger.error("Failed to load data from log files.")
    --- END OF FILE CONTENT ---


Directory: zhz_rag/finetuning/generated_data/
  File (skipped, extension not included): answer_finetune_samples_20250531.jsonl
  File: answer_finetune_samples_review_20250531.csv
    --- START OF FILE CONTENT ---
﻿prompt,completion,original_qwen_answer,gemini_scores,gemini_suggestion,source_of_ideal,interaction_id
"
<|im_start|>system
你是一个AI问答助手。你的任务是根据【上下文信息】回答【用户问题】。

**核心指令：**

1.  **尝试直接回答：** 请首先仔细阅读【上下文信息】，如果其中包含能直接回答【用户问题】的内容，请用上下文中的信息直接、简洁地回答。
2.  **忠实原文：** 你的回答必须严格基于【上下文信息】，禁止加入任何外部知识或个人观点。
3.  **如果无法回答：** 如果你分析了【上下文信息】后，确认其中确实没有能回答【用户问题】的明确信息，那么请只回答以下这句话：
    ""根据目前提供的资料，我无法找到关于您问题的明确信息。""
    **不要添加任何其他解释、建议或反问。**

**请直接给出答案，或者只给出上述那句固定的“无法找到信息”的回复。**
<|im_start|>user
用户问题: 总结一下关于AI在医疗领域应用的主要内容。

上下文信息:
Source Type: vector_chroma, Score: 0.9200
Content: AI技术在医疗影像分析方面取得了显著进展，能够帮助医生更早发现癌症等疾病。例如，深度学习模型可以分析X光片、CT扫描结果，其准确率有时甚至超过人类专家。

---

Source Type: vector_chroma, Score: 0.8800
Content: 个性化医疗是未来的趋势，AI通过分析大量的患者数据，包括基因序列、生活方式、病史等，为每位患者量身定制治疗方案和用药指导，从而提高疗效并减少副作用。

---

Source Type: keyword_bm25s, Score: 15.5000
Content: 药物研发周期长、成本高是长期存在的问题。AI可以通过模拟筛选化合物、预测药物相互作用，极大地缩短新药从研发到上市的时间。
<|im_end|>
<|im_start|>assistant
",AI在医疗领域的主要应用包括辅助诊断（如通过图像识别分析医疗影像，识别病变），个性化治疗方案的制定（基于患者基因和生活习惯数据），以及新药研发加速（通过预测分子结构和药物反应）。,AI在医疗领域的主要应用包括辅助诊断（如通过图像识别分析医疗影像，识别病变），个性化治疗方案的制定（基于患者基因和生活习惯数据），以及新药研发加速（通过预测分子结构和药物反应）。,"{'overall': 5, 'faithfulness': 5, 'relevance': 5, 'completeness': 5, 'context_sufficiency': 'Sufficient'}",No improvement needed.,qwen_high_score_by_gemini,a1-rag-001
"
<|im_start|>system
你是一个AI问答助手。你的任务是根据【上下文信息】回答【用户问题】。

**核心指令：**

1.  **尝试直接回答：** 请首先仔细阅读【上下文信息】，如果其中包含能直接回答【用户问题】的内容，请用上下文中的信息直接、简洁地回答。
2.  **忠实原文：** 你的回答必须严格基于【上下文信息】，禁止加入任何外部知识或个人观点。
3.  **如果无法回答：** 如果你分析了【上下文信息】后，确认其中确实没有能回答【用户问题】的明确信息，那么请只回答以下这句话：
    ""根据目前提供的资料，我无法找到关于您问题的明确信息。""
    **不要添加任何其他解释、建议或反问。**

**请直接给出答案，或者只给出上述那句固定的“无法找到信息”的回复。**
<|im_start|>user
用户问题: 我们公司上季度的营销活动预算是多少？

上下文信息:
Source Type: vector_chroma, Score: 0.9500
Content: 会议纪要：Q3营销计划讨论...市场部张经理提出，本季度营销活动预算初步定为50万元，主要用于线上推广和内容制作...

---

Source Type: knowledge_graph, Score: 1.0000
Content: 事实: 营销活动Q3 -> 预算: 50万元
<|im_end|>
<|im_start|>assistant
",根据提供的信息，我们公司上季度的营销活动预算是50万元。,根据提供的信息，我们公司上季度的营销活动预算是50万元。,"{'overall': 4, 'faithfulness': 5, 'relevance': 5, 'completeness': 4, 'context_sufficiency': 'Sufficient'}",可以考虑在答案中提及信息来源，例如：'根据会议纪要和知识图谱信息，公司上季度营销活动预算为50万元。',qwen_high_score_by_gemini_with_minor_gemini_suggestion,a2-rag-002
"
<|im_start|>system
你是一个AI问答助手。你的任务是根据【上下文信息】回答【用户问题】。

**核心指令：**

1.  **尝试直接回答：** 请首先仔细阅读【上下文信息】，如果其中包含能直接回答【用户问题】的内容，请用上下文中的信息直接、简洁地回答。
2.  **忠实原文：** 你的回答必须严格基于【上下文信息】，禁止加入任何外部知识或个人观点。
3.  **如果无法回答：** 如果你分析了【上下文信息】后，确认其中确实没有能回答【用户问题】的明确信息，那么请只回答以下这句话：
    ""根据目前提供的资料，我无法找到关于您问题的明确信息。""
    **不要添加任何其他解释、建议或反问。**

**请直接给出答案，或者只给出上述那句固定的“无法找到信息”的回复。**
<|im_start|>user
用户问题: 请告诉我关于月球的三个有趣事实。

上下文信息:
Source Type: vector_chroma, Score: 0.8500
Content: 月球是地球的唯一天然卫星，其直径约为地球的四分之一。

---

Source Type: vector_chroma, Score: 0.9000
Content: 由于月球的自转周期和公转周期相等（潮汐锁定），我们从地球上始终只能看到月球的同一面。
<|im_end|>
<|im_start|>assistant
",月球是地球唯一的天然卫星，它的自转周期与公转周期相同，因此总是以同一面朝向地球。此外，月球的引力导致了地球上的潮汐现象，并且月球上没有大气层。,月球是地球唯一的天然卫星，它的自转周期与公转周期相同，因此总是以同一面朝向地球。此外，月球的引力导致了地球上的潮汐现象，并且月球上没有大气层。,"{'overall': 2, 'faithfulness': 2, 'relevance': 4, 'completeness': 2, 'context_sufficiency': 'Partially Sufficient'}",答案应严格基于提供的上下文。如果上下文不足以提供三个事实，应诚实说明，例如：'根据提供的信息，月球是地球唯一的天然卫星，并且总是以同一面朝向地球。目前信息不足以提供第三个事实。',qwen_low_score_with_gemini_suggestion_for_review,a3-rag-003
"
<|im_start|>system
你是一个AI问答助手。你的任务是根据【上下文信息】回答【用户问题】。

**核心指令：**

1.  **尝试直接回答：** 请首先仔细阅读【上下文信息】，如果其中包含能直接回答【用户问题】的内容，请用上下文中的信息直接、简洁地回答。
2.  **忠实原文：** 你的回答必须严格基于【上下文信息】，禁止加入任何外部知识或个人观点。
3.  **如果无法回答：** 如果你分析了【上下文信息】后，确认其中确实没有能回答【用户问题】的明确信息，那么请只回答以下这句话：
    ""根据目前提供的资料，我无法找到关于您问题的明确信息。""
    **不要添加任何其他解释、建议或反问。**

**请直接给出答案，或者只给出上述那句固定的“无法找到信息”的回复。**
<|im_start|>user
用户问题: Alpha项目的负责人是谁？

上下文信息:
Source Type: vector_chroma, Score: 0.7000
Content: 项目Beta的负责人是李明，项目Gamma的负责人是王芳。

---

Source Type: keyword_bm25s, Score: 10.1000
Content: 所有项目列表：项目Beta, 项目Gamma, 项目Delta。
<|im_end|>
<|im_start|>assistant
",根据目前提供的资料，我无法找到关于您问题的明确信息。,根据目前提供的资料，我无法找到关于您问题的明确信息。,"{'overall': 5, 'faithfulness': 5, 'relevance': 5, 'completeness': 5, 'context_sufficiency': 'Insufficient'}",No improvement needed.,qwen_standard_no_answer_confirmed_by_gemini_context_insufficient,a4-rag-004
"
<|im_start|>system
你是一个AI问答助手。你的任务是根据【上下文信息】回答【用户问题】。

**核心指令：**

1.  **尝试直接回答：** 请首先仔细阅读【上下文信息】，如果其中包含能直接回答【用户问题】的内容，请用上下文中的信息直接、简洁地回答。
2.  **忠实原文：** 你的回答必须严格基于【上下文信息】，禁止加入任何外部知识或个人观点。
3.  **如果无法回答：** 如果你分析了【上下文信息】后，确认其中确实没有能回答【用户问题】的明确信息，那么请只回答以下这句话：
    ""根据目前提供的资料，我无法找到关于您问题的明确信息。""
    **不要添加任何其他解释、建议或反问。**

**请直接给出答案，或者只给出上述那句固定的“无法找到信息”的回复。**
<|im_start|>user
用户问题: 解释一下什么是量子纠缠？

上下文信息:
Source Type: vector_chroma, Score: 0.9300
Content: 根据量子力学，当两个或多个粒子共同组成一个量子态时，即使它们在空间上分离，对其中一个粒子的测量也会瞬间影响到其他粒子的状态，这种现象被称为量子纠缠。

---

Source Type: vector_chroma, Score: 0.7800
Content: 爱因斯坦称量子纠缠为“鬼魅般的超距作用”。
<|im_end|>
<|im_start|>assistant
",量子纠缠是指当几个粒子在彼此相互作用后，由于各个粒子所拥有的特性已综合成为整体性质，无法单独描述各个粒子的性质，只能描述整体系统的性质的现象。,量子纠缠是指当几个粒子在彼此相互作用后，由于各个粒子所拥有的特性已综合成为整体性质，无法单独描述各个粒子的性质，只能描述整体系统的性质的现象。,"{'overall': 3, 'faithfulness': 3, 'relevance': 4, 'completeness': 2, 'context_sufficiency': 'Sufficient'}",答案应该更好地利用上下文信息，特别是上下文片段1中关于'即使它们在空间上分离，对其中一个粒子的测量也会瞬间影响到其他粒子的状态'的描述，这比当前答案更易于理解。可以考虑直接改写或总结上下文片段1的核心内容。,qwen_low_score_with_gemini_suggestion_for_review,a5-rag-005

    --- END OF FILE CONTENT ---

  File (skipped, extension not included): cypher_finetune_samples_20250531.jsonl
  File: cypher_finetune_samples_review_20250531.csv
    --- START OF FILE CONTENT ---
﻿prompt,completion,original_qwen_cypher,gemini_score,source_of_ideal,interaction_id
"<|im_start|>system
你是顶级Neo4j Cypher查询生成专家... (省略完整Schema和指令) ...【知识图谱Schema描述】
:ExtractedEntity {text: STRING, label: STRING (PERSON, ORGANIZATION, TASK)}
(:ExtractedEntity {label:""PERSON""})-[:WORKS_AT]->(:ExtractedEntity {label:""ORGANIZATION""})
(:ExtractedEntity {label:""TASK""})-[:ASSIGNED_TO]->(:ExtractedEntity {label:""PERSON""})
... (省略示例) ...现在，请根据以下用户问题和上述Schema及规则生成Cypher查询。
<|im_end|>
<|im_start|>user
用户问题: 张三在哪里工作？
<|im_end|>
<|im_start|>assistant
","MATCH (p:ExtractedEntity {text: '张三', label: 'PERSON'})-[:WORKS_AT]->(org:ExtractedEntity {label: 'ORGANIZATION'}) RETURN org.text AS organizationName","MATCH (p:ExtractedEntity {text: '张三', label: 'PERSON'})-[:WORKS_AT]->(org:ExtractedEntity {label: 'ORGANIZATION'}) RETURN org.text AS organizationName",5,qwen_high_score_by_gemini,c1-rag-001
"<|im_start|>system
你是顶级Neo4j Cypher查询生成专家... (同上) ...
<|im_end|>
<|im_start|>user
用户问题: 李四负责了哪些任务，并且这些任务的负责人是谁？
<|im_end|>
<|im_start|>assistant
","为了更准确地回答，查询应该确保只返回李四作为负责人的任务。可以考虑修改为：MATCH (task:ExtractedEntity {label: 'TASK'})-[:ASSIGNED_TO]->(p:ExtractedEntity {text: '李四', label: 'PERSON'}) RETURN task.text AS taskName, p.text AS assigneeName","MATCH (task:ExtractedEntity {label: 'TASK'})-[:ASSIGNED_TO]->(p:ExtractedEntity {text: '李四', label: 'PERSON'}), (task)-[:ASSIGNED_TO]->(assignee:ExtractedEntity {label: 'PERSON'}) RETURN task.text AS taskName, assignee.text AS assigneeName",3,gemini_suggestion_for_review,c2-rag-002
"<|im_start|>system
你是顶级Neo4j Cypher查询生成专家... (同上) ...
<|im_end|>
<|im_start|>user
用户问题: 项目Alpha的状态是什么？
<|im_end|>
<|im_start|>assistant
",无法生成Cypher查询.,MATCH (t:Task {name: '项目Alpha'}) RETURN t.status,1,gemini_explicitly_cannot_generate_or_qwen_low_quality,c3-rag-003
"<|im_start|>system
你是顶级Neo4j Cypher查询生成专家... (同上) ...
<|im_end|>
<|im_start|>user
用户问题: 王五的直属上级是谁？
<|im_end|>
<|im_start|>assistant
",无法生成Cypher查询.,无法生成Cypher查询.,5,qwen_and_gemini_cannot_generate,c4-rag-004
"<|im_start|>system
你是顶级Neo4j Cypher查询生成专家... (同上) ...
<|im_end|>
<|im_start|>user
用户问题: 数据库中有多少人？
<|im_end|>
<|im_start|>assistant
",无法生成Cypher查询.,无法生成Cypher查询.,1,qwen_and_gemini_cannot_generate,c6-rag-006

    --- END OF FILE CONTENT ---


Directory: zhz_rag/stored_data/
  File: __init__.py
    --- File is empty ---

  File (skipped, extension not included): kuzu_default_db.lockfile

Directory: zhz_rag/stored_data/rag_interaction_logs/
  File (skipped, extension not included): rag_interactions_20250605.jsonl

Directory: zhz_rag/stored_data/evaluation_results_logs/

Directory: zhz_rag/stored_data/kuzu_default_db/
  File (skipped, extension not included): .lock
  File (skipped, extension not included): .shadow
  File (skipped, extension not included): .wal
  File (skipped, extension not included): catalog.kz
  File (skipped, extension not included): data.kz
  File (skipped, extension not included): metadata.kz
  File (skipped, extension not included): n-0.hindex
  File (skipped, extension not included): n-0.hindex.ovf

Directory: zhz_rag/stored_data/chromadb_index/
  File (skipped, extension not included): chroma.sqlite3

Directory: zhz_rag/stored_data/chromadb_index/fc39f817-3d12-4574-a52b-5bdfe2d8f24b/
  File (skipped, extension not included): data_level0.bin
  File (skipped, extension not included): header.bin
  File (skipped, extension not included): length.bin
  File (skipped, extension not included): link_lists.bin

Directory: zhz_rag/stored_data/kuzu_test_visibility_db/
  File (skipped, extension not included): .lock
  File (skipped, extension not included): .shadow
  File (skipped, extension not included): .wal
  File (skipped, extension not included): catalog.kz
  File (skipped, extension not included): data.kz
  File (skipped, extension not included): metadata.kz
  File (skipped, extension not included): n-0.hindex
  File (skipped, extension not included): n-0.hindex.ovf

Directory: zhz_rag/stored_data/bm25_index/
  File (skipped, extension not included): data.csc.index.npy
  File (skipped, extension not included): doc_ids.pkl
  File (skipped, extension not included): indices.csc.index.npy
  File (skipped, extension not included): indptr.csc.index.npy
  File: params.index.json
    --- START OF FILE CONTENT ---
{
    "k1": 1.5,
    "b": 0.75,
    "delta": 0.5,
    "method": "lucene",
    "idf_method": "lucene",
    "dtype": "float32",
    "int_dtype": "int32",
    "num_docs": 2,
    "version": "0.2.13",
    "backend": "numpy"
}
    --- END OF FILE CONTENT ---

  File: vocab.index.json
    --- START OF FILE CONTENT ---
{"分配":0,"处理":1,"项目":2,"。":3,"自然":4,"语言":5,"第二":6,"自然语言":7,"任务分配":8,"工作":9,"技术":10,"第二个":11,"给":12,"Alpha":13,"文档":14,"张三在":15,"张":16,"的":17,"编写":18,"三":19,"了":20,"讨论":21,"任务":22,"二个":23,"谷歌":24,"":25}
    --- END OF FILE CONTENT ---


Directory: zhz_rag/evaluation/
  File: __init__.py
    --- File is empty ---

  File: analyze_answer.py
    --- START OF FILE CONTENT ---
# zhz_rag/evaluation/analyze_answer.py
import json
import os
import pandas as pd
from typing import List, Dict, Any, Optional
from collections import Counter
from datetime import datetime
import glob 

# --- 从项目中导入必要的模块 ---
try:
    from zhz_rag.utils.common_utils import (
        load_jsonl_file, # <--- 使用新的通用函数
        EVALUATION_RESULTS_LOGS_DIR # 导入评估日志目录常量
    )
except ImportError as e:
    print(f"ERROR: Could not import necessary modules in analyze_answer.py: {e}")
    print("Make sure this script is run in an environment where 'zhz_rag' package is accessible.")
    exit(1)

import logging

# --- 配置此脚本的logger ---
analyze_answer_logger = logging.getLogger("AnalyzeAnswerLogger")
analyze_answer_logger.setLevel(logging.INFO)
if not analyze_answer_logger.hasHandlers():
    _console_handler = logging.StreamHandler()
    _formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
    _console_handler.setFormatter(_formatter)
    analyze_answer_logger.addHandler(_console_handler)
    analyze_answer_logger.propagate = False
analyze_answer_logger.info("--- AnalyzeAnswerLogger configured ---")

# --- 核心功能函数 ---

def extract_answer_evaluation_details(log_entry: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """
    从单条已解析的答案评估日志条目中提取关键信息。
    这个函数与您之前在 analyze_answer.py 中的版本基本一致，稍作调整。
    """
    details = {}
    eval_data = log_entry.get("eval_llm_processed_output_json")

    if not eval_data or not isinstance(eval_data, dict):
        analyze_answer_logger.warning(f"Skipping log entry due to missing or invalid 'eval_llm_processed_output_json'. Interaction ID ref: {log_entry.get('original_interaction_id_ref')}")
        return None

    details["interaction_id_ref"] = log_entry.get("original_interaction_id_ref")
    details["user_question"] = log_entry.get("user_question_for_eval")
    details["generated_answer"] = log_entry.get("generated_answer_for_eval")
    
    summary = eval_data.get("evaluation_summary", {})
    dimensions = eval_data.get("dimensions", {})
    
    details["overall_answer_quality_score"] = summary.get("overall_answer_quality_score")
    details["main_strengths_answer"] = summary.get("main_strengths_answer")
    details["main_weaknesses_answer"] = summary.get("main_weaknesses_answer")
    
    faithfulness = dimensions.get("faithfulness", {})
    details["faithfulness_score"] = faithfulness.get("score")
    details["faithfulness_reasoning"] = faithfulness.get("reasoning")
    # 确保 problematic_answer_segments_faithfulness 是列表，然后 join
    problematic_segments = faithfulness.get("problematic_answer_segments_faithfulness", [])
    if isinstance(problematic_segments, list):
        details["faithfulness_problematic_segments"] = "; ".join(problematic_segments)
    else:
        details["faithfulness_problematic_segments"] = str(problematic_segments) # 以防万一不是列表

    relevance = dimensions.get("relevance", {})
    details["relevance_score"] = relevance.get("score")
    details["relevance_reasoning"] = relevance.get("reasoning")
    
    completeness = dimensions.get("completeness", {})
    details["completeness_context_sufficiency"] = completeness.get("context_sufficiency_assessment")
    details["completeness_context_reasoning"] = completeness.get("context_sufficiency_reasoning")
    details["completeness_score"] = completeness.get("score")
    details["completeness_reasoning"] = completeness.get("reasoning")
    
    coherence = dimensions.get("coherence_fluency", {}) # 键名可能与prompt中的一致
    details["coherence_score"] = coherence.get("score")
    details["coherence_reasoning"] = coherence.get("reasoning")

    actionability = dimensions.get("actionability_usability", {}) # 键名可能与prompt中的一致
    details["actionability_score"] = actionability.get("score")
    details["actionability_reasoning"] = actionability.get("reasoning")
    
    details["gemini_suggestion_answer"] = eval_data.get("suggestion_for_answer_improvement")

    return details

def perform_answer_evaluation_analysis(
    evaluation_log_filepath: str,
    output_csv_filepath: str
) -> bool:
    """
    加载答案评估日志，进行分析，并保存结果到CSV。

    Args:
        evaluation_log_filepath (str): 答案评估结果日志文件的路径。
        output_csv_filepath (str): 分析结果CSV文件的保存路径。

    Returns:
        bool: 如果分析和保存成功则返回True，否则返回False。
    """
    analyze_answer_logger.info(f"Starting Answer evaluation analysis for log file: {evaluation_log_filepath}")
    analyze_answer_logger.info(f"Analysis results will be saved to: {output_csv_filepath}")

    evaluation_logs = load_jsonl_file(evaluation_log_filepath)

    if not evaluation_logs:
        analyze_answer_logger.warning(f"No evaluation logs found or loaded from {evaluation_log_filepath}. Analysis aborted.")
        return False

    extracted_details_list = []
    for log_entry in evaluation_logs:
        if log_entry.get("task_type") == "answer_evaluation_result": # 确保是答案评估日志
            details = extract_answer_evaluation_details(log_entry)
            if details:
                extracted_details_list.append(details)
        else:
            analyze_answer_logger.debug(f"Skipping log entry with task_type '{log_entry.get('task_type')}' as it's not 'answer_evaluation_result'.")


    if not extracted_details_list:
        analyze_answer_logger.info("No valid Answer evaluation details extracted from the logs. No CSV will be generated.")
        return False

    df = pd.DataFrame(extracted_details_list)
    
    score_columns = [
        "overall_answer_quality_score", "faithfulness_score", "relevance_score",
        "completeness_score", "coherence_score", "actionability_score"
    ]
    for col in score_columns:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')

    analyze_answer_logger.info(f"\n--- Preliminary Answer Evaluation Analysis (from {len(extracted_details_list)} entries) ---")
    analyze_answer_logger.info(f"Total evaluation entries processed: {len(df)}")

    for col_name in score_columns:
        if col_name in df.columns and not df[col_name].isnull().all():
            analyze_answer_logger.info(f"\nDimension: {col_name}")
            analyze_answer_logger.info(f"{df[col_name].describe()}")
            analyze_answer_logger.info("Score Distribution:")
            analyze_answer_logger.info(f"{df[col_name].value_counts(dropna=False).sort_index()}")
        else:
            analyze_answer_logger.info(f"\nDimension: {col_name} - No data or all NaN.")
            
    if "completeness_context_sufficiency" in df.columns and not df["completeness_context_sufficiency"].isnull().all():
        analyze_answer_logger.info("\nContext Sufficiency Assessment Distribution:")
        analyze_answer_logger.info(f"{df['completeness_context_sufficiency'].value_counts(dropna=False)}")
    else:
        analyze_answer_logger.info("\nContext Sufficiency Assessment Distribution: No data.")

    try:
        os.makedirs(os.path.dirname(output_csv_filepath), exist_ok=True)
        df.to_csv(output_csv_filepath, index=False, encoding='utf-8-sig')
        analyze_answer_logger.info(f"\nAnalysis results saved to: {output_csv_filepath}")
        return True
    except Exception as e:
        analyze_answer_logger.error(f"\nFailed to save CSV file: {e}", exc_info=True)
        return False

if __name__ == "__main__":
    EVALUATION_NAME_FOR_ANSWER = "answer_gemini_flash" 
    
    # --- 动态查找最新的评估结果日志文件 ---
    eval_logs_pattern = os.path.join(EVALUATION_RESULTS_LOGS_DIR, f"eval_results_{EVALUATION_NAME_FOR_ANSWER}_*.jsonl")
    all_eval_logs = sorted(glob.glob(eval_logs_pattern), key=os.path.getmtime, reverse=True)
    
    log_file_path_answer: Optional[str] = None
    output_csv_path_answer: Optional[str] = None

    if all_eval_logs:
        log_file_path_answer = all_eval_logs[0] # 获取最新的一个
        analyze_answer_logger.info(f"Found latest Answer evaluation log for analysis: {log_file_path_answer}")
        
        # 根据找到的日志文件名构造输出的 CSV 文件名
        base_log_name = os.path.basename(log_file_path_answer)
        # 从 "eval_results_answer_gemini_flash_YYYYMMDD.jsonl" 生成 "analysis_answer_gemini_flash_YYYYMMDD.csv"
        if base_log_name.startswith(f"eval_results_{EVALUATION_NAME_FOR_ANSWER}_") and base_log_name.endswith(".jsonl"):
            date_part_from_filename = base_log_name[len(f"eval_results_{EVALUATION_NAME_FOR_ANSWER}_"):-len(".jsonl")]
            output_csv_name_answer = f"analysis_{EVALUATION_NAME_FOR_ANSWER}_{date_part_from_filename}.csv"
            output_csv_path_answer = os.path.join(EVALUATION_RESULTS_LOGS_DIR, output_csv_name_answer)
        else: # Fallback naming for CSV
            today_str = datetime.now().strftime("%Y%m%d")
            output_csv_name_answer = f"analysis_{EVALUATION_NAME_FOR_ANSWER}_{today_str}_fallback.csv"
            output_csv_path_answer = os.path.join(EVALUATION_RESULTS_LOGS_DIR, output_csv_name_answer)
        analyze_answer_logger.info(f"Analysis CSV report will be saved to: {output_csv_path_answer}")
    else:
        analyze_answer_logger.error(f"No Answer evaluation log files found matching pattern: {eval_logs_pattern}")

    if log_file_path_answer and output_csv_path_answer and os.path.exists(log_file_path_answer):
        perform_answer_evaluation_analysis(
            evaluation_log_filepath=log_file_path_answer,
            output_csv_filepath=output_csv_path_answer
        )
    else:
        analyze_answer_logger.info("Answer evaluation analysis will not run as no suitable log file was identified or output path could not be determined.")
    --- END OF FILE CONTENT ---

  File: analyze_cypher.py
    --- START OF FILE CONTENT ---
# zhz_rag/evaluation/analyze_cypher.py
import json
import os
import pandas as pd
from typing import List, Dict, Any, Optional
from collections import Counter
from datetime import datetime
import glob 

# --- 从项目中导入必要的模块 ---
try:
    from zhz_rag.utils.common_utils import (
        load_jsonl_file, # <--- 使用新的通用函数
        EVALUATION_RESULTS_LOGS_DIR # 导入评估日志目录常量
    )
except ImportError as e:
    print(f"ERROR: Could not import necessary modules in analyze_cypher.py: {e}")
    print("Make sure this script is run in an environment where 'zhz_rag' package is accessible.")
    exit(1)

import logging

# --- 配置此脚本的logger ---
analyze_cypher_logger = logging.getLogger("AnalyzeCypherLogger")
analyze_cypher_logger.setLevel(logging.INFO)
if not analyze_cypher_logger.hasHandlers():
    _console_handler = logging.StreamHandler()
    _formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
    _console_handler.setFormatter(_formatter)
    analyze_cypher_logger.addHandler(_console_handler)
    analyze_cypher_logger.propagate = False
analyze_cypher_logger.info("--- AnalyzeCypherLogger configured ---")

# --- 核心功能函数 ---

def extract_cypher_evaluation_details(log_entry: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """
    从单条已解析的Cypher评估日志条目中提取关键信息。
    这个函数与您之前在 analyze_cypher.py 中的版本基本一致，稍作调整以适应通用加载。
    """
    details = {}
    # eval_llm_processed_output_json 字段包含了Gemini评估的JSON输出
    eval_data = log_entry.get("eval_llm_processed_output_json")

    if not eval_data or not isinstance(eval_data, dict):
        analyze_cypher_logger.warning(f"Skipping log entry due to missing or invalid 'eval_llm_processed_output_json'. Interaction ID ref: {log_entry.get('original_interaction_id_ref')}")
        return None # 如果核心评估数据缺失，则跳过此条目

    details["interaction_id_ref"] = log_entry.get("original_interaction_id_ref")
    details["user_question"] = log_entry.get("user_question_for_eval")
    details["generated_cypher"] = log_entry.get("generated_cypher_for_eval")
    
    summary = eval_data.get("evaluation_summary", {})
    dimensions = eval_data.get("dimensions", {})
    
    details["overall_quality_score"] = summary.get("overall_quality_score_cypher")
    details["main_strength"] = summary.get("main_strength_cypher")
    details["main_weakness"] = summary.get("main_weakness_cypher")
    
    syntax = dimensions.get("syntactic_correctness", {})
    details["syntax_score"] = syntax.get("score")
    details["syntax_reasoning"] = syntax.get("reasoning")
    
    schema = dimensions.get("schema_adherence", {})
    details["schema_overall_score"] = schema.get("overall_score")
    details["schema_node_label_correct"] = schema.get("node_label_correctness", {}).get("check_result")
    details["schema_entity_type_correct"] = schema.get("entity_type_property_correctness", {}).get("check_result")
    details["schema_rel_type_correct"] = schema.get("relationship_type_correctness", {}).get("check_result")
    details["schema_prop_name_correct"] = schema.get("property_name_correctness", {}).get("check_result")
    details["schema_hallucinated_present"] = schema.get("hallucinated_schema_elements", {}).get("check_result_hallucination_present")
    details["schema_hallucinated_elements"] = ", ".join(schema.get("hallucinated_schema_elements", {}).get("elements_found", []))
    details["schema_reasoning"] = schema.get("reasoning")
    
    intent = dimensions.get("intent_accuracy", {})
    details["intent_score"] = intent.get("score")
    details["intent_explanation_cypher"] = intent.get("explanation_of_cypher_retrieval")
    details["intent_alignment_notes"] = intent.get("semantic_alignment_with_question")
    details["intent_key_elements_notes"] = intent.get("key_element_coverage_notes")
    details["intent_reasoning"] = intent.get("reasoning")
    
    details["qwen_error_patterns"] = ", ".join(eval_data.get("qwen_error_patterns_identified", []))
    details["gemini_suggestion"] = eval_data.get("suggestion_for_improvement_cypher")

    return details

def perform_cypher_evaluation_analysis(
    evaluation_log_filepath: str,
    output_csv_filepath: str
) -> bool:
    """
    加载Cypher评估日志，进行分析，并保存结果到CSV。

    Args:
        evaluation_log_filepath (str): Cypher评估结果日志文件的路径。
        output_csv_filepath (str): 分析结果CSV文件的保存路径。

    Returns:
        bool: 如果分析和保存成功则返回True，否则返回False。
    """
    analyze_cypher_logger.info(f"Starting Cypher evaluation analysis for log file: {evaluation_log_filepath}")
    analyze_cypher_logger.info(f"Analysis results will be saved to: {output_csv_filepath}")

    # 使用通用函数加载评估日志
    # 注意：evaluate_cypher_with_gemini 保存的日志中 task_type 是 "cypher_evaluation_by_gemini"
    # load_jsonl_file 不关心 task_type，它会加载所有行
    evaluation_logs = load_jsonl_file(evaluation_log_filepath)

    if not evaluation_logs:
        analyze_cypher_logger.warning(f"No evaluation logs found or loaded from {evaluation_log_filepath}. Analysis aborted.")
        return False

    extracted_details_list = []
    for log_entry in evaluation_logs:
        # 确保只处理Cypher评估结果
        if log_entry.get("task_type") == "cypher_evaluation_by_gemini":
            details = extract_cypher_evaluation_details(log_entry)
            if details: # extract_cypher_evaluation_details 可能会返回 None
                extracted_details_list.append(details)
        else:
            analyze_cypher_logger.debug(f"Skipping log entry with task_type '{log_entry.get('task_type')}' as it's not 'cypher_evaluation_by_gemini'.")


    if not extracted_details_list:
        analyze_cypher_logger.info("No valid Cypher evaluation details extracted from the logs. No CSV will be generated.")
        return False

    df = pd.DataFrame(extracted_details_list)
    
    score_columns = ["overall_quality_score", "syntax_score", "schema_overall_score", "intent_score"]
    for col in score_columns:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')

    analyze_cypher_logger.info(f"\n--- Preliminary Cypher Evaluation Analysis (from {len(extracted_details_list)} entries) ---")
    analyze_cypher_logger.info(f"Total evaluation entries processed: {len(df)}")

    if "overall_quality_score" in df.columns and not df["overall_quality_score"].isnull().all():
        analyze_cypher_logger.info("\n1. Overall Quality Score:")
        analyze_cypher_logger.info(f"{df['overall_quality_score'].describe()}")
        analyze_cypher_logger.info("\nScore Distribution:")
        analyze_cypher_logger.info(f"{df['overall_quality_score'].value_counts(dropna=False).sort_index()}")
    else:
        analyze_cypher_logger.info("\n1. Overall Quality Score: No data or all NaN.")


    if "schema_overall_score" in df.columns and not df["schema_overall_score"].isnull().all():
        analyze_cypher_logger.info("\n2. Schema Adherence Overall Score:")
        analyze_cypher_logger.info(f"{df['schema_overall_score'].describe()}")
        analyze_cypher_logger.info("\nScore Distribution:")
        analyze_cypher_logger.info(f"{df['schema_overall_score'].value_counts(dropna=False).sort_index()}")
        
        schema_sub_checks = [
            "schema_node_label_correct", "schema_entity_type_correct", 
            "schema_rel_type_correct", "schema_prop_name_correct", 
            "schema_hallucinated_present"
        ]
        analyze_cypher_logger.info("\nSchema Adherence Sub-item Issues (False means issue, Hallucinated True means issue):")
        for check in schema_sub_checks:
            if check in df.columns:
                if check == "schema_hallucinated_present":
                    issue_count = df[df[check] == True].shape[0]
                    analyze_cypher_logger.info(f"  - {check} (Hallucination Present): {issue_count} entries")
                else:
                    issue_count = df[df[check] == False].shape[0]
                    analyze_cypher_logger.info(f"  - {check} (Incorrect): {issue_count} entries")
    else:
        analyze_cypher_logger.info("\n2. Schema Adherence Overall Score: No data or all NaN.")


    if "intent_score" in df.columns and not df["intent_score"].isnull().all():
        analyze_cypher_logger.info("\n3. Intent Accuracy Score:")
        analyze_cypher_logger.info(f"{df['intent_score'].describe()}")
        analyze_cypher_logger.info("\nScore Distribution:")
        analyze_cypher_logger.info(f"{df['intent_score'].value_counts(dropna=False).sort_index()}")
    else:
        analyze_cypher_logger.info("\n3. Intent Accuracy Score: No data or all NaN.")


    if "qwen_error_patterns" in df.columns and not df["qwen_error_patterns"].isnull().all():
        analyze_cypher_logger.info("\n4. Identified Qwen Error Patterns (Top 5):")
        all_patterns = []
        for pattern_list_str in df["qwen_error_patterns"].dropna():
            if pattern_list_str and isinstance(pattern_list_str, str):
                all_patterns.extend([p.strip() for p in pattern_list_str.split(",") if p.strip()])
        pattern_counts = Counter(all_patterns)
        analyze_cypher_logger.info(f"{pattern_counts.most_common(5)}")
    else:
        analyze_cypher_logger.info("\n4. Identified Qwen Error Patterns: No data.")
        
    try:
        # Ensure output directory exists
        os.makedirs(os.path.dirname(output_csv_filepath), exist_ok=True)
        df.to_csv(output_csv_filepath, index=False, encoding='utf-8-sig')
        analyze_cypher_logger.info(f"\nAnalysis results saved to: {output_csv_filepath}")
        return True
    except Exception as e:
        analyze_cypher_logger.error(f"\nFailed to save CSV file: {e}", exc_info=True)
        return False

if __name__ == "__main__":
    EVALUATION_NAME_FOR_CYPHER = "cypher_gemini_flash" 

    eval_logs_pattern = os.path.join(EVALUATION_RESULTS_LOGS_DIR, f"eval_results_{EVALUATION_NAME_FOR_CYPHER}_*.jsonl")
    all_eval_logs = sorted(glob.glob(eval_logs_pattern), key=os.path.getmtime, reverse=True)
    
    log_file_path_cypher: Optional[str] = None
    output_csv_path_cypher: Optional[str] = None

    if all_eval_logs:
        log_file_path_cypher = all_eval_logs[0]
        analyze_cypher_logger.info(f"Found latest Cypher evaluation log for analysis: {log_file_path_cypher}")
        
        base_log_name = os.path.basename(log_file_path_cypher)
        if base_log_name.startswith(f"eval_results_{EVALUATION_NAME_FOR_CYPHER}_") and base_log_name.endswith(".jsonl"):
            date_part_from_filename = base_log_name[len(f"eval_results_{EVALUATION_NAME_FOR_CYPHER}_"):-len(".jsonl")]
            output_csv_name_cypher = f"analysis_{EVALUATION_NAME_FOR_CYPHER}_{date_part_from_filename}.csv"
            output_csv_path_cypher = os.path.join(EVALUATION_RESULTS_LOGS_DIR, output_csv_name_cypher)
        else:
            today_str = datetime.now().strftime("%Y%m%d")
            output_csv_name_cypher = f"analysis_{EVALUATION_NAME_FOR_CYPHER}_{today_str}_fallback.csv"
            output_csv_path_cypher = os.path.join(EVALUATION_RESULTS_LOGS_DIR, output_csv_name_cypher)
        analyze_cypher_logger.info(f"Analysis CSV report will be saved to: {output_csv_path_cypher}")
    else:
        analyze_cypher_logger.error(f"No Cypher evaluation log files found matching pattern: {eval_logs_pattern}")

    if log_file_path_cypher and output_csv_path_cypher and os.path.exists(log_file_path_cypher):
        perform_cypher_evaluation_analysis(
            evaluation_log_filepath=log_file_path_cypher,
            output_csv_filepath=output_csv_path_cypher
        )
    else:
        analyze_cypher_logger.info("Cypher evaluation analysis will not run as no suitable log file was identified or output path could not be determined.")
    --- END OF FILE CONTENT ---

  File: batch_eval_answer.py
    --- START OF FILE CONTENT ---
# zhz_rag/evaluation/batch_eval_answer.py
import asyncio
import json
import os
from typing import List, Dict, Any, Optional, TYPE_CHECKING, Union
import glob
from datetime import datetime

try:
    from zhz_rag.evaluation.evaluator import evaluate_answer_with_gemini
    from zhz_rag.utils.common_utils import (
        find_latest_rag_interaction_log,
        load_jsonl_file,
        RAG_INTERACTION_LOGS_DIR
    )
    from zhz_rag.config.pydantic_models import RetrievedDocument
except ImportError as e:
    print(f"ERROR: Could not import necessary modules in batch_eval_answer.py: {e}")
    print("Make sure this script is run in an environment where 'zhz_rag' package is accessible.")
    exit(1)

if TYPE_CHECKING:
    from zhz_rag_pipeline_dagster.zhz_rag_pipeline.resources import GeminiAPIResource

import logging

batch_answer_eval_logger = logging.getLogger("BatchAnswerEvaluationLogger")
# 保留 DEBUG 级别，以便在需要时仍可查看详细日志，但常规 INFO 日志会更简洁
batch_answer_eval_logger.setLevel(logging.DEBUG) 
if not batch_answer_eval_logger.hasHandlers():
    _console_handler = logging.StreamHandler()
    _formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
    _console_handler.setFormatter(_formatter)
    batch_answer_eval_logger.addHandler(_console_handler)
    batch_answer_eval_logger.propagate = False
batch_answer_eval_logger.info("--- BatchAnswerEvaluationLogger configured (Level: DEBUG) ---")


def format_contexts_for_evaluation(context_docs_raw: List[Dict[str, Any]]) -> str:
    formatted_contexts = []
    if not context_docs_raw or not isinstance(context_docs_raw, list):
        batch_answer_eval_logger.warning("format_contexts_for_evaluation received no context or invalid format.")
        batch_answer_eval_logger.debug("DEBUG_FORMAT_CTX: context_docs_raw is empty or not a list.") 
        return "No context provided or context in unexpected format."

    batch_answer_eval_logger.debug(f"DEBUG_FORMAT_CTX: --- format_contexts_for_evaluation ---")
    batch_answer_eval_logger.debug(f"DEBUG_FORMAT_CTX: Received context_docs_raw length: {len(context_docs_raw)}")
    if context_docs_raw:
        batch_answer_eval_logger.debug(f"DEBUG_FORMAT_CTX: First item of context_docs_raw (type: {type(context_docs_raw[0])}): {str(context_docs_raw[0])[:500]}...")

    for i, doc_data in enumerate(context_docs_raw): 
        batch_answer_eval_logger.debug(f"DEBUG_FORMAT_CTX:   Processing doc_data item {i} (type: {type(doc_data)}): {str(doc_data)[:300]}...")
        try:
            content = doc_data.get("content", "[Content not available]") 
            source_type = doc_data.get("source_type", "unknown_source") 
            score = doc_data.get("score") 
            metadata = doc_data.get("metadata", {})
            chunk_id = metadata.get("chunk_id") or metadata.get("id") 

            context_str = f"--- Context Snippet {i+1} ---\n"
            context_str += f"Source Type: {source_type}\n"
            if score is not None:
                try:
                    context_str += f"Original Score: {float(score):.4f}\n"
                except (ValueError, TypeError):
                    context_str += f"Original Score: {score}\n" 
            if chunk_id:
                 context_str += f"Chunk ID: {chunk_id}\n"
            context_str += f"Content: {content}\n"
            formatted_contexts.append(context_str)
            batch_answer_eval_logger.debug(f"DEBUG_FORMAT_CTX:     Formatted context snippet {i+1} (content part first 100 chars): {str(content)[:100]}...")

        except Exception as e:
            batch_answer_eval_logger.warning(f"Could not parse a context document fully in format_contexts_for_evaluation: {doc_data}. Error: {e}")
            content = doc_data.get("content", "[Content not available]") 
            source_type = doc_data.get("source_type", "unknown_source") 
            formatted_contexts.append(f"--- Context Snippet {i+1} (Parsing Warning) ---\nSource Type: {source_type}\nContent: {content}\n")
    
    final_formatted_str = "\n\n".join(formatted_contexts) if formatted_contexts else "No context provided."
    batch_answer_eval_logger.debug(f"DEBUG_FORMAT_CTX: --- format_contexts_for_evaluation: Final formatted string (first 500 chars): {final_formatted_str[:500]}...")
    return final_formatted_str


async def run_answer_batch_evaluation(
    gemini_resource_for_evaluator: 'GeminiAPIResource',
    rag_interaction_log_filepath: str,
    app_version: str = "0.1.0",
    use_simulated_api: bool = False,
    api_call_delay: float = 4.1,
    target_task_types: Union[str, List[str]] = "rag_query_processing_full_log",
    field_mapping: Optional[Dict[str, Union[str, List[str]]]] = None
) -> Dict[str, int]:
    batch_answer_eval_logger.info(f"Starting batch Answer evaluation for log file: {rag_interaction_log_filepath}")
    batch_answer_eval_logger.info(f"Parameters: app_version='{app_version}', use_simulated_api={use_simulated_api}, api_call_delay={api_call_delay}s")
    batch_answer_eval_logger.info(f"Target task types: {target_task_types}")
    batch_answer_eval_logger.info(f"Field mapping: {field_mapping}")

    processed_count = 0
    evaluated_count = 0
    skipped_missing_data_count = 0
    skipped_task_type_mismatch = 0

    if use_simulated_api:
        batch_answer_eval_logger.warning("Batch Answer evaluation is USING SIMULATED Gemini responses.")
    else:
        batch_answer_eval_logger.info("Batch Answer evaluation is using REAL Gemini API calls.")

    interaction_logs = load_jsonl_file(rag_interaction_log_filepath)

    if not interaction_logs:
        batch_answer_eval_logger.error(f"No data loaded from RAG interaction log file: {rag_interaction_log_filepath}. Exiting.")
        return {"processed": 0, "evaluated": 0, "skipped_missing_data": 0, "skipped_task_type_mismatch": 0, "file_not_found_or_empty": 1}

    # --- 移除了临时筛选特定 interaction_id 的代码 ---

    if isinstance(target_task_types, str):
        target_task_types_list = [target_task_types]
    else:
        target_task_types_list = target_task_types

    default_field_map = {
        "user_query": ["user_query", "original_user_query", "query"],
        "generated_answer": ["generated_answer", "processed_llm_output", "final_answer_from_llm", "final_answer"],
        "interaction_id": ["interaction_id", "original_interaction_id"],
        "context_docs": ["final_context_docs_full", "final_context_docs_summary"]
    }
    current_field_map = default_field_map.copy()
    if field_mapping:
        for key, value in field_mapping.items():
            if isinstance(value, str):
                current_field_map[key] = [value] 
            else:
                current_field_map[key] = value

    def get_field_value(log_entry: Dict[str, Any], field_key: str) -> Any:
        for actual_field_name in current_field_map.get(field_key, []):
            if actual_field_name in log_entry:
                return log_entry[actual_field_name]
        return None

    for line_number, interaction_log in enumerate(interaction_logs, 1):
        processed_count += 1
        current_task_type = interaction_log.get("task_type")

        if current_task_type not in target_task_types_list:
            skipped_task_type_mismatch +=1
            continue
        
        # --- 添加日志打印 interaction_log 的键，用于调试 ---
        batch_answer_eval_logger.debug(f"DEBUG_INTERACTION_LOG: Keys in interaction_log for entry {line_number}: {list(interaction_log.keys())}")
        if "final_context_docs_full" in interaction_log:
            batch_answer_eval_logger.debug(f"DEBUG_INTERACTION_LOG: interaction_log['final_context_docs_full'] (first item preview): {str(interaction_log['final_context_docs_full'][0])[:200] if interaction_log['final_context_docs_full'] else 'Empty or None'}")
        else:
            batch_answer_eval_logger.debug(f"DEBUG_INTERACTION_LOG: 'final_context_docs_full' NOT in interaction_log for entry {line_number}.")
        if "final_context_docs_summary" in interaction_log:
            batch_answer_eval_logger.debug(f"DEBUG_INTERACTION_LOG: interaction_log['final_context_docs_summary'] (first item preview): {str(interaction_log['final_context_docs_summary'][0])[:200] if interaction_log['final_context_docs_summary'] else 'Empty or None'}")
        else:
            batch_answer_eval_logger.debug(f"DEBUG_INTERACTION_LOG: 'final_context_docs_summary' NOT in interaction_log for entry {line_number}.")
        # --- 结束日志打印 ---


        user_question = get_field_value(interaction_log, "user_query")
        generated_answer = get_field_value(interaction_log, "generated_answer")
        original_id = get_field_value(interaction_log, "interaction_id")
        context_docs_raw = get_field_value(interaction_log, "context_docs")
        
        batch_answer_eval_logger.debug(
            f"Log entry {line_number}: task_type='{current_task_type}', id='{original_id}', "
            f"q_present={bool(user_question)}, ans_present={bool(generated_answer)}, ctx_present_is_list={isinstance(context_docs_raw, list) if context_docs_raw else False}"
        )

        if user_question and generated_answer and original_id and context_docs_raw and isinstance(context_docs_raw, list):
            batch_answer_eval_logger.info(f"DEBUG_EVAL: For ID {original_id}, UserQ: '{str(user_question)[:50]}...', GenAns: '{str(generated_answer)[:50]}...', Contexts count: {len(context_docs_raw)}")
            retrieved_contexts_str_for_eval = format_contexts_for_evaluation(context_docs_raw)
            
            batch_answer_eval_logger.info(f"Evaluating Answer for interaction_id: {original_id} - User Question: '{str(user_question)[:50]}...' - Generated Answer: '{str(generated_answer)[:50]}...'")
            
            evaluation_result = await evaluate_answer_with_gemini(
                gemini_resource=gemini_resource_for_evaluator,
                user_question=str(user_question),
                retrieved_contexts=retrieved_contexts_str_for_eval,
                generated_answer=str(generated_answer),
                original_interaction_id=str(original_id),
                app_version=app_version
            )

            if evaluation_result:
                evaluated_count += 1
                summary = evaluation_result.get("evaluation_summary", {})
                overall_score = summary.get("overall_answer_quality_score", "N/A")
                batch_answer_eval_logger.info(f"Successfully evaluated Answer for interaction_id: {original_id}. Overall Score: {overall_score}")
            else:
                batch_answer_eval_logger.warning(f"Answer evaluation returned None or failed for interaction_id: {original_id}")
            
            if not use_simulated_api and evaluated_count > 0 : 
                batch_answer_eval_logger.info(f"Waiting for {api_call_delay} seconds before next API call...")
                await asyncio.sleep(api_call_delay)
        else:
            skipped_missing_data_count += 1
            log_preview = {
                "interaction_id": original_id, 
                "task_type": current_task_type, 
                "user_question_found": bool(user_question),
                "generated_answer_found": bool(generated_answer),
                "context_docs_found_and_list": isinstance(context_docs_raw, list) if context_docs_raw else False
            }
            batch_answer_eval_logger.warning(f"Skipping RAG log entry {line_number} due to missing critical data. Details: {log_preview}")
        
        if processed_count > 0 and processed_count % 10 == 0:
            batch_answer_eval_logger.info(f"Progress: Processed {processed_count} log entries. Evaluated {evaluated_count} answers so far. Skipped (type mismatch): {skipped_task_type_mismatch}. Skipped (missing data): {skipped_missing_data_count}.")

    summary = {
        "total_log_entries_read": processed_count,
        "target_task_type_entries_found": processed_count - skipped_task_type_mismatch,
        "answers_evaluated_successfully": evaluated_count,
        "skipped_due_to_missing_data_in_target_entries": skipped_missing_data_count,
    }
    batch_answer_eval_logger.info(f"Batch Answer evaluation finished. Summary: {summary}")
    return summary


if __name__ == "__main__":
    try:
        from zhz_rag_pipeline_dagster.zhz_rag_pipeline.resources import GeminiAPIResource, GeminiAPIResourceConfig
        
        gemini_model_name_env = os.getenv("GEMINI_MODEL_FOR_EVAL", "gemini/gemini-1.5-flash-latest")
        gemini_proxy_url_env = os.getenv("LITELLM_PROXY_URL") 

        gemini_resource_config = GeminiAPIResourceConfig(
            model_name=gemini_model_name_env,
            proxy_url=gemini_proxy_url_env
        )
        gemini_eval_resource = GeminiAPIResource(
            model_name=gemini_resource_config.model_name,
            proxy_url=gemini_resource_config.proxy_url,
            default_temperature=gemini_resource_config.default_temperature,
            default_max_tokens=gemini_resource_config.default_max_tokens
        )
        class MockContext: 
            def __init__(self):
                self.log = batch_answer_eval_logger
        
        if hasattr(gemini_eval_resource, 'setup_for_execution'):
             gemini_eval_resource.setup_for_execution(MockContext())
        batch_answer_eval_logger.info(f"GeminiAPIResource for evaluation initialized successfully using Dagster's resource class.")

    except ImportError:
        batch_answer_eval_logger.critical("CRITICAL: Could not import GeminiAPIResource. Ensure Dagster modules are in PYTHONPATH or installed.")
        gemini_eval_resource = None 
    except Exception as e_res_init:
        batch_answer_eval_logger.critical(f"CRITICAL: Error initializing GeminiAPIResource: {e_res_init}", exc_info=True)
        gemini_eval_resource = None


    log_file_to_evaluate = find_latest_rag_interaction_log(RAG_INTERACTION_LOGS_DIR)
    use_simulated_env = os.getenv("USE_SIMULATED_GEMINI_ANSWER_EVAL", "false").lower() == "true"
    api_delay_env = float(os.getenv("GEMINI_API_CALL_DELAY_SECONDS", "4.1"))
    app_version_tag_env = os.getenv("APP_VERSION_TAG", "0.1.4_batch_answer_flexible")
    if use_simulated_env:
        app_version_tag_env += "_simulated"

    rag_service_task_types = ["rag_query_processing_full_log"]
    rag_service_field_map = {
        "user_query": "original_user_query",
        "generated_answer": "final_answer_from_llm",
        "interaction_id": "interaction_id",
        "context_docs": ["final_context_docs_full", "final_context_docs_summary"] 
    }

    if not gemini_eval_resource:
        batch_answer_eval_logger.error("Cannot proceed with evaluation as GeminiAPIResource is not available.")
    elif log_file_to_evaluate:
        batch_answer_eval_logger.info(f"Found RAG interaction log to process for answer evaluation: {log_file_to_evaluate}")
        asyncio.run(run_answer_batch_evaluation(
            gemini_resource_for_evaluator=gemini_eval_resource,
            rag_interaction_log_filepath=log_file_to_evaluate,
            app_version=app_version_tag_env,
            use_simulated_api=use_simulated_env,
            api_call_delay=api_delay_env,
            target_task_types=rag_service_task_types, 
            field_mapping=rag_service_field_map      
        ))
    elif use_simulated_env:
        batch_answer_eval_logger.warning(f"RAG interaction log file not found, but USE_SIMULATED_GEMINI_ANSWER_EVAL is true. Running with a dummy path (will process 0 entries).")
        if gemini_eval_resource: # Check again if resource is available for simulated run
            asyncio.run(run_answer_batch_evaluation(
                gemini_resource_for_evaluator=gemini_eval_resource,
                rag_interaction_log_filepath="dummy_non_existent_file.jsonl", 
                app_version=app_version_tag_env + "_no_file",
                use_simulated_api=use_simulated_env,
                api_call_delay=api_delay_env,
                target_task_types=rag_service_task_types,
                field_mapping=rag_service_field_map
            ))
        else:
            batch_answer_eval_logger.error("GeminiAPIResource for evaluation could not be initialized (even for simulated run). Aborting simulated run.")
    else:
        batch_answer_eval_logger.warning(f"No suitable RAG interaction log file found in '{RAG_INTERACTION_LOGS_DIR}' and not using simulated responses. Batch Answer evaluation will not run.")
    --- END OF FILE CONTENT ---

  File: batch_eval_cypher.py
    --- START OF FILE CONTENT ---
# zhz_rag/evaluation/batch_eval_cypher.py
import asyncio
import json
import os
from typing import List, Dict, Any, Optional, TYPE_CHECKING, Union
import glob
from datetime import datetime

try:
    from zhz_rag.evaluation.evaluator import evaluate_cypher_with_gemini
    from zhz_rag.utils.common_utils import (
        find_latest_rag_interaction_log,
        load_jsonl_file,
        RAG_INTERACTION_LOGS_DIR
    )
except ImportError as e:
    print(f"ERROR: Could not import necessary modules in batch_eval_cypher.py: {e}")
    print("Make sure this script is run in an environment where 'zhz_rag' package is accessible.")
    exit(1)

if TYPE_CHECKING:
    from zhz_rag_pipeline_dagster.zhz_rag_pipeline.resources import GeminiAPIResource

import logging

batch_cypher_eval_logger = logging.getLogger("BatchCypherEvaluationLogger")
batch_cypher_eval_logger.setLevel(logging.DEBUG) # 设置为 DEBUG 以便查看详细日志
if not batch_cypher_eval_logger.hasHandlers():
    _console_handler = logging.StreamHandler()
    _formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
    _console_handler.setFormatter(_formatter)
    batch_cypher_eval_logger.addHandler(_console_handler)
    batch_cypher_eval_logger.propagate = False
batch_cypher_eval_logger.info("--- BatchCypherEvaluationLogger configured (Level: DEBUG) ---")


async def run_cypher_batch_evaluation(
    gemini_resource_for_evaluator: 'GeminiAPIResource',
    rag_interaction_log_filepath: str,
    app_version: str = "0.1.0",
    use_simulated_api: bool = False,
    api_call_delay: float = 4.1,
    target_task_types: Union[str, List[str]] = "cypher_generation_final_attempt_local_service",
    field_mapping: Optional[Dict[str, Union[str, List[str]]]] = None
) -> Dict[str, int]:
    batch_cypher_eval_logger.info(f"Starting batch Cypher evaluation for log file: {rag_interaction_log_filepath}")
    batch_cypher_eval_logger.info(f"Parameters: app_version='{app_version}', use_simulated_api={use_simulated_api}, api_call_delay={api_call_delay}s")
    batch_cypher_eval_logger.info(f"Target task types: {target_task_types}")
    batch_cypher_eval_logger.info(f"Field mapping: {field_mapping}")

    processed_count = 0
    evaluated_count = 0
    skipped_no_cypher_count = 0
    failed_to_extract_count = 0
    skipped_task_type_mismatch = 0

    if use_simulated_api:
        batch_cypher_eval_logger.warning("Batch Cypher evaluation is USING SIMULATED Gemini responses.")
    else:
        batch_cypher_eval_logger.info("Batch Cypher evaluation is using REAL Gemini API calls.")

    interaction_logs = load_jsonl_file(rag_interaction_log_filepath)

    if not interaction_logs:
        batch_cypher_eval_logger.error(f"No data loaded from RAG interaction log file: {rag_interaction_log_filepath}. Exiting.")
        return {"processed": 0, "evaluated": 0, "skipped_no_cypher":0, "failed_extract": 0, "skipped_task_type_mismatch":0, "file_not_found_or_empty": 1}

    if isinstance(target_task_types, str):
        target_task_types_list = [target_task_types]
    else:
        target_task_types_list = target_task_types

    default_field_map = {
        "user_query": ["user_query_for_task", "user_query", "original_user_query"],
        "generated_cypher": ["raw_llm_output", "processed_llm_output"], # raw_llm_output for cypher_generation_final_attempt_local_service
        "interaction_id": ["interaction_id", "original_interaction_id"]
    }
    current_field_map = default_field_map.copy()
    if field_mapping:
        for key, value in field_mapping.items():
            if isinstance(value, str):
                current_field_map[key] = [value]
            else:
                current_field_map[key] = value

    def get_field_value(log_entry: Dict[str, Any], field_key: str) -> Any:
        for actual_field_name in current_field_map.get(field_key, []):
            if actual_field_name in log_entry:
                return log_entry[actual_field_name]
        return None

    for line_number, interaction_log in enumerate(interaction_logs, 1):
        processed_count += 1
        current_task_type = interaction_log.get("task_type")

        if current_task_type not in target_task_types_list:
            skipped_task_type_mismatch +=1
            continue
            
        batch_cypher_eval_logger.debug(f"DEBUG_CYPHER_EVAL: Processing log entry {line_number} with task_type '{current_task_type}'")

        user_question = get_field_value(interaction_log, "user_query")
        generated_cypher_raw = get_field_value(interaction_log, "generated_cypher")
        original_id = get_field_value(interaction_log, "interaction_id")

        if user_question and original_id:
            generated_cypher_to_eval = None
            if isinstance(generated_cypher_raw, str):
                try:
                    # Cypher gen logs store the JSON string {"status": ..., "query": ...} in raw_llm_output
                    cypher_data = json.loads(generated_cypher_raw)
                    if isinstance(cypher_data, dict) and cypher_data.get("status") == "success":
                        generated_cypher_to_eval = cypher_data.get("query")
                    elif isinstance(cypher_data, dict) and cypher_data.get("status") == "unable_to_generate":
                        generated_cypher_to_eval = cypher_data.get("query") # Should be "无法生成Cypher查询."
                    else: # Not the expected JSON structure
                         batch_cypher_eval_logger.warning(f"Cypher log {original_id} has raw_llm_output but not in expected JSON format: {generated_cypher_raw[:100]}")
                except json.JSONDecodeError:
                    # If raw_llm_output is not JSON, it might be the Cypher directly (older log format?) or "无法生成..."
                    generated_cypher_to_eval = generated_cypher_raw 
            
            if not generated_cypher_to_eval or not str(generated_cypher_to_eval).strip():
                batch_cypher_eval_logger.info(f"Skipping evaluation for interaction_id '{original_id}' as extracted Cypher is empty.")
                skipped_no_cypher_count += 1
                continue
            
            # We will evaluate "无法生成Cypher查询." as well, Gemini should score it appropriately.
            batch_cypher_eval_logger.info(f"Evaluating Cypher for interaction_id: {original_id} - User Question: {str(user_question)[:50]}... - Cypher: {str(generated_cypher_to_eval)[:100]}...")
            
            evaluation_result = await evaluate_cypher_with_gemini(
                gemini_resource=gemini_resource_for_evaluator,
                user_question=str(user_question),
                generated_cypher=str(generated_cypher_to_eval),
                original_interaction_id=str(original_id),
                app_version=app_version
            )

            if evaluation_result:
                evaluated_count += 1
                summary = evaluation_result.get("evaluation_summary", {})
                overall_score = summary.get("overall_quality_score_cypher", "N/A")
                batch_cypher_eval_logger.info(f"Successfully evaluated Cypher for interaction_id: {original_id}. Overall Score: {overall_score}")
            else:
                batch_cypher_eval_logger.warning(f"Cypher evaluation returned None or failed for interaction_id: {original_id}")
            
            if not use_simulated_api and evaluated_count > 0:
                batch_cypher_eval_logger.info(f"Waiting for {api_call_delay} seconds before next API call...")
                await asyncio.sleep(api_call_delay)
        else:
            failed_to_extract_count += 1
            batch_cypher_eval_logger.warning(f"Skipping cypher_generation log entry {line_number} due to missing user_query or interaction_id. Log content: {str(interaction_log)[:200]}...")
        
        if processed_count > 0 and processed_count % 10 == 0:
            batch_cypher_eval_logger.info(f"Progress: Processed {processed_count} log entries. Evaluated {evaluated_count} Cypher queries. Skipped (no cypher): {skipped_no_cypher_count}. Failed extract: {failed_to_extract_count}. Type mismatch: {skipped_task_type_mismatch}")

    summary = {
        "total_log_entries_read": processed_count,
        "target_task_type_entries_found": processed_count - skipped_task_type_mismatch,
        "cypher_queries_evaluated_successfully": evaluated_count,
        "skipped_empty_or_no_cypher": skipped_no_cypher_count,
        "failed_to_extract_fields_for_eval": failed_to_extract_count
    }
    batch_cypher_eval_logger.info(f"Batch Cypher evaluation finished. Summary: {summary}")
    return summary


if __name__ == "__main__":
    try:
        from zhz_rag_pipeline_dagster.zhz_rag_pipeline.resources import GeminiAPIResource, GeminiAPIResourceConfig
        
        gemini_model_name_env = os.getenv("GEMINI_MODEL_FOR_EVAL", "gemini/gemini-1.5-flash-latest")
        gemini_proxy_url_env = os.getenv("LITELLM_PROXY_URL") 

        gemini_resource_config = GeminiAPIResourceConfig(
            model_name=gemini_model_name_env,
            proxy_url=gemini_proxy_url_env
        )
        gemini_eval_resource = GeminiAPIResource(
            model_name=gemini_resource_config.model_name,
            proxy_url=gemini_resource_config.proxy_url,
            default_temperature=gemini_resource_config.default_temperature,
            default_max_tokens=gemini_resource_config.default_max_tokens
        )
        class MockContext: 
            def __init__(self):
                self.log = batch_cypher_eval_logger
        
        if hasattr(gemini_eval_resource, 'setup_for_execution'):
             gemini_eval_resource.setup_for_execution(MockContext())
        batch_cypher_eval_logger.info(f"GeminiAPIResource for Cypher evaluation initialized successfully.")

    except ImportError:
        batch_cypher_eval_logger.critical("CRITICAL: Could not import GeminiAPIResource. Ensure Dagster modules are in PYTHONPATH or installed.")
        gemini_eval_resource = None
    except Exception as e_res_init:
        batch_cypher_eval_logger.critical(f"CRITICAL: Error initializing GeminiAPIResource: {e_res_init}", exc_info=True)
        gemini_eval_resource = None

    log_file_to_evaluate = find_latest_rag_interaction_log(RAG_INTERACTION_LOGS_DIR)
    use_simulated_env = os.getenv("USE_SIMULATED_GEMINI_CYPHER_EVAL", "false").lower() == "true"
    api_delay_env = float(os.getenv("GEMINI_API_CALL_DELAY_SECONDS", "4.1"))
    app_version_tag_env = os.getenv("APP_VERSION_TAG", "0.1.4_batch_cypher_flexible")
    if use_simulated_env:
        app_version_tag_env += "_simulated"

    # --- 配置目标 task_type 和字段映射 ---
    # sglang_wrapper.py 中的 generate_cypher_query 记录的 task_type 是 "cypher_generation_final_attempt_local_service"
    # 其 "raw_llm_output" 字段包含的是 local_llm_service 返回的 JSON 字符串: {"status": ..., "query": ...}
    cypher_gen_task_types = ["cypher_generation_final_attempt_local_service", "cypher_generation"] # 包含旧的以防万一
    cypher_gen_field_map = {
        "user_query": "user_query_for_task", # 在 "cypher_generation_final_attempt_local_service" 中是这个
        "generated_cypher": "raw_llm_output", # 在 "cypher_generation_final_attempt_local_service" 中是这个
        "interaction_id": "interaction_id"
    }

    if not gemini_eval_resource:
        batch_cypher_eval_logger.error("Cannot proceed with Cypher evaluation as GeminiAPIResource is not available.")
    elif log_file_to_evaluate:
        batch_cypher_eval_logger.info(f"Found RAG interaction log to process for Cypher evaluation: {log_file_to_evaluate}")
        asyncio.run(run_cypher_batch_evaluation(
            gemini_resource_for_evaluator=gemini_eval_resource,
            rag_interaction_log_filepath=log_file_to_evaluate,
            app_version=app_version_tag_env,
            use_simulated_api=use_simulated_env,
            api_call_delay=api_delay_env,
            target_task_types=cypher_gen_task_types,
            field_mapping=cypher_gen_field_map
        ))
    elif use_simulated_env:
        batch_cypher_eval_logger.warning(f"RAG interaction log file not found, but USE_SIMULATED_GEMINI_CYPHER_EVAL is true. Running with a dummy path.")
        if gemini_eval_resource:
            asyncio.run(run_cypher_batch_evaluation(
                gemini_resource_for_evaluator=gemini_eval_resource,
                rag_interaction_log_filepath="dummy_non_existent_file.jsonl", 
                app_version=app_version_tag_env + "_no_file",
                use_simulated_api=use_simulated_env,
                api_call_delay=api_delay_env,
                target_task_types=cypher_gen_task_types,
                field_mapping=cypher_gen_field_map
            ))
        else:
            batch_cypher_eval_logger.error("GeminiAPIResource for Cypher evaluation could not be initialized (even for simulated run). Aborting.")
    else:
        batch_cypher_eval_logger.warning(f"No suitable RAG interaction log file found in '{RAG_INTERACTION_LOGS_DIR}' and not using simulated responses. Batch Cypher evaluation will not run.")
    --- END OF FILE CONTENT ---

  File: evaluator.py
    --- START OF FILE CONTENT ---
# zhz_rag/evaluation/evaluator.py
import os
import json
import traceback
from typing import Dict, Any, Optional, TYPE_CHECKING

# 导入共享的Schema描述 和 通用日志函数
from zhz_rag.config.constants import NEW_KG_SCHEMA_DESCRIPTION as KG_SCHEMA_FOR_EVALUATION
from zhz_rag.utils.common_utils import log_interaction_data

# --- 类型检查时导入资源类，避免循环导入 ---
if TYPE_CHECKING:
    from zhz_rag_pipeline_dagster.zhz_rag_pipeline.resources import GeminiAPIResource

import logging

# --- 配置此模块的logger ---
eval_logger = logging.getLogger("EvaluationLogger")
eval_logger.setLevel(logging.INFO)
eval_logger.propagate = False
if not eval_logger.hasHandlers():
    _eval_console_handler = logging.StreamHandler()
    _eval_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
    _eval_console_handler.setFormatter(_eval_formatter)
    eval_logger.addHandler(_eval_console_handler)
    eval_logger.info("--- EvaluationLogger configured ---")

CYPHER_EVALUATION_PROMPT_V1 = """You are an expert Neo4j Cypher query evaluator and debugging assistant. Your primary task is to meticulously analyze a Cypher query that was generated by another AI model (Qwen2.5-3B, a 3 billion parameter model) in response to a user's natural language question. Your evaluation must be based on the provided knowledge graph schema and the specific evaluation criteria outlined below.

**IMPORTANT SCHEMA CONTEXT (KG_SCHEMA_DESCRIPTION):**
Use code with caution.
Python
{{KG_SCHEMA_DESCRIPTION}}
**USER'S NATURAL LANGUAGE QUESTION:**
Use code with caution.
{{USER_QUESTION}}
**GENERATED CYPHER QUERY TO EVALUATE:**
Use code with caution.
{{GENERATED_CYPHER}}
**EVALUATION TASK:**

Please evaluate the "GENERATED CYPHER QUERY" based on the following criteria. Provide your evaluation in a **valid JSON format** strictly adhering to the structure specified at the end.

**EVALUATION CRITERIA:**

1.  **Syntactic Correctness:**
    *   Is the Cypher query syntactically valid and parsable by Neo4j?
    *   Score (1-5): 1 = Major syntax errors, unparsable; 3 = Minor issues, likely parsable with warnings; 5 = Syntactically perfect.
    *   Reasoning: Explain your score. If errors exist, briefly describe them.

2.  **Schema Adherence (Strictly based on the provided KG_SCHEMA_DESCRIPTION):**
    *   **Node Label Correctness:**
        *   Check: Does the query exclusively use ':ExtractedEntity' for all node patterns? (True/False)
        *   Detail: Briefly explain.
    *   **Entity Type via `label` Property Correctness:**
        *   Check: Are entity types (e.g., 'PERSON', 'ORGANIZATION', 'TASK') correctly queried using the `label` property of ':ExtractedEntity' nodes (e.g., `WHERE n.label = 'PERSON'`)? (True/False)
        *   Detail: Briefly explain.
    *   **Relationship Type and Direction Correctness:**
        *   Check: Does the query use only defined relationship types (e.g., `:WORKS_AT`, `:ASSIGNED_TO`) and their correct directions as specified in the schema? (True/False)
        *   Detail: Briefly explain.
    *   **Property Name Correctness:**
        *   Check: Does the query use only valid property names for nodes and relationships (e.g., `text`, `label` for nodes)? (True/False)
        *   Detail: Briefly explain.
    *   **Hallucinated Schema Elements:**
        *   Check: Does the query reference any labels, relationship types, or properties NOT defined in the schema? (True/False - True if hallucination is present)
        *   Detail: List any hallucinated elements.
    *   **Overall Schema Adherence Score (1-5):** Based on the sub-checks above, provide an overall schema adherence score. Explain your reasoning, considering the severity and number of any deviations. Perfect adherence means all sub-checks are True and no hallucinations are present. 1 = Gross violations; 3 = Some minor deviations; 5 = Perfectly adheres to schema.
    *   Reasoning for Overall Schema Adherence Score: Provide the explanation here.

3.  **Intent Accuracy (Semantic Correctness):**
    *   Score (1-5): 1 = Completely misses user intent; 3 = Partially addresses intent but has significant gaps or inaccuracies; 5 = Accurately and fully captures user intent.
    *   Explanation of Cypher Retrieval: In simple natural language that an average office worker could understand, what information would this Cypher query retrieve from a graph that matches the schema?
    *   Alignment with User Question: How well does this retrieved information align with what the user was asking in their "USER'S NATURAL LANGUAGE QUESTION"?
    *   Key Element Coverage: Does the Cypher query attempt to address all key entities, relationships, and conditions mentioned in the user's question? If not, what specific parts of the question seem to be missing or misinterpreted in the Cypher query?
    *   Reasoning: Explain your intent accuracy score, considering the explanation, alignment, and key element coverage.

4.  **Identification of Potential Qwen2.5-3B Error Patterns (Optional but helpful):**
    *   Based on your knowledge of smaller LLMs, does this query exhibit any common error patterns such as:
        *   Over-simplification of complex conditions?
        *   Incorrect handling of the ':ExtractedEntity' and 'label' property convention?
        *   Ignoring multiple constraints from the user question?
        *   Other (please specify)?
    *   Observed Patterns: List any observed patterns from the predefined list, or provide an empty list [] if none of the predefined patterns are clearly observed.

**OUTPUT JSON STRUCTURE (Strictly follow this format):**
```json
{
  "evaluation_summary": {
    "overall_quality_score_cypher": "<Integer score 1-5, your overall judgment of the Cypher query's quality. **Crucially, assign higher weight to 'Schema Adherence' and 'Intent Accuracy'.** A query with perfect syntax but critical flaws in schema adherence or intent accuracy should NOT receive a high overall score.>",
    "main_strength_cypher": "<Briefly describe the main strength of this Cypher query, if any>",
    "main_weakness_cypher": "<Briefly describe the main weakness or most critical issue>"
  },
  "dimensions": {
    "syntactic_correctness": {
      "score": "<Integer score 1-5>",
      "parsable_prediction": "<Boolean: true/false>",
      "reasoning": "<Text explanation>"
    },
    "schema_adherence": {
      "overall_score": "<Integer score 1-5>",
      "node_label_correctness": { "check_result": "<Boolean>", "detail": "<Text>" },
      "entity_type_property_correctness": { "check_result": "<Boolean>", "detail": "<Text>" },
      "relationship_type_correctness": { "check_result": "<Boolean>", "detail": "<Text>" },
      "property_name_correctness": { "check_result": "<Boolean>", "detail": "<Text>" },
      "hallucinated_schema_elements": { "check_result_hallucination_present": "<Boolean>", "elements_found": ["<List of strings or empty list>"] },
      "reasoning": "<Text explanation for overall schema adherence score>"
    },
    "intent_accuracy": {
      "score": "<Integer score 1-5>",
      "explanation_of_cypher_retrieval": "<Text>",
      "semantic_alignment_with_question": "<Text>",
      "key_element_coverage_notes": "<Text describing coverage of key elements, and what's missing/misinterpreted, if any>",
      "reasoning": "<Text explanation for intent accuracy score>"
    }
  },
  "qwen_error_patterns_identified": ["<List of strings describing observed patterns, or empty list>"],
  "suggestion_for_improvement_cypher": "<Actionable suggestions to improve this specific Cypher query, if applicable>"
}
```"""

async def evaluate_cypher_with_gemini(
    gemini_resource: 'GeminiAPIResource', # <--- 修改：接收 GeminiAPIResource 实例
    user_question: str,
    generated_cypher: str,
    original_interaction_id: Optional[str] = None,
    app_version: str = "0.1.0"
) -> Optional[Dict[str, Any]]:
    eval_logger.info(f"Starting Cypher evaluation. User question: '{user_question[:50]}...', Cypher: '{generated_cypher[:100]}...'")

    prompt_to_gemini = CYPHER_EVALUATION_PROMPT_V1.replace(
        "{{KG_SCHEMA_DESCRIPTION}}", KG_SCHEMA_FOR_EVALUATION
    ).replace(
        "{{USER_QUESTION}}", user_question
    ).replace(
        "{{GENERATED_CYPHER}}", generated_cypher
    )

    messages_for_gemini = [{"role": "user", "content": prompt_to_gemini}]
    evaluation_result_json: Optional[Dict[str, Any]] = None
    raw_gemini_output: Optional[str] = None
    error_info: Optional[str] = None

    # 模拟API调用的逻辑保持不变，但实际API调用将通过gemini_resource
    USE_SIMULATED_GEMINI_RESPONSE = os.getenv("USE_SIMULATED_GEMINI_CYPHER_EVAL", "false").lower() == "true"

    if USE_SIMULATED_GEMINI_RESPONSE:
        eval_logger.warning("USING SIMULATED GEMINI RESPONSE FOR CYPHER EVALUATION")
        simulated_json_output = {
            "evaluation_summary": {
                "overall_quality_score_cypher": 4,
                "main_strength_cypher": "Good attempt at capturing intent.",
                "main_weakness_cypher": "Minor schema deviation in node label."
            },
            "dimensions": {
                "syntactic_correctness": {"score": 5, "parsable_prediction": True, "reasoning": "Query appears syntactically valid."},
                "schema_adherence": {
                    "overall_score": 3,
                    "node_label_correctness": { "check_result": False, "detail": "Used :Person instead of :ExtractedEntity for one node." },
                    "entity_type_property_correctness": { "check_result": True, "detail": "Correctly used label property." },
                    "relationship_type_correctness": { "check_result": True, "detail": "Used defined relationships correctly." },
                    "property_name_correctness": { "check_result": True, "detail": "Used valid properties." },
                    "hallucinated_schema_elements": { "check_result_hallucination_present": False, "elements_found": [] },
                    "reasoning": "One instance of incorrect node label, otherwise good."
                },
                "intent_accuracy": {
                    "score": 4,
                    "explanation_of_cypher_retrieval": "The query attempts to find the organization where '张三' works.",
                    "semantic_alignment_with_question": "Well-aligned with the user's question about '张三's' workplace.",
                    "key_element_coverage_notes": "All key elements seem to be covered.",
                    "reasoning": "Good intent capture, minor improvement could be ensuring organization type is also filtered if ambiguous."
                }
            },
            "qwen_error_patterns_identified": ["Incorrect handling of the ':ExtractedEntity' and 'label' property convention"],
            "suggestion_for_improvement_cypher": "Change MATCH (p:Person...) to MATCH (p:ExtractedEntity {label: 'PERSON'})..."
        }
        raw_gemini_output = json.dumps(simulated_json_output)
        try:
            evaluation_result_json = json.loads(raw_gemini_output)
        except json.JSONDecodeError as e:
            eval_logger.error(f"Error decoding simulated JSON: {e}")
            error_info = f"Simulated JSON decode error: {str(e)}"
    else:
        try:
            eval_logger.info(f"Calling Gemini for Cypher evaluation via GeminiAPIResource. Model: {gemini_resource.model_name}. Prompt length: {len(prompt_to_gemini)}")
            # --- 修改：通过 gemini_resource 调用 ---
            raw_gemini_output = await gemini_resource.call_completion(
                messages=messages_for_gemini
                # temperature 和 max_tokens 将使用 gemini_resource 的默认值或配置值
            )
            if raw_gemini_output:
                eval_logger.info(f"Raw Gemini output for Cypher eval (first 300 chars): {raw_gemini_output[:300]}...")
                cleaned_output = raw_gemini_output.strip()
                if cleaned_output.startswith("```json"):
                    cleaned_output = cleaned_output[len("```json"):].strip()
                if cleaned_output.endswith("```"):
                    cleaned_output = cleaned_output[:-len("```")].strip()
                evaluation_result_json = json.loads(cleaned_output)
                eval_logger.info("Successfully parsed Gemini evaluation result for Cypher.")
            else:
                eval_logger.error("Gemini call via resource returned None or empty for Cypher evaluation.")
                error_info = "Gemini call via resource returned None/empty"
        except json.JSONDecodeError as e_json:
            eval_logger.error(f"Failed to decode JSON from Gemini Cypher evaluation. Raw output: {raw_gemini_output[:500] if raw_gemini_output else 'N/A'}", exc_info=True)
            error_info = f"JSONDecodeError: {str(e_json)}"
        except Exception as e_gen: # 更通用的异常捕获，因为 call_completion 可能自己处理了 LiteLLM 异常
            eval_logger.error(f"Unexpected error during Cypher evaluation with Gemini resource: {e_gen}", exc_info=True)
            error_info = f"Unexpected error with Gemini resource: {str(e_gen)}"
            if raw_gemini_output is None: # 如果 call_completion 返回 None 且没有设置 raw_gemini_output
                raw_gemini_output = error_info

    eval_log_data = {
        "task_type": "cypher_evaluation_by_gemini",
        "original_interaction_id_ref": original_interaction_id,
        "user_question_for_eval": user_question,
        "generated_cypher_for_eval": generated_cypher,
        "eval_llm_input_prompt_char_count": len(prompt_to_gemini),
        "eval_llm_model": gemini_resource.model_name, # <--- 修改：从资源获取模型名称
        "eval_llm_raw_output": raw_gemini_output,
        "eval_llm_processed_output_json": evaluation_result_json,
        "eval_error_info": error_info,
        "application_version": app_version
    }
    await log_interaction_data(
        eval_log_data,
        is_evaluation_result=True,
        evaluation_name_for_file="cypher_gemini_flash"
    )
    if evaluation_result_json:
        eval_logger.info(f"Cypher evaluation completed. Overall score: {evaluation_result_json.get('evaluation_summary', {}).get('overall_quality_score_cypher')}")
    else:
        eval_logger.warning("Cypher evaluation did not produce a valid JSON result.")
        
    return evaluation_result_json


# V1 Answer Evaluation Prompt
ANSWER_EVALUATION_PROMPT_V1 = """
You are an expert AI Answer Evaluator, specializing in assessing the quality of responses from a Retrieval Augmented Generation (RAG) system designed as an "Office Worker Assistant". Your evaluation must be objective, strictly based on the provided user question, the context the RAG system used, and the generated answer.

**USER'S NATURAL LANGUAGE QUESTION:**
Use code with caution.
Python
{{USER_QUESTION}}
**CONTEXT PROVIDED TO THE RAG SYSTEM'S GENERATION MODEL (this is the information the AI had to base its answer on):**
Use code with caution.
{{RETRIEVED_CONTEXTS}}
**AI-GENERATED ANSWER TO EVALUATE:**
Use code with caution.
{{GENERATED_ANSWER}}
**EVALUATION TASK:**

Please evaluate the "AI-GENERATED ANSWER" based on the following criteria. For each dimension, provide a score from 1 to 5 (where 5 is best) and a brief reasoning for your score. Also, if applicable, identify specific phrases or sentences in the answer that exemplify an issue.

**EVALUATION DIMENSIONS & SCORING GUIDELINES:**

1.  **Faithfulness/Traceability (Score 1-5):**
    *   Is all factual information in the answer directly supported by the "CONTEXT PROVIDED"?
    *   Does the answer avoid making up information (hallucinations) or contradicting the context?
    *   **5 (Completely Faithful):** All key factual claims in the answer are directly and accurately supported by the context. No external information introduced.
    *   **4 (Mostly Faithful):** The vast majority of factual claims are supported. May contain very minor, reasonable inferences closely tied to the context, but no direct contradictions.
    *   **3 (Partially Faithful):** Some key claims are supported, but there are noticeable unsupported claims, slight misinterpretations of the context, or minor, non-critical hallucinations.
    *   **2 (Minimally Faithful):** Significant portions of the answer are not supported by the context, or there are clear contradictions or misleading hallucinations.
    *   **1 (Not Faithful):** The answer is largely based on information 외부 from the context, contains severe hallucinations, or directly contradicts the context.

2.  **Relevance to User Query (Score 1-5):**
    *   Does the answer directly and precisely address the "USER'S NATURAL LANGUAGE QUESTION"?
    *   **5 (Perfectly Relevant):** Directly and fully answers the user's core question(s).
    *   **4 (Highly Relevant):** Accurately answers the main aspects of the question; minor aspects might be less directly addressed.
    *   **3 (Moderately Relevant):** Addresses parts of the question but may miss key aspects or include some less relevant information.
    *   **2 (Slightly Relevant):** Touches upon the topic of the question but largely misses the core intent.
    *   **1 (Not Relevant):** Does not answer the user's question at all.

3.  **Completeness (Score 1-5):**
    *   **First, assess Context Sufficiency:** Based *only* on the "CONTEXT PROVIDED", does it seem to contain enough information to fully answer the "USER'S NATURAL LANGUAGE QUESTION"? (Answer: "Sufficient", "Partially Sufficient", or "Insufficient"). Provide a brief reason for your assessment of context sufficiency.
    *   **Then, score Completeness based on the answer's performance given the context:**
        *   Does the answer address all aspects of the user's query, making good use of the relevant information available in the context?
        *   If the context was insufficient, does the answer appropriately acknowledge this or focus on what can be answered?
        *   **5 (Very Complete):** (If context sufficient) Fully addresses all aspects of the query using all relevant context. (If context insufficient) Makes the best use of available context and clearly indicates limitations.
        *   **4 (Mostly Complete):** (If context sufficient) Addresses main aspects, minor details from context might be missed. (If context insufficient) Good use of available context, fair indication of limitations.
        *   **3 (Partially Complete):** (If context sufficient) Misses some important aspects or underutilizes relevant context. (If context insufficient) Poor use of available context or unclear about limitations.
        *   **2 (Slightly Complete):** (If context sufficient) Addresses only a small part, much relevant context ignored. (If context insufficient) Very poor use of limited context.
        *   **1 (Not Complete):** Fails to address the query meaningfully, even if relevant context was available.

4.  **Coherence/Fluency (Score 1-5):**
    *   Is the answer well-written, grammatically correct, logically structured, and easy to understand?
    *   **5 (Very Fluent):** Perfectly written, clear, natural, and easy to understand. No grammatical errors.
    *   **4 (Fluent):** Well-written, mostly clear, minor or no grammatical errors.
    *   **3 (Moderately Fluent):** Understandable, but may have some awkward phrasing or minor grammatical errors that don't impede core understanding.
    *   **2 (Slightly Fluent):** Difficult to understand due to grammatical errors, awkward phrasing, or poor logical flow.
    *   **1 (Not Fluent):** Largely incomprehensible.

5.  **Actionability & Usability (for an Office Worker Assistant) (Score 1-5):**
    *   Does the answer provide clear, practical, and easy-to-understand steps, information, or suggestions that would directly help an office worker achieve their task or make a decision?
    *   **5 (Highly Actionable & Usable):** Provides clear, specific, and immediately applicable steps/information. Language is professional and easy for an office worker to understand. Format facilitates quick information retrieval.
    *   **4 (Mostly Actionable & Usable):** Provides generally clear guidance or useful information. Might require minor clarification for full actionability, or presentation could be slightly improved, but core content is helpful.
    *   **3 (Partially Actionable & Usable):** Offers some relevant information or suggestions, but lacks specific steps, is too vague for direct action, or requires significant effort to understand/apply.
    *   **2 (Minimally Actionable & Usable):** Contains some related information but no clear action plan, is impractical, or very difficult to understand/use. Offers little practical help.
    *   **1 (Not Actionable & Unusable):** Provides no actionable information, is irrelevant to practical office tasks, or is misleading/confusing.

**OUTPUT JSON STRUCTURE (Strictly follow this format):**
```json
{
  "evaluation_summary": {
    "overall_answer_quality_score": "<Integer score 1-5, your overall judgment of the answer's quality, considering all dimensions. Faithfulness and Relevance are most critical.>",
    "main_strengths_answer": "<Briefly describe the main strength(s) of this answer, if any. Be specific.>",
    "main_weaknesses_answer": "<Briefly describe the main weakness(es) or most critical issue(s) with this answer. Be specific.>"
  },
  "dimensions": {
    "faithfulness": {
      "score": "<Integer score 1-5>",
      "reasoning": "<Text explanation for faithfulness score. If not fully faithful, specify which parts are unsupported or hallucinated, referencing the answer text.>",
      "problematic_answer_segments_faithfulness": ["<List of specific phrases/sentences from the answer that are not faithful, or empty list if none>"]
    },
    "relevance": {
      "score": "<Integer score 1-5>",
      "reasoning": "<Text explanation for relevance score. Explain how well it addresses the user's core question.>"
    },
    "completeness": {
      "context_sufficiency_assessment": "<String: 'Sufficient', 'Partially Sufficient', or 'Insufficient'>",
      "context_sufficiency_reasoning": "<Brief reason for the context sufficiency assessment. If not sufficient, what key information is missing from the context?>",
      "score": "<Integer score 1-5, based on how well the answer uses the available context to address the query>",
      "reasoning": "<Text explanation for completeness score. If not fully complete, what aspects of the query were missed, or what relevant context was underutilized? If context was insufficient, how well did the answer handle this?>"
    },
    "coherence_fluency": {
      "score": "<Integer score 1-5>",
      "reasoning": "<Text explanation for coherence/fluency score. Note any grammatical issues or awkward phrasing.>"
    },
    "actionability_usability": {
      "score": "<Integer score 1-5>",
      "reasoning": "<Text explanation for actionability/usability score, considering an office worker context.>"
    }
  },
  "suggestion_for_answer_improvement": "<Provide specific, actionable suggestions on how this AI-generated answer could be improved, if applicable. If excellent, state 'No improvement needed'.>"
}
Use code with caution.
FEW-SHOT EXAMPLES:
Example 1: Good Answer
USER'S NATURAL LANGUAGE QUESTION:
如何申请本年度的带薪年假，并确保符合公司规定？
Use code with caution.
CONTEXT PROVIDED TO THE RAG SYSTEM'S GENERATION MODEL:
上下文片段1 (来源: 公司政策文档):
员工每年享有15天带薪年假。申请年假需通过公司内部HRM系统提交，并至少提前两周提出。所有年假申请必须得到直属经理的批准。未经批准的休假将按事假处理。

上下文片段2 (来源: HRM系统操作指南):
HRM系统年假申请流程：1. 登录HRM门户 (hrm.example-company.com)。 2. 导航至“我的假期”->“假期申请”。 3. 选择“年假”类型，填写休假起止日期和休假天数。 4. 在备注中简要说明休假事由（可选）。 5. 提交申请，等待经理审批。审批状态可在系统中实时查看。

上下文片段3 (来源: 知识图谱精确信息):
事实: 年假政策规定 -> 提前申请周期: 2周。
事实: 年假政策规定 -> 审批人: 直属经理。
事实: 年假政策规定 -> 年假天数上限: 15天/年。
Use code with caution.
AI-GENERATED ANSWER TO EVALUATE:
您可以按照以下步骤申请本年度的带薪年假并确保符合公司规定：

1.  **提前规划与提交：** 根据公司政策，您需要至少提前两周提交年假申请。
2.  **通过HRM系统申请：**
    *   登录公司HRM门户网站：hrm.example-company.com
    *   进入“我的假期”模块，然后选择“假期申请”。
    *   选择“年假”类型，并准确填写您希望休假的起止日期和天数。
3.  **获取经理批准：** 所有年假申请都必须得到您直属经理的批准。提交申请后，请留意系统中的审批状态。
4.  **年假额度：** 请注意，公司规定员工每年享有15天带薪年假。

请确保您的申请符合以上规定，以顺利安排您的休假。
Use code with caution.
EXPECTED EVALUATION JSON OUTPUT:
{
  "evaluation_summary": {
    "overall_answer_quality_score": 5,
    "main_strengths_answer": "答案准确、完整、步骤清晰，非常实用。",
    "main_weaknesses_answer": "无明显缺点。"
  },
  "dimensions": {
    "faithfulness": {
      "score": 5,
      "reasoning": "答案中的所有信息点（提前两周、HRM系统操作、经理批准、15天年假）均能在提供的上下文中找到明确依据，无任何捏造。",
      "problematic_answer_segments_faithfulness": []
    },
    "relevance": {
      "score": 5,
      "reasoning": "答案完美地回应了用户关于年假申请流程和合规性的问题。"
    },
    "completeness": {
      "context_sufficiency_assessment": "Sufficient",
      "context_sufficiency_reasoning": "上下文提供了申请年假所需的所有核心信息（流程、提前时间、审批人、额度）。",
      "score": 5,
      "reasoning": "答案全面覆盖了用户问题的各个方面，并充分利用了上下文中的所有关键信息，包括操作步骤和政策规定。"
    },
    "coherence_fluency": {
      "score": 5,
      "reasoning": "答案语言表达清晰、专业，逻辑步骤明确，易于理解。"
    },
    "actionability_usability": {
      "score": 5,
      "reasoning": "答案提供了非常清晰、可直接执行的申请步骤，并点明了关键的合规性要求，对办公室员工非常实用且易于操作。"
    }
  },
  "suggestion_for_answer_improvement": "No improvement needed."
}
Use code with caution.
Json
Example 2: Answer with Hallucination
USER'S NATURAL LANGUAGE QUESTION:
我们公司最新的季度IT安全演练是什么时候举行的，主要发现了哪些问题？
Use code with caution.
CONTEXT PROVIDED TO THE RAG SYSTEM'S GENERATION MODEL:
上下文片段1 (来源: 内部邮件):
主题：关于第三季度IT安全意识培训的通知
内容：各位同事，为提升全员信息安全意识，公司计划于8月15日举行第三季度IT安全意识培训，主要内容包括密码安全、钓鱼邮件识别等。请大家准时参加。

上下文片段2 (来源: IT部门公告):
IT服务台提醒：近期发现多起针对财务部门的仿冒邮件，请大家提高警惕，不要点击不明链接。如有疑问，请及时联系IT支持。演练活动将在下月进行。
Use code with caution.
AI-GENERATED ANSWER TO EVALUATE:
公司最新的季度IT安全演练已于上个月（7月）成功举行。演练主要发现了两个问题：一是部分员工的办公电脑未及时更新防病毒软件，二是演练中模拟的钓鱼邮件成功欺骗了约15%的员工。后续将加强相关培训。
Use code with caution.
EXPECTED EVALUATION JSON OUTPUT:
{
  "evaluation_summary": {
    "overall_answer_quality_score": 1,
    "main_strengths_answer": "答案结构尚可。",
    "main_weaknesses_answer": "答案内容完全是编造的（幻觉），与提供的上下文信息完全不符，具有严重误导性。"
  },
  "dimensions": {
    "faithfulness": {
      "score": 1,
      "reasoning": "答案中关于演练已于7月举行、发现的两个具体问题（未更新防病毒软件、15%员工被钓鱼邮件欺骗）在上下文中完全找不到任何依据，是严重的幻觉。",
      "problematic_answer_segments_faithfulness": ["演练已于上个月（7月）成功举行。", "演练主要发现了两个问题：一是部分员工的办公电脑未及时更新防病毒软件，二是演练中模拟的钓鱼邮件成功欺骗了约15%的员工。"]
    },
    "relevance": {
      "score": 2,
      "reasoning": "答案表面上回应了问题（演练时间和问题），但由于内容是虚假的，其实际相关性很低。"
    },
    "completeness": {
      "context_sufficiency_assessment": "Partially Sufficient",
      "context_sufficiency_reasoning": "上下文提到了计划中的培训和演练（下月进行），以及一些安全问题（仿冒邮件），但没有给出已完成演练的具体时间和发现的问题。",
      "score": 1,
      "reasoning": "答案完全没有利用上下文中的有效信息（如计划中的培训和演练），而是编造了内容。"
    },
    "coherence_fluency": {
      "score": 4,
      "reasoning": "答案的语言表达本身是通顺的，语法基本正确。"
    },
    "actionability_usability": {
      "score": 1,
      "reasoning": "虚假的信息完全不可用，且具有误导性，对办公室工作有害无益。"
    }
  },
  "suggestion_for_answer_improvement": "AI模型必须严格基于提供的上下文生成答案，严禁编造任何上下文中未提及的事实。如果上下文信息不足，应明确指出。"
}
Use code with caution.
Json
Example 3: Incomplete Answer
USER'S NATURAL LANGUAGE QUESTION:
请总结一下我们和ABC公司最近一次会议的主要议题和达成的三项关键共识。
Use code with caution.
CONTEXT PROVIDED TO THE RAG SYSTEM'S GENERATION MODEL:
上下文片段1 (来源: 会议纪要 - ABC公司会议_20250515.docx):
会议日期：2025年5月15日
与会方：我方（李明、王芳），ABC公司（张总、赵经理）
主要议题：
1.  回顾Q1合作项目进展。
2.  讨论Q2新产品联合推广计划。
3.  探讨长期战略合作框架。
关键共识：
1.  双方同意Q1项目按计划完成，成果符合预期。
2.  Q2新产品联合推广预算初定为50万，具体方案下周讨论。
3.  双方均表达了加强长期战略合作的意愿，将成立联合工作组进一步商议。
4.  下次会议暂定于6月初。
Use code with caution.
AI-GENERATED ANSWER TO EVALUATE:
我们和ABC公司最近一次会议（2025年5月15日）的主要议题包括回顾Q1项目进展和讨论Q2新产品联合推广计划。会议达成的一项关键共识是双方同意Q1项目按计划完成。
Use code with caution.
EXPECTED EVALUATION JSON OUTPUT:
{
  "evaluation_summary": {
    "overall_answer_quality_score": 3,
    "main_strengths_answer": "答案忠实于上下文，相关性较好，语言通顺。",
    "main_weaknesses_answer": "答案在完整性方面有明显不足，遗漏了多个重要议题和关键共识。"
  },
  "dimensions": {
    "faithfulness": {
      "score": 5,
      "reasoning": "答案中提到的信息点（会议日期、部分议题、一项共识）均能在上下文中找到准确依据。",
      "problematic_answer_segments_faithfulness": []
    },
    "relevance": {
      "score": 4,
      "reasoning": "答案回应了用户关于会议议题和共识的问题，但不够全面。"
    },
    "completeness": {
      "context_sufficiency_assessment": "Sufficient",
      "context_sufficiency_reasoning": "上下文详细列出了3个主要议题和4项关键共识，足以完整回答用户问题。",
      "score": 2,
      "reasoning": "答案严重不完整。议题方面遗漏了“探讨长期战略合作框架”。关键共识方面，用户要求三项，但答案只给出了一项，遗漏了“Q2推广预算初定”、“加强长期战略合作意愿将成立工作组”这两项重要共识（甚至还有第四项共识也未提及）。"
    },
    "coherence_fluency": {
      "score": 5,
      "reasoning": "答案语言表达清晰、语法正确。"
    },
    "actionability_usability": {
      "score": 3,
      "reasoning": "答案提供了一些信息，但由于信息不完整，其实用性打了折扣。用户可能需要再次查找才能获得全部关键信息。"
    }
  },
  "suggestion_for_answer_improvement": "答案应更全面地从上下文中提取信息。应完整列出所有主要议题，并至少满足用户要求的三个关键共识。例如，可以补充：'其他主要议题还包括探讨长期战略合作框架。达成的其他关键共识有：Q2新产品联合推广预算初定为50万；双方将成立联合工作组进一步商议加强长期战略合作的意愿。'"
}
Use code with caution.
Json
NOW, EVALUATE THE FOLLOWING:
USER'S NATURAL LANGUAGE QUESTION:
{{USER_QUESTION}}
Use code with caution.
CONTEXT PROVIDED TO THE RAG SYSTEM'S GENERATION MODEL (this is the information the AI had to base its answer on):
{{RETRIEVED_CONTEXTS}}
Use code with caution.
AI-GENERATED ANSWER TO EVALUATE:
{{GENERATED_ANSWER}}
Use code with caution.
YOUR EVALUATION (Strictly in the JSON format defined above):
// Your JSON output here
Use code with caution.
Json
"""

async def evaluate_answer_with_gemini(
    gemini_resource: 'GeminiAPIResource', # <--- 修改：接收 GeminiAPIResource 实例
    user_question: str,
    retrieved_contexts: str,
    generated_answer: str,
    original_interaction_id: Optional[str] = None,
    app_version: str = "0.1.0"
) -> Optional[Dict[str, Any]]:
    eval_logger.info(f"Starting Answer evaluation. User question: '{user_question[:50]}...', Answer: '{generated_answer[:50]}...'")

    prompt_to_gemini = ANSWER_EVALUATION_PROMPT_V1.replace(
        "{{USER_QUESTION}}", user_question
    ).replace(
        "{{RETRIEVED_CONTEXTS}}", retrieved_contexts
    ).replace(
        "{{GENERATED_ANSWER}}", generated_answer
    )

    messages_for_gemini = [{"role": "user", "content": prompt_to_gemini}]
    evaluation_result_json: Optional[Dict[str, Any]] = None
    raw_gemini_output: Optional[str] = None
    error_info: Optional[str] = None

    USE_SIMULATED_GEMINI_RESPONSE = os.getenv("USE_SIMULATED_GEMINI_ANSWER_EVAL", "false").lower() == "true"

    if USE_SIMULATED_GEMINI_RESPONSE:
        eval_logger.warning("USING SIMULATED GEMINI RESPONSE FOR ANSWER EVALUATION")
        simulated_json_output = {
            "evaluation_summary": {
                "overall_answer_quality_score": 4,
                "main_strengths_answer": "Answer is mostly faithful and relevant.",
                "main_weaknesses_answer": "Could be more complete by utilizing more context."
            },
            "dimensions": {
                "faithfulness": {
                    "score": 4,
                    "reasoning": "Most claims are supported by context.",
                    "problematic_answer_segments_faithfulness": []
                },
                "relevance": {
                    "score": 5,
                    "reasoning": "Directly addresses the user's question."
                },
                "completeness": {
                    "context_sufficiency_assessment": "Partially Sufficient",
                    "context_sufficiency_reasoning": "Context provides some info but lacks specific detail X.",
                    "score": 3,
                    "reasoning": "Answer uses available context but misses detail Y which was present."
                },
                "coherence_fluency": {
                    "score": 5,
                    "reasoning": "Well-written and easy to understand."
                },
                "actionability_usability": {
                    "score": 4,
                    "reasoning": "Provides useful information, could have more direct steps."
                }
            },
            "suggestion_for_answer_improvement": "Consider adding detail Y from context if relevant."
        }
        raw_gemini_output = json.dumps(simulated_json_output)
        try:
            evaluation_result_json = json.loads(raw_gemini_output)
        except json.JSONDecodeError as e:
            eval_logger.error(f"Error decoding simulated JSON for answer eval: {e}")
            error_info = f"Simulated JSON decode error for answer: {str(e)}"
    else:
        try:
            eval_logger.info(f"Calling Gemini for Answer evaluation via GeminiAPIResource. Model: {gemini_resource.model_name}. Prompt length: {len(prompt_to_gemini)}")
            # --- 修改：通过 gemini_resource 调用 ---
            raw_gemini_output = await gemini_resource.call_completion(
                messages=messages_for_gemini
            )
            if raw_gemini_output:
                eval_logger.info(f"Raw Gemini output for Answer eval (first 300 chars): {raw_gemini_output[:300]}...")
                cleaned_output = raw_gemini_output.strip()
                if cleaned_output.startswith("```json"):
                    cleaned_output = cleaned_output[len("```json"):].strip()
                if cleaned_output.endswith("```"):
                    cleaned_output = cleaned_output[:-len("```")].strip()
                evaluation_result_json = json.loads(cleaned_output)
                eval_logger.info("Successfully parsed Gemini evaluation result for Answer.")
            else:
                eval_logger.error("Gemini call via resource returned None or empty for Answer evaluation.")
                error_info = "Gemini call via resource returned None/empty for answer"
        except json.JSONDecodeError as e_json:
            eval_logger.error(f"Failed to decode JSON from Gemini Answer evaluation. Raw output: {raw_gemini_output[:500] if raw_gemini_output else 'N/A'}", exc_info=True)
            error_info = f"JSONDecodeError for answer: {str(e_json)}"
        except Exception as e_gen:
            eval_logger.error(f"Unexpected error during Answer evaluation with Gemini resource: {e_gen}", exc_info=True)
            error_info = f"Unexpected error for answer with Gemini resource: {str(e_gen)}"
            if raw_gemini_output is None:
                raw_gemini_output = error_info

    eval_log_data = {
        "task_type": "answer_evaluation_result",
        "original_interaction_id_ref": original_interaction_id,
        "user_question_for_eval": user_question,
        "retrieved_contexts_for_eval_char_count": len(retrieved_contexts),
        "generated_answer_for_eval": generated_answer,
        "eval_llm_input_prompt_char_count": len(prompt_to_gemini),
        "eval_llm_model": gemini_resource.model_name, # <--- 修改：从资源获取模型名称
        "eval_llm_raw_output": raw_gemini_output,
        "eval_llm_processed_output_json": evaluation_result_json,
        "eval_error_info": error_info,
        "application_version": app_version
    }
    await log_interaction_data(
        eval_log_data,
        is_evaluation_result=True,
        evaluation_name_for_file="answer_gemini_flash"
    )

    if evaluation_result_json:
        eval_logger.info(f"Answer evaluation completed. Overall score: {evaluation_result_json.get('evaluation_summary', {}).get('overall_answer_quality_score')}")
    else:
        eval_logger.warning("Answer evaluation did not produce a valid JSON result.")
        
    return evaluation_result_json

if __name__ == '__main__':
    import asyncio

    # --- 模拟一个 GeminiAPIResource 实例用于测试 ---
    # 在实际的 Dagster 环境中，这个资源会被正确注入
    # 为了让这里的测试能跑通，我们需要一个模拟的资源或配置
    
    class MockGeminiAPIResource:
        def __init__(self, api_key, model_name, proxy_url=None, temp=0.1, max_tokens=1024):
            self.api_key = api_key
            self.model_name = model_name
            self.proxy_url = proxy_url
            self.default_temperature = temp
            self.default_max_tokens = max_tokens
            self._logger = eval_logger # 使用eval_logger进行模拟
            self._logger.info("MockGeminiAPIResource initialized for testing.")
            if not self.api_key:
                 self._logger.warning("MockGeminiAPIResource: API key is missing!")


        async def call_completion(self, messages: List[Dict[str, str]], **kwargs) -> Optional[str]:
            self._logger.info(f"MockGeminiAPIResource.call_completion called with {len(messages)} messages.")
            # 这是一个非常简化的模拟，实际测试时可能需要更复杂的模拟响应
            # 或者直接依赖环境变量 USE_SIMULATED_GEMINI_..._EVAL 来触发 evaluator 内部的模拟
            if "cypher" in messages[0]["content"].lower():
                return json.dumps({
                    "evaluation_summary": {"overall_quality_score_cypher": 5, "main_strength_cypher": "Mocked Cypher OK", "main_weakness_cypher": "None"},
                    "dimensions": {}, "qwen_error_patterns_identified": [], "suggestion_for_improvement_cypher": "Mocked suggestion"
                })
            else:
                return json.dumps({
                     "evaluation_summary": {"overall_answer_quality_score": 5, "main_strengths_answer": "Mocked Answer OK", "main_weaknesses_answer": "None"},
                     "dimensions": {}, "suggestion_for_answer_improvement": "Mocked suggestion for answer"
                })

    async def test_evaluators_with_mock_resource():
        eval_logger.info("--- Running tests for evaluators with MockGeminiAPIResource ---")
        
        mock_api_key = os.getenv("GEMINI_API_KEY") or "MOCK_API_KEY_IF_NOT_SET"
        mock_resource = MockGeminiAPIResource(
            api_key=mock_api_key,
            model_name="gemini-1.5-flash-latest" # 与资源默认值一致
        )

        # --- Test Cypher Evaluation ---
        eval_logger.info("\n--- Testing Cypher Evaluation with Mock Resource ---")
        cypher_result = await evaluate_cypher_with_gemini(
            gemini_resource=mock_resource,
            user_question="Test Cypher Question?",
            generated_cypher="MATCH (n) RETURN n",
            original_interaction_id="test_cypher_eval_001"
        )
        if cypher_result:
            eval_logger.info(f"Mocked Cypher Eval Result: {json.dumps(cypher_result, indent=2, ensure_ascii=False)}")
        else:
            eval_logger.warning("Mocked Cypher Eval returned None.")

        # --- Test Answer Evaluation ---
        eval_logger.info("\n--- Testing Answer Evaluation with Mock Resource ---")
        answer_result = await evaluate_answer_with_gemini(
            gemini_resource=mock_resource,
            user_question="Test Answer Question?",
            retrieved_contexts="Some sample context.",
            generated_answer="A sample answer based on context.",
            original_interaction_id="test_answer_eval_001"
        )
        if answer_result:
            eval_logger.info(f"Mocked Answer Eval Result: {json.dumps(answer_result, indent=2, ensure_ascii=False)}")
        else:
            eval_logger.warning("Mocked Answer Eval returned None.")

    # 如果希望在直接运行 evaluator.py 时执行测试：
    # asyncio.run(test_evaluators_with_mock_resource())
    
    # 保留您之前的测试代码（如果需要独立运行并实际调用API，但要注意资源注入）
    # os.environ["USE_SIMULATED_GEMINI_ANSWER_EVAL"] = "true"
    # asyncio.run(test_answer_evaluation()) # 您之前的测试函数名
    pass # 通常 evaluator.py 不会直接运行，而是被其他脚本调用
    --- END OF FILE CONTENT ---


Directory: zhz_rag/api/
  File: __init__.py
    --- File is empty ---

  File: main_api.py
    --- START OF FILE CONTENT ---
# zhz_agent/main.py
from fastapi import FastAPI
from contextlib import asynccontextmanager
import uvicorn
import logging
from typing import Type, List, Dict, Any, Optional, ClassVar

# --- [修改] 导入 -> 改为绝对导入 ---
from zhz_rag.utils.db_utils import database, sqlalchemy_engine, Base, get_scheduler
from zhz_rag.api.task_manager_api import router as tasks_router
from zhz_rag.task_management import db_models # 确保 SQLAlchemy 模型被导入

@asynccontextmanager
async def lifespan(app_instance: FastAPI):
    print("--- Main FastAPI 应用启动 (已集成任务管理API 和 APScheduler) ---") # [修改] 更新了描述

    # --- 数据库初始化 ---
    await database.connect()
    print("数据库已连接。")
    try:
        Base.metadata.create_all(bind=sqlalchemy_engine)
        print("数据库表已执行 create_all。")
        from sqlalchemy import inspect
        inspector = inspect(sqlalchemy_engine)
        if inspector.has_table("tasks"):
            print("'tasks' 表已成功创建/存在于数据库中。")
        else:
            print("警告: 'tasks' 表在 create_all 之后仍未找到！这通常意味着模型没有在 create_all 之前被正确导入。")
            print(f"   已知的表: {inspector.get_table_names()}") # 打印所有实际创建的表
            print(f"   Base.metadata.tables: {Base.metadata.tables.keys()}") # 打印 SQLAlchemy 元数据中已注册的表
    except Exception as e:
        print(f"创建或检查数据库表时出错: {e}")
        import traceback
        traceback.print_exc() # 打印详细的异常堆栈


    # --- [修改] APScheduler 初始化 (使用 get_scheduler) ---
    current_scheduler = get_scheduler() # <--- 获取调度器实例
    try:
        logging.getLogger('apscheduler').setLevel(logging.DEBUG) # 设置为 DEBUG 级别

        if not current_scheduler.running: # <--- 只有在未运行时才启动
            current_scheduler.start()
            print("APScheduler 已启动并使用数据库作业存储。")
        else:
            print("APScheduler 已在运行。")
    except Exception as e:
        print(f"APScheduler 启动失败: {e}")

    print("RAG 组件的初始化和管理在 zhz_agent_mcp_server.py。")
    print("任务管理API已在 /tasks 路径下可用。")

    yield # FastAPI 应用在此运行

    print("--- Main FastAPI 应用关闭 ---")
    current_scheduler_on_shutdown = get_scheduler() # <--- 再次获取以确保是同一个实例
    if current_scheduler_on_shutdown and current_scheduler_on_shutdown.running:
        current_scheduler_on_shutdown.shutdown()
        print("APScheduler 已关闭。")
    await database.disconnect()
    print("数据库已断开连接。")
    print("RAG 组件的清理在 zhz_agent_mcp_server.py。")

# --- App 定义 (保持不变) ---
app = FastAPI(
    title="Hybrid RAG Backend with Task Management",
    description="主 FastAPI 应用，负责接收请求、编排 Agent，并提供任务管理API。",
    version="0.2.1",
    lifespan=lifespan
)

app.include_router(tasks_router)

@app.get("/")
async def read_root():
    return {
        "message": "Welcome to the Hybrid RAG Backend Main App.",
        "available_services": {
            "task_management": "/tasks/docs",
            "rag_via_mcpo": "mcpo proxy at port 8006 (see mcpo_servers.json)"
        }
    }

if __name__ == "__main__":
    print("--- 启动 Main FastAPI 服务器 (包含任务管理API) ---")
    uvicorn.run("zhz_agent.main:app", host="0.0.0.0", port=8000, reload=True) # Ensure correct run command
    --- END OF FILE CONTENT ---

  File: rag_mcp_service.py
    --- START OF FILE CONTENT ---
import os
import json
import asyncio
import traceback
from contextlib import asynccontextmanager
from typing import List, Dict, Any, Optional, AsyncIterator
from dataclasses import dataclass, field # 确保导入 field
import time
import logging
import sys
import hashlib # <--- 添加
from datetime import datetime, timezone # <--- 添加
import uuid # <--- 添加


# MCP 框架导入
from mcp.server.fastmcp import FastMCP, Context

# --- 配置 rag_service 的专用日志 ---
_rag_service_py_dir = os.path.dirname(os.path.abspath(__file__))
_rag_service_log_file = os.path.join(_rag_service_py_dir, 'rag_service_debug.log')

rag_logger = logging.getLogger("RagServiceLogger")
rag_logger.setLevel(logging.DEBUG)
rag_logger.propagate = False

if rag_logger.hasHandlers():
    rag_logger.handlers.clear()

try:
    _file_handler = logging.FileHandler(_rag_service_log_file, mode='w')
    _file_handler.setLevel(logging.DEBUG)
    _formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
    _file_handler.setFormatter(_formatter)
    rag_logger.addHandler(_file_handler)
    rag_logger.info("--- RagServiceLogger configured to write to rag_service_debug.log ---")
except Exception as e:
    print(f"CRITICAL: Failed to configure RagServiceLogger: {e}")


# --- 从项目内部导入所有 RAG 模块 ---
from zhz_rag.config.pydantic_models import QueryRequest, HybridRAGResponse, RetrievedDocument
from zhz_rag.llm.sglang_wrapper import (
    generate_answer_from_context,
    generate_expanded_queries,
    generate_cypher_query,
    generate_clarification_question,
    generate_intent_classification,
    NO_ANSWER_PHRASE_ANSWER_CLEAN
)
from zhz_rag.core_rag.retrievers.chromadb_retriever import ChromaDBRetriever
from zhz_rag.core_rag.retrievers.file_bm25_retriever import FileBM25Retriever
from zhz_rag.core_rag.kg_retriever import KGRetriever
from zhz_rag.core_rag.fusion_engine import FusionEngine
from zhz_rag.utils.common_utils import log_interaction_data

from dotenv import load_dotenv

# 加载 .env 文件
# __file__ 是当前 rag_service.py 的路径: /home/zhz/zhz_agent/rag_service.py
# os.path.dirname(os.path.abspath(__file__)) 是 /home/zhz/zhz_agent 目录
# .env 文件与 rag_service.py 在同一个目录下 (zhz_agent 目录)
current_dir = os.path.dirname(os.path.abspath(__file__))
dotenv_path = os.path.join(current_dir, '.env')

if os.path.exists(dotenv_path):
    load_dotenv(dotenv_path=dotenv_path)
    rag_logger.info(f"Loaded .env file from: {dotenv_path}")
else:
    rag_logger.warning(f".env file not found at {dotenv_path}, will rely on environment variables or defaults.")
    # 仍然尝试加载，因为python-dotenv的默认行为是查找当前工作目录和上级目录的.env
    load_dotenv()

# --- 应用上下文 Dataclass ---
@dataclass
class AppContext:
    # vector_retriever: VectorRetriever # 旧的
    chroma_retriever: Optional[ChromaDBRetriever] = None # 新的
    kg_retriever: Optional[KGRetriever] = None
    # bm25_retriever: BM25Retriever # 旧的
    file_bm25_retriever: Optional[FileBM25Retriever] = None # 新的
    fusion_engine: Optional[FusionEngine] = None
    # llm_generator: Optional[Any] = None # LLMGenerator在您的代码中没有被实例化并放入AppContext

# --- MCP 服务器生命周期管理 ---
@asynccontextmanager
async def app_lifespan_for_rag_service(server: FastMCP) -> AsyncIterator[AppContext]:
    rag_logger.info("--- RAG Service (FastMCP): 正在初始化 RAG 组件 (新版) ---")
    
    chroma_retriever_instance: Optional[ChromaDBRetriever] = None
    kg_retriever_instance: Optional[KGRetriever] = None
    file_bm25_retriever_instance: Optional[FileBM25Retriever] = None
    fusion_engine_instance: Optional[FusionEngine] = None

    # 初始化 ChromaDB Retriever
    try:
        # 这些路径和名称应该与Dagster流水线中配置的一致
        # 优先从环境变量读取，如果不存在则使用默认值（如果适用）
        chroma_persist_dir = os.getenv("CHROMA_PERSIST_DIRECTORY", "/home/zhz/dagster_home/chroma_data")
        chroma_collection_name = os.getenv("CHROMA_COLLECTION_NAME", "rag_documents")
        embedding_model_path = os.getenv("EMBEDDING_MODEL_PATH", "/home/zhz/models/bge-small-zh-v1.5")

        if not os.path.isdir(chroma_persist_dir):
                rag_logger.warning(f"ChromaDB persist directory '{chroma_persist_dir}' not found. Retrieval may fail or use an in-memory DB if ChromaDBRetriever handles this.")
        if not os.path.exists(embedding_model_path): # embedding_model_path 应该是目录
            rag_logger.warning(f"Embedding model path '{embedding_model_path}' not found. ChromaDBRetriever initialization might fail.")

        chroma_retriever_instance = ChromaDBRetriever(
            collection_name=chroma_collection_name,
            persist_directory=chroma_persist_dir,
            embedding_model_name_or_path=embedding_model_path
        )
        rag_logger.info("RAG Service: ChromaDBRetriever 初始化成功。")
    except Exception as e:
        rag_logger.error(f"RAG Service: ChromaDBRetriever 初始化失败: {e}", exc_info=True)
        # 不在此处抛出异常，允许服务在部分组件失败时仍能启动（如果设计如此）
    
    # 初始化 File BM25 Retriever
    try:
        bm25_index_dir = os.getenv("BM25_INDEX_DIRECTORY", "/home/zhz/dagster_home/bm25_index_data/")
        if not os.path.isdir(bm25_index_dir):
            rag_logger.warning(f"BM25 index directory '{bm25_index_dir}' not found. FileBM25Retriever initialization might fail.")
            
        file_bm25_retriever_instance = FileBM25Retriever(
            index_directory_path=bm25_index_dir
        )
        rag_logger.info("RAG Service: FileBM25Retriever 初始化成功。")
    except Exception as e:
        rag_logger.error(f"RAG Service: FileBM25Retriever 初始化失败: {e}", exc_info=True)

    # 初始化 KG Retriever
    try:
        # KGRetriever 内部会从环境变量读取NEO4J配置
        # generate_cypher_query 是从 zhz_agent.llm 导入的
        kg_retriever_instance = KGRetriever(llm_cypher_generator_func=generate_cypher_query)
        rag_logger.info("RAG Service: KGRetriever 初始化成功。")
    except Exception as e:
        rag_logger.error(f"RAG Service: KGRetriever 初始化失败: {e}", exc_info=True)
        if kg_retriever_instance and hasattr(kg_retriever_instance, 'close'): # 确保在失败前尝试关闭
            kg_retriever_instance.close()
            
    # 初始化 Fusion Engine
    try:
        fusion_engine_instance = FusionEngine(logger=rag_logger)
        rag_logger.info("RAG Service: FusionEngine 初始化成功。")
    except Exception as e:
        rag_logger.error(f"RAG Service: FusionEngine 初始化失败: {e}", exc_info=True)

    rag_logger.info("--- RAG Service (FastMCP): RAG 组件初始化尝试完成。---")

    ctx = AppContext(
        chroma_retriever=chroma_retriever_instance,
        kg_retriever=kg_retriever_instance,
        file_bm25_retriever=file_bm25_retriever_instance,
        fusion_engine=fusion_engine_instance
    )
    try:
        yield ctx
    finally:
        rag_logger.info("--- RAG Service (FastMCP): 正在清理资源 ---")
        if kg_retriever_instance: # 确保只在成功初始化后才调用close
            kg_retriever_instance.close() 
        rag_logger.info("--- RAG Service (FastMCP): 清理完成 ---")

# --- 初始化 FastMCP 应用 ---
rag_mcp_application = FastMCP(
    name="zhz_agent_rag_service", # 修改了服务名称以区分
    description="Upgraded Hybrid RAG 服务，使用持久化知识库。",
    lifespan=app_lifespan_for_rag_service,
)

@rag_mcp_application.tool()
async def query_rag_v2( # 重命名工具函数以避免与旧的混淆 (如果需要)
    ctx: Context,
    query: str, # 直接使用 query 作为输入，而不是 QueryRequest 对象
    top_k_vector: int = 3,
    top_k_kg: int = 2,
    top_k_bm25: int = 3,
    top_k_final: int = 3 # 最终融合后返回的文档数
) -> str: 
    rag_logger.info(f"\n--- RAG Service (query_rag_v2): 接收到查询: '{query}' ---")
    rag_logger.info(f"      Params: top_k_vector={top_k_vector}, top_k_kg={top_k_kg}, top_k_bm25={top_k_bm25}, top_k_final={top_k_final}")
    start_time_total = time.time()

    app_ctx: AppContext = ctx.request_context.lifespan_context
    response_payload = {} 
    original_query_for_response = query 
    final_json_output = ""
    # --- [新增日志变量] ---
    log_all_raw_retrievals_summary: List[Dict[str, Any]] = []
    log_final_context_docs_summary: List[Dict[str, Any]] = [] # This will store the full doc model dumps
    log_fused_context_text_for_llm_snippet: str = "N/A"
    # --- [确保所有日志变量都有初始值] ---
    log_final_answer_from_llm: str = "N/A"
    log_intent_classification_result: Optional[Dict[str, Any]] = None
    log_expanded_queries: Optional[List[str]] = None
    # --- [结束确保所有日志变量都有初始值] ---

    try:
        # --- 1. LLM 驱动的意图分类和澄清触发 ---
        rag_logger.info(f"--- [TIME] 开始意图分类 at {time.time() - start_time_total:.2f}s ---")
        start_time_intent = time.time()
        intent_classification_result = await generate_intent_classification(query)
        log_intent_classification_result = intent_classification_result # <--- 记录日志
        rag_logger.info(f"--- [TIME] 结束意图分类, 耗时: {time.time() - start_time_intent:.2f}s. Result: {intent_classification_result}")

        if intent_classification_result.get("clarification_needed"):
            uncertainty_reason = intent_classification_result.get("reason", "查询可能存在歧义或信息不足。")
            clarification_question_text = await generate_clarification_question(query, uncertainty_reason)
            response_payload = {
                "status": "clarification_needed",
                "clarification_question": clarification_question_text,
                "original_query": original_query_for_response,
                "debug_info": {"uncertainty_reason": uncertainty_reason, "source": "intent_classification"}
            }
            rag_logger.info(f"--- 需要澄清，返回: {response_payload}")
            # final_json_output will be set before finally block

        else: 
             # --- 启用查询扩展 ---
            rag_logger.info(f"--- 查询清晰，无需澄清。将对原始查询 '{query}' 进行查询扩展 ---")
            start_time_expansion = time.time()
            expanded_queries = await generate_expanded_queries(query) # <--- 取消注释
            log_expanded_queries = expanded_queries # <--- 记录实际的扩展查询
            
            if not expanded_queries or query not in expanded_queries: # 确保原始查询一定在里面
                # 如果 generate_expanded_queries 返回空或不包含原始查询，至少处理原始查询
                if query not in (expanded_queries or []): # 处理 expanded_queries 可能为 None 的情况
                    expanded_queries = [query] + (expanded_queries or [])
                elif not expanded_queries:
                    expanded_queries = [query]

            rag_logger.info(f"--- 扩展后的查询列表 (共 {len(expanded_queries)} 个): {expanded_queries}. 耗时: {time.time() - start_time_expansion:.2f}s ---")
            
            all_raw_retrievals: List[RetrievedDocument] = []
            
            queries_to_process = expanded_queries # <--- 修改：现在处理所有扩展后的查询
            rag_logger.info(f"--- [TIME] 开始并行召回 for {len(queries_to_process)} queries at {time.time() - start_time_total:.2f}s ---")
            start_time_retrieval = time.time()

            for current_query_text in queries_to_process:
                rag_logger.info(f"Processing retrievals for query: '{current_query_text}'")
                
                # 向量检索 (ChromaDB)
                if app_ctx.chroma_retriever:
                    try:
                        chroma_docs_raw = app_ctx.chroma_retriever.retrieve(query_text=current_query_text, n_results=top_k_vector)
                        rag_logger.debug(f"   ChromaDB for '{current_query_text}' raw output: {chroma_docs_raw}") 
                        for doc_raw in chroma_docs_raw:
                            retrieved_doc = RetrievedDocument(
                                source_type="vector_chroma",
                                content=doc_raw.get("text", ""),
                                score=doc_raw.get("score", 0.0),
                                metadata={**doc_raw.get("metadata", {}), "original_query_part": current_query_text}
                            )
                            all_raw_retrievals.append(retrieved_doc)
                            log_all_raw_retrievals_summary.append(retrieved_doc.model_dump()) 
                        rag_logger.info(f"   ChromaDB for '{current_query_text}': found {len(chroma_docs_raw)} docs.")
                    except Exception as e_chroma:
                        rag_logger.error(f"   Error during ChromaDB retrieval for '{current_query_text}': {e_chroma}", exc_info=True)
                
                # 关键词检索 (BM25)
                if app_ctx.file_bm25_retriever:
                    try:
                        bm25_docs_raw = app_ctx.file_bm25_retriever.retrieve(query_text=current_query_text, n_results=top_k_bm25)
                        rag_logger.debug(f"   BM25 for '{current_query_text}' raw output (IDs and scores): {bm25_docs_raw}") 
                        for doc_raw_bm25 in bm25_docs_raw:
                            bm25_chunk_id = doc_raw_bm25.get("id")
                            text_content_for_bm25 = f"[BM25: Text for ID {bm25_chunk_id} pending]"
                            found_in_chroma = False
                            for existing_doc in all_raw_retrievals: 
                                if (existing_doc.metadata and (existing_doc.metadata.get("chunk_id") == bm25_chunk_id or existing_doc.metadata.get("id") == bm25_chunk_id)):
                                    text_content_for_bm25 = existing_doc.content
                                    found_in_chroma = True
                                    break
                            if not found_in_chroma and app_ctx.chroma_retriever and bm25_chunk_id: 
                                try:
                                    specific_chroma_doc = app_ctx.chroma_retriever._collection.get(ids=[bm25_chunk_id], include=["metadatas", "documents"]) # Also fetch documents for content
                                    if specific_chroma_doc:
                                        if specific_chroma_doc.get("documents") and specific_chroma_doc.get("documents")[0]:
                                            text_content_for_bm25 = specific_chroma_doc["documents"][0]
                                        elif specific_chroma_doc.get("metadatas") and specific_chroma_doc.get("metadatas")[0]: # Fallback to chunk_text in metadata
                                            text_content_for_bm25 = specific_chroma_doc["metadatas"][0].get("chunk_text", text_content_for_bm25)

                                except Exception as e_chroma_get:
                                    rag_logger.warning(f"   Failed to get text for BM25 ID {bm25_chunk_id} from Chroma: {e_chroma_get}")
                            
                            retrieved_doc = RetrievedDocument(
                                source_type="keyword_bm25s",
                                content=text_content_for_bm25,
                                score=doc_raw_bm25.get("score", 0.0),
                                metadata={"chunk_id": bm25_chunk_id, "original_query_part": current_query_text}
                            )
                            all_raw_retrievals.append(retrieved_doc)
                            log_all_raw_retrievals_summary.append(retrieved_doc.model_dump()) 
                        rag_logger.info(f"   BM25s for '{current_query_text}': found {len(bm25_docs_raw)} potential docs.")
                    except Exception as e_bm25:
                        rag_logger.error(f"   Error during BM25 retrieval for '{current_query_text}': {e_bm25}", exc_info=True)

                # 知识图谱检索
                if app_ctx.kg_retriever:
                    try:
                        rag_logger.info(f"   Performing KG retrieval for query: '{current_query_text}'")
                        kg_docs = await app_ctx.kg_retriever.retrieve_with_llm_cypher(query=current_query_text, top_k=top_k_kg)
                        rag_logger.debug(f"   KG for '{current_query_text}' raw output: {kg_docs}") 
                        for kg_doc_data in kg_docs: # kg_docs is List[Dict], needs conversion
                            retrieved_doc = RetrievedDocument(**kg_doc_data) # Convert dict to Pydantic model
                            if retrieved_doc.metadata:
                                retrieved_doc.metadata["original_query_part"] = current_query_text
                            else:
                                retrieved_doc.metadata = {"original_query_part": current_query_text}
                            all_raw_retrievals.append(retrieved_doc)
                            log_all_raw_retrievals_summary.append(retrieved_doc.model_dump()) 
                        rag_logger.info(f"   KG Retrieval for '{current_query_text}': found {len(kg_docs)} results.")
                    except Exception as e_kg:
                        rag_logger.error(f"   Error during KG retrieval for '{current_query_text}': {e_kg}", exc_info=True)
            
            rag_logger.info(f"--- [TIME] 结束所有召回, 耗时: {time.time() - start_time_retrieval:.2f}s ---")
            rag_logger.info(f"--- 总计从各路召回（所有查询处理后）的结果数: {len(all_raw_retrievals)} ---")
            if all_raw_retrievals:
                for i_doc, doc_retrieved in enumerate(all_raw_retrievals[:3]): # 日志只打印前3条摘要
                        rag_logger.debug(f"   Raw Doc {i_doc} (Summary): type={doc_retrieved.source_type}, score={doc_retrieved.score}, content='{str(doc_retrieved.content)[:50]}...'")

            if not all_raw_retrievals: 
                response_payload = {
                    "status": "success", 
                    "final_answer": "抱歉，根据您提供的查询，未能从知识库中找到相关信息。",
                    "original_query": original_query_for_response,
                    "retrieved_context_docs": [], 
                    "debug_info": {"message": "No documents retrieved from any source."}
                }
            else:
                rag_logger.info(f"--- [TIME] 开始结果融合与重排序 at {time.time() - start_time_total:.2f}s ---")
                start_time_fusion = time.time()
                final_context_docs: List[RetrievedDocument]
                if not app_ctx.fusion_engine:
                    rag_logger.error("FusionEngine not available! Skipping fusion and reranking.")
                    final_context_docs = sorted(all_raw_retrievals, key=lambda d: d.score if d.score is not None else -float('inf'), reverse=True)[:top_k_final]
                else:
                    final_context_docs = await app_ctx.fusion_engine.fuse_results(
                        all_raw_retrievals, 
                        original_query_for_response,
                        top_n_final=top_k_final
                    ) 
                log_final_context_docs_summary = [doc.model_dump() for doc in final_context_docs] 

                # --- 新增日志，检查 model_dump 的输出 ---
                if log_final_context_docs_summary:
                    rag_logger.info(f"DEBUG_MODEL_DUMP: First item of log_final_context_docs_summary (from model_dump()): {json.dumps(log_final_context_docs_summary[0], ensure_ascii=False, default=str)}")
                # --- 结束新增日志 ---

                rag_logger.info(f"--- [TIME] 结束结果融合与重排序, 耗时: {time.time() - start_time_fusion:.2f}s. Final context docs: {len(final_context_docs)} ---")
                if final_context_docs:
                    for i_fdoc, fdoc_retrieved in enumerate(final_context_docs[:3]): # 日志只打印前3条摘要
                        rag_logger.debug(f"   Fused Doc {i_fdoc} (Summary): type={fdoc_retrieved.source_type}, score={fdoc_retrieved.score}, content='{str(fdoc_retrieved.content)[:50]}...'")
                
                if not final_context_docs: 
                    fused_context_text_for_llm = "未在知识库中找到相关信息。"
                    final_answer_from_llm = "根据现有知识，未能找到您查询的相关信息。"
                    response_payload = {
                        "status": "success",
                        "final_answer": final_answer_from_llm,
                        "original_query": original_query_for_response,
                        "retrieved_context_docs": [], 
                        "debug_info": {"message": "No relevant context found after fusion."}
                    }
                else:
                    context_strings_for_llm = []
                    for doc in final_context_docs:
                        score_str = f"{doc.score:.4f}" if isinstance(doc.score, float) else str(doc.score if doc.score is not None else 'N/A')
                        context_strings_for_llm.append(
                            f"Source Type: {doc.source_type}, Score: {score_str}\nContent: {doc.content}"
                        )
                    fused_context_text_for_llm = "\n\n---\n\n".join(context_strings_for_llm)
                    log_fused_context_text_for_llm_snippet = fused_context_text_for_llm[:500] 

                    rag_logger.info(f"\n--- FUSED CONTEXT for LLM (length: {len(fused_context_text_for_llm)} chars) ---")
                    rag_logger.info(f"{fused_context_text_for_llm[:1000]}...") 
                    rag_logger.info(f"--- END OF FUSED CONTEXT ---\n")

                    rag_logger.info(f"--- [TIME] 开始最终答案生成 at {time.time() - start_time_total:.2f}s ---")
                    start_time_answer_gen = time.time()
                    final_answer_from_llm = await generate_answer_from_context(query, fused_context_text_for_llm)
                    log_final_answer_from_llm = final_answer_from_llm or "N/A" 
                    rag_logger.info(f"--- [TIME] 结束最终答案生成, 耗时: {time.time() - start_time_answer_gen:.2f}s ---")

                    if not final_answer_from_llm or final_answer_from_llm.strip() == NO_ANSWER_PHRASE_ANSWER_CLEAN:
                        final_answer_from_llm = "根据您提供的信息，我暂时无法给出明确的回答。"
                    
                    response_payload = {
                        "status": "success",
                        "final_answer": final_answer_from_llm,
                        "original_query": original_query_for_response,
                        "retrieved_context_docs": [doc.model_dump() for doc in final_context_docs], 
                        "debug_info": {"total_raw_retrievals_count": len(all_raw_retrievals)}
                    }

        final_json_output = json.dumps(response_payload, ensure_ascii=False)
        rag_logger.info(f"--- 'query_rag_v2' 逻辑执行完毕, 总耗时: {time.time() - start_time_total:.2f}s. ---")
        
    except Exception as e_main:
        rag_logger.error(f"RAG Service CRITICAL ERROR in 'query_rag_v2' (main try-except): {type(e_main).__name__} - {str(e_main)}", exc_info=True)
        user_query_for_err_log = original_query_for_response if 'original_query_for_response' in locals() and original_query_for_response else query
        response_payload = {
            "status": "error",
            "error_code": "RAG_SERVICE_INTERNAL_ERROR",
            "error_message": f"RAG服务内部发生未预期错误: {str(e_main)}",
            "original_query": user_query_for_err_log,
            "debug_info": {"exception_type": type(e_main).__name__}
        }
        final_json_output = json.dumps(response_payload, ensure_ascii=False)
    finally: 
        interaction_id_for_log = str(uuid.uuid4())
        current_app_version = "zhz_rag_mcp_service_0.2.1" 

        processed_final_context_docs_for_log = []
        temp_log_final_context_docs = locals().get('log_final_context_docs_summary') # 安全获取

        if temp_log_final_context_docs: # 如果 RAG 流程成功并且 final_context_docs 被处理了
            for doc_dict in temp_log_final_context_docs: # temp_log_final_context_docs 是 model_dump() 后的列表
                cleaned_doc = {}
                for key, value in doc_dict.items():
                    if isinstance(value, float) and (value != value or value == float('inf') or value == float('-inf')): 
                        cleaned_doc[key] = None 
                    else:
                        cleaned_doc[key] = value
                processed_final_context_docs_for_log.append(cleaned_doc)
        # 如果 temp_log_final_context_docs 为 None (例如澄清路径)，则 processed_final_context_docs_for_log 保持为 []

        full_log_entry = {
            "timestamp_utc": datetime.now(timezone.utc).isoformat(),
            "interaction_id": interaction_id_for_log,
            "original_query_interaction_id_ref": locals().get('original_query_interaction_id'), 
            "task_type": "rag_query_processing_full_log",
            "app_version": current_app_version,
            "original_user_query": locals().get('original_query_for_response', query), # query 总是有定义的
            "query_params": {
                "top_k_vector": top_k_vector, "top_k_kg": top_k_kg, 
                "top_k_bm25": top_k_bm25, "top_k_final": top_k_final
            },
            "intent_classification_result": locals().get('log_intent_classification_result'),
            "expanded_queries": locals().get('log_expanded_queries', []), # 默认为空列表
            "all_raw_retrievals_count": len(locals().get('log_all_raw_retrievals_summary', [])),
            "final_context_docs_count": len(processed_final_context_docs_for_log), # 使用清理后列表的长度
            "final_context_docs_summary": [ 
                {
                    "source_type": doc.get("source_type"), 
                    "score": doc.get("score"), 
                    "id": (doc.get("metadata",{}).get("chunk_id") or doc.get("metadata",{}).get("id")) if doc.get("metadata") else None, 
                    "content_preview": str(doc.get("content","N/A"))[:50]+"..."
                } 
                for doc in processed_final_context_docs_for_log[:5] # 使用清理后列表的摘要
            ], 
            "final_context_docs_full": processed_final_context_docs_for_log, # <--- 使用清理后的完整列表
            "fused_context_text_for_llm_snippet": locals().get('log_fused_context_text_for_llm_snippet', "N/A"),
            "final_answer_from_llm": locals().get('log_final_answer_from_llm', "N/A"),
            "final_response_payload_status": locals().get('response_payload', {}).get("status", "Unknown"),
            "total_processing_time_seconds": round(time.time() - start_time_total, 2) if 'start_time_total' in locals() else -1,
        }

        try:
            await log_interaction_data(full_log_entry) 
            rag_logger.info(f"Full RAG interaction log (ID: {interaction_id_for_log}) has been written.")
        except Exception as e_log_final:
            rag_logger.error(f"CRITICAL: Failed to write full RAG interaction log: {e_log_final}", exc_info=True)
        
        sys.stdout.flush(); sys.stderr.flush() 
    
    return final_json_output

if __name__ == "__main__":
    rag_logger.info("--- Starting RAG Service (FastMCP for mcpo via direct run) ---")
    rag_mcp_application.run()
    --- END OF FILE CONTENT ---

  File (skipped, extension not included): rag_service_debug.log
  File: task_manager_api.py
    --- START OF FILE CONTENT ---
# zhz_agent/task_manager_service.py
from fastapi import APIRouter, HTTPException, Depends, Body, Query, Path, status
from typing import List, Optional, Any, cast
from datetime import datetime, timedelta
import uuid
import traceback # 导入 traceback
import pytz

# --- [修改] 从 pydantic_models 导入我们定义的模型 -> 改为绝对导入 ---
from zhz_rag.config.pydantic_models import TaskModel, CreateTaskRequest, UpdateTaskRequest, TaskStatus, ReminderMethod

# --- [修改] 从 database_models 导入 SQLAlchemy 表模型 -> 改为绝对导入 ---
from zhz_rag.task_management.db_models import TaskDB

# --- [修改] 从新的 database.py 导入 database 对象 和 get_scheduler -> 改为绝对导入 ---
from zhz_rag.utils.db_utils import database, get_scheduler # 将 db_utils 修改为 database

# --- [修改] 从 .task_jobs 导入作业函数 -> 改为绝对导入 ---
from zhz_rag.task_management.jobs import send_task_reminder, execute_task_action
from apscheduler.triggers.date import DateTrigger # 用于指定精确的运行时间
from apscheduler.jobstores.base import JobLookupError # <--- [修改] 导入 JobLookupError 的正确路径

# APIRouter 实例
router = APIRouter(
    prefix="/tasks",
    tags=["Task Management"],
    responses={404: {"description": "Not found"}},
)

def _ensure_utc(dt: datetime) -> datetime:
    """确保 datetime 对象是 UTC 时区感知的。"""
    if dt.tzinfo is None:
        return pytz.utc.localize(dt) # 如果是朴素时间，假定它是UTC并设为UTC
    return dt.astimezone(pytz.utc) # 如果是其他时区，转换为UTC

def _schedule_task_jobs(task: TaskModel):
    current_scheduler = get_scheduler() # 获取 scheduler 实例
    print(f"DEBUG SCHEDULER: _schedule_task_jobs called. Scheduler instance: {current_scheduler}, Is running: {current_scheduler.running if current_scheduler else 'N/A'}")
    if not current_scheduler or not current_scheduler.running:
        print("SCHEDULER_ERROR: APScheduler 未运行，无法调度作业。")
        return

    # 提醒作业
    if task.reminder_time and task.status == TaskStatus.PENDING:
        reminder_job_id = f"reminder_{task.id}"
        try:
            reminder_methods_list = task.reminder_methods
            reminder_utc = _ensure_utc(task.reminder_time)
            print(f"SCHEDULER DEBUG: Passing reminder_methods to job: {reminder_methods_list}") # 添加日志

            current_scheduler.add_job(
                send_task_reminder,
                trigger=DateTrigger(run_date=reminder_utc),
                args=[task.id, task.title, reminder_methods_list], # <--- [修复] 直接传递列表
                id=reminder_job_id,
                name=f"Reminder for task {task.title[:20]}",
                replace_existing=True
            )
            print(f"SCHEDULER: 已为任务 '{task.id}' 添加/更新提醒作业，运行于 {task.reminder_time}")
        except Exception as e:
            print(f"SCHEDULER_ERROR: 添加提醒作业失败 for task '{task.id}': {e}")
            traceback.print_exc() # 打印详细错误堆栈

    # 执行作业
    if task.due_date and task.status == TaskStatus.PENDING:
        execution_job_id = f"execution_{task.id}"
        try:
            due_utc = _ensure_utc(task.due_date) # <--- [新增] 确保时间是 UTC 感知的
            print(f"SCHEDULER DEBUG: Adding execution job at {due_utc} ({due_utc.tzinfo})") # <--- [新增] 添加时区日志
            current_scheduler.add_job(
                execute_task_action,
                trigger=DateTrigger(run_date=due_utc),
                args=[task.id, task.action_type, task.action_payload],
                id=execution_job_id,
                name=f"Execution for task {task.title[:20]}",
                replace_existing=True
            )
            print(f"SCHEDULER: 已为任务 '{task.id}' 添加/更新执行作业，运行于 {task.due_date}")
        except Exception as e:
            print(f"SCHEDULER_ERROR: 添加执行作业失败 for task '{task.id}': {e}")

def _cancel_task_jobs(task_id: str):
    """从 APScheduler 取消作业"""
    current_scheduler = get_scheduler()
    if not current_scheduler or not current_scheduler.running:
        print("SCHEDULER_ERROR: APScheduler 未运行，无法取消作业。")
        return

    reminder_job_id = f"reminder_{task_id}"
    execution_job_id = f"execution_{task_id}"

    try:
        current_scheduler.remove_job(reminder_job_id)
        print(f"SCHEDULER: 已移除提醒作业 for task '{task_id}'")
    except JobLookupError:
        print(f"SCHEDULER_INFO: 提醒作业 '{reminder_job_id}' 未找到，无需移除。")
    except Exception as e:
        print(f"SCHEDULER_ERROR: 移除提醒作业失败 for task '{task_id}': {e}")

    try:
        current_scheduler.remove_job(execution_job_id)
        print(f"SCHEDULER: 已移除执行作业 for task '{task_id}'")
    except JobLookupError:
        print(f"SCHEDULER_INFO: 执行作业 '{execution_job_id}' 未找到，无需移除。")
    except Exception as e:
        print(f"SCHEDULER_ERROR: 移除执行作业失败 for task '{task_id}': {e}")

@router.post("/", response_model=TaskModel, status_code=status.HTTP_201_CREATED)
async def create_task(task_request: CreateTaskRequest = Body(...)):
    """
    创建一个新任务。
    """
    now = datetime.utcnow()
    task_id = str(uuid.uuid4())

    reminder_time_val = None
    if task_request.due_date and task_request.reminder_offset_minutes is not None:
        reminder_time_val = task_request.due_date - timedelta(minutes=task_request.reminder_offset_minutes)

    reminder_methods_values = [
        method.value if hasattr(method, 'value') else str(method)
        for method in (task_request.reminder_methods or [ReminderMethod.NOTIFICATION])
    ]

    insert_query = TaskDB.__table__.insert().values(
        id=task_id,
        title=task_request.title,
        description=task_request.description,
        status=TaskStatus.PENDING,
        created_at=now,
        updated_at=now,
        due_date=task_request.due_date,
        reminder_time=reminder_time_val,
        reminder_offset_minutes=task_request.reminder_offset_minutes,
        reminder_methods=reminder_methods_values, # <--- 确保存入的是字符串列表
        priority=task_request.priority or 0,
        tags=task_request.tags or [],
        action_type=task_request.action_type,
        action_payload=task_request.action_payload or {}
    )

    try:
        await database.execute(insert_query)
    except Exception as e:
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Failed to create task in database: {e}")

    created_task_db = await database.fetch_one(TaskDB.__table__.select().where(TaskDB.id == task_id))
    if not created_task_db:
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to retrieve task after creation")

    response_task = TaskModel.model_validate(dict(created_task_db))

    response_task.reminder_methods = [
        m.value if hasattr(m, 'value') else str(m)
        for m in response_task.reminder_methods
    ]

    _schedule_task_jobs(response_task)
    print(f"TASK_MANAGER: Created task '{response_task.id}' with title '{response_task.title}' in DB")
    return response_task

@router.get("/", response_model=List[TaskModel])
async def list_tasks(
    status_filter: Optional[TaskStatus] = Query(None, alias="status"),
    priority_filter: Optional[int] = Query(None, alias="priority"),
    skip: int = Query(0, ge=0),
    limit: int = Query(10, ge=1, le=100)
):
    """
    获取任务列表，支持过滤和分页。
    """
    query = TaskDB.__table__.select()
    if status_filter:
        query = query.where(TaskDB.status == status_filter)
    if priority_filter is not None:
        query = query.where(TaskDB.priority == priority_filter)

    query = query.order_by(TaskDB.created_at.desc()).offset(skip).limit(limit)

    db_tasks = await database.fetch_all(query)
    return [TaskModel.model_validate(dict(task)) for task in db_tasks]

@router.get("/{task_id}", response_model=TaskModel)
async def get_task(task_id: str = Path(..., description="要获取的任务ID")):
    """
    根据ID获取单个任务的详细信息。
    """
    query = TaskDB.__table__.select().where(TaskDB.id == task_id)
    db_task = await database.fetch_one(query)
    if not db_task:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Task not found")
    return TaskModel.model_validate(dict(db_task))

@router.put("/{task_id}", response_model=TaskModel)
async def update_task(
    task_id: str = Path(..., description="要更新的任务ID"),
    task_update: UpdateTaskRequest = Body(...)
):
    """
    更新现有任务。
    """
    existing_task_query = TaskDB.__table__.select().where(TaskDB.id == task_id)
    db_task = await database.fetch_one(existing_task_query)
    if not db_task:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Task not found")

    update_data = task_update.model_dump(exclude_unset=True)
    update_data["updated_at"] = datetime.utcnow()

    if "reminder_methods" in update_data and update_data["reminder_methods"] is not None:
        update_data["reminder_methods"] = [
            method.value if hasattr(method, 'value') else str(method)
            for method in update_data["reminder_methods"]
        ]

    current_due_date = update_data.get("due_date", cast(Optional[datetime], db_task.due_date))
    current_offset = update_data.get("reminder_offset_minutes", cast(Optional[int], db_task.reminder_offset_minutes))

    if current_due_date and current_offset is not None:
        update_data["reminder_time"] = current_due_date - timedelta(minutes=current_offset)
    elif "due_date" in update_data and current_offset is None:
         update_data["reminder_time"] = None


    update_query = TaskDB.__table__.update().where(TaskDB.id == task_id).values(**update_data)
    await database.execute(update_query)

    updated_db_task = await database.fetch_one(existing_task_query)
    if not updated_db_task:
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to retrieve task after update")

    response_task = TaskModel.model_validate(dict(updated_db_task))

    response_task.reminder_methods = [
        m.value if hasattr(m, 'value') else str(m)
        for m in response_task.reminder_methods
    ]

    _cancel_task_jobs(task_id)
    _schedule_task_jobs(response_task)
    print(f"TASK_MANAGER: Updated task '{response_task.id}' in DB")
    return response_task

@router.delete("/{task_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_task(task_id: str = Path(..., description="要删除的任务ID")):
    """
    删除一个任务。
    """
    existing_task_query = TaskDB.__table__.select().where(TaskDB.id == task_id)
    db_task = await database.fetch_one(existing_task_query)
    if not db_task:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Task not found")

    delete_query = TaskDB.__table__.delete().where(TaskDB.id == task_id)
    await database.execute(delete_query)

    _cancel_task_jobs(task_id)
    print(f"TASK_MANAGER: Deleted task '{task_id}' from DB")
    return None

@router.post("/{task_id}/complete", response_model=TaskModel)
async def mark_task_as_complete(task_id: str = Path(..., description="要标记为完成的任务ID")):
    """
    将任务标记为已完成。
    """
    existing_task_query = TaskDB.__table__.select().where(TaskDB.id == task_id)
    db_task_row = await database.fetch_one(existing_task_query)
    if not db_task_row:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Task not found")

    db_task = TaskModel.model_validate(dict(db_task_row))
    if db_task.status == TaskStatus.COMPLETED:
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="Task is already completed")

    update_data = {
        "status": TaskStatus.COMPLETED,
        "updated_at": datetime.utcnow()
    }
    update_query = TaskDB.__table__.update().where(TaskDB.id == task_id).values(**update_data)
    await database.execute(update_query)

    completed_db_task = await database.fetch_one(existing_task_query)
    if not completed_db_task:
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to retrieve task after marking complete")

    response_task = TaskModel.model_validate(dict(completed_db_task))
    _cancel_task_jobs(task_id)
    print(f"TASK_MANAGER: Marked task '{response_task.id}' as completed in DB")
    return response_task
    --- END OF FILE CONTENT ---


Directory: data/
  File: sample_documents.json
    --- START OF FILE CONTENT ---
[
  {
    "id": "doc_sales_q1",
    "content": "2024年第一季度销售报告显示，华东区域销售额增长最快，达到46.5万元，主要得益于新产品A的推出。华北区域总销售额为36.5万元，但利润率有所下降。",
    "metadata": {"source": "Q1_Sales_Report.xlsx", "date": "2024-04-15", "type": "sales_report"}
  },
  {
    "id": "doc_hr_policy",
    "content": "公司最新人力资源政策规定，员工每年享有15天带薪年假，病假需提供医生证明。弹性工作制适用于部分岗位，需部门经理批准。",
    "metadata": {"source": "HR_Policy_2024.pdf", "date": "2024-01-01", "type": "HR_policy"}
  },
  {
    "id": "doc_it_security",
    "content": "IT安全指南强调，所有员工必须定期更换密码，并启用双因素认证。任何可疑邮件应立即报告给IT部门。",
    "metadata": {"source": "IT_Security_Guide.docx", "date": "2023-11-20", "type": "IT_guideline"}
  },
  {
    "id": "doc_meeting_notes",
    "content": "周例会纪要：讨论了项目X的进度，张三负责市场调研，李四负责技术开发。王五提出了关于预算优化的建议。下周将进行客户演示。",
    "metadata": {"source": "Weekly_Meeting_2024-05-10.txt", "date": "2024-05-10", "type": "meeting_notes"}
  },
  {
    "id": "doc_product_launch",
    "content": "新产品A的发布取得了巨大成功，市场反馈积极。预计将在下半年推出产品B，进一步扩大市场份额。",
    "metadata": {"source": "Product_Launch_Review.pptx", "date": "2024-05-01", "type": "product_review"}
  }
]
    --- END OF FILE CONTENT ---

  File: sample_kg.json
    --- START OF FILE CONTENT ---
{
  "schema_description": "这是一个关于公司内部实体和关系的简化知识图谱。\n节点类型包括：\n- Person (人): 属性例如 name (姓名, String类型, 唯一)。\n- Product (产品): 属性例如 name (产品名称, String类型, 唯一)。\n- Region (区域): 属性例如 name (区域名称, String类型, 唯一, 如 '华东', '华北')。\n- Project (项目): 属性例如 name (项目名称, String类型, 唯一)。\n- Document (文档): 属性例如 id (文档ID, String类型, 唯一), content (内容摘要, String类型)。\n- SalesAmount (销售额记录): 属性例如 numeric_amount (数字金额, Float类型), unit (单位, String类型, 如 '万元'), period (时期, String类型, 如 '2024年第一季度')。\n\n关系类型包括：\n- WORKS_ON (人参与项目): (Person)-[:WORKS_ON]->(Project)。\n- AUTHORED_BY (文档由人撰写): (Document)-[:AUTHORED_BY]->(Person)。\n- LOCATED_IN (实体位于区域): 例如 (Person)-[:LOCATED_IN]->(Region)。\n- HAS_SALES_PRODUCT (区域销售了产品): (Region)-[:HAS_SALES_PRODUCT]->(Product)。\n- HAS_SALES_AMOUNT (区域有销售额记录): (Region)-[:HAS_SALES_AMOUNT]->(SalesAmount)。\n- RELATED_TO (实体间相关): 例如 (Product)-[:RELATED_TO]->(Document)。",
  "data_summary": "知识图谱中包含以下关键事实：\n- 人物：张三、李四、王五。\n- 产品：新产品A、产品B。\n- 区域：华东、华北。\n- 项目：项目X。\n- 关系：张三参与项目X，李四参与项目X。华东区域有新产品A的销售额。新产品A的发布文档是doc_product_launch。华东区域2024年第一季度销售额为46.5万元，华北区域2024年第一季度销售额为36.5万元。",
  "facts": [
    {"subject": "张三", "relation": "WORKS_ON", "object": "项目X", "type": "person_project"},
    {"subject": "李四", "relation": "WORKS_ON", "object": "项目X", "type": "person_project"},
    {"subject": "王五", "relation": "PROPOSED_IDEA", "object": "预算优化", "type": "person_idea"}, 
    {"subject": "华东", "relation": "HAS_SALES_PRODUCT", "object": "新产品A", "type": "region_product"},
    {"subject": "新产品A", "relation": "RELATED_TO", "object": "doc_product_launch", "type": "product_document"},
    {"subject": "华东", "relation": "HAS_SALES_AMOUNT", "object": "46.5万元", "type": "region_sales_amount", "period": "2024年第一季度"},
    {"subject": "华北", "relation": "HAS_SALES_AMOUNT", "object": "36.5万元", "type": "region_sales_amount", "period": "2024年第一季度"}
  ]
}
    --- END OF FILE CONTENT ---

  File (skipped, extension not included): test.xlsx

Directory: data/raw_documents/
  File: doc1.txt
    --- START OF FILE CONTENT ---
项目Alpha的文档编写任务分配给了张三。张三在谷歌工作。
    --- END OF FILE CONTENT ---

  File: doc2.txt
    --- START OF FILE CONTENT ---
第二个文档讨论了自然语言处理技术。

    --- END OF FILE CONTENT ---


Directory: scripts/
  File: __init__.py
    --- File is empty ---

  File: test_kg_retriever_directly.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/scripts/manual_tests/test_kg_retriever_directly.py
import asyncio
import os
import sys
import logging

# --- 配置项目根目录到 sys.path ---
# 这使得我们可以像在项目根目录运行一样导入模块
# 例如 from zhz_rag.core_rag.kg_retriever import KGRetriever
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)
# --- 结束 sys.path 配置 ---

try:
    from zhz_rag.core_rag.kg_retriever import KGRetriever
    from zhz_rag.llm.sglang_wrapper import generate_cypher_query # KGRetriever 依赖它
    from zhz_rag.config.constants import NEW_KG_SCHEMA_DESCRIPTION # generate_cypher_query 依赖它
except ImportError as e:
    print(f"Error importing necessary modules: {e}")
    print("Please ensure you are running this script from a context where 'zhz_rag' package is discoverable,")
    print(f"or that PYTHONPATH is set correctly. Current sys.path: {sys.path}")
    sys.exit(1)

# --- 日志配置 (与 KGRetriever 内部的日志级别和格式一致或更详细) ---
# KGRetriever 和 sglang_wrapper 内部都有自己的 logger
# 为了看到它们的输出，我们可以配置根 logger，或者确保它们的 logger 能输出到控制台
logging.basicConfig(
    level=logging.DEBUG, # 设置为 DEBUG 可以看到更多信息
    format='%(asctime)s - %(name)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)] # 输出到标准输出
)
logger = logging.getLogger(__name__)

async def main():
    logger.info("--- Starting KGRetriever Direct Test ---")

    # 确保 local_llm_service.py 正在运行 (端口 8088)
    # 确保 KuzuDB 数据库路径正确且包含数据

    # 从环境变量获取 KuzuDB 路径，如果 KGRetriever 内部也这样做的话
    # KGRetriever 默认使用 KUZU_DB_PATH_ENV，其默认值是 "/home/zhz/zhz_agent/zhz_rag/stored_data/kuzu_default_db"
    # 如果您的 .env 文件或环境变量中设置了不同的 KUZU_DB_PATH，请确保这里也一致，或者让 KGRetriever 使用其默认值。
    kuzu_db_path_for_test = os.getenv("KUZU_DB_PATH", "/home/zhz/zhz_agent/zhz_rag/stored_data/kuzu_default_db")
    logger.info(f"Using KuzuDB path for test: {kuzu_db_path_for_test}")


    # 实例化 KGRetriever
    # 它会使用 sglang_wrapper.generate_cypher_query 作为默认的 Cypher 生成函数
    # generate_cypher_query 会调用 local_llm_service.py
    try:
        kg_retriever = KGRetriever(db_path=kuzu_db_path_for_test)
        logger.info("KGRetriever instance created.")
    except Exception as e_init:
        logger.error(f"Failed to initialize KGRetriever: {e_init}", exc_info=True)
        return

    test_queries = [
        {"query": "张三在哪里工作？", "top_k": 2},
        {"query": "项目Alpha的文档编写任务分配给了谁？", "top_k": 2},
        {"query": "法国的首都是哪里？", "top_k": 2} # 测试无法生成 Cypher 的情况
    ]

    for test_case in test_queries:
        user_query = test_case["query"]
        top_k_results = test_case["top_k"]
        logger.info(f"\n--- Testing query: '{user_query}' with top_k={top_k_results} ---")
        
        try:
            retrieved_kg_docs = await kg_retriever.retrieve_with_llm_cypher(
                query=user_query,
                top_k=top_k_results
            )

            if retrieved_kg_docs:
                logger.info(f"Retrieved {len(retrieved_kg_docs)} documents from KG for query '{user_query}':")
                for i, doc in enumerate(retrieved_kg_docs):
                    logger.info(f"  Doc {i+1}:")
                    logger.info(f"    Source Type: {doc.get('source_type')}")
                    logger.info(f"    Content: {doc.get('content')}")
                    logger.info(f"    Score: {doc.get('score')}")
                    logger.info(f"    Metadata: {doc.get('metadata')}")
            else:
                logger.info(f"No documents retrieved from KG for query '{user_query}'. This might be expected if Cypher was '无法生成'.")

        except Exception as e_retrieve:
            logger.error(f"Error during KG retrieval for query '{user_query}': {e_retrieve}", exc_info=True)
        
        logger.info("--- End of test case ---")

    # 关闭 KGRetriever (如果它有 close 方法且需要显式关闭)
    # KGRetriever 的 close 方法会删除 _db 对象，依赖其 __del__
    if hasattr(kg_retriever, 'close'):
        kg_retriever.close()
        logger.info("KGRetriever closed.")

if __name__ == "__main__":
    # 确保 local_llm_service.py 在端口 8088 上运行
    # 确保 KuzuDB 数据库路径正确且包含数据
    if not os.getenv("SGLANG_API_URL"): # sglang_wrapper.py 会使用这个环境变量
        os.environ["SGLANG_API_URL"] = "http://localhost:8088/v1/chat/completions" # 指向我们的本地服务
        logger.info(f"SGLANG_API_URL not set, defaulting to: {os.environ['SGLANG_API_URL']}")

    asyncio.run(main())
    logger.info("--- KGRetriever Direct Test Finished ---")
    --- END OF FILE CONTENT ---


Directory: scripts/manual_tests/
  File: __init__.py
    --- File is empty ---

  File: test_kuzu.py
    --- START OF FILE CONTENT ---
# test_kuzu.py
import kuzu
import os
import pandas as pd
import re # <--- 添加导入
import unicodedata # <--- 添加导入

def normalize_text_for_id(text: str) -> str:
    if not isinstance(text, str):
        return str(text)
    try:
        normalized_text = unicodedata.normalize('NFKD', text)
        normalized_text = normalized_text.lower()
        normalized_text = normalized_text.strip()
        normalized_text = re.sub(r'\s+', ' ', normalized_text)
        return normalized_text
    except Exception as e:
        return text

DB_PATH = "/home/zhz/zhz_agent/zhz_rag/stored_data/kuzu_default_db"

def run_queries(conn: kuzu.Connection):
    print("\n--- Querying Data ---")

    # 规范化查询条件中的文本
    task_text_orig = "项目Alpha的文档编写任务"
    person_text_orig = "张三"
    org_text_orig = "谷歌"

    task_text_norm = normalize_text_for_id(task_text_orig)
    person_text_norm = normalize_text_for_id(person_text_orig)
    org_text_norm = normalize_text_for_id(org_text_orig)

    print(f"Normalized for query: '{task_text_orig}' -> '{task_text_norm}'")
    print(f"Normalized for query: '{person_text_orig}' -> '{person_text_norm}'")
    print(f"Normalized for query: '{org_text_orig}' -> '{org_text_norm}'")


    queries_to_run = {
        "Total ExtractedEntity Nodes": "MATCH (n:ExtractedEntity) RETURN count(n) AS total_entities;",
        "All ExtractedEntity Nodes (Limit 5)": "MATCH (n:ExtractedEntity) RETURN n.id_prop, n.text, n.label LIMIT 5;",
        # 使用规范化后的文本进行查询
        "Specific Entity (张三 - normalized)": f"MATCH (n:ExtractedEntity {{text: '{person_text_norm}', label: 'PERSON'}}) RETURN n.id_prop, n.text, n.label;",
        "Specific Entity (项目Alpha... - normalized)": f"MATCH (n:ExtractedEntity {{text: '{task_text_norm}', label: 'TASK'}}) RETURN n.id_prop, n.text, n.label;",
        "Specific Entity (谷歌 - normalized)": f"MATCH (n:ExtractedEntity {{text: '{org_text_norm}', label: 'ORGANIZATION'}}) RETURN n.id_prop, n.text, n.label;",
        
        "Total WorksAt Relationships": "MATCH ()-[r:WorksAt]->() RETURN count(r) AS total_works_at_rels;",
        "Total AssignedTo Relationships": "MATCH ()-[r:AssignedTo]->() RETURN count(r) AS total_assigned_to_rels;",
        
        # 关系查询也使用规范化文本（如果条件中包含文本）
        "Who works at 谷歌? (normalized)": f"MATCH (p:ExtractedEntity {{label: 'PERSON'}})-[r:WorksAt]->(o:ExtractedEntity {{text: '{org_text_norm}', label: 'ORGANIZATION'}}) RETURN p.text AS person_name;",
        "Where does 张三 work? (normalized)": f"MATCH (p:ExtractedEntity {{text: '{person_text_norm}', label: 'PERSON'}})-[r:WorksAt]->(o:ExtractedEntity {{label: 'ORGANIZATION'}}) RETURN o.text AS organization_name;",
        "What task is assigned to 张三? (normalized)": f"MATCH (t:ExtractedEntity {{label: 'TASK'}})-[r:AssignedTo]->(p:ExtractedEntity {{text: '{person_text_norm}', label: 'PERSON'}}) RETURN t.text AS task_name;",
        "Who is the task '项目Alpha...' assigned to? (normalized)": f"MATCH (t:ExtractedEntity {{text: '{task_text_norm}', label: 'TASK'}})-[r:AssignedTo]->(p:ExtractedEntity {{label: 'PERSON'}}) RETURN p.text AS person_name;",
        
        "All WorksAt Relationships (Source and Target Text)": "MATCH (src:ExtractedEntity)-[r:WorksAt]->(tgt:ExtractedEntity) RETURN src.text AS source_text, src.label AS source_label, tgt.text AS target_text, tgt.label AS target_label;",
        "All AssignedTo Relationships (Source and Target Text)": "MATCH (src:ExtractedEntity)-[r:AssignedTo]->(tgt:ExtractedEntity) RETURN src.text AS source_text, src.label AS source_label, tgt.text AS target_text, tgt.label AS target_label;",
    }

    for description, query_str in queries_to_run.items():
        print(f"\nExecuting Query: {description}")
        print(f"Cypher: {query_str}")
        try:
            results = conn.execute(query_str)
            # 使用 pandas 显示结果更美观
            df = pd.DataFrame(results.get_as_df())
            if not df.empty:
                print(df.to_string())
            else:
                print("  Query returned no results.")
            results.close() # 记得关闭结果集
        except Exception as e:
            print(f"  Error executing query: {e}")

def main():
    print(f"Kuzu Python client version: {kuzu.__version__}")
    print(f"Attempting to connect to KuzuDB at: {DB_PATH}")

    if not os.path.exists(DB_PATH):
        print(f"ERROR: Database directory not found at {DB_PATH}")
        print("Please ensure the Dagster KuzuDB pipeline has run successfully and created the database.")
        return

    db = None
    conn = None
    try:
        # --- 连接到已存在的数据库 (只读模式足够用于查询) ---
        # 如果需要执行写操作（例如，在测试中临时修改），可以使用 read_only=False
        db = kuzu.Database(DB_PATH, read_only=True)
        conn = kuzu.Connection(db)
        print(f"Successfully connected to KuzuDB. Database path: {os.path.abspath(DB_PATH)}")

        # --- 列出所有表 (使用我们从研究报告中知道的方法) ---
        print("\n--- Listing Tables ---")
        try:
            node_tables = conn._get_node_table_names()
            print(f"Node Tables: {node_tables}")
            rel_tables_info = conn._get_rel_table_names()
            rel_tables = [info['name'] for info in rel_tables_info]
            print(f"Rel Tables: {rel_tables}")
            all_tables = node_tables + rel_tables
            print(f"All Tables: {all_tables}")

            # 验证核心表是否存在
            expected_tables = ["ExtractedEntity", "WorksAt", "AssignedTo"]
            missing = [t for t in expected_tables if t not in all_tables]
            if not missing:
                print("Core tables (ExtractedEntity, WorksAt, AssignedTo) are present.")
            else:
                print(f"WARNING: Missing core tables: {missing}")

        except Exception as e_list_tables:
            print(f"Error listing tables: {e_list_tables}")
            print("This might indicate an issue with the KuzuDB connection or version compatibility for these internal methods.")

        # --- 执行查询 ---
        if conn: # 确保连接有效
            run_queries(conn)

    except Exception as e:
        print(f"An error occurred: {e}")
        import traceback
        traceback.print_exc()
    finally:
        print("\nKuzuDB test script finished.")
        # Kuzu Connection 没有显式的 close() 方法
        # Kuzu Database 对象在其 __del__ 方法中处理关闭
        if db is not None:
            del db # 确保数据库对象被垃圾回收，从而关闭

if __name__ == "__main__":
    main()
    --- END OF FILE CONTENT ---

  File: test_litellm_local.py
    --- START OF FILE CONTENT ---
# test_litellm_local.py
import litellm
import os
import traceback
import json

# 不再依赖 LITELLM_CONFIG_PATH 来测试这个本地模型
# if "LITELLM_CONFIG_PATH" in os.environ:
#     del os.environ["LITELLM_CONFIG_PATH"] # 确保它不干扰

litellm.set_verbose = True 

messages = [
    {"role": "system", "content": "You are a helpful assistant."},
    {"role": "user", "content": "中国的首都是哪里？请用中文回答。并简要介绍一下它。"}
]

custom_llm_model_name_in_litellm = "local/qwen3-1.7b-gguf" 
custom_llm_api_base = "http://localhost:8088/v1"        
custom_llm_api_key = "nokey"                            

print("--- Test Case: Calling local LLM via LiteLLM with direct parameters ---")
try:
    print(f"Attempting to call '{custom_llm_model_name_in_litellm}' via LiteLLM...")
    print(f"  Target API Base: {custom_llm_api_base}")
    print(f"  Using API Key: {custom_llm_api_key}")
    print(f"  Messages: {json.dumps(messages, ensure_ascii=False, indent=2)}")
    
    response = litellm.completion(
        model=custom_llm_model_name_in_litellm,
        messages=messages,
        max_tokens=1024, 
        temperature=0.7,
        custom_llm_provider="openai", 
        api_base=custom_llm_api_base,
        api_key=custom_llm_api_key 
    )
    
    print(f"\n--- Response from '{custom_llm_model_name_in_litellm}' ---")
    if response.choices and response.choices[0].message and response.choices[0].message.content is not None:
        print("\nProcessed Content (from LiteLLM response):")
        print(response.choices[0].message.content)
    else:
        print("\nNo content found in LiteLLM response choices.")
        print("Full Response (if any):")
        print(response)

    print("\nUsage Information (from LiteLLM response):")
    if response.usage:
        print(f"  Prompt Tokens: {response.usage.prompt_tokens}")
        print(f"  Completion Tokens: {response.usage.completion_tokens}")
        print(f"  Total Tokens: {response.usage.total_tokens}")
    else:
        print("  Usage information not available.")

except Exception as e:
    print(f"\n--- ERROR ---")
    print(f"Error calling '{custom_llm_model_name_in_litellm}' via LiteLLM: {e}")
    traceback.print_exc()

print("\n--- test_litellm_local.py finished ---")
    --- END OF FILE CONTENT ---

  File: test_llama_cpp.py
    --- START OF FILE CONTENT ---
from llama_cpp import Llama
import os

MODEL_DIR = "/home/zhz/models/Qwen3-1.7B-GGUF" # 您存放模型的目录
# 列出该目录下所有的 .gguf 文件，让用户选择或自动选择一个
gguf_files = [f for f in os.listdir(MODEL_DIR) if f.endswith(".gguf")]

if not gguf_files:
    print(f"No GGUF models found in {MODEL_DIR}")
    exit()

print("Available GGUF models:")
for i, fname in enumerate(gguf_files):
    print(f"{i+1}. {fname}")

# 为了测试，我们直接使用您下载的 Qwen 模型文件名
# 您可以根据实际文件名修改，或者实现一个选择逻辑
# 假设您的模型文件名中包含 "Qwen1.7B-GGUF" 或类似标识
qwen_model_filename = None
for fname in gguf_files:
    if "Qwen" in fname and "1.7B" in fname and fname.endswith(".gguf"): # 简单匹配
        qwen_model_filename = fname
        break

if not qwen_model_filename:
    print("Could not automatically find a Qwen 1.7B GGUF model. Please specify one.")
    # 或者让用户输入选择：
    # choice = int(input(f"Select a model by number (1-{len(gguf_files)}): ")) - 1
    # qwen_model_filename = gguf_files[choice]
    exit()

MODEL_PATH = os.path.join(MODEL_DIR, qwen_model_filename)
print(f"\nUsing model: {MODEL_PATH}")

# --- LLM 初始化参数 ---
# n_gpu_layers: 设置为 > 0 可以将部分层卸载到 GPU (如果兼容且已安装 GPU 支持的 llama.cpp 版本)
# 设置为 0 则完全使用 CPU。对于 RTX 3060 (12GB)，可以尝试设置一个较大的值，例如 20-30，具体取决于模型大小和VRAM。
# 对于1.7B模型，如果VRAM足够，可以尝试更高的值，甚至 -1 (全部卸载)。
# 我们先从 CPU 开始测试，n_gpu_layers=0
N_GPU_LAYERS = 0
N_CTX = 2048 #模型的上下文窗口大小，Qwen1.7B 可能支持更长的，具体查阅模型卡片
N_BATCH = 512 # 提示处理的批处理大小

try:
    print(f"Initializing Llama model from: {MODEL_PATH}...")
    print(f"Parameters: n_gpu_layers={N_GPU_LAYERS}, n_ctx={N_CTX}, n_batch={N_BATCH}")
    
    llm = Llama(
        model_path=MODEL_PATH,
        n_gpu_layers=N_GPU_LAYERS,
        n_ctx=N_CTX,
        n_batch=N_BATCH,
        verbose=True # 打印更多加载信息
    )
    print("Llama model initialized successfully.")

    # --- 测试 1: 简单文本生成 ---
    prompt_simple = "中国的首都是哪里？请用中文回答。"
    print(f"\n--- Test 1: Simple Text Generation ---")
    print(f"Prompt: {prompt_simple}")
    output_simple = llm(
        prompt_simple,
        max_tokens=50, # 生成的最大 token 数
        stop=["\n", " Human:", " Assistant:"], # 停止词
        echo=False # 不回显 prompt
    )
    print(f"LLM Output: {output_simple['choices'][0]['text'].strip()}")

    # --- 测试 2: Text-to-Cypher (初步) ---
    # 使用简化的 Schema 和问题进行测试
    # 您可以将 NEW_KG_SCHEMA_DESCRIPTION 从 constants.py 复制过来或导入
    # 这里为了独立性，我们先用一个极简的描述
    kg_schema_simple = """
    节点标签: ExtractedEntity (属性: text, label)
    关系类型: WORKS_AT (从 PERSON 到 ORGANIZATION), ASSIGNED_TO (从 TASK 到 PERSON)
    实体标签值: PERSON, ORGANIZATION, TASK
    """
    user_question_cypher = "张三在哪里工作？"
    prompt_cypher = f"""
    你是一个将自然语言问题转换为Cypher查询的助手。
    严格根据以下Schema生成Cypher查询：
    {kg_schema_simple}
    用户问题: {user_question_cypher}
    Cypher查询: MATCH (p:ExtractedEntity {{text: '张三', label: 'PERSON'}})-[:WORKS_AT]->(o:ExtractedEntity {{label: 'ORGANIZATION'}}) RETURN o.text
    用户问题: 项目Alpha分配给了谁？
    Cypher查询: MATCH (t:ExtractedEntity {{text: '项目Alpha', label: 'TASK'}})-[:ASSIGNED_TO]->(p:ExtractedEntity {{label: 'PERSON'}}) RETURN p.text
    用户问题: {user_question_cypher}
    Cypher查询:""" # Few-shot prompting

    print(f"\n--- Test 2: Text-to-Cypher (Simple) ---")
    print(f"User question for Cypher: {user_question_cypher}")
    # print(f"Cypher Prompt (simplified): {prompt_cypher}") # Prompt 可能很长
    
    output_cypher = llm(
        prompt_cypher,
        max_tokens=150,
        stop=["\n", "用户问题:"],
        temperature=0.1, # 对于代码生成，低temperature通常更好
        echo=False
    )
    generated_cypher = output_cypher['choices'][0]['text'].strip()
    print(f"Generated Cypher: {generated_cypher}")

    # --- 测试 3: 上下文问答 (初步) ---
    context_qa = "根据KuzuDB的文档，它是一个嵌入式的图数据库，支持Cypher查询语言。"
    user_question_qa = "KuzuDB支持什么查询语言？"
    prompt_qa = f"""
    根据以下上下文回答问题。
    上下文: {context_qa}
    问题: {user_question_qa}
    答案:"""

    print(f"\n--- Test 3: Contextual QA (Simple) ---")
    print(f"Context: {context_qa}")
    print(f"User question for QA: {user_question_qa}")
    output_qa = llm(
        prompt_qa,
        max_tokens=100,
        stop=["\n", "问题:"],
        temperature=0.2,
        echo=False
    )
    generated_answer_qa = output_qa['choices'][0]['text'].strip()
    print(f"Generated Answer: {generated_answer_qa}")

except Exception as e:
    print(f"An error occurred with Llama.cpp: {e}")
    import traceback
    traceback.print_exc()
finally:
    print("\nLlama.cpp test script finished.")
    if 'llm' in locals() and hasattr(llm, 'close'): # llama_cpp Llama对象没有close方法
         pass # llm对象在Python中通常通过垃圾回收来释放资源
    --- END OF FILE CONTENT ---

  File: test_rag_mcp_service.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/scripts/manual_tests/test_rag_mcp_service.py
import httpx
import json
import asyncio
import os # <--- 添加 os 模块导入
import sys # <--- 添加 sys 模块导入

# --- 配置项目根目录到 sys.path ---
# 这使得我们可以像在项目根目录运行一样导入模块
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)
# --- 结束 sys.path 配置 ---

MCPO_BASE_URL = os.getenv("MCPO_BASE_URL", "http://localhost:8006") # 从环境变量或默认值获取
RAG_SERVICE_PATH = "zhz_rag_mcp_service/query_rag_v2"
TARGET_URL = f"{MCPO_BASE_URL}/{RAG_SERVICE_PATH}"

async def call_rag_service(query: str, top_k_vector: int = 3, top_k_kg: int = 2, top_k_bm25: int = 3, top_k_final: int = 3):
    payload = {
        "query": query,
        "top_k_vector": top_k_vector,
        "top_k_kg": top_k_kg,
        "top_k_bm25": top_k_bm25,
        "top_k_final": top_k_final
    }
    print(f"\n--- Sending request to RAG MCP Service for query: '{query}' ---")
    print(f"URL: {TARGET_URL}")
    print(f"Payload: {json.dumps(payload, ensure_ascii=False, indent=2)}")

    try:
        async with httpx.AsyncClient(timeout=120.0) as client: 
            response = await client.post(TARGET_URL, json=payload)
        
        print(f"\nResponse Status Code: {response.status_code}")
        try:
            response_data = response.json()
            print("Response JSON:")
            print(json.dumps(response_data, ensure_ascii=False, indent=2))
            
            if "final_answer" in response_data:
                print(f"\nFinal Answer: {response_data['final_answer']}")
            if "retrieved_context_docs" in response_data and isinstance(response_data["retrieved_context_docs"], list):
                print(f"Number of retrieved context docs for answer: {len(response_data['retrieved_context_docs'])}")

        except json.JSONDecodeError:
            print("Error: Response is not valid JSON.")
            print(f"Raw Response Text (first 500 chars):\n{response.text[:500]}")
        except Exception as e_resp:
            print(f"Error processing response content: {e_resp}")
            print(f"Raw Response Text (first 500 chars):\n{response.text[:500]}")

    except httpx.RequestError as e_req:
        print(f"HTTP Request Error: {e_req}")
    except Exception as e:
        print(f"An unexpected error occurred: {e}")
    print("--- End of RAG Service request ---")

async def main():
    print("--- Starting RAG MCP Service Test Script ---")
    
    # 确保 mcpo 和依赖服务 (local_llm_service.py) 已启动
    # 确保 .env 文件被加载 (如果脚本依赖环境变量，例如 MCPO_BASE_URL)
    from dotenv import load_dotenv
    dotenv_path_script = os.path.join(PROJECT_ROOT, '.env') # 假设 .env 在项目根目录
    if os.path.exists(dotenv_path_script):
        load_dotenv(dotenv_path=dotenv_path_script)
        print(f"Loaded .env file from: {dotenv_path_script}")
    else:
        print(f".env file not found at {dotenv_path_script}, using defaults or existing env vars.")


    test_queries = [
        "张三在哪里工作？",
        "项目Alpha的文档编写任务分配给了谁？",
        "法国的首都是哪里？"
    ]

    for query in test_queries:
        await call_rag_service(query)
        await asyncio.sleep(1) 

    print("\n--- RAG MCP Service Test Script Finished ---")

if __name__ == "__main__":
    asyncio.run(main())
    --- END OF FILE CONTENT ---


Directory: kuzu_test_db/
  File (skipped, extension not included): .lock
  File (skipped, extension not included): .shadow
  File (skipped, extension not included): .wal
  File (skipped, extension not included): catalog.kz
  File (skipped, extension not included): data.kz
  File (skipped, extension not included): metadata.kz
  File (skipped, extension not included): n-0.hindex
  File (skipped, extension not included): n-0.hindex.ovf

Directory: utils/
  File: __init__.py
    --- File is empty ---

  File: common_utils.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/utils/common_utils.py

import httpx
import json
import traceback
import os
import logging

# --- 日志配置 ---
logger = logging.getLogger(__name__)
# (您可以根据需要添加更详细的日志配置)

# --- MCP 配置 ---
MCPO_BASE_URL = os.getenv("MCPO_BASE_URL", "http://localhost:8006")

async def call_mcpo_tool(tool_name_with_prefix: str, payload: dict) -> dict:
    """
    异步调用MCP工具服务。
    tool_name_with_prefix 例如: "zhz_rag_mcp_service/query_rag_v2" 或 "ddgsearch/search"
    payload 是传递给MCP工具的参数字典。
    返回一个字典，成功时包含工具的输出，失败时包含 "error" 键。
    """
    api_url = f"{MCPO_BASE_URL}/{tool_name_with_prefix}"
    cleaned_payload = {k: v for k, v in (payload or {}).items() if v is not None}

    logger.info(f"Calling MCP endpoint: POST {api_url} with payload: {json.dumps(cleaned_payload, ensure_ascii=False)}")

    async with httpx.AsyncClient(timeout=120.0) as client: # 增加超时
        response = None
        try:
            headers = {"Content-Type": "application/json"}
            response = await client.post(api_url, json=cleaned_payload, headers=headers)
            logger.info(f"MCP status code for {tool_name_with_prefix}: {response.status_code}")

            if response.status_code == 200:
                try:
                    result_data = response.json()
                    # 检查 MCP 服务本身是否返回了错误结构 (例如 MCP 框架的错误包装)
                    if isinstance(result_data, dict) and result_data.get("isError"): # 假设 MCP 错误格式
                        error_text = result_data.get("content", [{"text": "Unknown error from MCP tool"}])[0].get("text")
                        logger.error(f"MCP Tool '{tool_name_with_prefix}' execution failed (isError=true): {error_text}")
                        return {"error": f"Tool '{tool_name_with_prefix}' failed via MCP: {error_text}"}
                    return result_data # 假设成功时直接返回工具的 JSON 输出
                except json.JSONDecodeError:
                    logger.warning(f"MCP call to '{tool_name_with_prefix}' returned status 200 but response is not JSON. Raw text: {response.text[:500]}...")
                    return {"error": "Non-JSON response from MCP tool", "raw_response": response.text}
            else:
                error_text = f"MCP call to '{tool_name_with_prefix}' failed with status {response.status_code}. Response: {response.text[:500]}..."
                logger.error(error_text)
                return {"error": error_text, "status_code": response.status_code}

        except httpx.RequestError as exc:
            error_msg = f"HTTP RequestError calling MCP tool '{tool_name_with_prefix}': {type(exc).__name__} - {exc}"
            logger.error(error_msg, exc_info=True)
            return {"error": error_msg, "exception_type": type(exc).__name__}
        except Exception as exc:
            error_msg = f"Unexpected error calling MCP tool '{tool_name_with_prefix}': {type(exc).__name__} - {exc}"
            response_text_snippet = response.text[:500] if response and hasattr(response, 'text') else "N/A"
            logger.error(f"{error_msg}. Response snippet: {response_text_snippet}", exc_info=True)
            return {"error": error_msg, "exception_type": type(exc).__name__}

# 确保 utils 目录也有一个 __init__.py 文件
# touch /home/zhz/zhz_agent/utils/__init__.py (如果不存在)
    --- END OF FILE CONTENT ---


Directory: stored_data/

Directory: stored_data/rag_interaction_logs/

Directory: local_agent/
  File: local_agent_app.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/local_agent/local_agent_app.py

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from typing import Dict, Any, List, Optional
import pandas as pd # 用于处理 Excel 数据
from pandasql import PandaSQL # 用于在 Pandas DataFrame 上执行 SQL 查询
import os
import logging # 用于日志记录
import traceback # 用于更详细的错误信息

# --- [新增] 导入 winotify 用于发送 Windows 桌面通知 ---
from winotify import Notification #

# --- 配置常量 ---
LOCAL_AGENT_PORT = 8003 # 本地代理服务监听的端口
LOG_LEVEL = logging.INFO # 日志级别

# --- 日志配置 ---
# 配置日志记录器，方便调试和追踪
logging.basicConfig(level=LOG_LEVEL, format='%(asctime)s - %(levelname)s - LocalAgent - %(message)s') #
logger = logging.getLogger(__name__) #

# --- FastAPI 应用实例 ---
app = FastAPI(
    title="ZhzAI Local Agent", #
    description="本地代理程序，提供处理本地文件（如Excel）和执行特定本地任务（如桌面通知）的MCP服务。", #
    version="0.1.1" # 更新版本号以反映添加了通知功能
)

# --- Pydantic 模型定义 ---

# 用于 Excel SQO (Structured Query Object) 操作的请求体
class ExecuteSQORequest(BaseModel): #
    sqo: Dict[str, Any] = Field(description="必需。一个结构化查询对象 (SQO) 的JSON字典，用于描述要对Excel执行的操作。") #

# Excel SQO 操作的通用响应体
class SQOResponse(BaseModel): #
    success: bool # 操作是否成功
    result: Any = None # 成功时的结果数据
    error: Optional[str] = None # 失败时的简要错误信息
    error_details: Optional[str] = None # 失败时的详细技术错误信息或堆栈

# 用于桌面通知的请求体
class NotificationRequest(BaseModel): #
    title: str = Field(default="任务提醒", description="通知的标题") #
    message: str = Field(description="通知的主要内容") #
    app_name: str = Field(default="终端大脑助手", description="显示在通知来源的应用名称") #
    # icon_path: Optional[str] = Field(None, description="通知图标的绝对路径 (可选)") #

# 桌面通知操作的响应体
class NotificationResponse(BaseModel): #
    success: bool # 操作是否成功 #
    message: str # 返回的消息，成功或失败的说明 #

# --- 核心功能函数 ---

def apply_filters_to_dataframe(df: pd.DataFrame, filters: List[Dict[str, Any]]) -> pd.DataFrame: #
    """
    将一系列过滤器应用于 Pandas DataFrame。
    这是 Excel SQO 操作的核心辅助函数，用于在执行聚合、查询等操作前筛选数据。
    """
    if not filters: #
        return df #

    df_filtered = df.copy() #
    for f_obj in filters: #
        column = f_obj.get("column") #
        operator = f_obj.get("operator") #
        value = f_obj.get("value") #
        is_date = f_obj.get("is_date", False) #

        if not all([column, operator, value is not None]): #
            logger.warning(f"Skipping invalid filter object: {f_obj}") #
            continue #

        if column not in df_filtered.columns: #
            logger.warning(f"Filter column '{column}' not found in DataFrame. Available: {df_filtered.columns.tolist()}. Skipping filter: {f_obj}") #
            continue #

        try: #
            series_to_filter = df_filtered[column] # 操作副本以避免 SettingWithCopyWarning #
            
            # 尝试转换日期，如果失败则记录警告并可能跳过
            if is_date: #
                try: #
                    series_to_filter = pd.to_datetime(series_to_filter, errors='coerce') #
                    filter_value_dt = pd.to_datetime(value, errors='coerce') #
                    # 如果日期转换失败且操作符不是处理列表的类型（列表值在后面处理）
                    if pd.isna(filter_value_dt) and not (isinstance(value, list) and operator in ["in", "is_in_list", "not in", "is_not_in_list"]): #
                        logger.warning(f"Cannot convert filter value '{value}' to date for column '{column}'. Skipping filter.") #
                        continue #
                    value = filter_value_dt # 更新 value 为日期对象或 NaT #
                except Exception as e_date: #
                    logger.warning(f"Error converting column '{column}' or value '{value}' to datetime: {e_date}. Skipping filter.") #
                    continue #
            
            condition = None #
            # 根据操作符构建筛选条件
            if operator == "equals" or operator == "==": #
                condition = (series_to_filter == value) #
            elif operator == "not_equals" or operator == "!=": #
                condition = (series_to_filter != value) #
            elif operator == "greater_than" or operator == ">": #
                condition = (series_to_filter > value) #
            elif operator == "greater_than_or_equals" or operator == ">=": #
                condition = (series_to_filter >= value) #
            elif operator == "less_than" or operator == "<": #
                condition = (series_to_filter < value) #
            elif operator == "less_than_or_equals" or operator == "<=": #
                condition = (series_to_filter <= value) #
            elif operator == "contains": #
                if is_date: #
                    logger.warning(f"'contains' operator is not directly applicable to date column '{column}'. Skipping filter.") #
                    continue #
                condition = (series_to_filter.astype(str).str.contains(str(value), case=False, na=False)) #
            elif operator == "not_contains": #
                if is_date: #
                    logger.warning(f"'not_contains' operator is not directly applicable to date column '{column}'. Skipping filter.") #
                    continue #
                condition = (~series_to_filter.astype(str).str.contains(str(value), case=False, na=False)) #
            elif operator == "is_in_list" or operator == "in": #
                if not isinstance(value, list): #
                    logger.warning(f"'is_in_list' operator expects a list value for column '{column}'. Skipping filter.") #
                    continue #
                if is_date: #
                    list_value_dt = pd.to_datetime(value, errors='coerce').dropna().tolist() #
                    if not list_value_dt: #
                        logger.warning(f"Cannot convert list values to dates for 'is_in_list' on column '{column}'. Skipping filter.") #
                        continue #
                    condition = (series_to_filter.isin(list_value_dt)) #
                else: #
                    condition = (series_to_filter.isin(value)) #
            elif operator == "is_not_in_list" or operator == "not in": #
                if not isinstance(value, list): #
                    logger.warning(f"'is_not_in_list' operator expects a list value for column '{column}'. Skipping filter.") #
                    continue #
                if is_date: #
                    list_value_dt = pd.to_datetime(value, errors='coerce').dropna().tolist() #
                    if not list_value_dt: #
                         logger.warning(f"Cannot convert list values to dates for 'is_not_in_list' on column '{column}'. Skipping filter.") #
                         continue #
                    condition = (~series_to_filter.isin(list_value_dt)) #
                else: #
                    condition = (~series_to_filter.isin(value)) #
            elif operator == "between": #
                if not (isinstance(value, list) and len(value) == 2): #
                    logger.warning(f"'between' operator expects a list of two values for column '{column}'. Skipping filter.") #
                    continue #
                val1, val2 = value #
                if is_date: #
                    val1_dt = pd.to_datetime(val1, errors='coerce') #
                    val2_dt = pd.to_datetime(val2, errors='coerce') #
                    if pd.isna(val1_dt) or pd.isna(val2_dt): #
                        logger.warning(f"Cannot convert 'between' values to dates for column '{column}'. Skipping filter.") #
                        continue #
                    condition = (series_to_filter.between(min(val1_dt, val2_dt), max(val1_dt, val2_dt))) #
                else: #
                    condition = (series_to_filter.between(min(val1, val2), max(val1, val2))) #
            elif operator == "is_null" or operator == "isnull": #
                condition = (series_to_filter.isnull()) #
            elif operator == "is_not_null" or operator == "notnull": #
                condition = (series_to_filter.notnull()) #
            else: #
                logger.warning(f"Unsupported filter operator '{operator}' for column '{column}'. Skipping filter.") #
                continue # 跳过不支持的操作符 #

            if condition is not None: #
                 df_filtered = df_filtered[condition] #
            else: #
                logger.warning(f"Condition was None for filter {f_obj}. This should not happen if operator is supported.") #

        except Exception as e_filter: #
            logger.error(f"Error applying filter {f_obj} on column '{column}': {e_filter}", exc_info=True) #
            # 根据策略，可以选择跳过此过滤器或在此处引发异常/返回错误
            # 为保持流程继续，我们选择跳过错误的过滤器
            continue #
            
    return df_filtered #

# --- FastAPI 端点 ---

@app.post("/notify", response_model=NotificationResponse) #
async def send_desktop_notification(req: NotificationRequest): #
    """
    接收通知请求并在 Windows 桌面上显示一个通知。
    """
    logger.info(f"Received notification request: Title='{req.title}', Message='{req.message}'") #
    try: #
        toast = Notification( #
            app_id=req.app_name, #
            title=req.title, #
            msg=req.message, #
            # duration="long", # 可选 #
            # icon=req.icon_path if req.icon_path and os.path.exists(req.icon_path) else "" # 可选 #
        )
        toast.show() #
        
        logger.info(f"Desktop notification successfully shown: '{req.title}'") #
        return NotificationResponse(success=True, message="Notification successfully shown.") #
    except Exception as e: #
        logger.error(f"Failed to show desktop notification: {e}", exc_info=True) #
        raise HTTPException(status_code=500, detail=f"Failed to show notification: {str(e)}") #


@app.post("/excel_sqo_mcp/execute_operation", response_model=SQOResponse) #
async def execute_excel_sqo_operation(request_data: ExecuteSQORequest): #
    """
    执行 Excel 结构化查询对象 (SQO) 操作。
    这个端点负责根据传入的 SQO 对指定的 Excel 文件进行数据读取、筛选、聚合等操作。
    """
    sqo = request_data.sqo #
    operation_type = sqo.get("operation_type") #
    file_path = sqo.get("file_path") #
    sheet_name = sqo.get("sheet_name", 0) # 默认为第一个工作表 #

    logger.info(f"Received SQO request: operation='{operation_type}', file='{file_path}', sheet='{sheet_name}'") #
    logger.debug(f"Full SQO received: {sqo}") #

    if not file_path or not operation_type: #
        logger.error("SQO missing 'file_path' or 'operation_type'.") #
        return SQOResponse(success=False, error="SQO中缺少 'file_path' 或 'operation_type' 参数。") #

    # 安全性提示：在生产环境中，需要对 file_path 进行严格校验，防止路径遍历等安全风险。
    # 例如，限制只能访问特定目录下的文件。
    if not os.path.exists(file_path): #
        logger.error(f"Excel file not found: {file_path}") #
        return SQOResponse(success=False, error=f"Excel文件未找到 '{file_path}'") #

    try: #
        # 读取 Excel 文件到 Pandas DataFrame
        df = pd.read_excel(file_path, sheet_name=sheet_name) #
        logger.info(f"Successfully loaded DataFrame. Shape: {df.shape}, Columns: {df.columns.tolist()}") #

        result_data = None #

        # --- 根据操作类型执行不同逻辑 ---

        if operation_type == "get_unique_values": #
            column_name = sqo.get("column_name") #
            if not column_name or column_name not in df.columns: #
                error_msg = f"'get_unique_values' 操作缺少有效 'column_name' 或列 '{column_name}' 不存在。可用列: {df.columns.tolist()}" #
                logger.error(error_msg) #
                return SQOResponse(success=False, error=error_msg) #
            
            df_to_process = df.copy() #
            filters_from_sqo = sqo.get("filters") #
            if filters_from_sqo and isinstance(filters_from_sqo, list): #
                logger.info(f"Applying filters for get_unique_values: {filters_from_sqo}") #
                df_to_process = apply_filters_to_dataframe(df_to_process, filters_from_sqo) #
                logger.info(f"DataFrame shape after filtering for get_unique_values: {df_to_process.shape}") #

            unique_values_series = df_to_process[column_name].unique() #
            if sqo.get('options', {}).get('drop_na', False): # 默认为 False #
                unique_values = pd.Series(unique_values_series).dropna().tolist() #
            else: #
                unique_values = unique_values_series.tolist() #
            result_data = unique_values #

        elif operation_type == "group_by_aggregate": #
            group_by_cols = sqo.get("group_by_columns") #
            agg_col = sqo.get("aggregation_column") #
            agg_func = sqo.get("aggregation_function") #
            if not (group_by_cols and agg_col and agg_func): #
                error_msg = "'group_by_aggregate' 操作缺少 'group_by_columns', 'aggregation_column', 或 'aggregation_function' 参数。" #
                logger.error(error_msg) #
                return SQOResponse(success=False, error=error_msg) #
            
            columns_to_check = [] #
            if isinstance(group_by_cols, list): columns_to_check.extend(group_by_cols) #
            elif isinstance(group_by_cols, str): columns_to_check.append(group_by_cols) #
            if agg_col: columns_to_check.append(agg_col) #
            for col in columns_to_check: #
                if col not in df.columns: #
                    error_msg = f"列 '{col}' 在Excel中未找到。可用列: {df.columns.tolist()}" #
                    logger.error(error_msg) #
                    return SQOResponse(success=False, error=error_msg) #
            
            df_to_group = df.copy() #
            filters_from_sqo = sqo.get("filters") #
            if filters_from_sqo and isinstance(filters_from_sqo, list): #
                logger.info(f"Applying filters for group_by_aggregate: {filters_from_sqo}") #
                df_to_group = apply_filters_to_dataframe(df_to_group, filters_from_sqo) #
                logger.info(f"DataFrame shape after filtering: {df_to_group.shape}") #
            elif "filters" in sqo and sqo["filters"]: #
                 logger.warning(f"SQO中的 'filters' 字段格式不正确（期望列表），将被忽略。Filters: {sqo['filters']}") #

            grouped_data = df_to_group.groupby(group_by_cols)[agg_col].agg(agg_func) #
            output_col_name = sqo.get('options', {}).get('output_column_name') #
            if output_col_name: #
                if isinstance(grouped_data, pd.Series): grouped_data = grouped_data.rename(output_col_name) #
                elif isinstance(grouped_data, pd.DataFrame) and len(grouped_data.columns)==1: grouped_data.columns = [output_col_name] #
            result_data = grouped_data.reset_index().to_dict(orient='records') #

        elif operation_type == "find_top_n_rows": #
            select_columns = sqo.get("select_columns") #
            condition_col = sqo.get("condition_column") #
            sort_order_str = sqo.get("sort_order", "descending").lower() #
            n_rows_param = sqo.get("n_rows", 1) # 使用 n_rows_param 避免与内置名称冲突 #
            if not (select_columns and condition_col): #
                error_msg = "'find_top_n_rows' 操作缺少 'select_columns' 或 'condition_column' 参数。" #
                logger.error(error_msg) #
                return SQOResponse(success=False, error=error_msg) #
            
            columns_to_check_top_n = [] # 使用不同变量名以避免作用域问题 #
            if isinstance(select_columns, list): columns_to_check_top_n.extend(select_columns) #
            elif isinstance(select_columns, str): columns_to_check_top_n.append(select_columns) #
            if condition_col: columns_to_check_top_n.append(condition_col) #
            for col_top_n in columns_to_check_top_n: # 使用不同迭代变量 #
                 if col_top_n not in df.columns: #
                    error_msg = f"列 '{col_top_n}' 在Excel中未找到。可用列: {df.columns.tolist()}" #
                    logger.error(error_msg) #
                    return SQOResponse(success=False, error=error_msg) #

            if sort_order_str not in ['ascending', 'descending']: #
                error_msg = f"无效的 sort_order: {sort_order_str}" #
                logger.error(error_msg) #
                return SQOResponse(success=False, error=error_msg) #
            if not isinstance(n_rows_param, int) or n_rows_param <= 0: #
                error_msg = f"n_rows 必须是正整数，但收到: {n_rows_param}" #
                logger.error(error_msg) #
                return SQOResponse(success=False, error=error_msg) #

            df_to_sort = df.copy() #
            filters_from_sqo_top_n = sqo.get("filters") # 使用不同变量名 #
            if filters_from_sqo_top_n and isinstance(filters_from_sqo_top_n, list): #
                logger.info(f"Applying filters for find_top_n_rows: {filters_from_sqo_top_n}") #
                df_to_sort = apply_filters_to_dataframe(df_to_sort, filters_from_sqo_top_n) #
                logger.info(f"DataFrame shape after filtering: {df_to_sort.shape}") #
            elif "filters" in sqo and sqo["filters"]: #
                 logger.warning(f"SQO中的 'filters' 字段格式不正确（期望列表），将被忽略。Filters: {sqo['filters']}") #
            
            ascending_order = True if sort_order_str == 'ascending' else False #
            sorted_df = df_to_sort.sort_values(by=condition_col, ascending=ascending_order) #
            result_df = sorted_df.head(n_rows_param) # 使用 n_rows_param #
            result_data = result_df[select_columns].to_dict(orient='records') #
        
        elif operation_type == "direct_sql_query": #
            sql_query_from_sqo = sqo.get("sql_query") #
            if not sql_query_from_sqo: #
                error_msg = "'direct_sql_query' 操作缺少 'sql_query' 参数。" #
                logger.error(error_msg) #
                return SQOResponse(success=False, error=error_msg) #
            
            pdsql = PandaSQL(persist=False) #
            logger.debug(f"Executing direct_sql_query: {sql_query_from_sqo} on columns: {df.columns.tolist()}") #
            # 在执行SQL之前，对DataFrame应用过滤器（如果提供了）
            df_for_sql = df.copy() #
            filters_for_sql = sqo.get("filters") #
            if filters_for_sql and isinstance(filters_for_sql, list): #
                logger.info(f"Applying filters before direct_sql_query: {filters_for_sql}") #
                df_for_sql = apply_filters_to_dataframe(df_for_sql, filters_for_sql) #
                logger.info(f"DataFrame shape after filtering for direct_sql_query: {df_for_sql.shape}") #

            query_result_df = pdsql(sql_query_from_sqo, env={'df': df_for_sql}) # 在过滤后的df上执行 #
            
            if query_result_df is None: #
                result_data = "SQL查询成功执行，但没有返回数据。" #
            elif query_result_df.empty: #
                result_data = "SQL查询成功执行，但未找到符合条件的数据。" #
            else: #
                result_list_of_dicts = query_result_df.to_dict(orient='records') #
                if len(result_list_of_dicts) == 1: #
                    if len(result_list_of_dicts[0]) == 1: result_data = list(result_list_of_dicts[0].values())[0] #
                    else: result_data = result_list_of_dicts[0] #
                else: #
                    if result_list_of_dicts and len(result_list_of_dicts[0]) == 1: # 添加检查 result_list_of_dicts 是否为空 #
                        single_col_name = list(result_list_of_dicts[0].keys())[0] #
                        result_data = [row[single_col_name] for row in result_list_of_dicts] #
                    else: result_data = result_list_of_dicts #
        else: #
            error_msg = f"不支持的操作类型 '{operation_type}'。" #
            logger.error(error_msg) #
            return SQOResponse(success=False, error=error_msg) #

        logger.info(f"SQO operation '{operation_type}' executed successfully.") #
        return SQOResponse(success=True, result=result_data) #

    except ImportError: #
        logger.critical("Pandas或PandaSQL未正确安装。") #
        return SQOResponse(success=False, error="执行Excel查询所需的库 (Pandas/PandaSQL) 未安装。", error_details=traceback.format_exc()) #
    except FileNotFoundError as e_fnf: # 更具体的异常捕获 #
        logger.error(f"File not found during operation: {e_fnf}", exc_info=True) #
        return SQOResponse(success=False, error=str(e_fnf), error_details=traceback.format_exc()) #
    except pd.errors.EmptyDataError as e_empty: #
        logger.error(f"Pandas EmptyDataError for file {file_path}, sheet {sheet_name}: {e_empty}", exc_info=True) #
        return SQOResponse(success=False, error=f"无法读取Excel文件 '{os.path.basename(file_path)}' (工作表: {sheet_name})，文件可能为空或格式不正确。", error_details=traceback.format_exc()) #
    except KeyError as e_key: # 捕获列名不存在等错误 #
        logger.error(f"KeyError during operation '{operation_type}': {e_key}. SQO: {sqo}", exc_info=True) #
        return SQOResponse(success=False, error=f"操作失败：列名 '{str(e_key)}' 可能不存在或不正确。请检查SQO参数和Excel列名。可用列: {df.columns.tolist() if 'df' in locals() else '未知'}", error_details=traceback.format_exc()) #
    except Exception as e: #
        error_message = f"执行SQO操作 '{operation_type}' 时发生内部错误: {type(e).__name__} - {str(e)}" #
        logger.error(error_message, exc_info=True) #
        return SQOResponse(success=False, error=error_message, error_details=traceback.format_exc()) #


# --- 用于本地直接运行测试此本地代理服务 ---
if __name__ == "__main__": #
    import uvicorn #
    logger.info(f"Starting Local Agent server on http://0.0.0.0:{LOCAL_AGENT_PORT}") #
    uvicorn.run("local_agent_app:app", host="0.0.0.0", port=LOCAL_AGENT_PORT, reload=True) # 添加 reload=True #

    --- END OF FILE CONTENT ---

  File: local_agent_app.py.txt
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/local_agent/local_agent_app.py

from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from typing import Dict, Any, List, Optional
import pandas as pd # 用于处理 Excel 数据
from pandasql import PandaSQL # 用于在 Pandas DataFrame 上执行 SQL 查询
import os
import logging # 用于日志记录
import traceback # 用于更详细的错误信息

# --- [新增] 导入 winotify 用于发送 Windows 桌面通知 ---
from winotify import Notification #

# --- 配置常量 ---
LOCAL_AGENT_PORT = 8003 # 本地代理服务监听的端口
LOG_LEVEL = logging.INFO # 日志级别

# --- 日志配置 ---
# 配置日志记录器，方便调试和追踪
logging.basicConfig(level=LOG_LEVEL, format='%(asctime)s - %(levelname)s - LocalAgent - %(message)s') #
logger = logging.getLogger(__name__) #

# --- FastAPI 应用实例 ---
app = FastAPI(
    title="ZhzAI Local Agent", #
    description="本地代理程序，提供处理本地文件（如Excel）和执行特定本地任务（如桌面通知）的MCP服务。", #
    version="0.1.1" # 更新版本号以反映添加了通知功能
)

# --- Pydantic 模型定义 ---

# 用于 Excel SQO (Structured Query Object) 操作的请求体
class ExecuteSQORequest(BaseModel): #
    sqo: Dict[str, Any] = Field(description="必需。一个结构化查询对象 (SQO) 的JSON字典，用于描述要对Excel执行的操作。") #

# Excel SQO 操作的通用响应体
class SQOResponse(BaseModel): #
    success: bool # 操作是否成功
    result: Any = None # 成功时的结果数据
    error: Optional[str] = None # 失败时的简要错误信息
    error_details: Optional[str] = None # 失败时的详细技术错误信息或堆栈

# 用于桌面通知的请求体
class NotificationRequest(BaseModel): #
    title: str = Field(default="任务提醒", description="通知的标题") #
    message: str = Field(description="通知的主要内容") #
    app_name: str = Field(default="终端大脑助手", description="显示在通知来源的应用名称") #
    # icon_path: Optional[str] = Field(None, description="通知图标的绝对路径 (可选)") #

# 桌面通知操作的响应体
class NotificationResponse(BaseModel): #
    success: bool # 操作是否成功 #
    message: str # 返回的消息，成功或失败的说明 #

# --- 核心功能函数 ---

def apply_filters_to_dataframe(df: pd.DataFrame, filters: List[Dict[str, Any]]) -> pd.DataFrame: #
    """
    将一系列过滤器应用于 Pandas DataFrame。
    这是 Excel SQO 操作的核心辅助函数，用于在执行聚合、查询等操作前筛选数据。
    """
    if not filters: #
        return df #

    df_filtered = df.copy() #
    for f_obj in filters: #
        column = f_obj.get("column") #
        operator = f_obj.get("operator") #
        value = f_obj.get("value") #
        is_date = f_obj.get("is_date", False) #

        if not all([column, operator, value is not None]): #
            logger.warning(f"Skipping invalid filter object: {f_obj}") #
            continue #

        if column not in df_filtered.columns: #
            logger.warning(f"Filter column '{column}' not found in DataFrame. Available: {df_filtered.columns.tolist()}. Skipping filter: {f_obj}") #
            continue #

        try: #
            series_to_filter = df_filtered[column] # 操作副本以避免 SettingWithCopyWarning #
            
            # 尝试转换日期，如果失败则记录警告并可能跳过
            if is_date: #
                try: #
                    series_to_filter = pd.to_datetime(series_to_filter, errors='coerce') #
                    filter_value_dt = pd.to_datetime(value, errors='coerce') #
                    # 如果日期转换失败且操作符不是处理列表的类型（列表值在后面处理）
                    if pd.isna(filter_value_dt) and not (isinstance(value, list) and operator in ["in", "is_in_list", "not in", "is_not_in_list"]): #
                        logger.warning(f"Cannot convert filter value '{value}' to date for column '{column}'. Skipping filter.") #
                        continue #
                    value = filter_value_dt # 更新 value 为日期对象或 NaT #
                except Exception as e_date: #
                    logger.warning(f"Error converting column '{column}' or value '{value}' to datetime: {e_date}. Skipping filter.") #
                    continue #
            
            condition = None #
            # 根据操作符构建筛选条件
            if operator == "equals" or operator == "==": #
                condition = (series_to_filter == value) #
            elif operator == "not_equals" or operator == "!=": #
                condition = (series_to_filter != value) #
            elif operator == "greater_than" or operator == ">": #
                condition = (series_to_filter > value) #
            elif operator == "greater_than_or_equals" or operator == ">=": #
                condition = (series_to_filter >= value) #
            elif operator == "less_than" or operator == "<": #
                condition = (series_to_filter < value) #
            elif operator == "less_than_or_equals" or operator == "<=": #
                condition = (series_to_filter <= value) #
            elif operator == "contains": #
                if is_date: #
                    logger.warning(f"'contains' operator is not directly applicable to date column '{column}'. Skipping filter.") #
                    continue #
                condition = (series_to_filter.astype(str).str.contains(str(value), case=False, na=False)) #
            elif operator == "not_contains": #
                if is_date: #
                    logger.warning(f"'not_contains' operator is not directly applicable to date column '{column}'. Skipping filter.") #
                    continue #
                condition = (~series_to_filter.astype(str).str.contains(str(value), case=False, na=False)) #
            elif operator == "is_in_list" or operator == "in": #
                if not isinstance(value, list): #
                    logger.warning(f"'is_in_list' operator expects a list value for column '{column}'. Skipping filter.") #
                    continue #
                if is_date: #
                    list_value_dt = pd.to_datetime(value, errors='coerce').dropna().tolist() #
                    if not list_value_dt: #
                        logger.warning(f"Cannot convert list values to dates for 'is_in_list' on column '{column}'. Skipping filter.") #
                        continue #
                    condition = (series_to_filter.isin(list_value_dt)) #
                else: #
                    condition = (series_to_filter.isin(value)) #
            elif operator == "is_not_in_list" or operator == "not in": #
                if not isinstance(value, list): #
                    logger.warning(f"'is_not_in_list' operator expects a list value for column '{column}'. Skipping filter.") #
                    continue #
                if is_date: #
                    list_value_dt = pd.to_datetime(value, errors='coerce').dropna().tolist() #
                    if not list_value_dt: #
                         logger.warning(f"Cannot convert list values to dates for 'is_not_in_list' on column '{column}'. Skipping filter.") #
                         continue #
                    condition = (~series_to_filter.isin(list_value_dt)) #
                else: #
                    condition = (~series_to_filter.isin(value)) #
            elif operator == "between": #
                if not (isinstance(value, list) and len(value) == 2): #
                    logger.warning(f"'between' operator expects a list of two values for column '{column}'. Skipping filter.") #
                    continue #
                val1, val2 = value #
                if is_date: #
                    val1_dt = pd.to_datetime(val1, errors='coerce') #
                    val2_dt = pd.to_datetime(val2, errors='coerce') #
                    if pd.isna(val1_dt) or pd.isna(val2_dt): #
                        logger.warning(f"Cannot convert 'between' values to dates for column '{column}'. Skipping filter.") #
                        continue #
                    condition = (series_to_filter.between(min(val1_dt, val2_dt), max(val1_dt, val2_dt))) #
                else: #
                    condition = (series_to_filter.between(min(val1, val2), max(val1, val2))) #
            elif operator == "is_null" or operator == "isnull": #
                condition = (series_to_filter.isnull()) #
            elif operator == "is_not_null" or operator == "notnull": #
                condition = (series_to_filter.notnull()) #
            else: #
                logger.warning(f"Unsupported filter operator '{operator}' for column '{column}'. Skipping filter.") #
                continue # 跳过不支持的操作符 #

            if condition is not None: #
                 df_filtered = df_filtered[condition] #
            else: #
                logger.warning(f"Condition was None for filter {f_obj}. This should not happen if operator is supported.") #

        except Exception as e_filter: #
            logger.error(f"Error applying filter {f_obj} on column '{column}': {e_filter}", exc_info=True) #
            # 根据策略，可以选择跳过此过滤器或在此处引发异常/返回错误
            # 为保持流程继续，我们选择跳过错误的过滤器
            continue #
            
    return df_filtered #

# --- FastAPI 端点 ---

@app.post("/notify", response_model=NotificationResponse) #
async def send_desktop_notification(req: NotificationRequest): #
    """
    接收通知请求并在 Windows 桌面上显示一个通知。
    """
    logger.info(f"Received notification request: Title='{req.title}', Message='{req.message}'") #
    try: #
        toast = Notification( #
            app_id=req.app_name, #
            title=req.title, #
            msg=req.message, #
            # duration="long", # 可选 #
            # icon=req.icon_path if req.icon_path and os.path.exists(req.icon_path) else "" # 可选 #
        )
        toast.show() #
        
        logger.info(f"Desktop notification successfully shown: '{req.title}'") #
        return NotificationResponse(success=True, message="Notification successfully shown.") #
    except Exception as e: #
        logger.error(f"Failed to show desktop notification: {e}", exc_info=True) #
        raise HTTPException(status_code=500, detail=f"Failed to show notification: {str(e)}") #


@app.post("/excel_sqo_mcp/execute_operation", response_model=SQOResponse) #
async def execute_excel_sqo_operation(request_data: ExecuteSQORequest): #
    """
    执行 Excel 结构化查询对象 (SQO) 操作。
    这个端点负责根据传入的 SQO 对指定的 Excel 文件进行数据读取、筛选、聚合等操作。
    """
    sqo = request_data.sqo #
    operation_type = sqo.get("operation_type") #
    file_path = sqo.get("file_path") #
    sheet_name = sqo.get("sheet_name", 0) # 默认为第一个工作表 #

    logger.info(f"Received SQO request: operation='{operation_type}', file='{file_path}', sheet='{sheet_name}'") #
    logger.debug(f"Full SQO received: {sqo}") #

    if not file_path or not operation_type: #
        logger.error("SQO missing 'file_path' or 'operation_type'.") #
        return SQOResponse(success=False, error="SQO中缺少 'file_path' 或 'operation_type' 参数。") #

    # 安全性提示：在生产环境中，需要对 file_path 进行严格校验，防止路径遍历等安全风险。
    # 例如，限制只能访问特定目录下的文件。
    if not os.path.exists(file_path): #
        logger.error(f"Excel file not found: {file_path}") #
        return SQOResponse(success=False, error=f"Excel文件未找到 '{file_path}'") #

    try: #
        # 读取 Excel 文件到 Pandas DataFrame
        df = pd.read_excel(file_path, sheet_name=sheet_name) #
        logger.info(f"Successfully loaded DataFrame. Shape: {df.shape}, Columns: {df.columns.tolist()}") #

        result_data = None #

        # --- 根据操作类型执行不同逻辑 ---

        if operation_type == "get_unique_values": #
            column_name = sqo.get("column_name") #
            if not column_name or column_name not in df.columns: #
                error_msg = f"'get_unique_values' 操作缺少有效 'column_name' 或列 '{column_name}' 不存在。可用列: {df.columns.tolist()}" #
                logger.error(error_msg) #
                return SQOResponse(success=False, error=error_msg) #
            
            df_to_process = df.copy() #
            filters_from_sqo = sqo.get("filters") #
            if filters_from_sqo and isinstance(filters_from_sqo, list): #
                logger.info(f"Applying filters for get_unique_values: {filters_from_sqo}") #
                df_to_process = apply_filters_to_dataframe(df_to_process, filters_from_sqo) #
                logger.info(f"DataFrame shape after filtering for get_unique_values: {df_to_process.shape}") #

            unique_values_series = df_to_process[column_name].unique() #
            if sqo.get('options', {}).get('drop_na', False): # 默认为 False #
                unique_values = pd.Series(unique_values_series).dropna().tolist() #
            else: #
                unique_values = unique_values_series.tolist() #
            result_data = unique_values #

        elif operation_type == "group_by_aggregate": #
            group_by_cols = sqo.get("group_by_columns") #
            agg_col = sqo.get("aggregation_column") #
            agg_func = sqo.get("aggregation_function") #
            if not (group_by_cols and agg_col and agg_func): #
                error_msg = "'group_by_aggregate' 操作缺少 'group_by_columns', 'aggregation_column', 或 'aggregation_function' 参数。" #
                logger.error(error_msg) #
                return SQOResponse(success=False, error=error_msg) #
            
            columns_to_check = [] #
            if isinstance(group_by_cols, list): columns_to_check.extend(group_by_cols) #
            elif isinstance(group_by_cols, str): columns_to_check.append(group_by_cols) #
            if agg_col: columns_to_check.append(agg_col) #
            for col in columns_to_check: #
                if col not in df.columns: #
                    error_msg = f"列 '{col}' 在Excel中未找到。可用列: {df.columns.tolist()}" #
                    logger.error(error_msg) #
                    return SQOResponse(success=False, error=error_msg) #
            
            df_to_group = df.copy() #
            filters_from_sqo = sqo.get("filters") #
            if filters_from_sqo and isinstance(filters_from_sqo, list): #
                logger.info(f"Applying filters for group_by_aggregate: {filters_from_sqo}") #
                df_to_group = apply_filters_to_dataframe(df_to_group, filters_from_sqo) #
                logger.info(f"DataFrame shape after filtering: {df_to_group.shape}") #
            elif "filters" in sqo and sqo["filters"]: #
                 logger.warning(f"SQO中的 'filters' 字段格式不正确（期望列表），将被忽略。Filters: {sqo['filters']}") #

            grouped_data = df_to_group.groupby(group_by_cols)[agg_col].agg(agg_func) #
            output_col_name = sqo.get('options', {}).get('output_column_name') #
            if output_col_name: #
                if isinstance(grouped_data, pd.Series): grouped_data = grouped_data.rename(output_col_name) #
                elif isinstance(grouped_data, pd.DataFrame) and len(grouped_data.columns)==1: grouped_data.columns = [output_col_name] #
            result_data = grouped_data.reset_index().to_dict(orient='records') #

        elif operation_type == "find_top_n_rows": #
            select_columns = sqo.get("select_columns") #
            condition_col = sqo.get("condition_column") #
            sort_order_str = sqo.get("sort_order", "descending").lower() #
            n_rows_param = sqo.get("n_rows", 1) # 使用 n_rows_param 避免与内置名称冲突 #
            if not (select_columns and condition_col): #
                error_msg = "'find_top_n_rows' 操作缺少 'select_columns' 或 'condition_column' 参数。" #
                logger.error(error_msg) #
                return SQOResponse(success=False, error=error_msg) #
            
            columns_to_check_top_n = [] # 使用不同变量名以避免作用域问题 #
            if isinstance(select_columns, list): columns_to_check_top_n.extend(select_columns) #
            elif isinstance(select_columns, str): columns_to_check_top_n.append(select_columns) #
            if condition_col: columns_to_check_top_n.append(condition_col) #
            for col_top_n in columns_to_check_top_n: # 使用不同迭代变量 #
                 if col_top_n not in df.columns: #
                    error_msg = f"列 '{col_top_n}' 在Excel中未找到。可用列: {df.columns.tolist()}" #
                    logger.error(error_msg) #
                    return SQOResponse(success=False, error=error_msg) #

            if sort_order_str not in ['ascending', 'descending']: #
                error_msg = f"无效的 sort_order: {sort_order_str}" #
                logger.error(error_msg) #
                return SQOResponse(success=False, error=error_msg) #
            if not isinstance(n_rows_param, int) or n_rows_param <= 0: #
                error_msg = f"n_rows 必须是正整数，但收到: {n_rows_param}" #
                logger.error(error_msg) #
                return SQOResponse(success=False, error=error_msg) #

            df_to_sort = df.copy() #
            filters_from_sqo_top_n = sqo.get("filters") # 使用不同变量名 #
            if filters_from_sqo_top_n and isinstance(filters_from_sqo_top_n, list): #
                logger.info(f"Applying filters for find_top_n_rows: {filters_from_sqo_top_n}") #
                df_to_sort = apply_filters_to_dataframe(df_to_sort, filters_from_sqo_top_n) #
                logger.info(f"DataFrame shape after filtering: {df_to_sort.shape}") #
            elif "filters" in sqo and sqo["filters"]: #
                 logger.warning(f"SQO中的 'filters' 字段格式不正确（期望列表），将被忽略。Filters: {sqo['filters']}") #
            
            ascending_order = True if sort_order_str == 'ascending' else False #
            sorted_df = df_to_sort.sort_values(by=condition_col, ascending=ascending_order) #
            result_df = sorted_df.head(n_rows_param) # 使用 n_rows_param #
            result_data = result_df[select_columns].to_dict(orient='records') #
        
        elif operation_type == "direct_sql_query": #
            sql_query_from_sqo = sqo.get("sql_query") #
            if not sql_query_from_sqo: #
                error_msg = "'direct_sql_query' 操作缺少 'sql_query' 参数。" #
                logger.error(error_msg) #
                return SQOResponse(success=False, error=error_msg) #
            
            pdsql = PandaSQL(persist=False) #
            logger.debug(f"Executing direct_sql_query: {sql_query_from_sqo} on columns: {df.columns.tolist()}") #
            # 在执行SQL之前，对DataFrame应用过滤器（如果提供了）
            df_for_sql = df.copy() #
            filters_for_sql = sqo.get("filters") #
            if filters_for_sql and isinstance(filters_for_sql, list): #
                logger.info(f"Applying filters before direct_sql_query: {filters_for_sql}") #
                df_for_sql = apply_filters_to_dataframe(df_for_sql, filters_for_sql) #
                logger.info(f"DataFrame shape after filtering for direct_sql_query: {df_for_sql.shape}") #

            query_result_df = pdsql(sql_query_from_sqo, env={'df': df_for_sql}) # 在过滤后的df上执行 #
            
            if query_result_df is None: #
                result_data = "SQL查询成功执行，但没有返回数据。" #
            elif query_result_df.empty: #
                result_data = "SQL查询成功执行，但未找到符合条件的数据。" #
            else: #
                result_list_of_dicts = query_result_df.to_dict(orient='records') #
                if len(result_list_of_dicts) == 1: #
                    if len(result_list_of_dicts[0]) == 1: result_data = list(result_list_of_dicts[0].values())[0] #
                    else: result_data = result_list_of_dicts[0] #
                else: #
                    if result_list_of_dicts and len(result_list_of_dicts[0]) == 1: # 添加检查 result_list_of_dicts 是否为空 #
                        single_col_name = list(result_list_of_dicts[0].keys())[0] #
                        result_data = [row[single_col_name] for row in result_list_of_dicts] #
                    else: result_data = result_list_of_dicts #
        else: #
            error_msg = f"不支持的操作类型 '{operation_type}'。" #
            logger.error(error_msg) #
            return SQOResponse(success=False, error=error_msg) #

        logger.info(f"SQO operation '{operation_type}' executed successfully.") #
        return SQOResponse(success=True, result=result_data) #

    except ImportError: #
        logger.critical("Pandas或PandaSQL未正确安装。") #
        return SQOResponse(success=False, error="执行Excel查询所需的库 (Pandas/PandaSQL) 未安装。", error_details=traceback.format_exc()) #
    except FileNotFoundError as e_fnf: # 更具体的异常捕获 #
        logger.error(f"File not found during operation: {e_fnf}", exc_info=True) #
        return SQOResponse(success=False, error=str(e_fnf), error_details=traceback.format_exc()) #
    except pd.errors.EmptyDataError as e_empty: #
        logger.error(f"Pandas EmptyDataError for file {file_path}, sheet {sheet_name}: {e_empty}", exc_info=True) #
        return SQOResponse(success=False, error=f"无法读取Excel文件 '{os.path.basename(file_path)}' (工作表: {sheet_name})，文件可能为空或格式不正确。", error_details=traceback.format_exc()) #
    except KeyError as e_key: # 捕获列名不存在等错误 #
        logger.error(f"KeyError during operation '{operation_type}': {e_key}. SQO: {sqo}", exc_info=True) #
        return SQOResponse(success=False, error=f"操作失败：列名 '{str(e_key)}' 可能不存在或不正确。请检查SQO参数和Excel列名。可用列: {df.columns.tolist() if 'df' in locals() else '未知'}", error_details=traceback.format_exc()) #
    except Exception as e: #
        error_message = f"执行SQO操作 '{operation_type}' 时发生内部错误: {type(e).__name__} - {str(e)}" #
        logger.error(error_message, exc_info=True) #
        return SQOResponse(success=False, error=error_message, error_details=traceback.format_exc()) #


# --- 用于本地直接运行测试此本地代理服务 ---
if __name__ == "__main__": #
    import uvicorn #
    logger.info(f"Starting Local Agent server on http://0.0.0.0:{LOCAL_AGENT_PORT}") #
    uvicorn.run("local_agent_app:app", host="0.0.0.0", port=LOCAL_AGENT_PORT, reload=True) # 添加 reload=True #

    --- END OF FILE CONTENT ---


Directory: core/
  File: __init__.py
    --- File is empty ---

  File: llm_manager.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/core/llm_manager.py

import os
from typing import Any, Optional

# 从相对路径导入包装器
from .llms.custom_litellm_wrapper import CustomLiteLLMWrapper

# --- 配置常量 (可以从项目级配置文件或环境变量加载) ---
# 本地 LLM 服务地址 (Qwen3)
LOCAL_QWEN_API_BASE = os.getenv("LOCAL_LLM_API_BASE", "http://localhost:8088/v1")
LOCAL_QWEN_MODEL_NAME_FOR_LITELLM = os.getenv("LOCAL_LLM_MODEL_NAME_FOR_LITELLM", "local/qwen3-1.7b-gguf") # 与 LiteLLM 调用匹配的名称

# 云端 LiteLLM 网关地址 (用于 Gemini 等)
CLOUD_LITELLM_GW_API_BASE = os.getenv("CLOUD_LITELLM_GW_API_BASE", "YOUR_CLOUD_LITELLM_GATEWAY_URL_HERE/v1")
GEMINI_MODEL_NAME_FOR_LITELLM = os.getenv("CLOUD_LLM_MODEL_NAME_FOR_LITELLM", "gemini/gemini-1.5-flash-latest")
CLOUD_LITELLM_GATEWAY_API_KEY = os.getenv("CLOUD_LITELLM_GATEWAY_API_KEY") # 网关本身可能需要的 API Key

# 默认 LLM 参数
DEFAULT_TEMPERATURE_LOCAL = 0.7
DEFAULT_MAX_TOKENS_LOCAL = 2048

DEFAULT_TEMPERATURE_CLOUD = 0.5
DEFAULT_MAX_TOKENS_CLOUD = 4096


def get_local_qwen_llm(
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
    agent_tools: Optional[list] = None, # CrewAI Agent 的工具列表
    tool_config: Optional[dict] = None  # Gemini 风格的 tool_config
) -> CustomLiteLLMWrapper:
    """
    获取配置好的本地 Qwen3 LLM 实例 (CrewAI 兼容)。
    """
    temp = temperature if temperature is not None else DEFAULT_TEMPERATURE_LOCAL
    mt = max_tokens if max_tokens is not None else DEFAULT_MAX_TOKENS_LOCAL

    print(f"LLM Manager: Creating Local Qwen LLM instance.")
    print(f"  Model: {LOCAL_QWEN_MODEL_NAME_FOR_LITELLM}, API Base: {LOCAL_QWEN_API_BASE}")
    print(f"  Temp: {temp}, Max Tokens: {mt}")
    
    return CustomLiteLLMWrapper(
        model=LOCAL_QWEN_MODEL_NAME_FOR_LITELLM,
        api_base=LOCAL_QWEN_API_BASE,
        api_key="nokey", # 本地服务通常不需要 key
        custom_llm_provider="openai", # 因为我们的本地服务是 OpenAI 兼容的
        temperature=temp,
        max_tokens=mt,
        agent_tools=agent_tools,
        tool_config=tool_config
    )

def get_cloud_gemini_llm(
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
    agent_tools: Optional[list] = None, # CrewAI Agent 的工具列表
    tool_config: Optional[dict] = None  # Gemini 风格的 tool_config
) -> Optional[CustomLiteLLMWrapper]:
    """
    获取配置好的云端 Gemini LLM 实例 (CrewAI 兼容)。
    如果 CLOUD_LITELLM_GW_API_BASE 未配置，则尝试直接调用 Gemini (api_base=None)。
    """
    temp = temperature if temperature is not None else DEFAULT_TEMPERATURE_CLOUD
    mt = max_tokens if max_tokens is not None else DEFAULT_MAX_TOKENS_CLOUD

    actual_gemini_api_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
    gateway_specific_key = CLOUD_LITELLM_GATEWAY_API_KEY # 这个是网关本身的key

    print(f"LLM Manager: Preparing Cloud Gemini LLM instance.")
    print(f"  Model: {GEMINI_MODEL_NAME_FOR_LITELLM}")
    print(f"  Gateway API Base from env: {CLOUD_LITELLM_GW_API_BASE}")
    print(f"  Temp: {temp}, Max Tokens: {mt}")

    effective_api_base = CLOUD_LITELLM_GW_API_BASE
    key_to_use = gateway_specific_key # 默认使用网关key

    if CLOUD_LITELLM_GW_API_BASE == "YOUR_CLOUD_LITELLM_GATEWAY_URL_HERE/v1" or not CLOUD_LITELLM_GW_API_BASE:
        print("LLM Manager INFO: CLOUD_LITELLM_GW_API_BASE is not configured or is a placeholder. Will attempt direct Gemini call with api_base=None.")
        effective_api_base = None # 对于 LiteLLM 直接调用 Gemini，api_base 应为 None
        key_to_use = actual_gemini_api_key # 直接调用 Gemini 时，使用 Gemini 的 API Key
        if not key_to_use:
            print("LLM Manager WARNING: GEMINI_API_KEY (or GOOGLE_API_KEY) is not set. Direct Gemini calls will likely fail.")
            # 即使 key 未设置，我们仍然返回实例，让 LiteLLM 尝试并可能在调用时失败
    else:
        # 如果配置了网关，则使用网关的key（如果网关需要key的话）
        print(f"LLM Manager INFO: Using configured CLOUD_LITELLM_GW_API_BASE: {effective_api_base}")
        key_to_use = gateway_specific_key


    return CustomLiteLLMWrapper(
        model=GEMINI_MODEL_NAME_FOR_LITELLM,
        api_base=effective_api_base,
        api_key=key_to_use, # 使用根据情况选择的 API Key
        temperature=temp,
        max_tokens=mt,
        agent_tools=agent_tools,
        tool_config=tool_config
        # custom_llm_provider 对于直接调用 gemini/ 模型通常不需要显式设置，LiteLLM 会识别
    )

# --- (可选) 一个选择 LLM 的辅助函数 ---
def get_llm_instance(
    llm_type: str = "local_qwen", # "local_qwen" 或 "cloud_gemini"
    **kwargs # 其他传递给具体 LLM 创建函数的参数
) -> Optional[CustomLiteLLMWrapper]:
    if llm_type == "local_qwen":
        return get_local_qwen_llm(**kwargs)
    elif llm_type == "cloud_gemini":
        return get_cloud_gemini_llm(**kwargs)
    else:
        print(f"LLM Manager ERROR: Unknown LLM type '{llm_type}'. Returning None.")
        return None

if __name__ == "__main__":
    # 测试
    print("--- Testing LLM Manager ---")
    
    print("\n--- Getting Local Qwen LLM ---")
    local_llm = get_llm_instance("local_qwen", temperature=0.2)
    if local_llm:
        print(f"Local LLM instance: {local_llm.model_name}, Temp: {local_llm.litellm_params.get('temperature')}")
        # 可以在这里尝试调用，但需要运行 local_llm_service
        # try:
        #     response = local_llm.call(messages=[{"role":"user", "content":"你好"}])
        #     print(f"Test call to local_llm: {response}")
        # except Exception as e:
        #     print(f"Error calling local_llm: {e}")
    
    print("\n--- Getting Cloud Gemini LLM ---")
    # 需要设置环境变量 CLOUD_LITELLM_GW_API_BASE 和 CLOUD_LITELLM_GATEWAY_API_KEY
    # os.environ["CLOUD_LITELLM_GW_API_BASE"] = "http://your-gateway-url/v1" 
    # os.environ["CLOUD_LITELLM_GATEWAY_API_KEY"] = "your_gateway_key"
    cloud_llm = get_llm_instance("cloud_gemini", max_tokens=100)
    if cloud_llm:
        print(f"Cloud LLM instance: {cloud_llm.model_name}, Max Tokens: {cloud_llm.litellm_params.get('max_tokens')}")
    else:
        print("Cloud LLM instance could not be created (check gateway config).")
    --- END OF FILE CONTENT ---


Directory: core/grammars/
  File (skipped, extension not included): cypher_or_unable_output.gbnf

Directory: core/agents/
  File: __init__.py
    --- File is empty ---


Directory: core/tools/
  File: __init__.py
    --- File is empty ---

  File: calculator_tool.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/core/tools/calculator_tool.py
from typing import Type, Any
from pydantic import BaseModel, Field
from crewai.tools import BaseTool
import logging
import re

logger = logging.getLogger(__name__)

class CalculateToolInput(BaseModel):
    expression: str = Field(description="要计算的数学表达式字符串。例如：'(10 + 20) * 3 / 5 - 10'")

class CalculateTool(BaseTool):
    name: str = "calculate_tool"
    description: str = (
        "【计算器工具】执行数学表达式的计算并返回数值结果。支持常见的算术运算（加、减、乘、除、括号）。"
    )
    args_schema: Type[BaseModel] = CalculateToolInput

    def _run(self, expression: str, **kwargs: Any) -> str:
        logger.info(f"CalculateTool._run called with expression: '{expression}'")
        
        # 安全性检查：只允许数字、操作符、括号和点
        # 移除了对字母的允许，以防止执行不安全代码
        # 允许科学计数法中的 'e' 或 'E'
        if not re.match(r"^[0-9+\-*/().\sEe]+$", expression):
            logger.error(f"Invalid characters in expression: '{expression}'")
            return "错误：表达式中包含无效字符。只允许数字、操作符 (+, -, *, /)、括号、小数点和空格。"
        
        # 另一个安全措施：避免非常长的表达式
        if len(expression) > 100: # 限制表达式长度
            logger.error(f"Expression too long: '{expression}'")
            return "错误：表达式过长。"

        try:
            # 使用 eval 进行计算，前面已经做了基本的字符过滤
            # 注意：eval 仍然有潜在风险，如果过滤不完美。
            # 对于生产环境，强烈建议使用更安全的表达式求值库，如 numexpr 或 ast.literal_eval (但后者功能有限)
            result = eval(expression, {"__builtins__": {}}, {}) # 限制 eval 的命名空间
            logger.info(f"Expression '{expression}' evaluated to: {result}")
            return f"计算结果: {expression} = {result}"
        except ZeroDivisionError:
            logger.error(f"Error evaluating expression '{expression}': Division by zero")
            return "错误：表达式中存在除以零的操作。"
        except SyntaxError:
            logger.error(f"Error evaluating expression '{expression}': Syntax error")
            return "错误：数学表达式语法错误。"
        except Exception as e:
            logger.error(f"Error evaluating expression '{expression}': {e}", exc_info=True)
            return f"计算表达式时发生错误: {str(e)}"
    --- END OF FILE CONTENT ---

  File: enhanced_rag_tool.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/core/tools/enhanced_rag_tool.py

import asyncio
from typing import Type, Optional, Dict, Any
from pydantic import BaseModel, Field
from crewai.tools import BaseTool
import json
import traceback

from utils.common_utils import call_mcpo_tool

import logging
logger = logging.getLogger(__name__)


class EnhancedRAGToolInput(BaseModel):
    query: str = Field(description="用户提出的原始查询文本。")
    top_k_vector: int = Field(default=5, description="期望检索的向量搜索结果数量。")
    top_k_kg: int = Field(default=3, description="期望检索的知识图谱结果数量。")
    top_k_bm25: int = Field(default=3, description="期望检索的 BM25 关键词搜索结果数量。")

class EnhancedRAGTool(BaseTool):
    name: str = "enhanced_rag_tool"
    description: str = "【核心RAG工具】用于从本地知识库查找信息、回答复杂问题，整合了向量、关键词和图谱检索。"
    args_schema: Type[BaseModel] = EnhancedRAGToolInput
    mcp_service_name: str = "zhz_rag_mcp_service"
    mcp_tool_path: str = "query_rag_v2"

    async def _acall_mcp(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        """
        异步调用 MCP 工具服务，并处理其包装响应。
        目标是返回 RAG 服务本身的业务响应字典。
        """
        tool_path_on_mcp = f"{self.mcp_service_name}/{self.mcp_tool_path}"
        logger.critical(f"!!! EnhancedRAGTool._acall_mcp: ENTERING. Calling MCP endpoint '{tool_path_on_mcp}' with payload: {payload}")
        try:
            mcp_wrapper_response = await call_mcpo_tool(tool_path_on_mcp, payload)

            logger.critical(f"!!! EnhancedRAGTool._acall_mcp: RECEIVED from call_mcpo_tool - Type: {type(mcp_wrapper_response)}")
            if isinstance(mcp_wrapper_response, dict):
                logger.critical(f"!!! EnhancedRAGTool._acall_mcp: RECEIVED keys: {list(mcp_wrapper_response.keys())}")
            logger.critical(f"!!! EnhancedRAGTool._acall_mcp: RECEIVED content preview: {str(mcp_wrapper_response)[:500]}")

            # 场景1: call_mcpo_tool 返回 MCP 框架的成功响应 ({"success": True, "data": RAG_response})
            if isinstance(mcp_wrapper_response, dict) and mcp_wrapper_response.get("success") is True:
                rag_service_data = mcp_wrapper_response.get("data")
                if isinstance(rag_service_data, dict):
                    logger.info("!!! EnhancedRAGTool._acall_mcp: MCP call successful (success:True), returning RAG 'data' field.")
                    return rag_service_data
                else:
                    logger.error(f"!!! EnhancedRAGTool._acall_mcp: MCP 'data' field is not a dict. Got {type(rag_service_data)}. Payload: {payload}")
                    return {
                        "status": "error", "original_query": payload.get("query", "N/A"), "retrieved_context_docs": [],
                        "error_message": f"Internal error: RAG service data from MCP was not a dict. Got: {str(rag_service_data)[:200]}",
                        "error_code": "INVALID_RAG_DATA_FROM_MCP_SUCCESS"
                    }

            # 场景2: call_mcpo_tool 返回 MCP 框架的失败响应 ({"success": False, "error": ...})
            elif isinstance(mcp_wrapper_response, dict) and mcp_wrapper_response.get("success") is False:
                error_msg = mcp_wrapper_response.get("error", "Unknown error from MCP call wrapper.")
                error_type = mcp_wrapper_response.get("error_type", "MCP_CALL_FAILED")
                status_code = mcp_wrapper_response.get("status_code")
                logger.error(f"!!! EnhancedRAGTool._acall_mcp: MCP call failed (success:False). Type: {error_type}, Error: {error_msg}, Status Code: {status_code}")
                return { # 构造一个符合RAG错误格式的字典
                    "status": "error", "original_query": payload.get("query", "N/A"), "retrieved_context_docs": [],
                    "error_message": error_msg, "error_code": error_type,
                    "debug_info": {"mcp_error_type": error_type, "mcp_status_code": status_code}
                }

            # 场景3: call_mcpo_tool 直接返回了 RAG 服务的业务 JSON (例如，包含 "status" 但不含 "success")
            # 这种情况通常是 MCP 代理透传了下游服务的响应。
            elif isinstance(mcp_wrapper_response, dict) and "status" in mcp_wrapper_response:
                logger.info("!!! EnhancedRAGTool._acall_mcp: Received a direct business response from RAG service (contains 'status' key).")
                return mcp_wrapper_response

            # 场景4: call_mcpo_tool 返回了其他无法识别的字典格式
            elif isinstance(mcp_wrapper_response, dict):
                logger.error(f"!!! EnhancedRAGTool._acall_mcp: call_mcpo_tool returned an unexpected dictionary format: {str(mcp_wrapper_response)[:200]}")
                return {
                    "status": "error", "original_query": payload.get("query", "N/A"), "retrieved_context_docs": [],
                    "error_message": f"Internal error: MCP wrapper response format unknown. Got: {str(mcp_wrapper_response)[:200]}",
                    "error_code": "INVALID_MCP_WRAPPER_DICT_FORMAT_UNKNOWN"
                }
            
            # 场景5: call_mcpo_tool 返回的不是字典
            else:
                logger.error(f"!!! EnhancedRAGTool._acall_mcp: call_mcpo_tool returned non-dict type: {type(mcp_wrapper_response)}. Content: {str(mcp_wrapper_response)[:200]}")
                return {
                    "status": "error", "original_query": payload.get("query", "N/A"), "retrieved_context_docs": [],
                    "error_message": f"Internal error: MCP wrapper returned non-dict. Got: {str(mcp_wrapper_response)[:200]}",
                    "error_code": "INVALID_MCP_WRAPPER_NON_DICT"
                }

        except Exception as e_acall:
            logger.critical(f"!!! EnhancedRAGTool._acall_mcp: EXCEPTION during call_mcpo_tool or its processing: {e_acall}", exc_info=True)
            return {
                "status": "error", "original_query": payload.get("query", "N/A"), "retrieved_context_docs": [],
                "error_message": f"Exception in _acall_mcp: {str(e_acall)}",
                "error_code": "ACALL_MCP_UNHANDLED_EXCEPTION",
                "traceback": traceback.format_exc()
            }

    def _run(
        self,
        query: str,
        top_k_vector: int = 5,
        top_k_kg: int = 3,
        top_k_bm25: int = 3,
        **kwargs: Any
    ) -> str:
        logger.critical(f"!!! EnhancedRAGTool._run: ENTERING. Query: '{query}'")
        payload = {
            "query": query,
            "top_k_vector": top_k_vector,
            "top_k_kg": top_k_kg,
            "top_k_bm25": top_k_bm25,
        }
        rag_service_response_data: Dict[str, Any] = {
            "status": "error", "original_query": query, "retrieved_context_docs": [],
            "error_message": "Initialization error in _run before async call",
            "error_code": "RUN_INIT_ERROR"
        }

        try:
            logger.critical("!!! EnhancedRAGTool._run: Attempting to get event loop...")
            loop = asyncio.get_event_loop()
            logger.critical(f"!!! EnhancedRAGTool._run: Event loop obtained. Is running: {loop.is_running()}")

            if loop.is_running():
                logger.critical("!!! EnhancedRAGTool._run: Loop is running. Using ThreadPoolExecutor.")
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                    logger.critical("!!! EnhancedRAGTool._run: Submitting _acall_mcp to executor...")
                    future = executor.submit(asyncio.run, self._acall_mcp(payload))
                    logger.critical("!!! EnhancedRAGTool._run: Waiting for future.result()...")
                    rag_service_response_data = future.result(timeout=120)
                    logger.critical("!!! EnhancedRAGTool._run: future.result() RETURNED.")
            else:
                logger.critical("!!! EnhancedRAGTool._run: Loop is NOT running. Using asyncio.run directly.")
                rag_service_response_data = asyncio.run(self._acall_mcp(payload))
                logger.critical("!!! EnhancedRAGTool._run: asyncio.run(_acall_mcp) RETURNED.")

        except asyncio.TimeoutError as e_timeout:
            logger.critical(f"!!! EnhancedRAGTool._run: ASYNCIO TIMEOUT during RAG tool execution: {e_timeout}", exc_info=True)
            rag_service_response_data = {
                "status": "error", "original_query": query, "retrieved_context_docs": [],
                "error_message": f"RAG tool execution timed out: {str(e_timeout)}",
                "error_code": "ASYNC_TIMEOUT_IN_RUN"
            }
        except Exception as e_run:
            logger.critical(f"!!! EnhancedRAGTool._run: EXCEPTION during RAG tool execution: {e_run}", exc_info=True)
            rag_service_response_data = {
                "status": "error", "original_query": query, "retrieved_context_docs": [],
                "error_message": f"Exception in _run: {str(e_run)}",
                "error_code": "RUN_UNHANDLED_EXCEPTION",
                "traceback": traceback.format_exc()
            }

        logger.critical(f"!!! EnhancedRAGTool._run: Raw RAG service data before _handle_mcp_result: {str(rag_service_response_data)[:500]}")

        if not isinstance(rag_service_response_data, dict):
            logger.error(f"!!! EnhancedRAGTool._run: rag_service_response_data is not a dict, type: {type(rag_service_response_data)}. Wrapping in error dict.")
            rag_service_response_data = {
                "status": "error", "original_query": query, "retrieved_context_docs": [],
                "error_message": f"Internal error: _acall_mcp did not return a dict as expected. Got: {str(rag_service_response_data)[:200]}",
                "error_code": "INVALID_ACALL_RETURN_TYPE_FINAL"
            }
        # 确保 rag_service_response_data 至少包含 'status' 键，如果它在 _acall_mcp 中因为某些路径没有返回标准错误结构
        if "status" not in rag_service_response_data:
            logger.warning(f"!!! EnhancedRAGTool._run: rag_service_response_data missing 'status' key. Data: {str(rag_service_response_data)[:200]}. Defaulting to error status.")
            rag_service_response_data["status"] = "error"
            rag_service_response_data.setdefault("error_message", "Malformed response from RAG service call.")
            rag_service_response_data.setdefault("error_code", "MALFORMED_RAG_RESPONSE_IN_RUN")
            rag_service_response_data.setdefault("original_query", query)
            rag_service_response_data.setdefault("retrieved_context_docs", [])


        return self._handle_mcp_result(rag_service_response_data)

    def _handle_mcp_result(self, rag_service_data: Dict[str, Any]) -> str:
        """
        处理来自 RAG 服务 (zhz_rag_mcp_service) 的直接响应。
        """
        logger.info(f"EnhancedRAGTool._handle_mcp_result received RAG service data: {str(rag_service_data)[:500]}...")

        if not isinstance(rag_service_data, dict):
            error_msg = f"TOOL_ERROR: {self.name} received an invalid response format (expected dict from RAG service, got {type(rag_service_data)}). Content: {str(rag_service_data)[:200]}"
            logger.error(error_msg)
            return error_msg

        status_from_rag = rag_service_data.get("status")

        if status_from_rag == "success":
            final_answer = rag_service_data.get("final_answer")
            retrieved_docs_raw = rag_service_data.get("retrieved_context_docs")

            if final_answer is None:
                no_answer_msg = f"TOOL_INFO: {self.name} RAG service status is 'success', but did not provide a final_answer."
                logger.warning(no_answer_msg)
                return "RAG service processed successfully but found no specific answer."

            response_parts = [f"RAG Answer: {str(final_answer).strip()}"]
            if retrieved_docs_raw and isinstance(retrieved_docs_raw, list) and retrieved_docs_raw:
                response_parts.append("\n\nSupporting Context Snippets (Top 2):")
                for i, doc_data in enumerate(retrieved_docs_raw[:2]):
                    content = doc_data.get("content", "N/A")
                    source = doc_data.get("source_type", "N/A")
                    score = doc_data.get("score")
                    score_str = f"{score:.2f}" if isinstance(score, float) else str(score if score is not None else 'N/A')
                    response_parts.append(f"  - Source: {source}, Score: {score_str}, Content: {str(content)[:100]}...")

            final_tool_output_str = "\n".join(response_parts)
            logger.info(f"EnhancedRAGTool: Successfully processed RAG success response. Output for agent (first 200 chars): {final_tool_output_str[:200]}...")
            return final_tool_output_str

        elif status_from_rag == "clarification_needed":
            clarification_question = rag_service_data.get("clarification_question", "需要您提供更多信息。")
            uncertainty_reason = rag_service_data.get("debug_info", {}).get("uncertainty_reason", "未知原因")
            clarification_output = f"CLARIFICATION_NEEDED: {clarification_question} (Reason: {uncertainty_reason})"
            logger.info(f"EnhancedRAGTool: RAG service requires clarification. Output for agent: {clarification_output}")
            return clarification_output

        elif status_from_rag == "error":
            rag_error_msg = rag_service_data.get("error_message", "RAG服务内部发生未知错误。")
            rag_error_code = rag_service_data.get("error_code", "RAG_UNKNOWN_ERROR")
            error_output = f"TOOL_ERROR: {self.name} failed. RAG service reported an error (Code: {rag_error_code}): {rag_error_msg}"
            logger.error(error_output)
            return error_output
        else:
            unknown_status_msg = f"TOOL_ERROR: {self.name} received an unknown or missing status '{status_from_rag}' from RAG service."
            logger.error(f"{unknown_status_msg} Full RAG data: {str(rag_service_data)[:300]}")
            return unknown_status_msg
    --- END OF FILE CONTENT ---

  File: excel_tool.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/core/tools/excel_tool.py

import httpx # 用于直接 HTTP 调用本地代理
import json
from typing import Type, Dict, Any, Union, List, Optional
from pydantic import BaseModel, Field
from crewai.tools import BaseTool
import  os

# --- 日志记录 ---
import logging
logger = logging.getLogger(__name__)

# 本地 Excel 代理服务的地址
# LOCAL_AGENT_BASE_URL = "http://localhost:8003" # 这个端口需要与 local_agent_app.py 中的一致
# 为了使其更灵活，从环境变量读取，并提供默认值
WINDOWS_HOST_IP = os.getenv("WINDOWS_HOST_IP_FOR_WSL", "192.168.3.11") # 确保这个IP正确
LOCAL_AGENT_PORT = os.getenv("LOCAL_AGENT_PORT", "8003")
LOCAL_AGENT_EXCEL_SQO_ENDPOINT = f"http://{WINDOWS_HOST_IP}:{LOCAL_AGENT_PORT}/excel_sqo_mcp/execute_operation"


class ExcelOperationToolInput(BaseModel):
    # Worker Agent 会迭代 Manager 生成的 SQO 列表，
    # 每次调用这个工具时，传递一个 SQO 字典，以及 file_path 和 sheet_name
    sqo_dict: Dict[str, Any] = Field(description="单个结构化查询对象 (SQO) 的JSON字典。")
    file_path: str = Field(description="目标Excel文件的绝对路径。")
    sheet_name: Union[str, int] = Field(default=0, description="目标工作表的名称或索引 (0-based)。")

class ExcelOperationTool(BaseTool):
    name: str = "excel_operation_tool"
    description: str = ("【Excel操作工具】通过结构化查询对象(SQO)对Excel文件执行复杂的数据查询、筛选、聚合等操作。"
                        "此工具接收单个SQO以及文件路径和工作表名，并调用本地代理执行。")
    args_schema: Type[BaseModel] = ExcelOperationToolInput

    def _run(
        self,
        sqo_dict: Dict[str, Any],
        file_path: str,
        sheet_name: Union[str, int] = 0,
        **kwargs: Any
    ) -> str:
        """
        同步执行方法，调用本地代理执行单个Excel SQO。
        """
        logger.info(f"ExcelOperationTool._run called with file_path: '{file_path}', sheet_name: '{sheet_name}'")
        logger.debug(f"SQO to execute: {sqo_dict}")

        if not isinstance(sqo_dict, dict):
            return "错误：传递给 ExcelOperationTool 的 sqo_dict 参数必须是一个字典。"
        if not sqo_dict.get("operation_type"):
            return "错误：SQO字典中缺少 'operation_type' 字段。"

        # 构造发送给 local_agent_app.py 的请求体
        # local_agent_app.py 的 /excel_sqo_mcp/execute_operation 端点期望的请求体是 ExecuteSQORequest 模型，
        # 它有一个名为 'sqo' 的字段，其值才是我们这里的 sqo_dict (已经包含了 operation_type, file_path, sheet_name)
        
        # 我们需要确保传递给本地代理的 SQO 中包含 file_path 和 sheet_name，
        # 因为 Manager Agent 生成的原始 SQO 列表中的字典不包含它们。
        # Worker Agent 在调用此工具前，应该已经将它们补充完整。
        # 但为了工具的健壮性，我们在这里也检查一下，并可以覆盖或添加。
        
        sqo_for_local_agent = sqo_dict.copy() # 创建副本以修改
        sqo_for_local_agent["file_path"] = file_path # 确保或覆盖 file_path
        sqo_for_local_agent["sheet_name"] = sheet_name # 确保或覆盖 sheet_name

        request_payload_to_local_agent = {"sqo": sqo_for_local_agent}

        logger.info(f"Sending request to Local Excel Agent: {LOCAL_AGENT_EXCEL_SQO_ENDPOINT}")
        logger.debug(f"Payload to Local Excel Agent: {request_payload_to_local_agent}")

        try:
            # 使用 httpx 进行同步调用
            with httpx.Client(timeout=60.0) as client:
                response = client.post(LOCAL_AGENT_EXCEL_SQO_ENDPOINT, json=request_payload_to_local_agent)
            
            logger.info(f"Local Excel Agent response status: {response.status_code}")
            response_json = response.json() # local_agent_app.py 返回的是 SQOResponse 模型

            if response.status_code == 200 and response_json.get("success"):
                result = response_json.get("result")
                logger.info(f"Excel operation successful. Result type: {type(result)}")
                # 将结果转换为字符串以便 Agent 处理
                if isinstance(result, (list, dict)):
                    try:
                        return json.dumps(result, ensure_ascii=False, indent=2)
                    except TypeError: # 处理无法JSON序列化的类型，例如某些Pandas特殊类型
                        return str(result)
                return str(result)
            else:
                error_message = response_json.get("error", "未知错误")
                error_details = response_json.get("error_details")
                full_error = f"本地Excel代理错误: {error_message}"
                if error_details:
                    full_error += f" | 详情: {str(error_details)[:200]}" # 限制详情长度
                logger.error(full_error)
                return full_error

        except httpx.RequestError as e:
            error_msg = f"请求本地Excel代理时出错: {e}"
            logger.error(error_msg, exc_info=True)
            return error_msg
        except json.JSONDecodeError as e:
            error_msg = f"解析本地Excel代理响应JSON时出错: {e}. 响应文本: {response.text[:200]}"
            logger.error(error_msg, exc_info=True)
            return error_msg
        except Exception as e:
            error_msg = f"执行Excel操作时发生未知错误: {e}"
            logger.error(error_msg, exc_info=True)
            return error_msg

# 确保 core/tools 目录也有一个 __init__.py 文件
# touch /home/zhz/zhz_agent/core/tools/__init__.py (如果不存在)
    --- END OF FILE CONTENT ---

  File: search_tool.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/core/tools/search_tool.py

import asyncio
from typing import Type, Optional, Dict, Any, List
from pydantic import BaseModel, Field
from crewai.tools import BaseTool
import json
from utils.common_utils import call_mcpo_tool # 确保从正确的相对路径导入
import traceback

# --- 日志记录 ---
import logging
logger = logging.getLogger(__name__)


class WebSearchToolInput(BaseModel):
    query: str = Field(description="要进行网络搜索的关键词或问题。")
    max_results: Optional[int] = Field(default=5, description="希望返回的最大搜索结果数量。")

class WebSearchTool(BaseTool):
    name: str = "web_search_tool" # Agent将使用这个名字
    description: str = ("【网络搜索工具】使用DuckDuckGo搜索引擎在互联网上查找与用户查询相关的信息。"
                        "返回搜索结果列表，每个结果包含标题、链接和摘要。")
    args_schema: Type[BaseModel] = WebSearchToolInput
    mcp_service_name: str = "ddgsearch" # 与 mcpo_servers.json 中定义的服务名一致
    mcp_tool_path: str = "search"      # DuckDuckGo 搜索服务提供的端点

    async def _acall_mcp(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        """
        实际调用 MCP 服务的异步方法。
        """
        tool_path_on_mcp = f"{self.mcp_service_name}/{self.mcp_tool_path}"
        logger.info(f"WebSearchTool: Calling MCP endpoint '{tool_path_on_mcp}' with payload: {payload}")
        # call_mcpo_tool 期望返回一个包含 success 和 data/error 的字典
        mcp_wrapper_response = await call_mcpo_tool(tool_path_on_mcp, payload)
        
        # 直接返回 call_mcpo_tool 的结果，让 _handle_mcp_result 处理
        return mcp_wrapper_response

    def _run(
        self,
        query: str,
        max_results: Optional[int] = 5,
        **kwargs: Any
    ) -> str:
        """
        同步执行方法，调用 MCP 的 ddgsearch 服务。
        """
        logger.info(f"WebSearchTool._run called with query: '{query}', max_results: {max_results}")
        payload = {
            "query": query,
            "max_results": max_results,
        }
        
        mcp_response_data: Dict[str, Any] = {
            "success": False, # 默认失败
            "error": "Initialization error in _run before async call for WebSearchTool",
            "error_type": "RUN_INIT_ERROR_WEBSEARCH"
        }

        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                    future = executor.submit(asyncio.run, self._acall_mcp(payload))
                    mcp_response_data = future.result(timeout=60) 
            else:
                mcp_response_data = asyncio.run(self._acall_mcp(payload))
        except Exception as e:
            logger.error(f"Error running WebSearchTool for query '{query}': {e}", exc_info=True)
            mcp_response_data = {
                "success": False,
                "error": f"Exception in WebSearchTool _run: {str(e)}",
                "error_type": "RUN_EXCEPTION_WEBSEARCH",
                "traceback": traceback.format_exc()
            }
        
        return self._handle_mcp_result(mcp_response_data)

    def _handle_mcp_result(self, mcp_response: Dict[str, Any]) -> str:
        """
        处理来自 MCP ddgsearch 服务的响应。
        call_mcpo_tool 会返回一个包含 "success": True/False 和 "data"/"error" 的字典。
        """
        logger.info(f"WebSearchTool._handle_mcp_result received: {str(mcp_response)[:500]}...")

        if not isinstance(mcp_response, dict):
            return f"TOOL_ERROR: {self.name} received invalid response format from MCP call (expected dict, got {type(mcp_response)})."

        if mcp_response.get("success") is False:
            error_msg = mcp_response.get("error", "Unknown error from ddgsearch MCP call.")
            logger.error(f"WebSearchTool failed via MCP: {error_msg}")
            return f"TOOL_ERROR: {self.name} failed: {error_msg}"

        # 如果 success 为 True，则 data 字段包含 ddgsearch 服务的实际输出
        ddg_service_data = mcp_response.get("data")
        if not isinstance(ddg_service_data, dict):
            logger.warning(f"WebSearchTool: 'data' field from successful MCP call is not a dict. Raw data: {str(ddg_service_data)[:200]}")
            return f"TOOL_ERROR: {self.name} received unexpected data format from ddgsearch service."

        # ddgsearch 服务成功时应该返回一个包含搜索结果列表的 JSON 结构
        # 参考 ddg-search MCP 工具的输出，它通常是 {"results": [{"title": ..., "href": ..., "body": ...}, ...]}
        search_results = ddg_service_data.get("results")
        
        if isinstance(search_results, list):
            if not search_results:
                return "网络搜索没有找到相关结果。"
            
            formatted_results = ["网络搜索结果："]
            for i, res in enumerate(search_results[:5]): # 最多展示5条给Agent
                if isinstance(res, dict):
                    title = res.get("title", "无标题")
                    link = res.get("href", "#")
                    snippet = res.get("body", "无摘要")
                    formatted_results.append(f"{i+1}. {title}\n   链接: {link}\n   摘要: {snippet[:150]}...\n")
                else:
                    formatted_results.append(f"{i+1}. {str(res)[:200]}...")
            return "\n".join(formatted_results)
        else:
            logger.warning(f"WebSearchTool: 'results' key not found or is not a list in ddgsearch service data. Raw service data: {str(ddg_service_data)[:300]}")
            return f"网络搜索未能获取到预期的结果列表。原始响应: {json.dumps(ddg_service_data, ensure_ascii=False, indent=2)}"
    --- END OF FILE CONTENT ---

  File: time_tool.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/core/tools/time_tool.py
from datetime import datetime
import pytz # 用于处理时区
from typing import Type, Optional, Any
from pydantic import BaseModel, Field
from crewai.tools import BaseTool
import logging

logger = logging.getLogger(__name__)

class GetCurrentTimeToolInput(BaseModel):
    timezone_str: Optional[str] = Field(
        default="Asia/Shanghai", 
        description="可选参数。IANA时区名称，例如 'Asia/Shanghai', 'America/New_York', 'UTC'. 默认为 'Asia/Shanghai'."
    )

class GetCurrentTimeTool(BaseTool):
    name: str = "get_current_time_tool"
    description: str = (
        "【时间工具】获取并返回当前的日期和时间。可以指定时区（例如 'Asia/Shanghai', 'America/New_York', 'UTC'），"
        "如果未指定，则默认为 'Asia/Shanghai' (中国标准时间)。"
        "当你需要知道“现在几点了”、“今天是什么日期”或在执行与时间相关的操作（如设置提醒）前获取基准时间时使用。"
    )
    args_schema: Type[BaseModel] = GetCurrentTimeToolInput

    def _run(self, timezone_str: Optional[str] = "Asia/Shanghai", **kwargs: Any) -> str:
        logger.info(f"GetCurrentTimeTool._run called with timezone_str: '{timezone_str}'")
        try:
            if not timezone_str or not timezone_str.strip():
                effective_timezone_str = "Asia/Shanghai"
                logger.info(f"Timezone was empty, defaulting to {effective_timezone_str}")
            else:
                effective_timezone_str = timezone_str

            target_tz = pytz.timezone(effective_timezone_str)
            now_in_tz = datetime.now(target_tz)
            formatted_time = now_in_tz.strftime("%Y-%m-%d %H:%M:%S %Z%z")
            logger.info(f"Current time in {effective_timezone_str}: {formatted_time}")
            return f"当前时间 ({effective_timezone_str}): {formatted_time}"
        except pytz.exceptions.UnknownTimeZoneError:
            logger.error(f"Unknown timezone provided: '{timezone_str}'")
            return f"错误：提供的时区 '{timezone_str}' 无效。请使用标准的IANA时区名称。"
        except Exception as e:
            logger.error(f"Error in GetCurrentTimeTool: {e}", exc_info=True)
            return f"获取当前时间时发生错误: {str(e)}"
    --- END OF FILE CONTENT ---


Directory: core/llms/
  File: __init__.py
    --- File is empty ---

  File: custom_litellm_wrapper.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/core/llms/custom_litellm_wrapper.py

import os
import json
import traceback
from typing import List, Dict, Any, Optional, Union, Sequence

import litellm
from crewai.llms.base_llm import BaseLLM as CrewAIBaseLLM
from crewai.tools import BaseTool # 用于类型提示

# --- 日志记录 ---
import logging
logger = logging.getLogger(__name__)
# logger.setLevel(logging.DEBUG) # 可以根据需要调整日志级别
# if not logger.hasHandlers():
#     handler = logging.StreamHandler()
#     formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
#     handler.setFormatter(formatter)
#     logger.addHandler(handler)

class CustomLiteLLMWrapper(CrewAIBaseLLM):
    """
    一个通用的 CrewAI LLM 包装器，通过 LiteLLM 调用各种 LLM 服务。
    能够处理本地 OpenAI 兼容的端点和通过 LiteLLM 支持的云端模型。
    """
    model_name: str # LiteLLM 使用的模型名称，例如 "gemini/gemini-1.5-flash-latest" 或 "local/qwen3-1.7b-gguf"
    api_base: Optional[str] = None
    api_key: Optional[str] = None
    custom_llm_provider: Optional[str] = None # 例如 "openai" 用于本地 OpenAI 兼容服务
    
    # LiteLLM 支持的额外参数，例如 temperature, max_tokens, top_p 等
    # 这些参数可以在实例化时传入，或者在调用 call 方法时覆盖
    litellm_params: Dict[str, Any] = {}

    # CrewAI 工具相关的参数
    tool_config: Optional[Dict[str, Any]] = None # 用于控制工具调用的模式，例如 Gemini 的 function_calling_config
    _cached_tools_for_litellm: Optional[List[Dict[str, Any]]] = None # 缓存转换后的工具定义

    def __init__(
        self,
        model: str, # CrewAI BaseLLM 需要 model 参数
        api_base: Optional[str] = None,
        api_key: Optional[str] = None,
        custom_llm_provider: Optional[str] = None,
        temperature: float = 0.7, # 默认温度
        max_tokens: Optional[int] = 2048, # 默认最大 token 数
        top_p: Optional[float] = None,
        stop_sequences: Optional[List[str]] = None,
        tool_config: Optional[Dict[str, Any]] = None, # 例如 {"function_calling_config": {"mode": "AUTO"}}
        agent_tools: Optional[List[BaseTool]] = None, # CrewAI Agent 的工具列表，用于转换为 LiteLLM 格式
        **kwargs: Any # 其他传递给 LiteLLM 的参数
    ):
        super().__init__(model=model) # 调用父类的构造函数

        # --- 新增日志 ---
        logger.info(f"CustomLiteLLMWrapper __init__ for '{model}': Received agent_tools type: {type(agent_tools)}")
        if agent_tools is not None:
            logger.info(f"CustomLiteLLMWrapper __init__ for '{model}': agent_tools content (names): {[tool.name for tool in agent_tools if hasattr(tool, 'name')]}")
        else:
            logger.info(f"CustomLiteLLMWrapper __init__ for '{model}': agent_tools is None.")
        # --- 结束新增日志 ---
        
        self.model_name = model
        self.api_base = api_base
        self.api_key = api_key
        self.custom_llm_provider = custom_llm_provider
        
        self.litellm_params = {
            "temperature": temperature,
            "max_tokens": max_tokens,
            "top_p": top_p,
            "stop": stop_sequences,
            **kwargs # 合并其他未知参数
        }
        # 清理 litellm_params 中的 None 值
        self.litellm_params = {k: v for k, v in self.litellm_params.items() if v is not None}

        self.tool_config = tool_config
        if agent_tools:
            self._cached_tools_for_litellm = self._convert_crewai_tools_to_litellm_format(agent_tools)
            logger.info(f"CustomLiteLLMWrapper for '{self.model_name}': Cached {len(self._cached_tools_for_litellm)} tools.")
        else:
            logger.info(f"CustomLiteLLMWrapper for '{self.model_name}': No agent_tools provided for caching.")

    def _remove_unwanted_fields_from_schema(self, schema: Dict[str, Any]) -> Dict[str, Any]:
        """
        递归移除 Pydantic schema 中可能导致某些 LLM (如 Gemini) 报错的字段，
        例如顶层的 'title' 和属性定义中的 'default'。
        也确保 'object' 类型的 schema 包含 'properties' 键。
        """
        if not isinstance(schema, dict):
            return schema

        schema.pop('title', None) # 移除顶层 title

        if "properties" in schema and isinstance(schema["properties"], dict):
            if "type" not in schema: # 确保 object 类型有 type 字段
                schema["type"] = "object"
            for prop_name, prop_def in list(schema["properties"].items()): # 使用 list 进行迭代以允许修改
                if isinstance(prop_def, dict):
                    prop_def.pop('default', None) # 移除属性的 default
                    prop_def.pop('title', None)   # 移除属性的 title
                    self._remove_unwanted_fields_from_schema(prop_def) # 递归处理嵌套 schema
        elif schema.get("type") == "object" and "properties" not in schema:
            # 如果是 object 类型但没有 properties，某些 LLM (如 Gemini) 会报错
            schema["properties"] = {}

        # 移除顶层的 default (如果存在且不应该存在于顶层)
        # 通常 default 应该在属性级别，但以防万一
        # schema.pop('default', None) # 这个可能过于激进，先注释掉

        # 递归处理其他嵌套字典
        for key, value in schema.items():
            if isinstance(value, dict):
                self._remove_unwanted_fields_from_schema(value)
            elif isinstance(value, list):
                for i, item in enumerate(value):
                    if isinstance(item, dict):
                        value[i] = self._remove_unwanted_fields_from_schema(item)
        return schema
        
    def _convert_crewai_tools_to_litellm_format(self, tools: Optional[List[BaseTool]]) -> Optional[List[Dict[str, Any]]]:
        """
        将 CrewAI 的 BaseTool 列表转换为 LiteLLM期望的 "tools" 格式。
        LiteLLM 的格式通常与 OpenAI 的 function calling 格式类似。
        """
        if not tools:
            return None
        
        litellm_tool_definitions = []
        for tool_instance in tools:
            tool_name = tool_instance.name
            tool_description = tool_instance.description
            
            parameters_schema: Dict[str, Any]
            if not hasattr(tool_instance, 'args_schema') or not tool_instance.args_schema:
                # 如果工具没有定义参数 schema，则默认为一个没有参数的 object 类型
                parameters_schema = {"type": "object", "properties": {}}
            else:
                try:
                    # Pydantic V2 使用 model_json_schema(), V1 使用 schema()
                    if hasattr(tool_instance.args_schema, 'model_json_schema'):
                        pydantic_schema = tool_instance.args_schema.model_json_schema()
                    else:
                        pydantic_schema = tool_instance.args_schema.schema() # type: ignore
                    
                    # 清理 schema，移除 'title' 和 'default' 等字段
                    cleaned_schema = self._remove_unwanted_fields_from_schema(pydantic_schema.copy())
                    parameters_schema = cleaned_schema
                except Exception as e:
                    logger.error(f"Error processing schema for tool {tool_name}: {e}. Defaulting to empty params.")
                    parameters_schema = {"type": "object", "properties": {}}
            
            litellm_tool_definitions.append({
                "type": "function", # LiteLLM/OpenAI 的标准类型
                "function": {
                    "name": tool_name,
                    "description": tool_description,
                    "parameters": parameters_schema
                }
            })
        logger.debug(f"Converted CrewAI tools to LiteLLM format: {json.dumps(litellm_tool_definitions, indent=2)}")
        return litellm_tool_definitions

    def call(self, messages: List[Dict[str, str]], tools: Optional[List[Dict[str, Any]]] = None, **kwargs: Any) -> str:
        """
        使用 LiteLLM 调用 LLM。
        CrewAI 的 BaseLLM.call 方法期望返回一个字符串。
        """
        logger.info(f"CustomLiteLLMWrapper.call for '{self.model_name}' invoked.")
        logger.debug(f"  Messages: {json.dumps(messages, indent=2, ensure_ascii=False)}")
        logger.debug(f"  Tools provided to call: {'Yes' if tools else 'No'}")
        logger.debug(f"  kwargs: {kwargs}")

        # 合并参数，调用时传入的 kwargs 优先级更高
        current_litellm_params = {**self.litellm_params, **kwargs}

        litellm_call_args: Dict[str, Any] = {
            "model": self.model_name,
            "messages": messages,
            "api_base": self.api_base,
            "api_key": self.api_key,
            "custom_llm_provider": self.custom_llm_provider,
            **current_litellm_params # temperature, max_tokens, etc.
        }

        # 处理工具（函数调用）
        # CrewAI 在调用 Agent 的 LLM 时，会根据 Agent 的 tools 属性自动准备 tools 参数
        # 我们需要确保这里的 tools 参数与 LiteLLM 兼容
        final_tools_for_litellm = tools # 直接使用 CrewAI 传递过来的 tools
        if not final_tools_for_litellm and self._cached_tools_for_litellm:
            logger.info("  No tools passed to call, using cached tools for LiteLLM.")
            final_tools_for_litellm = self._cached_tools_for_litellm
        
        if final_tools_for_litellm:
            litellm_call_args["tools"] = final_tools_for_litellm
            # 根据 self.tool_config 设置 tool_choice
            if self.tool_config and "function_calling_config" in self.tool_config:
                fc_config = self.tool_config["function_calling_config"]
                mode = fc_config.get("mode", "AUTO").upper() # "AUTO", "ANY", "NONE"
                
                if mode == "ANY" and fc_config.get("allowed_function_names"):
                    # 对于 Gemini，如果 mode 是 ANY 且指定了函数名，tool_choice 应该是一个特定结构
                    # 对于 OpenAI，tool_choice 可以是 {"type": "function", "function": {"name": "my_function"}}
                    # LiteLLM 会尝试适配，我们先按 OpenAI 的方式设置
                    litellm_call_args["tool_choice"] = {
                        "type": "function", 
                        "function": {"name": fc_config["allowed_function_names"][0]} # 假设只取第一个
                    }
                elif mode in ["AUTO", "ANY", "NONE"]: # ANY 在没有指定函数名时，行为类似 AUTO
                    litellm_call_args["tool_choice"] = mode.lower()
                else: # 默认为 auto
                    litellm_call_args["tool_choice"] = "auto"
                logger.debug(f"  Setting tool_choice to: {litellm_call_args['tool_choice']}")
            else: # 如果没有 tool_config，默认 tool_choice 为 auto
                 litellm_call_args["tool_choice"] = "auto"
                 logger.debug(f"  No tool_config, defaulting tool_choice to 'auto'.")


        # 移除值为 None 的参数，因为 litellm.completion 不喜欢 None 的 api_key 等
        litellm_call_args_cleaned = {k: v for k, v in litellm_call_args.items() if v is not None}

        # --- 新增：如果 api_base 为 None (表示直接调用云端模型)，则尝试使用 LITELLM_PROXY_URL ---
        if self.api_base is None:
            local_proxy_url = os.getenv("LITELLM_PROXY_URL")
            if local_proxy_url:
                litellm_call_args_cleaned["proxy"] = { # LiteLLM 的 proxy 参数期望一个字典
                    "http": local_proxy_url,
                    "https": local_proxy_url,
                }
                logger.info(f"  Using local proxy for direct cloud call: {local_proxy_url}")
            else:
                logger.info("  api_base is None, but LITELLM_PROXY_URL is not set. Proceeding without proxy.")
        # --- 结束新增代理逻辑 ---
        
        logger.info(f"  Attempting to call litellm.completion for model '{self.model_name}'...")
        # 在打印参数前确保 proxy 参数也被包含（如果设置了）
        debug_params_to_print = {k: v for k, v in litellm_call_args_cleaned.items() if k != 'messages'}
        if "proxy" in litellm_call_args_cleaned: # 确保打印时能看到 proxy
            debug_params_to_print["proxy_used"] = litellm_call_args_cleaned["proxy"]
            
        logger.debug(f"  LiteLLM Call Args (cleaned, messages excluded, proxy shown if used): {debug_params_to_print}")
        
        response = None 
        try:
            response = litellm.completion(**litellm_call_args_cleaned) 
            logger.info(f"  litellm.completion call for '{self.model_name}' succeeded.") # <--- 添加日志
            logger.debug(f"  LiteLLM Raw Response object type: {type(response)}")
            if hasattr(response, 'model_dump_json'):
                logger.debug(f"  LiteLLM Raw Response (JSON): {response.model_dump_json(indent=2)}")
            else:
                logger.debug(f"  LiteLLM Raw Response (str): {str(response)[:500]}")

            # --- 新增日志，检查原始的 usage ---
            if hasattr(response, 'usage') and response.usage:
                logger.info(f"  DEBUG USAGE (from LiteLLM response object): {response.usage}")
                # 如果 response.usage 是 Pydantic 模型，可以尝试打印其字典形式
                if hasattr(response.usage, 'model_dump'):
                    logger.info(f"  DEBUG USAGE (dict): {response.usage.model_dump()}")
                else:
                    logger.info(f"  DEBUG USAGE (raw object): {response.usage}")
            else:
                logger.warning("  DEBUG USAGE: LiteLLM response object does not have .usage or it's empty.")
            # --- 结束新增 ---

        except Exception as e:
            logger.error(f"LiteLLM completion call FAILED for model '{self.model_name}': {e}", exc_info=True) # <--- 修改日志
            return f"LLM_CALL_ERROR: 调用模型 '{self.model_name}' 失败: {str(e)}"

        # 从 LiteLLM 响应中提取内容或工具调用
        # LiteLLM 的 ModelResponse 结构与 OpenAI 的 ChatCompletion 类似
        llm_message_response = response.choices[0].message
        
        if hasattr(llm_message_response, 'tool_calls') and llm_message_response.tool_calls:
            logger.info(f"  LLM returned structured tool_calls: {llm_message_response.tool_calls}")
            # 构造 ReAct 格式的字符串
            tool_call = llm_message_response.tool_calls[0] # 假设只有一个工具调用
            action = tool_call.function.name
            action_input = tool_call.function.arguments # 这是 JSON 字符串
            
            # Gemini 可能也会在 content 中生成 Thought，如果它遵循 ReAct
            thought_prefix = ""
            if llm_message_response.content and "Thought:" in llm_message_response.content:
                thought_prefix = llm_message_response.content.split("Action:")[0] # 取 Action:之前的部分作为 Thought

            react_string = f"{thought_prefix.strip()}\nAction: {action}\nAction Input: {action_input}"
            logger.info(f"  Constructed ReAct string from tool_calls: {react_string}")
            return react_string.strip() # 返回 ReAct 字符串
        
        elif llm_message_response.content:
            content_str = llm_message_response.content
            logger.info(f"  LLM returned content (first 200 chars): {content_str[:200]}")
            # 如果 content 本身就是 ReAct 格式，也直接返回
            return content_str.strip() # 返回字符串
        
        else:
            logger.warning("  LLM response did not contain structured tool_calls or text content.")
            return ""

    def get_token_ids(self, text: str) -> List[int]:
        """
        获取文本的 token ID 列表。
        LiteLLM 提供了 litellm.encode 和 litellm.decode 方法。
        """
        try:
            # 注意：litellm.encode 可能需要 model 参数来确定使用哪个 tokenizer
            return litellm.encode(model=self.model_name, text=text)
        except Exception as e:
            logger.warning(f"get_token_ids failed for model '{self.model_name}': {e}. Returning empty list.")
            # CrewAI 在某些情况下即使这里返回空列表也能继续，但最好能正确实现
            return []

    # --- 添加这个方法 ---
    @property
    def supports_function_calling(self) -> bool:
        logger.debug(f"CustomLiteLLMWrapper.supports_function_calling() called for model {self.model_name}, returning True.")
        return True
    # --- 添加结束 ---

    # CrewAI 可能还会用到的一些属性
    @property
    def _llm_type(self) -> str:
        return f"custom_litellm_{self.model_name.replace('/', '_')}"

    @property
    def identifying_params(self) -> Dict[str, Any]:
        """返回用于标识此LLM实例的参数字典。"""
        return {
            "model_name": self.model_name,
            "api_base": self.api_base,
            "custom_llm_provider": self.custom_llm_provider,
            **self.litellm_params
        }

# --- 示例用法 (可选，用于测试此文件) ---
async def main_test_wrapper():
    logger.info("--- Testing CustomLiteLLMWrapper ---")

    # 测试本地 Qwen (假设服务在 http://localhost:8088/v1)
    try:
        logger.info("\n--- Testing Local Qwen ---")
        local_qwen_llm = CustomLiteLLMWrapper(
            model="local/qwen3-1.7b-gguf", # 这个名称需要与 LiteLLM 调用时匹配
            api_base="http://localhost:8088/v1",
            api_key="nokey",
            custom_llm_provider="openai",
            temperature=0.1
        )
        messages_qwen = [{"role": "user", "content": "你好，请用中文介绍一下你自己。不要超过50个字。"}]
        # CrewAI 通常是同步调用 call 方法，但我们的 call 内部是同步执行 litellm.completion
        # 如果要测试异步行为，需要 litellm.acompletion 和异步的 call
        response_qwen = local_qwen_llm.call(messages=messages_qwen)
        logger.info(f"Local Qwen Response: {response_qwen}")
        
        # 测试 token_ids
        # token_ids_qwen = local_qwen_llm.get_token_ids("你好，世界")
        # logger.info(f"Token IDs for '你好，世界' from Qwen (via LiteLLM encode): {token_ids_qwen}")

    except Exception as e:
        logger.error(f"Error testing local Qwen: {e}", exc_info=True)

    # 测试云端 Gemini (假设通过配置好的 LiteLLM 网关)
    # 需要设置 CLOUD_LITELLM_GW_API_BASE 环境变量
    # 例如: export CLOUD_LITELLM_GW_API_BASE="http://your-litellm-proxy.com/v1"
    #       export GEMINI_API_KEY="your_actual_gemini_key_if_proxy_doesnt_handle_it_or_proxy_key"
    # CLOUD_LITELLM_GW_API_BASE_TEST = os.getenv("CLOUD_LITELLM_GW_API_BASE_TEST")
    # GEMINI_API_KEY_TEST = os.getenv("GEMINI_API_KEY_TEST") # 或者网关的key

    # if CLOUD_LITELLM_GW_API_BASE_TEST and GEMINI_API_KEY_TEST:
    #     try:
    #         print("\n--- Testing Cloud Gemini via Gateway ---")
    #         gemini_llm = CustomLiteLLMWrapper(
    #             model="gemini/gemini-1.5-flash-latest",
    #             api_base=CLOUD_LITELLM_GW_API_BASE_TEST,
    #             api_key=GEMINI_API_KEY_TEST, # Key for the gateway or Gemini if gateway passes it
    #             temperature=0.5
    #         )
    #         messages_gemini = [{"role": "user", "content": "What is the capital of France?"}]
    #         response_gemini = gemini_llm.call(messages=messages_gemini)
    #         print(f"Cloud Gemini Response: {response_gemini}")
    #     except Exception as e:
    #         print(f"Error testing cloud Gemini: {e}")
    # else:
    #     print("\nSkipping Cloud Gemini test as CLOUD_LITELLM_GW_API_BASE_TEST or GEMINI_API_KEY_TEST is not set.")

if __name__ == "__main__":
    # import asyncio
    # asyncio.run(main_test_wrapper())
    pass
    --- END OF FILE CONTENT ---


--- End of Project Snapshot ---