Project Root: /home/zhz/zhz_agent
--- Project Structure and File Contents ---

Directory: /home/zhz/zhz_agent/
  File (skipped, extension not included): .env
  File (skipped, extension not included): .envrc
  File (skipped, extension not included): .gitignore
  File: README.md
    --- START OF FILE CONTENT ---
# zhz_agent
后端：RAG增强+多agent+dagster

    --- END OF FILE CONTENT ---

  File: __init__.py
    --- File is empty ---

  File: agent_orchestrator_service.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/agent_orchestrator_service.py

import os
import asyncio
import traceback
import json
from typing import List, Dict, Any, Optional, Union
from contextlib import asynccontextmanager
import re
import uvicorn
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from datetime import datetime 

import logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(module)s:%(lineno)d - %(message)s')
service_logger = logging.getLogger(__name__)

from crewai import Agent, Task, Crew, Process
from crewai.tools import BaseTool

# --- 添加开始 ---
from core.prompts.manager_prompts import get_manager_agent_goal, MANAGER_AGENT_BACKSTORY
# --- 添加结束 ---

from core.llm_manager import get_llm_instance, CustomLiteLLMWrapper
from core.tools.enhanced_rag_tool import EnhancedRAGTool
from core.tools.excel_tool import ExcelOperationTool
from core.tools.search_tool import WebSearchTool
from core.tools.time_tool import GetCurrentTimeTool
from core.tools.calculator_tool import CalculateTool


AGENT_SERVICE_PORT = int(os.getenv("AGENT_SERVICE_PORT", 8090))
AGENT_SERVICE_HOST = "0.0.0.0"

class AgentTaskRequest(BaseModel):
    user_query: str = Field(description="用户的原始文本查询。")

class AgentTaskResponse(BaseModel):
    answer: str = Field(description="Agent 系统生成的最终答案或响应。")
    status: str = Field(default="success", description="执行状态: 'success', 'needs_clarification', 'error', 'processing_plan', 'task_created'.")
    intermediate_plan: Optional[Dict[str, Any]] = Field(None, description="如果status是'processing_plan', 这里包含Manager Agent的规划结果。")
    task_id: Optional[str] = Field(None, description="如果任务是异步的，返回任务ID。")
    error_message: Optional[str] = Field(None, description="如果发生错误，此字段包含错误信息。")
    debug_info: Optional[Dict[str, Any]] = Field(None, description="包含执行过程中的调试信息。")
    token_usage: Optional[Dict[str, Any]] = Field(None, description="LLM token 使用情况统计。")

class SubTaskDefinitionForManagerOutput(BaseModel):
    task_description: str = Field(description="用户的原始请求原文。")
    reasoning_for_plan: Optional[str] = Field(None, description="Manager Agent的决策思考过程。")
    selected_tool_names: List[str] = Field(description="选定的工具名称列表。如果直接回答，则为空列表。")
    direct_answer_content: Optional[str] = Field(None, description="如果选择直接回答，这里是答案内容。")
    tool_input_args: Optional[Dict[str, Any]] = Field(None, description="如果选择使用非Excel工具，这里是传递给该工具的参数。")
    excel_sqo_payload: Optional[List[Dict[str, Any]]] = Field(None, description="如果选择使用Excel工具，这里是SQO操作字典的列表。")

manager_llm: Optional[CustomLiteLLMWrapper] = None
worker_llm: Optional[CustomLiteLLMWrapper] = None
manager_agent_instance: Optional[Agent] = None
worker_agent_instance: Optional[Agent] = None
core_tools_instances: List[BaseTool] = []

# --- 覆盖开始 ---
CORE_TOOLS_ZHZ_AGENT = {
    "enhanced_rag_tool": "【核心RAG工具】用于从本地知识库查找信息、回答复杂问题，整合了向量、关键词和图谱检索。",
    "excel_operation_tool": "【Excel操作工具】通过结构化查询对象(SQO)对Excel文件执行复杂的数据查询、筛选、聚合等操作。此工具通过本地代理在Windows上运行。",
    "web_search_tool": "【网络搜索工具】使用DuckDuckGo搜索引擎在互联网上查找与用户查询相关的信息。此工具通过MCP调用。",
    "get_current_time_tool": "【时间工具】获取当前的日期和时间，可指定时区。此工具在Agent的Python环境中直接执行。",
    "calculate_tool": "【计算器工具】执行数学表达式的计算并返回数值结果。此工具在Agent的Python环境中直接执行。"
}
# --- 覆盖结束 ---
CORE_TOOL_NAMES_LIST = list(CORE_TOOLS_ZHZ_AGENT.keys()) # 这行会自动更新
TOOL_OPTIONS_STR_FOR_MANAGER = "\n".join( # 这行会自动更新
    [f"- '{name}': {desc}" for name, desc in CORE_TOOLS_ZHZ_AGENT.items()]
)

@asynccontextmanager
async def lifespan(app: FastAPI):
    global manager_llm, worker_llm, manager_agent_instance, worker_agent_instance, core_tools_instances
    # --- [修改] 为了让 Manager 的 prompt 也只看到 RAG 工具，我们需要在函数作用域内临时修改这些 ---
    global CORE_TOOL_NAMES_LIST, TOOL_OPTIONS_STR_FOR_MANAGER 
    
    print("--- Agent Orchestrator Service: Lifespan startup ---")

    # --- 步骤 1: 初始化核心工具实例 ---
    print("Initializing core tool instances...")
    enhanced_rag_tool_instance = None 
    excel_operation_tool_instance = None
    web_search_tool_instance = None
    get_current_time_tool_instance = None
    calculate_tool_instance = None
    try:
        enhanced_rag_tool_instance = EnhancedRAGTool()
        excel_operation_tool_instance = ExcelOperationTool() # 尝试实例化
        web_search_tool_instance = WebSearchTool()       # 尝试实例化
        get_current_time_tool_instance = GetCurrentTimeTool()
        calculate_tool_instance = CalculateTool()

        core_tools_instances = []
        if enhanced_rag_tool_instance:
            core_tools_instances.append(enhanced_rag_tool_instance)
        if get_current_time_tool_instance:
            core_tools_instances.append(get_current_time_tool_instance)
        if calculate_tool_instance:
            core_tools_instances.append(calculate_tool_instance)
        if excel_operation_tool_instance:
            core_tools_instances.append(excel_operation_tool_instance)
        if web_search_tool_instance:
            core_tools_instances.append(web_search_tool_instance)

        if not core_tools_instances:
            print("CRITICAL ERROR: No core tools could be initialized. Aborting LLM/Agent setup.")
            # 在这种情况下，后续的LLM和Agent初始化可能会失败或无意义
            # 可以考虑在这里抛出异常或设置一个全局状态阻止服务启动
        else:
            print(f"Successfully initialized tools: {[tool.name for tool in core_tools_instances]}")
        print(f"--- FULL TOOL CONFIGURATION WILL BE USED BY MANAGER (if successfully initialized) ---")
        print(f"Effective CORE_TOOL_NAMES_LIST for Manager (based on global def): {CORE_TOOL_NAMES_LIST}") # 这是全局的
        print(f"Effective TOOL_OPTIONS_STR_FOR_MANAGER for Manager (based on global def):\n{TOOL_OPTIONS_STR_FOR_MANAGER}") # 这是全局的
        print(f"Actually initialized tools for Worker: {[tool.name for tool in core_tools_instances]}")
        print(f"--- END OF TOOL CONFIGURATION ---")
        
    except Exception as e:
        print(f"ERROR during core tool initialization: {e}", exc_info=True)
        core_tools_instances = [] # 确保出错时为空列表
        # 后续LLM/Agent初始化时，如果 core_tools_instances 为空，它们可能需要特殊处理或报错

    # ... 后续的LLM和Agent初始化代码 ...
    # 确保 Manager Agent 的 tools=[] (它不直接调用工具)
    # 确保 Worker Agent 的 tools=core_tools_instances (它需要所有可用的工具实例)


    # --- 步骤 2: 初始化 LLM 实例 ---
    # LLM 初始化时，agent_tools 参数将使用上面步骤中已更新（且只包含RAG工具）的 core_tools_instances
    print("Initializing LLM instances...")
    try:
        gemini_tool_config = {"function_calling_config": {"mode": "AUTO"}}
        # Manager LLM 初始化
        manager_llm = get_llm_instance(
            llm_type="cloud_gemini", 
            temperature=0.1, 
            max_tokens=4096, 
            tool_config=gemini_tool_config,
            agent_tools=core_tools_instances # 使用已更新的 core_tools_instances
        )
        if not manager_llm:
            print("Failed to initialize Manager LLM (Cloud Gemini). Attempting fallback...")
            manager_llm = get_llm_instance(
                llm_type="local_qwen", 
                temperature=0.1, 
                max_tokens=3072, 
                tool_config=gemini_tool_config,
                agent_tools=core_tools_instances 
            )
        
        # Worker LLM 初始化
        print("Initializing Worker LLM (attempting Cloud Gemini first)...")
        worker_gemini_tool_config = {"function_calling_config": {"mode": "AUTO"}} 
        worker_llm = get_llm_instance(
            llm_type="cloud_gemini", 
            temperature=0.5, 
            max_tokens=3072,
            tool_config=worker_gemini_tool_config,
            agent_tools=core_tools_instances # 使用已更新的 core_tools_instances
        )
        if not worker_llm:
            print("Failed to initialize Worker LLM (Cloud Gemini). Attempting fallback to local_qwen...")
            worker_llm = get_llm_instance(
                llm_type="local_qwen", 
                temperature=0.6, 
                max_tokens=3072,
                agent_tools=core_tools_instances
            )

        if manager_llm: print(f"Manager LLM initialized: {manager_llm.model_name}")
        else: print("CRITICAL: Failed to initialize Manager LLM.")
        if worker_llm: print(f"Worker LLM initialized: {worker_llm.model_name}")
        else: print("CRITICAL: Failed to initialize Worker LLM.")
    except Exception as e:
        print(f"FATAL ERROR during LLM initialization: {e}") 
        traceback.print_exc() 
        manager_llm = None; worker_llm = None

    # --- 步骤 3: 初始化 Agent 实例 ---
    if manager_llm:
        current_manager_goal = get_manager_agent_goal(TOOL_OPTIONS_STR_FOR_MANAGER)
        manager_agent_instance = Agent(
        role='资深AI任务分解与Excel查询规划师 (Senior AI Task Decomposition and Excel Query Planner)',
        goal=current_manager_goal,
        backstory=MANAGER_AGENT_BACKSTORY,
        llm=manager_llm,
        verbose=True,
        allow_delegation=False,
        tools=[] 
    )
        print(f"Manager Agent initialized with LLM: {manager_llm.model_name}")
        # --- 添加的调试日志开始 ---
        if manager_agent_instance and manager_agent_instance.llm:
            print(f"DEBUG_LIFESPAN: Manager LLM type: {type(manager_agent_instance.llm)}")
            print(f"DEBUG_LIFESPAN: Is manager_llm.supports_function_calling callable? {callable(getattr(manager_agent_instance.llm, 'supports_function_calling', None))}")
            if callable(getattr(manager_agent_instance.llm, 'supports_function_calling', None)):
                print(f"DEBUG_LIFESPAN: manager_llm.supports_function_calling() returns: {manager_agent_instance.llm.supports_function_calling()}")
            else:
                print(f"DEBUG_LIFESPAN: manager_llm.supports_function_calling is NOT callable. Value: {getattr(manager_agent_instance.llm, 'supports_function_calling', 'Attribute not found')}")
        # --- 添加的调试日志结束 ---

    if worker_llm:
        worker_agent_instance = Agent(
            role='任务执行专家 (Task Execution Expert)',
            goal="根据Manager分配的具体任务描述和指定的工具，高效地执行任务并提供结果。",
            backstory="""我是一个AI执行者，专注于使用【Manager明确授权给我的工具】来解决问题。
                        我会严格遵循任务指令。如果任务是调用Excel工具并提供了SQO列表，我会按顺序迭代处理这些SQO，并整合结果。
                        对于像 'get_current_time_tool' 和 'calculate_tool' 这样的本地Python工具，我会直接在我的环境中执行它们。
                        对于其他工具，我会使用工具的名称（例如 'enhanced_rag_tool', 'excel_operation_tool', 'web_search_tool'）来调用它们。""",
            llm=worker_llm,
            verbose=True,
            allow_delegation=False,
            tools=core_tools_instances # Worker Agent 使用已更新（只含RAG工具）的 core_tools_instances
        )
        print(f"Worker Agent initialized with LLM: {worker_llm.model_name} and tools: {[t.name for t in worker_agent_instance.tools]}")

    if not manager_agent_instance or not worker_agent_instance:
        print("CRITICAL: One or more core agents failed to initialize. Service functionality will be severely limited.")
    elif not core_tools_instances and worker_agent_instance : 
        print("WARNING: Worker Agent initialized, but no core tools were successfully instantiated. Tool-based tasks will fail.")

    print("--- Agent Orchestrator Service: Lifespan startup complete ---")
    yield
    print("--- Agent Orchestrator Service: Lifespan shutdown ---")

app = FastAPI(
    title="Agent Orchestrator Service",
    description="接收用户请求，通过Manager/Worker Agent模型进行任务规划和执行。",
    version="0.1.1", # 版本更新
    lifespan=lifespan
)

@app.post("/v1/execute_task", response_model=AgentTaskResponse)
async def execute_task_endpoint(request: AgentTaskRequest):
    global manager_agent_instance, worker_agent_instance, core_tools_instances # 确保能访问全局 Agent 和工具实例
    
    print(f"Received agent task request: User Query='{request.user_query}'") # 使用 print 替代 logger

    if not manager_agent_instance or not worker_agent_instance:
        print("CRITICAL ERROR: Core agents are not initialized. Cannot process task.")
        raise HTTPException(status_code=503, detail="Service not ready: Core agents failed to initialize.")
    if not core_tools_instances:
        print("WARNING: Core tools are not initialized. Tool-based tasks may fail.")
    # --- 构建 Manager Task 的描述 ---
    # 我们将用户请求和可用的核心工具列表传递给 Manager Agent
    # Manager Agent 的 goal 和 backstory 已经包含了大部分指令
    # Task 的 description 主要用于传递动态信息，如当前用户查询
    manager_task_description_for_crewai = f"""
    用户的原始请求是：'{request.user_query}'

    你的任务是：
    1.  仔细分析此用户请求的意图。
    2.  严格回顾并遵循你在 `goal` 中被赋予的【决策规则与优先级】以及【决策示例】。
    3.  基于这些规则和示例，决定最佳的处理路径：是直接回答，还是选择一个最合适的工具。
    4.  如果选择了工具，请准备好调用该工具所需的参数。
    5.  严格按照 `SubTaskDefinitionForManagerOutput` 的JSON格式输出你的规划。`task_description` 字段必须是用户的原始请求原文: '{request.user_query}'。同时提供你的 `reasoning_for_plan`。
    """
    
    # --- Manager Task 的期望输出格式说明 ---
    manager_task_expected_output_description = f"""一个JSON对象，必须严格符合以下Pydantic模型的结构（你不需要输出 "SubTaskDefinitionForManagerOutput" 这个词本身）：
    {{
      "task_description": "string (固定为用户的原始请求: '{request.user_query}')",
      "reasoning_for_plan": "string (你的决策思考过程)",
      "selected_tool_names": ["list of strings (选定的工具名称列表。如果直接回答，则为空列表。如果使用工具，则为 ['enhanced_rag_tool'])"],
      "direct_answer_content": "string (可选, 仅当 selected_tool_names 为空列表时，这里是你的答案内容)",
      "tool_input_args": {{ "key": "value" }} (可选, 仅当 selected_tool_names 包含'enhanced_rag_tool'时，这里是给该工具的参数字典),
      "excel_sqo_payload": null # 当前Excel工具不可用，此字段应为null
    }}

    【重要输出规则】:
    - 如果你选择【直接回答】：`selected_tool_names` 必须是空列表 `[]`，`direct_answer_content` 必须包含你的答案，`tool_input_args` 和 `excel_sqo_payload` 应该为 `null` 或不存在。
    - 如果你选择使用【enhanced_rag_tool】：`selected_tool_names` 必须包含 `"enhanced_rag_tool"`，`direct_answer_content` 应该为 `null` 或不存在，`tool_input_args` 必须包含调用该工具所需的参数 (例如 `{{ "query": "{request.user_query}" }}` )，`excel_sqo_payload` 应该为 `null` 或不存在。

    示例输出 (直接回答):
    {{
      "task_description": "{request.user_query}",
      "reasoning_for_plan": "这是一个常识性问题，我可以根据我的内部知识直接回答。",
      "selected_tool_names": [],
      "direct_answer_content": "中国的首都是北京。",
      "tool_input_args": null,
      "excel_sqo_payload": null
    }}

    示例输出 (使用RAG工具):
    {{
      "task_description": "{request.user_query}",
      "reasoning_for_plan": "用户询问关于公司政策的问题，这需要从知识库中查找。",
      "selected_tool_names": ["enhanced_rag_tool"],
      "direct_answer_content": null,
      "tool_input_args": {{"query": "{request.user_query}", "top_k_vector": 5, "top_k_kg": 3, "top_k_bm25": 3}},
      "excel_sqo_payload": null
    }}
    请严格按照此JSON格式输出。
    """

    manager_task = Task(
        description=manager_task_description_for_crewai,
        expected_output=manager_task_expected_output_description,
        agent=manager_agent_instance,
        async_execution=False,
        output_pydantic=SubTaskDefinitionForManagerOutput
    )

    # --- 步骤 2: 执行 Manager Task ---
    print("Executing Manager Task...")
    manager_plan_object: Optional[SubTaskDefinitionForManagerOutput] = None
    manager_raw_output: Optional[str] = None

    try:
        manager_crew = Crew(
            agents=[manager_agent_instance],
            tasks=[manager_task],
            process=Process.sequential,
            verbose=True 
        )
        print(f"[{datetime.now()}] About to call manager_crew.kickoff()...") # <--- 添加日志
        manager_task_raw_result = manager_crew.kickoff(inputs={})
        print(f"[{datetime.now()}] manager_crew.kickoff() returned.") # <--- 添加日志

        # --- 新的解析逻辑，处理 CrewOutput ---
        actual_task_output: Optional[Any] = None # 用于存储实际的 TaskOutput 或字符串

        if hasattr(manager_task_raw_result, 'tasks_output') and isinstance(manager_task_raw_result.tasks_output, list) and manager_task_raw_result.tasks_output:
            # CrewOutput.tasks_output 是一个 TaskOutput 对象的列表
            actual_task_output = manager_task_raw_result.tasks_output[0] # 我们只有一个 manager_task
            print(f"Extracted TaskOutput from CrewOutput: {type(actual_task_output)}")
        elif hasattr(manager_task_raw_result, 'raw_output'): # 兼容直接返回 TaskOutput 的情况 (旧版或特定配置)
            actual_task_output = manager_task_raw_result
            print(f"kickoff returned a TaskOutput-like object directly: {type(actual_task_output)}")
        elif isinstance(manager_task_raw_result, str): # 直接返回字符串
            actual_task_output = manager_task_raw_result
            print(f"kickoff returned a raw string.")
        else:
            print(f"Warning: manager_crew.kickoff() returned an unexpected type: {type(manager_task_raw_result)}")
            actual_task_output = str(manager_task_raw_result)
        # --- 结束新的解析逻辑 ---


        # --- 后续的解析逻辑，现在基于 actual_task_output ---
        print(f"DEBUG: Type of actual_task_output: {type(actual_task_output)}")
        
        manager_raw_llm_output_str: Optional[str] = None
        if hasattr(actual_task_output, 'raw') and isinstance(actual_task_output.raw, str):
            manager_raw_llm_output_str = actual_task_output.raw.strip()
            print(f"DEBUG: actual_task_output.raw (LLM's original string output) IS:\n---\n{manager_raw_llm_output_str}\n---")
        else:
            print(f"DEBUG: actual_task_output.raw is not a string or does not exist. Value: {getattr(actual_task_output, 'raw', 'Attribute .raw not found')}")
            manager_raw_llm_output_str = str(actual_task_output) # Fallback

        # 优先尝试使用 CrewAI 已经解析好的 Pydantic 对象
        if hasattr(actual_task_output, 'pydantic_output') and \
           actual_task_output.pydantic_output is not None and \
           isinstance(actual_task_output.pydantic_output, SubTaskDefinitionForManagerOutput):
            print("DEBUG: Successfully using actual_task_output.pydantic_output.")
            manager_plan_object = actual_task_output.pydantic_output
        
        # 如果 Pydantic 对象不可用，但我们从 .raw 成功获取了字符串，则尝试解析它
        # 并且确保 manager_plan_object 之前没有被成功赋值
        elif manager_plan_object is None and manager_raw_llm_output_str: # <--- 添加 manager_plan_object is None 条件
            print(f"DEBUG: pydantic_output not available. Attempting to parse string from actual_task_output.raw:\n---\n{manager_raw_llm_output_str}\n---")
            json_to_parse_from_raw = manager_raw_llm_output_str # 使用我们从 .raw 获取的
            try:
                final_answer_marker = "## Final Answer:" # 虽然日志显示Qwen没输出这个，但保留以防万一
                if final_answer_marker in json_to_parse_from_raw:
                    json_to_parse_from_raw = json_to_parse_from_raw.split(final_answer_marker, 1)[-1].strip()
                
                cleaned_json_str = json_to_parse_from_raw.strip()
                if cleaned_json_str.startswith("```json"): cleaned_json_str = cleaned_json_str[len("```json"):].strip()
                if cleaned_json_str.endswith("```"): cleaned_json_str = cleaned_json_str[:-len("```")].strip()
                
                if not cleaned_json_str: raise ValueError("Cleaned JSON string (from .raw) is empty.")
                manager_plan_object = SubTaskDefinitionForManagerOutput(**json.loads(cleaned_json_str))
                print(f"Parsed Manager Plan (from actual_task_output.raw): {manager_plan_object.model_dump_json(indent=2)}")
            except (json.JSONDecodeError, ValueError, Exception) as e:
                print(f"Error parsing JSON from actual_task_output.raw: {e}. Raw content used: {json_to_parse_from_raw}")
        
        # 在所有尝试之后，如果 manager_plan_object 仍然是 None，才报告最终的解析失败
        if not manager_plan_object:
             # manager_raw_output 现在应该引用我们尝试过的 manager_raw_llm_output_str
             final_raw_output_for_debug = manager_raw_llm_output_str if manager_raw_llm_output_str else str(actual_task_output)
             print(f"Manager Task did not produce a valid Pydantic object after all attempts. Final Raw for debug: {final_raw_output_for_debug}")
             return AgentTaskResponse(answer="无法解析Manager Agent的规划结果。", status="error",
                                      error_message="Failed to parse manager plan after all attempts.",
                                      debug_info={"manager_raw_output_tried": final_raw_output_for_debug})
        
        # 如果 manager_plan_object 成功解析，则打印日志（这行已经在您的代码中）
        print(f"Parsed Manager Plan: {manager_plan_object.model_dump_json(indent=2)}")
        
    except Exception as e:
        print(f"Error executing Manager Task or its Crew: {e}") # <--- 修改后的第一行
        traceback.print_exc() # <--- 修改后的第二行
        return AgentTaskResponse(
            answer="执行Manager Agent任务时发生错误。", status="error", error_message=str(e),
            debug_info={"traceback": traceback.format_exc(), "manager_raw_output": manager_raw_output}
        )

    # --- 步骤 2: 根据 Manager 的规划执行后续操作 ---
    if not manager_plan_object.selected_tool_names and manager_plan_object.direct_answer_content:
        print(f"Manager decided to answer directly. Answer: {manager_plan_object.direct_answer_content}")
        return AgentTaskResponse(
            answer=manager_plan_object.direct_answer_content,
            status="success",
            debug_info={"manager_plan": manager_plan_object.model_dump()}
        )
    elif not manager_plan_object.selected_tool_names and not manager_plan_object.direct_answer_content:
        # 如果 Manager 既没有选择工具，也没有提供直接答案，这可能是一个规划错误
         print(f"Manager Warning: No tool selected and no direct answer provided. Reasoning: {manager_plan_object.reasoning_for_plan}")
         return AgentTaskResponse(
            answer=manager_plan_object.reasoning_for_plan or "Manager 未能提供明确的行动计划或答案。",
            status="success", # 或者 "error" 取决于您如何定义这种情况
            debug_info={"manager_plan": manager_plan_object.model_dump()}
        )

    # --- 步骤 3: 如果 Manager 规划使用工具，则创建并执行 Worker Task ---
    selected_tool_name = manager_plan_object.selected_tool_names[0] if manager_plan_object.selected_tool_names else None

    if not selected_tool_name:
        return AgentTaskResponse(
            answer="Manager规划使用工具但未在selected_tool_names中指定工具名称。", status="error",
            error_message="Tool name missing in manager plan's selected_tool_names.",
            debug_info={"manager_plan": manager_plan_object.model_dump()}
        )

    # 找到对应的工具实例
    tool_instance_for_worker = next((tool for tool in core_tools_instances if tool.name == selected_tool_name), None)

    if not tool_instance_for_worker:
        return AgentTaskResponse(
            answer=f"系统中未找到Manager规划使用的工具: {selected_tool_name}。", status="error",
            error_message=f"Tool '{selected_tool_name}' not found in core_tools_instances.",
            debug_info={"manager_plan": manager_plan_object.model_dump(), "available_tools": [t.name for t in core_tools_instances]}
        )

    print(f"Manager planned to use tool: {selected_tool_name}. Instance found: {tool_instance_for_worker is not None}")

    worker_task_description = ""
    worker_task_inputs = {} 

    if selected_tool_name == "enhanced_rag_tool":
        rag_query = manager_plan_object.tool_input_args.get("query", request.user_query) if manager_plan_object.tool_input_args else request.user_query
        top_k_v = manager_plan_object.tool_input_args.get("top_k_vector", 5) if manager_plan_object.tool_input_args else 5
        top_k_kg = manager_plan_object.tool_input_args.get("top_k_kg", 3) if manager_plan_object.tool_input_args else 3
        top_k_b = manager_plan_object.tool_input_args.get("top_k_bm25", 3) if manager_plan_object.tool_input_args else 3
        
        worker_task_description = f"请使用增强RAG工具回答以下问题：'{rag_query}'。\n使用以下参数进行检索：top_k_vector={top_k_v}, top_k_kg={top_k_kg}, top_k_bm25={top_k_b}。"
        worker_task_inputs = {
            "query": rag_query, 
            "top_k_vector": top_k_v, 
            "top_k_kg": top_k_kg, 
            "top_k_bm25": top_k_b
        }

    elif selected_tool_name == "excel_operation_tool":
        excel_sqo_list = manager_plan_object.excel_sqo_payload # excel_sqo_list 在这里赋值

        # --- 添加调试打印开始 ---
        print(f"DEBUG_EXCEL_TOOL: Type of manager_plan_object.excel_sqo_payload: {type(manager_plan_object.excel_sqo_payload)}")
        print(f"DEBUG_EXCEL_TOOL: Value of manager_plan_object.excel_sqo_payload: {manager_plan_object.excel_sqo_payload}")
        print(f"DEBUG_EXCEL_TOOL: Value of excel_sqo_list: {excel_sqo_list}")
        print(f"DEBUG_EXCEL_TOOL: Is excel_sqo_list None? {excel_sqo_list is None}")
        print(f"DEBUG_EXCEL_TOOL: Is excel_sqo_list an empty list? {isinstance(excel_sqo_list, list) and not excel_sqo_list}")
        # --- 添加调试打印结束 ---

        if not excel_sqo_list:
            return AgentTaskResponse(
                answer="Manager选择Excel工具但未提供SQO列表。", status="error",
                error_message="excel_sqo_payload is missing for excel_operation_tool.",
                debug_info={"manager_plan": manager_plan_object.model_dump()}
            )
        
        # --- 测试用硬编码 ---
        test_excel_file_path_on_windows = r"C:\\FlutterProjects\\data\\test2.xlsx" # 确保这个路径在Windows上是有效的
        test_excel_sheet_name: Union[str, int] = "Sheet1"
        # --- 测试用硬编码结束 ---
        
        worker_task_description = (
            f"你需要处理一个Excel文件相关的任务。目标文件路径是 '{test_excel_file_path_on_windows}'，工作表是 '{test_excel_sheet_name}'。\n"
            f"请严格按照以下SQO操作定义列表，顺序执行每一个操作，并整合所有操作的结果形成最终答案。\n"
            f"SQO操作定义列表: {json.dumps(excel_sqo_list, ensure_ascii=False)}\n\n"
            f"对于列表中的【每一个SQO操作定义字典】，你需要调用 '{tool_instance_for_worker.name}' 工具一次，"
            f"将该字典作为 'sqo_operation_definition' 参数，"
            f"同时传递 'target_excel_file_path': '{test_excel_file_path_on_windows}' 和 "
            f"'target_excel_sheet_name': '{test_excel_sheet_name}'。"
        )
        worker_task_inputs = {
            "excel_sqo_list_to_execute": excel_sqo_list, # Worker Agent会迭代这个
            "target_excel_file_path_for_worker": test_excel_file_path_on_windows, # Worker Agent使用这个传递给工具
            "target_excel_sheet_name_for_worker": test_excel_sheet_name # Worker Agent使用这个传递给工具
        }
        
    elif selected_tool_name == "web_search_tool": # 这是唯一且正确的 web_search_tool 分支
        search_query = manager_plan_object.tool_input_args.get("query", request.user_query) if manager_plan_object.tool_input_args else request.user_query
        max_results = manager_plan_object.tool_input_args.get("max_results", 5) if manager_plan_object.tool_input_args else 5
        worker_task_description = f"请使用网络搜索工具查找关于 '{search_query}' 的信息，返回最多 {max_results} 条结果。"
        worker_task_inputs = {"query": search_query, "max_results": max_results}
        
    elif selected_tool_name == "get_current_time_tool":
        timezone_str = manager_plan_object.tool_input_args.get("timezone", "Asia/Shanghai") if manager_plan_object.tool_input_args else "Asia/Shanghai"
        worker_task_description = f"请使用时间工具获取当前时间。时区参数为: '{timezone_str}'。"
        worker_task_inputs = {"timezone_str": timezone_str} 

    elif selected_tool_name == "calculate_tool":
        expression_str = manager_plan_object.tool_input_args.get("expression", "") if manager_plan_object.tool_input_args else ""
        if not expression_str:
             return AgentTaskResponse(
                answer="Manager选择计算器工具但未提供表达式。", status="error",
                error_message="expression is missing for calculate_tool.",
                debug_info={"manager_plan": manager_plan_object.model_dump()}
            )
        worker_task_description = f"请使用计算器工具计算以下表达式: '{expression_str}'。"
        worker_task_inputs = {"expression": expression_str}
        
    else: # 这是处理未知工具的 else
        return AgentTaskResponse(
            answer=f"未知的工具名称 '{selected_tool_name}' 被Manager规划。", status="error",
            error_message=f"Unknown tool '{selected_tool_name}' planned by manager.",
            debug_info={"manager_plan": manager_plan_object.model_dump()}
        )

    # 创建 Worker Task
    worker_task = Task(
        description=worker_task_description,
        expected_output="任务的执行结果，通常是一个字符串，其中包含答案或操作的状态。",
        agent=worker_agent_instance,
        tools=[tool_instance_for_worker], # 只给 Worker 当前任务需要的工具
        async_execution=False # Worker Task 通常也是同步的
    )

    print(f"Executing Worker Task with tool: {selected_tool_name}")
    print(f"Worker Task Description: {worker_task_description}")
    print(f"Worker Task Inputs: {worker_task_inputs}")
    
    worker_final_result: str = ""
    worker_crew_usage_metrics: Optional[Dict[str, Any]] = None # 用于存储 token usage
    token_usage_for_response: Optional[Dict[str, Any]] = None # <--- 在这里初始化
    try:
        # --- [关键修改] 为 Worker Task 创建并运行一个 Crew ---
        worker_crew = Crew(
            agents=[worker_agent_instance], # Worker Agent
            tasks=[worker_task],             # 它要执行的单个任务
            process=Process.sequential,
            verbose=True # 可以设为 True 或 2 来查看 Worker Crew 的详细日志
        )
        
        # 如果 worker_task_inputs 为空，则 inputs={}
        # CrewAI 的 kickoff 方法期望 inputs 是一个字典
        task_execution_inputs = worker_task_inputs if worker_task_inputs else {}
            
        # 执行 Worker Crew
        # kickoff 返回的是 CrewOutput 对象，或者在某些旧版本/配置下可能直接是结果字符串或 TaskOutput
        worker_crew_output = worker_crew.kickoff(inputs=task_execution_inputs)
        
        # 从 Worker Crew 的输出中提取结果
        # 这与我们处理 Manager Crew 输出的逻辑类似
        actual_worker_task_output: Optional[Any] = None
        if hasattr(worker_crew_output, 'tasks_output') and isinstance(worker_crew_output.tasks_output, list) and worker_crew_output.tasks_output:
            actual_worker_task_output = worker_crew_output.tasks_output[0] # 我们只有一个 worker_task
        elif hasattr(worker_crew_output, 'raw_output'): # 兼容直接返回 TaskOutput
            actual_worker_task_output = worker_crew_output
        elif isinstance(worker_crew_output, str): # 直接返回字符串
            actual_worker_task_output = worker_crew_output
        else: # 其他意外情况
            actual_worker_task_output = str(worker_crew_output)

        # 从 actual_worker_task_output 中提取最终的字符串结果
        if hasattr(actual_worker_task_output, 'raw') and isinstance(actual_worker_task_output.raw, str):
            worker_final_result = actual_worker_task_output.raw.strip()
        elif isinstance(actual_worker_task_output, str):
            worker_final_result = actual_worker_task_output.strip()
        else: # Fallback
            worker_final_result = str(actual_worker_task_output)
        
        print(f"Worker Task executed. Result: {worker_final_result}")
        
        # 获取 Worker Crew 的 token usage
        if hasattr(worker_crew, 'usage_metrics'):
            worker_crew_usage_metrics = worker_crew.usage_metrics
            print(f"DEBUG AGENT_ORCH: Raw worker_crew.usage_metrics object: {worker_crew_usage_metrics}")
            print(f"DEBUG AGENT_ORCH: Type of worker_crew.usage_metrics: {type(worker_crew_usage_metrics)}")
            if hasattr(worker_crew_usage_metrics, 'model_dump'):
                print(f"DEBUG AGENT_ORCH: worker_crew.usage_metrics.model_dump(): {worker_crew_usage_metrics.model_dump()}")
            elif isinstance(worker_crew_usage_metrics, dict):
                    print(f"DEBUG AGENT_ORCH: worker_crew.usage_metrics (is dict): {worker_crew_usage_metrics}")
            else:
                print(f"DEBUG AGENT_ORCH: worker_crew.usage_metrics is not a dict and has no model_dump.")
        else:
            print("DEBUG AGENT_ORCH: worker_crew does not have usage_metrics attribute.")
            worker_crew_usage_metrics = None # 确保它被定义
            # --- [结束关键修改] ---

            # 准备 token_usage_for_response
            token_usage_for_response = None
            if worker_crew_usage_metrics:
                if hasattr(worker_crew_usage_metrics, 'model_dump'):
                    token_usage_for_response = worker_crew_usage_metrics.model_dump()
                elif isinstance(worker_crew_usage_metrics, dict):
                    token_usage_for_response = worker_crew_usage_metrics
                else:
                    # 如果不是 Pydantic 模型或字典，尝试转换为字符串记录，但不作为结构化数据返回
                    service_logger.warning(f"Unexpected type for worker_crew_usage_metrics: {type(worker_crew_usage_metrics)}. Will not be included in structured token_usage.")

        return AgentTaskResponse(
                answer=worker_final_result,
                status="success",
                debug_info={
                    "manager_plan": manager_plan_object.model_dump(),
                    "worker_tool_used": selected_tool_name,
                    "worker_task_inputs": worker_task_inputs 
                },
                token_usage=token_usage_for_response # 使用处理后的 token_usage_for_response
            )

    except Exception as e:
    # 使用 traceback 打印详细错误
        print(f"Error executing Worker Task for tool {selected_tool_name}: {e}")
        traceback.print_exc() 
        return AgentTaskResponse(
            answer=f"执行工具 '{selected_tool_name}' 时发生错误。",
            status="error",
            error_message=str(e),
            debug_info={
                "manager_plan": manager_plan_object.model_dump(),
                "worker_tool_used": selected_tool_name,
                "worker_task_inputs": worker_task_inputs,
                "traceback": traceback.format_exc()
            }
        )

if __name__ == "__main__":
    print(f"--- Starting Agent Orchestrator FastAPI Service on {AGENT_SERVICE_HOST}:{AGENT_SERVICE_PORT} ---")
    uvicorn.run("agent_orchestrator_service:app", host=AGENT_SERVICE_HOST, port=AGENT_SERVICE_PORT, reload=True) # 确保模块名正确
    --- END OF FILE CONTENT ---

  File: batch_rename_and_replace.py
    --- START OF FILE CONTENT ---
import os
import shutil # 用于备份

# --- 配置 ---
ROOT_DIR = "/home/zhz/zhz_agent"  # 您项目的根目录
OLD_STRING = "sglang_wrapper"
NEW_STRING = "llm_interface"

# 需要处理的文件扩展名
TARGET_EXTENSIONS = ('.py', '.md', '.txt', '.json', '.yaml', '.yml') # 根据需要添加或修改

# 需要排除的目录名称 (精确匹配，小写)
EXCLUDE_DIRS = {
    '.git',
    '.venv',
    '__pycache__',
    'logs', # 假设您有日志目录不想扫描
    'stored_data', # 通常这里面是数据文件，不是代码引用
    'zhz_rag_pipeline_dagster_project.egg-info', # 构建产物
    'zhz_rag_core.egg-info', # 构建产物
    # 根据您的项目结构，可能还需要添加其他如 data, models, .vscode, node_modules 等
}

# 需要排除的文件名 (精确匹配，小写)
EXCLUDE_FILES = {
    'project_snapshot.txt', # 避免修改快照文件
    # 脚本自身会自动排除 (如果它在扫描路径下)
}

# --- 备份配置 ---
BACKUP_SUFFIX = ".bak_before_rename" # 备份文件的后缀

def should_process_file(filepath, filename_lower):
    """判断是否应该处理该文件"""
    if not filename_lower.endswith(TARGET_EXTENSIONS):
        return False
    if filename_lower in EXCLUDE_FILES:
        return False
    return True

def process_file_content(filepath):
    """读取文件内容，进行替换，如果发生改变则写回"""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception as e:
        print(f"  - Error reading file {filepath}: {e}")
        return False

    if OLD_STRING in content:
        print(f"  - Found '{OLD_STRING}' in: {filepath}")
        
        # 创建备份
        backup_filepath = filepath + BACKUP_SUFFIX
        try:
            shutil.copy2(filepath, backup_filepath)
            print(f"    - Backup created: {backup_filepath}")
        except Exception as e_backup:
            print(f"    - ERROR creating backup for {filepath}: {e_backup}. Skipping modification.")
            return False

        new_content = content.replace(OLD_STRING, NEW_STRING)
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(new_content)
            print(f"    - Replaced in: {filepath}")
            return True
        except Exception as e_write:
            print(f"    - Error writing to file {filepath}: {e_write}")
            # 如果写回失败，可以考虑是否恢复备份，但通常保留备份让用户手动处理更好
            return False
    return False

def rename_files_and_dirs(root_path):
    """重命名包含 OLD_STRING 的文件和目录"""
    for dirpath, dirnames, filenames in os.walk(root_path, topdown=False): # topdown=False 确保先处理子目录
        # 排除特定目录
        dirnames[:] = [d for d in dirnames if d.lower() not in EXCLUDE_DIRS]

        # 重命名文件
        for filename in filenames:
            if OLD_STRING in filename:
                old_filepath = os.path.join(dirpath, filename)
                new_filename = filename.replace(OLD_STRING, NEW_STRING)
                new_filepath = os.path.join(dirpath, new_filename)
                if os.path.exists(new_filepath):
                    print(f"  - SKIPPING rename (file): Target '{new_filepath}' already exists.")
                    continue
                try:
                    os.rename(old_filepath, new_filepath)
                    print(f"  - Renamed file: '{old_filepath}' to '{new_filepath}'")
                except Exception as e:
                    print(f"  - ERROR renaming file '{old_filepath}': {e}")
        
        # 重命名目录 (在处理完文件名之后)
        # 注意：os.walk 的 dirnames 是副本，直接修改它不会影响遍历
        # 我们需要在下一次 walk 时，新的目录名才会生效，或者在一次遍历后重新执行这部分
        # 为了简单，这里只重命名当前级别的目录，如果深层嵌套目录名也包含OLD_STRING，可能需要多次运行或更复杂的逻辑
        # 但通常我们只关心顶层的那个 llm 目录下的 sglang_wrapper.py
        # 如果您有 sglang_wrapper_utils 这样的目录名也想改，这个逻辑能处理
        for dirname in dirnames: # dirnames 已经是被 EXCLUDE_DIRS 过滤后的
            if OLD_STRING in dirname:
                old_dirpath = os.path.join(dirpath, dirname)
                new_dirname = dirname.replace(OLD_STRING, NEW_STRING)
                new_dirpath = os.path.join(dirpath, new_dirname)
                if os.path.exists(new_dirpath):
                     print(f"  - SKIPPING rename (dir): Target '{new_dirpath}' already exists.")
                     continue
                try:
                    # shutil.move 更安全，可以处理跨文件系统的情况，但 os.rename 对于同文件系统内的重命名通常足够
                    os.rename(old_dirpath, new_dirpath)
                    print(f"  - Renamed directory: '{old_dirpath}' to '{new_dirpath}'")
                except Exception as e:
                    print(f"  - ERROR renaming directory '{old_dirpath}': {e}")


def main():
    print(f"Starting search and replace in directory: {ROOT_DIR}")
    print(f"Replacing all occurrences of '{OLD_STRING}' with '{NEW_STRING}'")
    print(f"Target file extensions: {TARGET_EXTENSIONS}")
    print(f"Excluded directories: {EXCLUDE_DIRS}")
    print(f"Excluded files: {EXCLUDE_FILES}")
    print("-" * 30)

    # 第一步：先重命名文件和目录
    # 特别是我们知道要将 zhz_rag/llm/sglang_wrapper.py 重命名为 zhz_rag/llm/llm_interface.py
    # 这个脚本的 rename_files_and_dirs 会处理所有匹配的文件名和目录名
    print("\n--- Phase 1: Renaming files and directories ---")
    # 为了确保我们只重命名目标文件，可以先手动执行这个，或者更精确地指定路径
    # 这里我们先尝试全局重命名（除了排除项）
    # rename_files_and_dirs(ROOT_DIR) # 如果您只想重命名特定的那个文件，建议手动操作

    # 手动重命名我们最关心的那个文件
    specific_old_file = os.path.join(ROOT_DIR, "zhz_rag", "llm", f"{OLD_STRING}.py")
    specific_new_file = os.path.join(ROOT_DIR, "zhz_rag", "llm", f"{NEW_STRING}.py")
    if os.path.exists(specific_old_file):
        if os.path.exists(specific_new_file):
            print(f"  - SKIPPING rename: Target '{specific_new_file}' already exists.")
        else:
            try:
                os.rename(specific_old_file, specific_new_file)
                print(f"  - Successfully renamed: '{specific_old_file}' to '{specific_new_file}'")
            except Exception as e:
                print(f"  - ERROR renaming '{specific_old_file}': {e}")
    else:
        print(f"  - INFO: Specific file '{specific_old_file}' not found, assuming already renamed or not present.")


    print("\n--- Phase 2: Replacing content in files ---")
    files_changed_count = 0
    files_scanned_count = 0

    for dirpath, dirnames, filenames in os.walk(ROOT_DIR):
        # 排除特定目录
        dirnames[:] = [d for d in dirnames if d.lower() not in EXCLUDE_DIRS and not d.endswith(BACKUP_SUFFIX)]
        
        for filename in filenames:
            if filename.endswith(BACKUP_SUFFIX): # 跳过备份文件
                continue

            filepath = os.path.join(dirpath, filename)
            filename_lower = filename.lower()
            
            # 排除脚本自身
            if os.path.abspath(filepath) == os.path.abspath(__file__):
                continue

            if should_process_file(filepath, filename_lower):
                files_scanned_count += 1
                if process_file_content(filepath):
                    files_changed_count += 1
            
    print("-" * 30)
    print(f"Scan complete. {files_scanned_count} files scanned.")
    print(f"{files_changed_count} files were modified (backups created with {BACKUP_SUFFIX} suffix).")
    if files_changed_count > 0:
        print("IMPORTANT: Please review the changes carefully, for example using 'git diff'.")
        print("You can restore individual files from their .bak_before_rename backups if needed.")

if __name__ == "__main__":
    # 再次确认
    confirm = input(f"This script will modify files in '{ROOT_DIR}'.\n"
                    f"It will rename '{OLD_STRING}.py' to '{NEW_STRING}.py' in 'zhz_rag/llm/' if it exists,\n"
                    f"and then replace all occurrences of '{OLD_STRING}' with '{NEW_STRING}' in file contents.\n"
                    f"Backups of modified files will be created with '{BACKUP_SUFFIX}' suffix.\n"
                    "Are you sure you want to continue? (yes/no): ")
    if confirm.lower() == 'yes':
        main()
    else:
        print("Operation cancelled by user.")

    --- END OF FILE CONTENT ---

  File: config.yaml
    --- START OF FILE CONTENT ---
# config.yaml

model_list:
  - model_name: gemini/gemini-1.5-flash-latest # 您已有的 Gemini 配置 (如果存在)
    litellm_params:
      model: gemini/gemini-1.5-flash-latest
      api_key: os.environ/GEMINI_API_KEY
      # api_base: http://localhost:YOUR_CLASH_PORT # 如果需要

  # --- 修改我们本地的 Qwen3 1.7B 模型配置 ---
  - model_name: local/qwen3-1.7b-gguf
    litellm_params:
      model: openai/my-local-qwen # 使用 "openai/" 前缀，后面可以跟一个自定义的描述符
      api_base: http://localhost:8088/v1
      api_key: nokey

# ... (其他 LiteLLM 配置) ...
    --- END OF FILE CONTENT ---

  File: debug_kuzu_visibility.py
    --- START OF FILE CONTENT ---
# debug_kuzu_visibility.py
import kuzu
import os
import shutil
import time
import pandas as pd

DB_PATH = "./zhz_rag/stored_data/kuzu_test_visibility_db" # 使用一个新的测试路径

def setup_and_verify_schema():
    print(f"--- Phase 1: Setup and Initial Verification ---")
    if os.path.exists(DB_PATH):
        print(f"Removing existing database at {DB_PATH}")
        shutil.rmtree(DB_PATH)
    
    db_setup = None
    conn_setup = None
    try:
        print(f"Creating new database at {DB_PATH}")
        db_setup = kuzu.Database(DB_PATH, read_only=False)
        conn_setup = kuzu.Connection(db_setup)
        print("Database and connection created for setup.")

        ddl_queries = [
            "CREATE NODE TABLE IF NOT EXISTS ExtractedEntity (id_prop STRING, text STRING, label STRING, PRIMARY KEY (id_prop))",
            "CREATE REL TABLE IF NOT EXISTS WorksAt (FROM ExtractedEntity TO ExtractedEntity)",
            "CREATE REL TABLE IF NOT EXISTS AssignedTo (FROM ExtractedEntity TO ExtractedEntity)"
        ]
        for query in ddl_queries:
            print(f"Executing DDL: {query}")
            conn_setup.execute(query)
        print("DDL execution complete.")

        print("Executing CHECKPOINT...")
        conn_setup.execute("CHECKPOINT;")
        print("CHECKPOINT complete.")

        # Phase 1 Verification
        print("Verifying tables immediately after creation (same session)...")
        try:
            # 尝试 SHOW TABLES;
            print("Attempting 'SHOW TABLES;' in Phase 1...")
            result = conn_setup.execute("SHOW TABLES;") # <--- 修改这里
            df = pd.DataFrame(result.get_as_df())
            print(f"Tables found using 'SHOW TABLES;' (Phase 1):\n{df}")
            # 检查 'name' 列是否存在，并且 'ExtractedEntity' 是否在其中
            if not df.empty and 'name' in df.columns and "ExtractedEntity" in df["name"].tolist():
                print("Phase 1 Verification (SHOW TABLES;): ExtractedEntity table FOUND.")
            elif not df.empty and 'name' not in df.columns:
                print("Phase 1 Verification (SHOW TABLES;): 'name' column not found in SHOW TABLES result.")
                print(f"Columns available in SHOW TABLES result: {df.columns.tolist()}")
            else: # df is empty or ExtractedEntity not in name column
                print("Phase 1 Verification (SHOW TABLES;): ExtractedEntity table NOT FOUND or result was empty.")
        except Exception as e:
            print(f"Error during Phase 1 verification with 'SHOW TABLES;': {e}")
            print("Falling back to direct query for ExtractedEntity in Phase 1...")
            try:
                result_direct = conn_setup.execute("MATCH (e:ExtractedEntity) RETURN count(e) AS entity_count;")
                df_direct = pd.DataFrame(result_direct.get_as_df())
                print(f"Direct query result (Phase 1):\n{df_direct}")
                count_val = df_direct['entity_count'].iloc[0] if not df_direct.empty else -1
                print(f"Phase 1 Verification (Direct Query): ExtractedEntity table SEEMS TO EXIST (count: {count_val}).")
            except Exception as e_direct_phase1:
                print(f"Error during Phase 1 direct query verification: {e_direct_phase1}")

    except Exception as e:
        print(f"Error during Phase 1 setup: {e}")
    finally:
        if conn_setup:
            print("Closing setup connection.")
            # conn_setup.close() # Kuzu Connection 没有显式 close
        if db_setup:
            print("Deleting setup database instance reference (will trigger close).")
            del db_setup # 依赖 __del__
        print("--- Phase 1 Complete ---")

def verify_in_new_session():
    print(f"\n--- Phase 2: Verification in a New Session ---")
    if not os.path.exists(DB_PATH):
        print(f"Database at {DB_PATH} does not exist. Cannot perform Phase 2.")
        return

    db_verify = None
    conn_verify = None
    try:
        print(f"Opening existing database at {DB_PATH} for verification.")
        db_verify = kuzu.Database(DB_PATH, read_only=False) # 打开同一个数据库
        conn_verify = kuzu.Connection(db_verify)
        print("Database and connection created for verification.")

        # Phase 2 Verification
        print("Verifying tables in new session...")
        try:
            # 尝试 SHOW TABLES;
            print("Attempting 'SHOW TABLES;' in Phase 2...")
            result = conn_verify.execute("SHOW TABLES;") # <--- 修改这里
            df = pd.DataFrame(result.get_as_df())
            print(f"Tables found using 'SHOW TABLES;' (Phase 2):\n{df}")
            if not df.empty and 'name' in df.columns and "ExtractedEntity" in df["name"].tolist():
                print("Phase 2 Verification (SHOW TABLES;): ExtractedEntity table FOUND.")
            elif not df.empty and 'name' not in df.columns:
                print("Phase 2 Verification (SHOW TABLES;): 'name' column not found in SHOW TABLES result.")
                print(f"Columns available in SHOW TABLES result: {df.columns.tolist()}")
            else: # df is empty or ExtractedEntity not in name column
                print("Phase 2 Verification (SHOW TABLES;): ExtractedEntity table NOT FOUND or result was empty.")
        except Exception as e:
            print(f"Error during Phase 2 verification with 'SHOW TABLES;': {e}")
            print("Falling back to direct query for ExtractedEntity in Phase 2...")
            try:
                result_direct = conn_verify.execute("MATCH (e:ExtractedEntity) RETURN count(e) AS entity_count;")
                df_direct = pd.DataFrame(result_direct.get_as_df())
                print(f"Direct query result (Phase 2):\n{df_direct}")
                count_val = df_direct['entity_count'].iloc[0] if not df_direct.empty else -1
                print(f"Phase 2 Verification (Direct Query): ExtractedEntity table SEEMS TO EXIST (count: {count_val}).")
            except Exception as e_direct_phase2:
                print(f"Error during Phase 2 direct query verification: {e_direct_phase2}")

    except Exception as e:
        print(f"Error during Phase 2 setup: {e}")
    finally:
        if conn_verify:
            print("Closing verification connection.")
        if db_verify:
            print("Deleting verification database instance reference.")
            del db_verify
        print("--- Phase 2 Complete ---")

if __name__ == "__main__":
    print(f"Kuzu Python client version: {kuzu.__version__}")
    setup_and_verify_schema()
    print("\nWaiting a moment before trying to open in a new session (simulating process switch)...")
    time.sleep(2) # 短暂等待，模拟进程切换的间隙
    verify_in_new_session()
    --- END OF FILE CONTENT ---

  File: llm_setup.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/llm_setup.py

import os
from typing import Any, Optional, List, Dict

# LiteLLM 本身通常是直接调用其 API，例如 litellm.completion
# 如果要将其适配给 CrewAI 的 Agent，我们需要将其包装成一个符合 CrewAI BaseLLM 接口的类。
# CrewAI 自身也提供了通过 LiteLLM 使用各种模型的集成，我们也可以研究直接使用 CrewAI 的方式。
# 为简化起见，我们先创建一个函数，返回配置好的 LiteLLM 调用参数，或者一个简单的包装器。

# --- 从 agent_orchestrator_service.py 中获取的配置常量 ---
# 本地 LLM 服务地址 (Qwen3)
LOCAL_QWEN_API_BASE = os.getenv("LOCAL_LLM_API_BASE", "http://localhost:8088/v1") # 确保与 local_llm_service.py 的端口一致
LOCAL_QWEN_MODEL_NAME_FOR_LITELLM = os.getenv("LOCAL_LLM_MODEL_NAME", "local/qwen3-1.7b-gguf") # 这个是给 litellm 的 model 参数，需要与 LiteLLM 配置或调用方式对应

# 云端 LiteLLM 网关地址 (用于 Gemini 等)
CLOUD_LITELLM_GW_API_BASE = os.getenv("CLOUD_LITELLM_GW_API_BASE", "YOUR_CLOUD_LITELLM_GATEWAY_URL_HERE/v1") # 假设您的网关也提供 /v1 路径
GEMINI_MODEL_NAME_FOR_LITELLM = os.getenv("CLOUD_LLM_MODEL_NAME", "gemini/gemini-1.5-flash-latest") # 或您希望通过网关调用的模型

# API Keys (通常 LiteLLM 会在其自己的配置或环境变量中处理，但这里可以作为参考)
# GEMINI_API_KEY = os.getenv("GEMINI_API_KEY") # 如果直接调用 Gemini 而非通过网关
# 注意：本地模型的 API Key 通常是 "nokey" 或在服务中不强制

# --- CrewAI LLM 包装器 (如果需要直接传递给 CrewAI Agent) ---
# CrewAI 期望一个 LLM 对象，我们可以创建一个简单的包装器，或者使用 CrewAI 提供的 LiteLLM 集成
# from crewai.llms.base_llm import BaseLLM as CrewAIBaseLLM
# import litellm
#
# class LiteLLMWrapperForCrewAI(CrewAIBaseLLM):
#     model: str
#     api_base: Optional[str] = None
#     api_key: Optional[str] = None
#     custom_llm_provider: Optional[str] = None # 例如 "openai" for OpenAI-compatible endpoints
#     litellm_kwargs: Dict[str, Any] = {}
#
#     def __init__(self, model: str, api_base: Optional[str] = None, api_key: Optional[str] = None, custom_llm_provider: Optional[str] = None, **kwargs):
#         super().__init__(model=model) # CrewAI BaseLLM 需要 model 参数
#         self.model = model
#         self.api_base = api_base
#         self.api_key = api_key
#         self.custom_llm_provider = custom_llm_provider
#         self.litellm_kwargs = kwargs
#
#     def call(self, messages: List[Dict[str, str]], **kwargs: Any) -> str:
#         # CrewAI 的 BaseLLM.call 方法通常期望返回一个字符串
#         # LiteLLM 的 completion 返回一个 ModelResponse 对象
#         params = {
#             "model": self.model,
#             "messages": messages,
#             "api_base": self.api_base,
#             "api_key": self.api_key,
#             "custom_llm_provider": self.custom_llm_provider,
#             **self.litellm_kwargs, # 包含 temperature, max_tokens 等
#             **kwargs # 运行时可能传递的额外参数
#         }
#         # 移除值为 None 的参数，因为 litellm.completion 不喜欢 None 的 api_key 等
#         params_cleaned = {k: v for k, v in params.items() if v is not None}
#
#         response = litellm.completion(**params_cleaned)
#
#         # 从 LiteLLM 响应中提取内容
#         content = ""
#         if response.choices and response.choices[0].message and response.choices[0].message.content:
#             content = response.choices[0].message.content
#         return content
#
#     def get_token_ids(self, text: str) -> List[int]:
#         # LiteLLM 通常不直接暴露 tokenizer，但可以尝试通过 litellm.token_counter (如果它返回ids)
#         # 或者如果使用特定模型，可以尝试加载其 tokenizer
#         # 为简单起见，我们先返回空列表或引发 NotImplementedError
#         # return litellm.encode(model=self.model, text=text) # 如果 litellm.encode 可用
#         raise NotImplementedError("get_token_ids is not implemented for this LiteLLM wrapper.")


def get_local_qwen_llm_instance(
    temperature: float = 0.7,
    max_tokens: int = 2048,
    # crewai_wrapper: bool = False # 是否返回 CrewAI 兼容的包装器
) -> Any: # 返回 Any 以便后续决定是直接返回配置字典还是包装器实例
    """
    获取配置好的本地 Qwen3 LLM 实例（通过 LiteLLM 调用）。
    """
    print(f"LLM Setup: Configuring Local Qwen LLM via LiteLLM.")
    print(f"  Model: {LOCAL_QWEN_MODEL_NAME_FOR_LITELLM}")
    print(f"  API Base: {LOCAL_QWEN_API_BASE}")
    # if crewai_wrapper:
    #     return LiteLLMWrapperForCrewAI(
    #         model=LOCAL_QWEN_MODEL_NAME_FOR_LITELLM,
    #         api_base=LOCAL_QWEN_API_BASE,
    #         api_key="nokey", # 本地服务通常不需要 key
    #         custom_llm_provider="openai", # 因为我们的本地服务是 OpenAI 兼容的
    #         temperature=temperature,
    #         max_tokens=max_tokens
    #     )
    # else:
    # 返回一个配置字典，调用方可以使用 litellm.completion(**config, messages=...)
    return {
        "model": LOCAL_QWEN_MODEL_NAME_FOR_LITELLM,
        "api_base": LOCAL_QWEN_API_BASE,
        "api_key": "nokey",
        "custom_llm_provider": "openai",
        "temperature": temperature,
        "max_tokens": max_tokens
    }


def get_cloud_gemini_llm_instance(
    temperature: float = 0.5,
    max_tokens: int = 4096,
    # crewai_wrapper: bool = False
) -> Any:
    """
    获取配置好的云端 Gemini LLM 实例（通过云端 LiteLLM 网关调用）。
    """
    print(f"LLM Setup: Configuring Cloud Gemini LLM via LiteLLM Gateway.")
    print(f"  Model: {GEMINI_MODEL_NAME_FOR_LITELLM}")
    print(f"  API Base (Gateway): {CLOUD_LITELLM_GW_API_BASE}")

    if CLOUD_LITELLM_GW_API_BASE == "YOUR_CLOUD_LITELLM_GATEWAY_URL_HERE/v1":
        print("LLM Setup WARNING: CLOUD_LITELLM_GW_API_BASE is not configured. Cloud LLM calls will likely fail.")
        # 可以选择返回 None 或者一个无效的配置，让调用方处理
        return None

    # if crewai_wrapper:
    #     return LiteLLMWrapperForCrewAI(
    #         model=GEMINI_MODEL_NAME_FOR_LITELLM,
    #         api_base=CLOUD_LITELLM_GW_API_BASE,
    #         # API Key 通常由云端 LiteLLM 网关管理，客户端调用网关时可能不需要直接提供
    #         # 或者网关本身可能需要某种形式的认证 key
    #         api_key=os.getenv("CLOUD_LITELLM_GATEWAY_API_KEY"), # 假设网关可能需要一个 key
    #         # custom_llm_provider 可能不需要，因为网关会处理到具体云服务的转换
    #         temperature=temperature,
    #         max_tokens=max_tokens
    #     )
    # else:
    return {
        "model": GEMINI_MODEL_NAME_FOR_LITELLM,
        "api_base": CLOUD_LITELLM_GW_API_BASE,
        "api_key": os.getenv("CLOUD_LITELLM_GATEWAY_API_KEY"), # 网关本身的key
        "temperature": temperature,
        "max_tokens": max_tokens
    }

# --- 示例用法 (可选，用于测试此文件) ---
async def main_test_llm_setup():
    print("--- Testing LLM Setup ---")

    print("\n--- Getting Local Qwen Config ---")
    local_qwen_config = get_local_qwen_llm_instance()
    if local_qwen_config:
        print(f"Local Qwen Config: {local_qwen_config}")
        # 模拟调用
        try:
            messages = [{"role": "user", "content": "你好！"}]
            print(f"Simulating LiteLLM call with local Qwen config for: {messages}")
            # response = await litellm.acompletion(**local_qwen_config, messages=messages)
            # print(f"Simulated local Qwen response (first choice): {response.choices[0].message.content if response.choices else 'No response'}")
            print("Actual LiteLLM call commented out for setup test.")
        except Exception as e:
            print(f"Error simulating local Qwen call: {e}")
    else:
        print("Failed to get local Qwen config.")

    print("\n--- Getting Cloud Gemini Config ---")
    cloud_gemini_config = get_cloud_gemini_llm_instance()
    if cloud_gemini_config:
        print(f"Cloud Gemini Config: {cloud_gemini_config}")
        if cloud_gemini_config.get("api_base") == "YOUR_CLOUD_LITELLM_GATEWAY_URL_HERE/v1":
            print("Skipping simulated cloud Gemini call as gateway URL is a placeholder.")
        else:
            try:
                messages = [{"role": "user", "content": "Hello!"}]
                print(f"Simulating LiteLLM call with cloud Gemini config for: {messages}")
                # response = await litellm.acompletion(**cloud_gemini_config, messages=messages)
                # print(f"Simulated cloud Gemini response (first choice): {response.choices[0].message.content if response.choices else 'No response'}")
                print("Actual LiteLLM call commented out for setup test.")
            except Exception as e:
                print(f"Error simulating cloud Gemini call: {e}")
    else:
        print("Failed to get cloud Gemini config (likely due to placeholder URL).")

if __name__ == "__main__":
    # 为了运行异步的 main_test_llm_setup
    # import asyncio
    # asyncio.run(main_test_llm_setup())
    pass
    --- END OF FILE CONTENT ---

  File: local_llm_service.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/local_llm_service.py
import os
import time
import uuid
import traceback
from contextlib import asynccontextmanager
from typing import List, Dict, Any, Optional, Union
import json


from zhz_rag.config.pydantic_models import ExtractedEntitiesAndRelationIntent


import uvicorn
from fastapi import FastAPI, HTTPException, Request as FastAPIRequest
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from llama_cpp import Llama, LlamaGrammar
import re

# --- 配置 ---
MODEL_DIR = os.getenv("LOCAL_LLM_MODEL_DIR", "/home/zhz/models/Qwen3-1.7B-GGUF")
MODEL_FILENAME = os.getenv("LOCAL_LLM_MODEL_FILENAME")
N_GPU_LAYERS = int(os.getenv("LOCAL_LLM_N_GPU_LAYERS", 0))
N_CTX = int(os.getenv("LOCAL_LLM_N_CTX", 4096))
N_BATCH = int(os.getenv("LOCAL_LLM_N_BATCH", 512))
SERVICE_PORT = int(os.getenv("LOCAL_LLM_SERVICE_PORT", 8088))
SERVICE_HOST = "0.0.0.0"

GBNF_FILE_PATH = os.path.join(os.path.dirname(__file__), "core", "grammars", "cypher_or_unable_output.gbnf")

# --- 全局变量 ---
llama_model: Optional[Llama] = None
model_path_global: Optional[str] = None
# logit_bias 相关的全局变量 (在 lifespan 中初始化)
failure_phrase_token_ids: List[int] = []
logit_bias_for_failure_phrase: Optional[Dict[int, float]] = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    global llama_model, model_path_global, failure_phrase_token_ids, logit_bias_for_failure_phrase
    print("--- Local LLM Service: Lifespan startup (with GBNF and logit_bias prep) ---")
    
    model_file_to_load = MODEL_FILENAME
    if not model_file_to_load:
        print(f"MODEL_FILENAME environment variable not set. Attempting to auto-detect GGUF file in {MODEL_DIR}...")
        try:
            gguf_files = [f for f in os.listdir(MODEL_DIR) if f.endswith(".gguf")]
            if not gguf_files:
                error_msg = f"No GGUF models found in directory: {MODEL_DIR}"
                print(f"ERROR: {error_msg}")
                app.state.cypher_path_grammar = None # Ensure state variable exists even on error
                raise RuntimeError(error_msg)
            if len(gguf_files) > 1:
                print(f"Warning: Multiple GGUF models found in {MODEL_DIR}. Using the first one: {gguf_files[0]}")
            model_file_to_load = gguf_files[0]
            print(f"Auto-detected GGUF file: {model_file_to_load}")
        except FileNotFoundError:
            error_msg = f"Model directory not found: {MODEL_DIR}"
            print(f"ERROR: {error_msg}")
            app.state.cypher_path_grammar = None
            raise RuntimeError(error_msg)
        except Exception as e_find_model:
            error_msg = f"Error auto-detecting GGUF file: {e_find_model}"
            print(f"ERROR: {error_msg}")
            app.state.cypher_path_grammar = None
            raise RuntimeError(error_msg)

    model_path_global = os.path.join(MODEL_DIR, model_file_to_load)
    print(f"Attempting to load Llama model from: {model_path_global}")
    print(f"Parameters: n_gpu_layers={N_GPU_LAYERS}, n_ctx={N_CTX}, n_batch={N_BATCH}")

    try:
        llama_model = Llama(
            model_path=model_path_global,
            n_gpu_layers=N_GPU_LAYERS,
            n_ctx=N_CTX,
            n_batch=N_BATCH,
            verbose=True
        )
        print("Llama model loaded successfully.")

        # --- 初始化 logit_bias 相关 ---
        failure_phrase_str = "无法生成Cypher查询."
        if llama_model:
            try:
                # add_bos=False, special=False (通常用于非起始的、纯文本的词元化)
                failure_phrase_token_ids = llama_model.tokenize(failure_phrase_str.encode("utf-8"), add_bos=False, special=False)
                # 为这些 token ID 设置正向偏置，例如 10.0 (可以调整)
                # 避免偏置 EOS token (如果它意外地出现在短语的词元化结果中)
                eos_token_id = llama_model.token_eos()
                logit_bias_for_failure_phrase = {
                    token_id: 10.0 for token_id in failure_phrase_token_ids if token_id != eos_token_id
                }
                print(f"Successfully tokenized failure phrase '{failure_phrase_str}' to IDs: {failure_phrase_token_ids}")
                print(f"Logit bias for failure phrase: {logit_bias_for_failure_phrase}")
            except Exception as e_tokenize:
                print(f"ERROR: Failed to tokenize failure phrase for logit_bias: {e_tokenize}")
                failure_phrase_token_ids = []
                logit_bias_for_failure_phrase = None
        # --- 结束 logit_bias 初始化 ---

        gbnf_grammar_instance: Optional[LlamaGrammar] = None
        print(f"Attempting to load GBNF grammar from: {GBNF_FILE_PATH}")
        if os.path.exists(GBNF_FILE_PATH):
            try:
                gbnf_grammar_instance = LlamaGrammar.from_file(GBNF_FILE_PATH)
                print("GBNF grammar (success/failure paths) loaded successfully into lifespan.")
            except Exception as e_gbnf:
                print(f"ERROR: Failed to load or parse GBNF grammar file '{GBNF_FILE_PATH}': {e_gbnf}")
                traceback.print_exc()
        else:
            print(f"ERROR: GBNF grammar file not found at '{GBNF_FILE_PATH}'.")
        
        app.state.cypher_path_grammar = gbnf_grammar_instance

    except Exception as e:
        print(f"FATAL: Failed to load Llama model or prepare GBNF/logit_bias: {e}")
        app.state.cypher_path_grammar = None
        logit_bias_for_failure_phrase = None # Ensure this is also cleared
    
    yield
    print("--- Local LLM Service: Lifespan shutdown ---")

app = FastAPI(
    title="Local LLM Service (OpenAI Compatible)",
    description="Uses GBNF with logit_bias for conditional JSON output.",
    version="0.1.7", 
    lifespan=lifespan
)

class ChatMessage(BaseModel):
    role: str
    content: str

class ChatCompletionRequest(BaseModel):
    model: str
    messages: List[ChatMessage]
    temperature: float = 0.7
    max_tokens: Optional[int] = 1024 # Default to 1024 or higher
    stream: bool = False
    stop: Optional[Union[str, List[str]]] = None

class ChatCompletionMessage(BaseModel):
    role: str
    content: Optional[str] = None

class ChatCompletionChoice(BaseModel):
    index: int
    message: ChatCompletionMessage
    finish_reason: Optional[str] = "stop"

class UsageInfo(BaseModel):
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int

class ChatCompletionResponse(BaseModel):
    id: str = Field(default_factory=lambda: f"chatcmpl-{uuid.uuid4().hex}")
    object: str = "chat.completion"
    created: int = Field(default_factory=lambda: int(time.time()))
    model: str
    choices: List[ChatCompletionChoice]
    usage: UsageInfo
    system_fingerprint: Optional[str] = None

def post_process_llm_output(content: Optional[str], finish_reason: Optional[str]) -> Optional[str]:
    if content is None:
        return None
    processed_content = content
    # Remove <think>...</think> blocks
    think_block_pattern = re.compile(r"<think[^>]*>.*?</think>\s*", flags=re.DOTALL | re.IGNORECASE)
    processed_content = think_block_pattern.sub("", processed_content)

    # Handle potentially incomplete <think> tags if generation was cut short
    if finish_reason == "length" and \
       re.match(r"<think[^>]*>", processed_content.strip(), flags=re.IGNORECASE) and \
       not re.search(r"</think\s*>", processed_content, flags=re.IGNORECASE):
        print("DEBUG_POST_PROCESS: Incomplete think block due to length, attempting to remove partial tag.")
        # More aggressive removal of any leading <think...> tag if it's incomplete
        processed_content = re.sub(r"^<think[^>]*>","", processed_content.strip(), flags=re.IGNORECASE).strip()

    # Remove any remaining stray <think> or </think> tags
    stray_think_tag_pattern = re.compile(r"</?\s*think[^>]*?>\s*", flags=re.IGNORECASE)
    processed_content = stray_think_tag_pattern.sub("", processed_content)
    return processed_content.strip()

@app.post("/v1/chat/completions", response_model=ChatCompletionResponse)
async def create_chat_completion_endpoint(fastapi_req: FastAPIRequest, request: ChatCompletionRequest):
    global llama_model, model_path_global, logit_bias_for_failure_phrase
    
    # --- 添加调试日志 ---
    print("\n--- local_llm_service: Received /v1/chat/completions request ---")
    try:
        print(f"Request Body (raw): {await fastapi_req.body()}") # 打印原始请求体
        print(f"Request Model: {request.model}")
        print(f"Request Messages (count): {len(request.messages) if request.messages else 0}")
        print(f"Request Temperature: {request.temperature}")
        print(f"Request Max Tokens: {request.max_tokens}")
        print(f"Request Stop: {request.stop}")
    except Exception as e_req_log:
        print(f"Error logging request details: {e_req_log}")
    # --- 调试日志结束 ---


    loaded_cypher_path_grammar: Optional[LlamaGrammar] = getattr(fastapi_req.app.state, 'cypher_path_grammar', None)
    
    if llama_model is None:
        raise HTTPException(status_code=503, detail="Llama model is not loaded or failed to load.")
    if request.stream:
        raise HTTPException(status_code=400, detail="Streaming responses are not yet supported by this service.")

    response_content_raw: Optional[str] = None
    prompt_tokens = 0
    completion_tokens = 0
    total_tokens = 0
    finish_reason = "stop"
    request_model_name = request.model 
    final_json_output_str_for_client: Optional[str] = None

    if not hasattr(llama_model, "create_chat_completion"):
        print("CRITICAL_ERROR: llama_model.create_chat_completion method not found.")
        # Construct a proper JSON string for the error case directly
        error_response_obj = {"status": "unable_to_generate", "query": "LLM service misconfiguration."}
        final_json_output_str_for_client = json.dumps(error_response_obj, ensure_ascii=False)
        # Early return with the error structure if model is misconfigured
        return ChatCompletionResponse(
            id=f"chatcmpl-{uuid.uuid4().hex}",
            object="chat.completion",
            created=int(time.time()),
            model=request_model_name or (os.path.basename(model_path_global) if model_path_global else "local-llm-misconfigured"),
            choices=[ChatCompletionChoice(index=0, message=ChatCompletionMessage(role="assistant", content=final_json_output_str_for_client), finish_reason="error")],
            usage=UsageInfo(prompt_tokens=0, completion_tokens=0, total_tokens=0)
        )

    try:
        dict_messages = [msg.model_dump() for msg in request.messages]
        completion_params: Dict[str, Any] = {
            "messages": dict_messages,
            "temperature": request.temperature, 
            "max_tokens": request.max_tokens or 1024,
            "stop": request.stop,
        }

            # --- 新增：根据模型名称判断是否启用JSON模式 ---
        # 假设模型名称中包含 "kg_entity_extraction" 表示我们期望JSON输出
        if "kg_entity_extraction" in request.model.lower():
            try:
                json_schema_for_extraction = ExtractedEntitiesAndRelationIntent.model_json_schema()
                completion_params["response_format"] = {
                    "type": "json_object",
                    "schema": json_schema_for_extraction
                }
                print(f"DEBUG_FastAPI: Enabled JSON mode with schema for model {request.model}")
                # print(f"DEBUG_FastAPI: JSON Schema: {json.dumps(json_schema_for_extraction, indent=2)}") # 可选打印
            except Exception as e_schema:
                print(f"ERROR_FastAPI: Failed to generate JSON schema for ExtractedEntitiesAndRelationIntent: {e_schema}")
        # --- 结束新增 ---

        is_cypher_gen_task = False
        if dict_messages and dict_messages[0]["role"] == "system":
            system_content_for_check = dict_messages[0]["content"]
            keyword_to_check = "知识图谱结构 (KuzuDB) 与 Cypher 查询生成规则" # Ensure this matches constants.py
            processed_system_content = system_content_for_check.lower()
            processed_keyword = keyword_to_check.lower()
            print(f"DEBUG_FastAPI_CypherTaskCheck: System prompt content (LOWERCASED, first 300 chars): '{processed_system_content[:300]}...'")
            print(f"DEBUG_FastAPI_CypherTaskCheck: Keyword to check (LOWERCASED): '{processed_keyword}'")
            if processed_keyword in processed_system_content:
                is_cypher_gen_task = True
                print("DEBUG_FastAPI_CypherTaskCheck: Cypher generation task DETECTED (after lowercasing and keyword adjustment).")
            else:
                print("DEBUG_FastAPI_CypherTaskCheck: Cypher generation task NOT DETECTED (keyword missing after lowercasing).")
        else:
            print("DEBUG_FastAPI_CypherTaskCheck: No system message found or messages empty, not a Cypher task.")
         
        if is_cypher_gen_task:
            print("DEBUG_FastAPI: Cypher generation task DETECTED.")
            if loaded_cypher_path_grammar is not None:
                print("DEBUG_FastAPI: Applying GBNF grammar (success/failure paths) FROM APP.STATE.")
                completion_params["grammar"] = loaded_cypher_path_grammar
                
                if logit_bias_for_failure_phrase:
                    print(f"DEBUG_FastAPI: Applying logit_bias for failure phrase: {logit_bias_for_failure_phrase}")
                    completion_params["logit_bias"] = logit_bias_for_failure_phrase
            else:
                print("DEBUG_FastAPI: GBNF grammar FROM APP.STATE IS NONE. Proceeding without grammar for Cypher task.")
        else:
            print("DEBUG_FastAPI: Not a Cypher task. No grammar or specific logit_bias applied.")
        
        print(f"DEBUG_FastAPI: Calling llama_model.create_chat_completion with params (preview): "
          f"model={request.model}, temp={completion_params['temperature']}, "
          f"max_tokens={completion_params['max_tokens']}, stop={completion_params['stop']}, "
          f"json_mode_enabled={'response_format' in completion_params}")
    
        completion = llama_model.create_chat_completion(**completion_params)

        # --- 添加调试日志 ---
        print(f"DEBUG_FastAPI: Raw completion object from llama_model.create_chat_completion: {completion}")
        # --- 调试日志结束 ---

        response_content_raw = completion['choices'][0]['message']['content']
        prompt_tokens = completion['usage']['prompt_tokens']
        completion_tokens = completion['usage']['completion_tokens']
        total_tokens = completion['usage']['total_tokens']
        finish_reason = completion['choices'][0].get('finish_reason', 'stop')
        print(f"DEBUG_FastAPI: Raw content from llama_model: '{response_content_raw}'")

    except Exception as e:
        print(f"Error during llama_model.create_chat_completion: {e}")
        traceback.print_exc()
        # Ensure this is a JSON string
        final_json_output_str_for_client = json.dumps({"status": "unable_to_generate", "query": "LLM call failed during generation."})
        finish_reason = "error" # Indicate an error finish

    if final_json_output_str_for_client is None: # Only process if no error above set this
        processed_llm_text = post_process_llm_output(response_content_raw, finish_reason)
        
        if is_cypher_gen_task:
            standard_success_template = {"status": "success", "query": ""} 
            standard_unable_json_obj = {"status": "unable_to_generate", "query": "无法生成Cypher查询."}
            final_json_to_return_obj = standard_unable_json_obj # Default to unable

            if processed_llm_text:
                cleaned_text_for_json_parse = processed_llm_text.strip()
                # Remove Markdown code block fences if present
                if cleaned_text_for_json_parse.startswith("```json"):
                    cleaned_text_for_json_parse = cleaned_text_for_json_parse[len("```json"):].strip()
                if cleaned_text_for_json_parse.endswith("```"):
                    cleaned_text_for_json_parse = cleaned_text_for_json_parse[:-len("```")].strip()
                
                try:
                    data = json.loads(cleaned_text_for_json_parse)
                    if isinstance(data, dict) and "status" in data and "query" in data:
                        if data.get("status") == "success" and isinstance(data.get("query"), str) and data.get("query").strip():
                            final_json_to_return_obj = data
                            print(f"DEBUG_FastAPI: LLM output is a valid 'success' JSON (GBNF success path likely): {json.dumps(final_json_to_return_obj, ensure_ascii=False)}")
                        elif data.get("status") == "unable_to_generate" and data.get("query") == "无法生成Cypher查询.":
                            final_json_to_return_obj = data # Already standard
                            print(f"DEBUG_FastAPI: LLM output is a valid 'unable_to_generate' JSON (GBNF failure path likely): {json.dumps(final_json_to_return_obj, ensure_ascii=False)}")
                        else: # JSON has status/query but not matching expected values
                            print(f"DEBUG_FastAPI: LLM JSON has unexpected status/query content. Status: '{data.get('status')}', Query: '{str(data.get('query'))[:100]}'. Defaulting to standard 'unable_to_generate'.")
                            # final_json_to_return_obj remains standard_unable_json_obj
                    else: 
                        print(f"DEBUG_FastAPI: LLM output parsed as JSON, but not the expected dict with status/query: '{cleaned_text_for_json_parse}'. Defaulting to standard 'unable_to_generate'.")
                except json.JSONDecodeError:
                    print(f"DEBUG_FastAPI: LLM output was not valid JSON. Raw (after post_process): '{processed_llm_text}'. Defaulting to standard 'unable_to_generate'.")
                except Exception as e_parse:
                     print(f"DEBUG_FastAPI: Unexpected error parsing LLM output: {e_parse}. Raw: '{processed_llm_text}'. Defaulting to 'unable_to_generate'.")
            else:
                print("DEBUG_FastAPI: LLM output was empty after post_processing. Defaulting to standard 'unable_to_generate' JSON.")
            
            final_json_output_str_for_client = json.dumps(final_json_to_return_obj, ensure_ascii=False)
        else: 
            # For non-Cypher tasks, return the processed text directly
            final_json_output_str_for_client = processed_llm_text if processed_llm_text is not None else ""


    print(f"DEBUG_FastAPI: Final content string to be returned to client: '{final_json_output_str_for_client}'")

    response_id = f"chatcmpl-{uuid.uuid4().hex}"
    created_time = int(time.time())
    effective_model_name = request_model_name
    if not effective_model_name and model_path_global:
        effective_model_name = os.path.basename(model_path_global)
    elif not effective_model_name:
        effective_model_name = "local-llm-unknown"
    
    # Ensure final_json_output_str_for_client is a string, even if empty (for non-Cypher tasks)
    if final_json_output_str_for_client is None:
        final_json_output_str_for_client = "" # Or some other default string

    return ChatCompletionResponse(
        id=response_id,
        object="chat.completion",
        created=created_time,
        model=effective_model_name,
        choices=[
            ChatCompletionChoice(
                index=0,
                message=ChatCompletionMessage(role="assistant", content=final_json_output_str_for_client),
                finish_reason=finish_reason 
            )
        ],
        usage=UsageInfo(
            prompt_tokens=prompt_tokens,
            completion_tokens=completion_tokens,
            total_tokens=total_tokens
        )
    )

@app.get("/v1/models", response_model=Dict[str, Any])
async def list_models():
    global model_path_global
    model_id_for_clients = "qwen3local_gguf_gbnf_logit_bias"
    model_name_to_display = "Qwen3-1.7B-GGUF (GBNF+LogitBias)"
    
    if model_path_global and os.path.exists(model_path_global):
        model_name_to_display = os.path.basename(model_path_global)
        created_timestamp = int(os.path.getctime(model_path_global))
    else:
        # Fallback if model_path_global is not set or file doesn't exist
        model_name_to_display = "Qwen3-1.7B-GGUF (Model path not resolved)"
        created_timestamp = int(time.time())

    return {
        "object": "list",
        "data": [
            {
                "id": model_id_for_clients,
                "object": "model",
                "created": created_timestamp,
                "owned_by": "user",
                "description": f"Locally hosted GGUF model: {model_name_to_display}. Uses GBNF and logit_bias for Cypher tasks."
            }
        ]
    }

if __name__ == "__main__":
    print(f"--- Starting Local LLM FastAPI Service on {SERVICE_HOST}:{SERVICE_PORT} ---")
    print(f"--- Model will be loaded from DIR: {MODEL_DIR}, FILE: {MODEL_FILENAME or 'Auto-detected GGUF'} ---")
    print(f"--- GBNF Grammar for Cypher/Unable output will be loaded from: {GBNF_FILE_PATH} ---")
    uvicorn.run("local_llm_service:app", host=SERVICE_HOST, port=SERVICE_PORT, reload=False)
    --- END OF FILE CONTENT ---

  File: mcp_servers.json
    --- START OF FILE CONTENT ---
{
  "mcpServers": {
    "zhz_rag_mcp_service": { 
      "command": "/home/zhz/zhz_agent/.venv/bin/python", 
      "args": [
        "-m", 
        "zhz_rag.api.rag_mcp_service" 
      ],
      "cwd": "/home/zhz/zhz_agent", 
      "env": {
        "PYTHONPATH": "/home/zhz/zhz_agent:/home/zhz/zhz_agent/zhz_rag",
        "SGLANG_API_URL": "http://localhost:8088/v1/chat/completions",
        "CHROMA_PERSIST_DIRECTORY": "/home/zhz/zhz_agent/zhz_rag/stored_data/chromadb_index",
        "BM25_INDEX_DIRECTORY": "/home/zhz/zhz_agent/zhz_rag/stored_data/bm25_index",
        "KUZU_DB_PATH": "/home/zhz/zhz_agent/zhz_rag/stored_data/kuzu_default_db"
      }
    },    
    "ddgsearch": {
      "command": "uvx",
      "args": ["duckduckgo-mcp-server"], 
      "disabled": false,
      "autoApprove": ["search", "fetch_content"] 
    },   
    "amap-maps": {
      "command": "node",
      "args": [
        "/home/zhz/mcp/amap-maps-mcp-server/build/index.js"
      ],
      "cwd": "/home/zhz/mcp/amap-maps-mcp-server",
      "env": {
        "AMAP_MAPS_API_KEY": "1ca72c66e1c964ce73eecd3822174442"
      },
      "disabled": false
    }
  }
}
    --- END OF FILE CONTENT ---

  File: project_snapshot.txt
    --- START OF FILE CONTENT ---
Project Root: /home/zhz/zhz_agent
--- Project Structure and File Contents ---

Directory: /home/zhz/zhz_agent/
  File (skipped, extension not included): .env
  File (skipped, extension not included): .envrc
  File (skipped, extension not included): .gitignore
  File: README.md
    --- START OF FILE CONTENT ---
# zhz_agent
后端：RAG增强+多agent+dagster

    --- END OF FILE CONTENT ---

  File: __init__.py
    --- File is empty ---

  File: agent_orchestrator_service.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/agent_orchestrator_service.py

import os
import asyncio
import traceback
import json
from typing import List, Dict, Any, Optional, Union
from contextlib import asynccontextmanager
import re
import uvicorn
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from datetime import datetime 

import logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(module)s:%(lineno)d - %(message)s')
service_logger = logging.getLogger(__name__)

from crewai import Agent, Task, Crew, Process
from crewai.tools import BaseTool

# --- 添加开始 ---
from core.prompts.manager_prompts import get_manager_agent_goal, MANAGER_AGENT_BACKSTORY
# --- 添加结束 ---

from core.llm_manager import get_llm_instance, CustomLiteLLMWrapper
from core.tools.enhanced_rag_tool import EnhancedRAGTool
from core.tools.excel_tool import ExcelOperationTool
from core.tools.search_tool import WebSearchTool
from core.tools.time_tool import GetCurrentTimeTool
from core.tools.calculator_tool import CalculateTool


AGENT_SERVICE_PORT = int(os.getenv("AGENT_SERVICE_PORT", 8090))
AGENT_SERVICE_HOST = "0.0.0.0"

class AgentTaskRequest(BaseModel):
    user_query: str = Field(description="用户的原始文本查询。")

class AgentTaskResponse(BaseModel):
    answer: str = Field(description="Agent 系统生成的最终答案或响应。")
    status: str = Field(default="success", description="执行状态: 'success', 'needs_clarification', 'error', 'processing_plan', 'task_created'.")
    intermediate_plan: Optional[Dict[str, Any]] = Field(None, description="如果status是'processing_plan', 这里包含Manager Agent的规划结果。")
    task_id: Optional[str] = Field(None, description="如果任务是异步的，返回任务ID。")
    error_message: Optional[str] = Field(None, description="如果发生错误，此字段包含错误信息。")
    debug_info: Optional[Dict[str, Any]] = Field(None, description="包含执行过程中的调试信息。")
    token_usage: Optional[Dict[str, Any]] = Field(None, description="LLM token 使用情况统计。")

class SubTaskDefinitionForManagerOutput(BaseModel):
    task_description: str = Field(description="用户的原始请求原文。")
    reasoning_for_plan: Optional[str] = Field(None, description="Manager Agent的决策思考过程。")
    selected_tool_names: List[str] = Field(description="选定的工具名称列表。如果直接回答，则为空列表。")
    direct_answer_content: Optional[str] = Field(None, description="如果选择直接回答，这里是答案内容。")
    tool_input_args: Optional[Dict[str, Any]] = Field(None, description="如果选择使用非Excel工具，这里是传递给该工具的参数。")
    excel_sqo_payload: Optional[List[Dict[str, Any]]] = Field(None, description="如果选择使用Excel工具，这里是SQO操作字典的列表。")

manager_llm: Optional[CustomLiteLLMWrapper] = None
worker_llm: Optional[CustomLiteLLMWrapper] = None
manager_agent_instance: Optional[Agent] = None
worker_agent_instance: Optional[Agent] = None
core_tools_instances: List[BaseTool] = []

# --- 覆盖开始 ---
CORE_TOOLS_ZHZ_AGENT = {
    "enhanced_rag_tool": "【核心RAG工具】用于从本地知识库查找信息、回答复杂问题，整合了向量、关键词和图谱检索。",
    "excel_operation_tool": "【Excel操作工具】通过结构化查询对象(SQO)对Excel文件执行复杂的数据查询、筛选、聚合等操作。此工具通过本地代理在Windows上运行。",
    "web_search_tool": "【网络搜索工具】使用DuckDuckGo搜索引擎在互联网上查找与用户查询相关的信息。此工具通过MCP调用。",
    "get_current_time_tool": "【时间工具】获取当前的日期和时间，可指定时区。此工具在Agent的Python环境中直接执行。",
    "calculate_tool": "【计算器工具】执行数学表达式的计算并返回数值结果。此工具在Agent的Python环境中直接执行。"
}
# --- 覆盖结束 ---
CORE_TOOL_NAMES_LIST = list(CORE_TOOLS_ZHZ_AGENT.keys()) # 这行会自动更新
TOOL_OPTIONS_STR_FOR_MANAGER = "\n".join( # 这行会自动更新
    [f"- '{name}': {desc}" for name, desc in CORE_TOOLS_ZHZ_AGENT.items()]
)

@asynccontextmanager
async def lifespan(app: FastAPI):
    global manager_llm, worker_llm, manager_agent_instance, worker_agent_instance, core_tools_instances
    # --- [修改] 为了让 Manager 的 prompt 也只看到 RAG 工具，我们需要在函数作用域内临时修改这些 ---
    global CORE_TOOL_NAMES_LIST, TOOL_OPTIONS_STR_FOR_MANAGER 
    
    print("--- Agent Orchestrator Service: Lifespan startup ---")

    # --- 步骤 1: 初始化核心工具实例 ---
    print("Initializing core tool instances...")
    enhanced_rag_tool_instance = None 
    excel_operation_tool_instance = None
    web_search_tool_instance = None
    get_current_time_tool_instance = None
    calculate_tool_instance = None
    try:
        enhanced_rag_tool_instance = EnhancedRAGTool()
        excel_operation_tool_instance = ExcelOperationTool() # 尝试实例化
        web_search_tool_instance = WebSearchTool()       # 尝试实例化
        get_current_time_tool_instance = GetCurrentTimeTool()
        calculate_tool_instance = CalculateTool()

        core_tools_instances = []
        if enhanced_rag_tool_instance:
            core_tools_instances.append(enhanced_rag_tool_instance)
        if get_current_time_tool_instance:
            core_tools_instances.append(get_current_time_tool_instance)
        if calculate_tool_instance:
            core_tools_instances.append(calculate_tool_instance)
        if excel_operation_tool_instance:
            core_tools_instances.append(excel_operation_tool_instance)
        if web_search_tool_instance:
            core_tools_instances.append(web_search_tool_instance)

        if not core_tools_instances:
            print("CRITICAL ERROR: No core tools could be initialized. Aborting LLM/Agent setup.")
            # 在这种情况下，后续的LLM和Agent初始化可能会失败或无意义
            # 可以考虑在这里抛出异常或设置一个全局状态阻止服务启动
        else:
            print(f"Successfully initialized tools: {[tool.name for tool in core_tools_instances]}")
        print(f"--- FULL TOOL CONFIGURATION WILL BE USED BY MANAGER (if successfully initialized) ---")
        print(f"Effective CORE_TOOL_NAMES_LIST for Manager (based on global def): {CORE_TOOL_NAMES_LIST}") # 这是全局的
        print(f"Effective TOOL_OPTIONS_STR_FOR_MANAGER for Manager (based on global def):\n{TOOL_OPTIONS_STR_FOR_MANAGER}") # 这是全局的
        print(f"Actually initialized tools for Worker: {[tool.name for tool in core_tools_instances]}")
        print(f"--- END OF TOOL CONFIGURATION ---")
        
    except Exception as e:
        print(f"ERROR during core tool initialization: {e}", exc_info=True)
        core_tools_instances = [] # 确保出错时为空列表
        # 后续LLM/Agent初始化时，如果 core_tools_instances 为空，它们可能需要特殊处理或报错

    # ... 后续的LLM和Agent初始化代码 ...
    # 确保 Manager Agent 的 tools=[] (它不直接调用工具)
    # 确保 Worker Agent 的 tools=core_tools_instances (它需要所有可用的工具实例)


    # --- 步骤 2: 初始化 LLM 实例 ---
    # LLM 初始化时，agent_tools 参数将使用上面步骤中已更新（且只包含RAG工具）的 core_tools_instances
    print("Initializing LLM instances...")
    try:
        gemini_tool_config = {"function_calling_config": {"mode": "AUTO"}}
        # Manager LLM 初始化
        manager_llm = get_llm_instance(
            llm_type="cloud_gemini", 
            temperature=0.1, 
            max_tokens=4096, 
            tool_config=gemini_tool_config,
            agent_tools=core_tools_instances # 使用已更新的 core_tools_instances
        )
        if not manager_llm:
            print("Failed to initialize Manager LLM (Cloud Gemini). Attempting fallback...")
            manager_llm = get_llm_instance(
                llm_type="local_qwen", 
                temperature=0.1, 
                max_tokens=3072, 
                tool_config=gemini_tool_config,
                agent_tools=core_tools_instances 
            )
        
        # Worker LLM 初始化
        print("Initializing Worker LLM (attempting Cloud Gemini first)...")
        worker_gemini_tool_config = {"function_calling_config": {"mode": "AUTO"}} 
        worker_llm = get_llm_instance(
            llm_type="cloud_gemini", 
            temperature=0.5, 
            max_tokens=3072,
            tool_config=worker_gemini_tool_config,
            agent_tools=core_tools_instances # 使用已更新的 core_tools_instances
        )
        if not worker_llm:
            print("Failed to initialize Worker LLM (Cloud Gemini). Attempting fallback to local_qwen...")
            worker_llm = get_llm_instance(
                llm_type="local_qwen", 
                temperature=0.6, 
                max_tokens=3072,
                agent_tools=core_tools_instances
            )

        if manager_llm: print(f"Manager LLM initialized: {manager_llm.model_name}")
        else: print("CRITICAL: Failed to initialize Manager LLM.")
        if worker_llm: print(f"Worker LLM initialized: {worker_llm.model_name}")
        else: print("CRITICAL: Failed to initialize Worker LLM.")
    except Exception as e:
        print(f"FATAL ERROR during LLM initialization: {e}") 
        traceback.print_exc() 
        manager_llm = None; worker_llm = None

    # --- 步骤 3: 初始化 Agent 实例 ---
    if manager_llm:
        current_manager_goal = get_manager_agent_goal(TOOL_OPTIONS_STR_FOR_MANAGER)
        manager_agent_instance = Agent(
        role='资深AI任务分解与Excel查询规划师 (Senior AI Task Decomposition and Excel Query Planner)',
        goal=current_manager_goal,
        backstory=MANAGER_AGENT_BACKSTORY,
        llm=manager_llm,
        verbose=True,
        allow_delegation=False,
        tools=[] 
    )
        print(f"Manager Agent initialized with LLM: {manager_llm.model_name}")
        # --- 添加的调试日志开始 ---
        if manager_agent_instance and manager_agent_instance.llm:
            print(f"DEBUG_LIFESPAN: Manager LLM type: {type(manager_agent_instance.llm)}")
            print(f"DEBUG_LIFESPAN: Is manager_llm.supports_function_calling callable? {callable(getattr(manager_agent_instance.llm, 'supports_function_calling', None))}")
            if callable(getattr(manager_agent_instance.llm, 'supports_function_calling', None)):
                print(f"DEBUG_LIFESPAN: manager_llm.supports_function_calling() returns: {manager_agent_instance.llm.supports_function_calling()}")
            else:
                print(f"DEBUG_LIFESPAN: manager_llm.supports_function_calling is NOT callable. Value: {getattr(manager_agent_instance.llm, 'supports_function_calling', 'Attribute not found')}")
        # --- 添加的调试日志结束 ---

    if worker_llm:
        worker_agent_instance = Agent(
            role='任务执行专家 (Task Execution Expert)',
            goal="根据Manager分配的具体任务描述和指定的工具，高效地执行任务并提供结果。",
            backstory="""我是一个AI执行者，专注于使用【Manager明确授权给我的工具】来解决问题。
                        我会严格遵循任务指令。如果任务是调用Excel工具并提供了SQO列表，我会按顺序迭代处理这些SQO，并整合结果。
                        对于像 'get_current_time_tool' 和 'calculate_tool' 这样的本地Python工具，我会直接在我的环境中执行它们。
                        对于其他工具，我会使用工具的名称（例如 'enhanced_rag_tool', 'excel_operation_tool', 'web_search_tool'）来调用它们。""",
            llm=worker_llm,
            verbose=True,
            allow_delegation=False,
            tools=core_tools_instances # Worker Agent 使用已更新（只含RAG工具）的 core_tools_instances
        )
        print(f"Worker Agent initialized with LLM: {worker_llm.model_name} and tools: {[t.name for t in worker_agent_instance.tools]}")

    if not manager_agent_instance or not worker_agent_instance:
        print("CRITICAL: One or more core agents failed to initialize. Service functionality will be severely limited.")
    elif not core_tools_instances and worker_agent_instance : 
        print("WARNING: Worker Agent initialized, but no core tools were successfully instantiated. Tool-based tasks will fail.")

    print("--- Agent Orchestrator Service: Lifespan startup complete ---")
    yield
    print("--- Agent Orchestrator Service: Lifespan shutdown ---")

app = FastAPI(
    title="Agent Orchestrator Service",
    description="接收用户请求，通过Manager/Worker Agent模型进行任务规划和执行。",
    version="0.1.1", # 版本更新
    lifespan=lifespan
)

@app.post("/v1/execute_task", response_model=AgentTaskResponse)
async def execute_task_endpoint(request: AgentTaskRequest):
    global manager_agent_instance, worker_agent_instance, core_tools_instances # 确保能访问全局 Agent 和工具实例
    
    print(f"Received agent task request: User Query='{request.user_query}'") # 使用 print 替代 logger

    if not manager_agent_instance or not worker_agent_instance:
        print("CRITICAL ERROR: Core agents are not initialized. Cannot process task.")
        raise HTTPException(status_code=503, detail="Service not ready: Core agents failed to initialize.")
    if not core_tools_instances:
        print("WARNING: Core tools are not initialized. Tool-based tasks may fail.")
    # --- 构建 Manager Task 的描述 ---
    # 我们将用户请求和可用的核心工具列表传递给 Manager Agent
    # Manager Agent 的 goal 和 backstory 已经包含了大部分指令
    # Task 的 description 主要用于传递动态信息，如当前用户查询
    manager_task_description_for_crewai = f"""
    用户的原始请求是：'{request.user_query}'

    你的任务是：
    1.  仔细分析此用户请求的意图。
    2.  严格回顾并遵循你在 `goal` 中被赋予的【决策规则与优先级】以及【决策示例】。
    3.  基于这些规则和示例，决定最佳的处理路径：是直接回答，还是选择一个最合适的工具。
    4.  如果选择了工具，请准备好调用该工具所需的参数。
    5.  严格按照 `SubTaskDefinitionForManagerOutput` 的JSON格式输出你的规划。`task_description` 字段必须是用户的原始请求原文: '{request.user_query}'。同时提供你的 `reasoning_for_plan`。
    """
    
    # --- Manager Task 的期望输出格式说明 ---
    manager_task_expected_output_description = f"""一个JSON对象，必须严格符合以下Pydantic模型的结构（你不需要输出 "SubTaskDefinitionForManagerOutput" 这个词本身）：
    {{
      "task_description": "string (固定为用户的原始请求: '{request.user_query}')",
      "reasoning_for_plan": "string (你的决策思考过程)",
      "selected_tool_names": ["list of strings (选定的工具名称列表。如果直接回答，则为空列表。如果使用工具，则为 ['enhanced_rag_tool'])"],
      "direct_answer_content": "string (可选, 仅当 selected_tool_names 为空列表时，这里是你的答案内容)",
      "tool_input_args": {{ "key": "value" }} (可选, 仅当 selected_tool_names 包含'enhanced_rag_tool'时，这里是给该工具的参数字典),
      "excel_sqo_payload": null # 当前Excel工具不可用，此字段应为null
    }}

    【重要输出规则】:
    - 如果你选择【直接回答】：`selected_tool_names` 必须是空列表 `[]`，`direct_answer_content` 必须包含你的答案，`tool_input_args` 和 `excel_sqo_payload` 应该为 `null` 或不存在。
    - 如果你选择使用【enhanced_rag_tool】：`selected_tool_names` 必须包含 `"enhanced_rag_tool"`，`direct_answer_content` 应该为 `null` 或不存在，`tool_input_args` 必须包含调用该工具所需的参数 (例如 `{{ "query": "{request.user_query}" }}` )，`excel_sqo_payload` 应该为 `null` 或不存在。

    示例输出 (直接回答):
    {{
      "task_description": "{request.user_query}",
      "reasoning_for_plan": "这是一个常识性问题，我可以根据我的内部知识直接回答。",
      "selected_tool_names": [],
      "direct_answer_content": "中国的首都是北京。",
      "tool_input_args": null,
      "excel_sqo_payload": null
    }}

    示例输出 (使用RAG工具):
    {{
      "task_description": "{request.user_query}",
      "reasoning_for_plan": "用户询问关于公司政策的问题，这需要从知识库中查找。",
      "selected_tool_names": ["enhanced_rag_tool"],
      "direct_answer_content": null,
      "tool_input_args": {{"query": "{request.user_query}", "top_k_vector": 5, "top_k_kg": 3, "top_k_bm25": 3}},
      "excel_sqo_payload": null
    }}
    请严格按照此JSON格式输出。
    """

    manager_task = Task(
        description=manager_task_description_for_crewai,
        expected_output=manager_task_expected_output_description,
        agent=manager_agent_instance,
        async_execution=False,
        output_pydantic=SubTaskDefinitionForManagerOutput
    )

    # --- 步骤 2: 执行 Manager Task ---
    print("Executing Manager Task...")
    manager_plan_object: Optional[SubTaskDefinitionForManagerOutput] = None
    manager_raw_output: Optional[str] = None

    try:
        manager_crew = Crew(
            agents=[manager_agent_instance],
            tasks=[manager_task],
            process=Process.sequential,
            verbose=True 
        )
        print(f"[{datetime.now()}] About to call manager_crew.kickoff()...") # <--- 添加日志
        manager_task_raw_result = manager_crew.kickoff(inputs={})
        print(f"[{datetime.now()}] manager_crew.kickoff() returned.") # <--- 添加日志

        # --- 新的解析逻辑，处理 CrewOutput ---
        actual_task_output: Optional[Any] = None # 用于存储实际的 TaskOutput 或字符串

        if hasattr(manager_task_raw_result, 'tasks_output') and isinstance(manager_task_raw_result.tasks_output, list) and manager_task_raw_result.tasks_output:
            # CrewOutput.tasks_output 是一个 TaskOutput 对象的列表
            actual_task_output = manager_task_raw_result.tasks_output[0] # 我们只有一个 manager_task
            print(f"Extracted TaskOutput from CrewOutput: {type(actual_task_output)}")
        elif hasattr(manager_task_raw_result, 'raw_output'): # 兼容直接返回 TaskOutput 的情况 (旧版或特定配置)
            actual_task_output = manager_task_raw_result
            print(f"kickoff returned a TaskOutput-like object directly: {type(actual_task_output)}")
        elif isinstance(manager_task_raw_result, str): # 直接返回字符串
            actual_task_output = manager_task_raw_result
            print(f"kickoff returned a raw string.")
        else:
            print(f"Warning: manager_crew.kickoff() returned an unexpected type: {type(manager_task_raw_result)}")
            actual_task_output = str(manager_task_raw_result)
        # --- 结束新的解析逻辑 ---


        # --- 后续的解析逻辑，现在基于 actual_task_output ---
        print(f"DEBUG: Type of actual_task_output: {type(actual_task_output)}")
        
        manager_raw_llm_output_str: Optional[str] = None
        if hasattr(actual_task_output, 'raw') and isinstance(actual_task_output.raw, str):
            manager_raw_llm_output_str = actual_task_output.raw.strip()
            print(f"DEBUG: actual_task_output.raw (LLM's original string output) IS:\n---\n{manager_raw_llm_output_str}\n---")
        else:
            print(f"DEBUG: actual_task_output.raw is not a string or does not exist. Value: {getattr(actual_task_output, 'raw', 'Attribute .raw not found')}")
            manager_raw_llm_output_str = str(actual_task_output) # Fallback

        # 优先尝试使用 CrewAI 已经解析好的 Pydantic 对象
        if hasattr(actual_task_output, 'pydantic_output') and \
           actual_task_output.pydantic_output is not None and \
           isinstance(actual_task_output.pydantic_output, SubTaskDefinitionForManagerOutput):
            print("DEBUG: Successfully using actual_task_output.pydantic_output.")
            manager_plan_object = actual_task_output.pydantic_output
        
        # 如果 Pydantic 对象不可用，但我们从 .raw 成功获取了字符串，则尝试解析它
        # 并且确保 manager_plan_object 之前没有被成功赋值
        elif manager_plan_object is None and manager_raw_llm_output_str: # <--- 添加 manager_plan_object is None 条件
            print(f"DEBUG: pydantic_output not available. Attempting to parse string from actual_task_output.raw:\n---\n{manager_raw_llm_output_str}\n---")
            json_to_parse_from_raw = manager_raw_llm_output_str # 使用我们从 .raw 获取的
            try:
                final_answer_marker = "## Final Answer:" # 虽然日志显示Qwen没输出这个，但保留以防万一
                if final_answer_marker in json_to_parse_from_raw:
                    json_to_parse_from_raw = json_to_parse_from_raw.split(final_answer_marker, 1)[-1].strip()
                
                cleaned_json_str = json_to_parse_from_raw.strip()
                if cleaned_json_str.startswith("```json"): cleaned_json_str = cleaned_json_str[len("```json"):].strip()
                if cleaned_json_str.endswith("```"): cleaned_json_str = cleaned_json_str[:-len("```")].strip()
                
                if not cleaned_json_str: raise ValueError("Cleaned JSON string (from .raw) is empty.")
                manager_plan_object = SubTaskDefinitionForManagerOutput(**json.loads(cleaned_json_str))
                print(f"Parsed Manager Plan (from actual_task_output.raw): {manager_plan_object.model_dump_json(indent=2)}")
            except (json.JSONDecodeError, ValueError, Exception) as e:
                print(f"Error parsing JSON from actual_task_output.raw: {e}. Raw content used: {json_to_parse_from_raw}")
        
        # 在所有尝试之后，如果 manager_plan_object 仍然是 None，才报告最终的解析失败
        if not manager_plan_object:
             # manager_raw_output 现在应该引用我们尝试过的 manager_raw_llm_output_str
             final_raw_output_for_debug = manager_raw_llm_output_str if manager_raw_llm_output_str else str(actual_task_output)
             print(f"Manager Task did not produce a valid Pydantic object after all attempts. Final Raw for debug: {final_raw_output_for_debug}")
             return AgentTaskResponse(answer="无法解析Manager Agent的规划结果。", status="error",
                                      error_message="Failed to parse manager plan after all attempts.",
                                      debug_info={"manager_raw_output_tried": final_raw_output_for_debug})
        
        # 如果 manager_plan_object 成功解析，则打印日志（这行已经在您的代码中）
        print(f"Parsed Manager Plan: {manager_plan_object.model_dump_json(indent=2)}")
        
    except Exception as e:
        print(f"Error executing Manager Task or its Crew: {e}") # <--- 修改后的第一行
        traceback.print_exc() # <--- 修改后的第二行
        return AgentTaskResponse(
            answer="执行Manager Agent任务时发生错误。", status="error", error_message=str(e),
            debug_info={"traceback": traceback.format_exc(), "manager_raw_output": manager_raw_output}
        )

    # --- 步骤 2: 根据 Manager 的规划执行后续操作 ---
    if not manager_plan_object.selected_tool_names and manager_plan_object.direct_answer_content:
        print(f"Manager decided to answer directly. Answer: {manager_plan_object.direct_answer_content}")
        return AgentTaskResponse(
            answer=manager_plan_object.direct_answer_content,
            status="success",
            debug_info={"manager_plan": manager_plan_object.model_dump()}
        )
    elif not manager_plan_object.selected_tool_names and not manager_plan_object.direct_answer_content:
        # 如果 Manager 既没有选择工具，也没有提供直接答案，这可能是一个规划错误
         print(f"Manager Warning: No tool selected and no direct answer provided. Reasoning: {manager_plan_object.reasoning_for_plan}")
         return AgentTaskResponse(
            answer=manager_plan_object.reasoning_for_plan or "Manager 未能提供明确的行动计划或答案。",
            status="success", # 或者 "error" 取决于您如何定义这种情况
            debug_info={"manager_plan": manager_plan_object.model_dump()}
        )

    # --- 步骤 3: 如果 Manager 规划使用工具，则创建并执行 Worker Task ---
    selected_tool_name = manager_plan_object.selected_tool_names[0] if manager_plan_object.selected_tool_names else None

    if not selected_tool_name:
        return AgentTaskResponse(
            answer="Manager规划使用工具但未在selected_tool_names中指定工具名称。", status="error",
            error_message="Tool name missing in manager plan's selected_tool_names.",
            debug_info={"manager_plan": manager_plan_object.model_dump()}
        )

    # 找到对应的工具实例
    tool_instance_for_worker = next((tool for tool in core_tools_instances if tool.name == selected_tool_name), None)

    if not tool_instance_for_worker:
        return AgentTaskResponse(
            answer=f"系统中未找到Manager规划使用的工具: {selected_tool_name}。", status="error",
            error_message=f"Tool '{selected_tool_name}' not found in core_tools_instances.",
            debug_info={"manager_plan": manager_plan_object.model_dump(), "available_tools": [t.name for t in core_tools_instances]}
        )

    print(f"Manager planned to use tool: {selected_tool_name}. Instance found: {tool_instance_for_worker is not None}")

    worker_task_description = ""
    worker_task_inputs = {} 

    if selected_tool_name == "enhanced_rag_tool":
        rag_query = manager_plan_object.tool_input_args.get("query", request.user_query) if manager_plan_object.tool_input_args else request.user_query
        top_k_v = manager_plan_object.tool_input_args.get("top_k_vector", 5) if manager_plan_object.tool_input_args else 5
        top_k_kg = manager_plan_object.tool_input_args.get("top_k_kg", 3) if manager_plan_object.tool_input_args else 3
        top_k_b = manager_plan_object.tool_input_args.get("top_k_bm25", 3) if manager_plan_object.tool_input_args else 3
        
        worker_task_description = f"请使用增强RAG工具回答以下问题：'{rag_query}'。\n使用以下参数进行检索：top_k_vector={top_k_v}, top_k_kg={top_k_kg}, top_k_bm25={top_k_b}。"
        worker_task_inputs = {
            "query": rag_query, 
            "top_k_vector": top_k_v, 
            "top_k_kg": top_k_kg, 
            "top_k_bm25": top_k_b
        }

    elif selected_tool_name == "excel_operation_tool":
        excel_sqo_list = manager_plan_object.excel_sqo_payload # excel_sqo_list 在这里赋值

        # --- 添加调试打印开始 ---
        print(f"DEBUG_EXCEL_TOOL: Type of manager_plan_object.excel_sqo_payload: {type(manager_plan_object.excel_sqo_payload)}")
        print(f"DEBUG_EXCEL_TOOL: Value of manager_plan_object.excel_sqo_payload: {manager_plan_object.excel_sqo_payload}")
        print(f"DEBUG_EXCEL_TOOL: Value of excel_sqo_list: {excel_sqo_list}")
        print(f"DEBUG_EXCEL_TOOL: Is excel_sqo_list None? {excel_sqo_list is None}")
        print(f"DEBUG_EXCEL_TOOL: Is excel_sqo_list an empty list? {isinstance(excel_sqo_list, list) and not excel_sqo_list}")
        # --- 添加调试打印结束 ---

        if not excel_sqo_list:
            return AgentTaskResponse(
                answer="Manager选择Excel工具但未提供SQO列表。", status="error",
                error_message="excel_sqo_payload is missing for excel_operation_tool.",
                debug_info={"manager_plan": manager_plan_object.model_dump()}
            )
        
        # --- 测试用硬编码 ---
        test_excel_file_path_on_windows = r"C:\\FlutterProjects\\data\\test2.xlsx" # 确保这个路径在Windows上是有效的
        test_excel_sheet_name: Union[str, int] = "Sheet1"
        # --- 测试用硬编码结束 ---
        
        worker_task_description = (
            f"你需要处理一个Excel文件相关的任务。目标文件路径是 '{test_excel_file_path_on_windows}'，工作表是 '{test_excel_sheet_name}'。\n"
            f"请严格按照以下SQO操作定义列表，顺序执行每一个操作，并整合所有操作的结果形成最终答案。\n"
            f"SQO操作定义列表: {json.dumps(excel_sqo_list, ensure_ascii=False)}\n\n"
            f"对于列表中的【每一个SQO操作定义字典】，你需要调用 '{tool_instance_for_worker.name}' 工具一次，"
            f"将该字典作为 'sqo_operation_definition' 参数，"
            f"同时传递 'target_excel_file_path': '{test_excel_file_path_on_windows}' 和 "
            f"'target_excel_sheet_name': '{test_excel_sheet_name}'。"
        )
        worker_task_inputs = {
            "excel_sqo_list_to_execute": excel_sqo_list, # Worker Agent会迭代这个
            "target_excel_file_path_for_worker": test_excel_file_path_on_windows, # Worker Agent使用这个传递给工具
            "target_excel_sheet_name_for_worker": test_excel_sheet_name # Worker Agent使用这个传递给工具
        }
        
    elif selected_tool_name == "web_search_tool": # 这是唯一且正确的 web_search_tool 分支
        search_query = manager_plan_object.tool_input_args.get("query", request.user_query) if manager_plan_object.tool_input_args else request.user_query
        max_results = manager_plan_object.tool_input_args.get("max_results", 5) if manager_plan_object.tool_input_args else 5
        worker_task_description = f"请使用网络搜索工具查找关于 '{search_query}' 的信息，返回最多 {max_results} 条结果。"
        worker_task_inputs = {"query": search_query, "max_results": max_results}
        
    elif selected_tool_name == "get_current_time_tool":
        timezone_str = manager_plan_object.tool_input_args.get("timezone", "Asia/Shanghai") if manager_plan_object.tool_input_args else "Asia/Shanghai"
        worker_task_description = f"请使用时间工具获取当前时间。时区参数为: '{timezone_str}'。"
        worker_task_inputs = {"timezone_str": timezone_str} 

    elif selected_tool_name == "calculate_tool":
        expression_str = manager_plan_object.tool_input_args.get("expression", "") if manager_plan_object.tool_input_args else ""
        if not expression_str:
             return AgentTaskResponse(
                answer="Manager选择计算器工具但未提供表达式。", status="error",
                error_message="expression is missing for calculate_tool.",
                debug_info={"manager_plan": manager_plan_object.model_dump()}
            )
        worker_task_description = f"请使用计算器工具计算以下表达式: '{expression_str}'。"
        worker_task_inputs = {"expression": expression_str}
        
    else: # 这是处理未知工具的 else
        return AgentTaskResponse(
            answer=f"未知的工具名称 '{selected_tool_name}' 被Manager规划。", status="error",
            error_message=f"Unknown tool '{selected_tool_name}' planned by manager.",
            debug_info={"manager_plan": manager_plan_object.model_dump()}
        )

    # 创建 Worker Task
    worker_task = Task(
        description=worker_task_description,
        expected_output="任务的执行结果，通常是一个字符串，其中包含答案或操作的状态。",
        agent=worker_agent_instance,
        tools=[tool_instance_for_worker], # 只给 Worker 当前任务需要的工具
        async_execution=False # Worker Task 通常也是同步的
    )

    print(f"Executing Worker Task with tool: {selected_tool_name}")
    print(f"Worker Task Description: {worker_task_description}")
    print(f"Worker Task Inputs: {worker_task_inputs}")
    
    worker_final_result: str = ""
    worker_crew_usage_metrics: Optional[Dict[str, Any]] = None # 用于存储 token usage
    token_usage_for_response: Optional[Dict[str, Any]] = None # <--- 在这里初始化
    try:
        # --- [关键修改] 为 Worker Task 创建并运行一个 Crew ---
        worker_crew = Crew(
            agents=[worker_agent_instance], # Worker Agent
            tasks=[worker_task],             # 它要执行的单个任务
            process=Process.sequential,
            verbose=True # 可以设为 True 或 2 来查看 Worker Crew 的详细日志
        )
        
        # 如果 worker_task_inputs 为空，则 inputs={}
        # CrewAI 的 kickoff 方法期望 inputs 是一个字典
        task_execution_inputs = worker_task_inputs if worker_task_inputs else {}
            
        # 执行 Worker Crew
        # kickoff 返回的是 CrewOutput 对象，或者在某些旧版本/配置下可能直接是结果字符串或 TaskOutput
        worker_crew_output = worker_crew.kickoff(inputs=task_execution_inputs)
        
        # 从 Worker Crew 的输出中提取结果
        # 这与我们处理 Manager Crew 输出的逻辑类似
        actual_worker_task_output: Optional[Any] = None
        if hasattr(worker_crew_output, 'tasks_output') and isinstance(worker_crew_output.tasks_output, list) and worker_crew_output.tasks_output:
            actual_worker_task_output = worker_crew_output.tasks_output[0] # 我们只有一个 worker_task
        elif hasattr(worker_crew_output, 'raw_output'): # 兼容直接返回 TaskOutput
            actual_worker_task_output = worker_crew_output
        elif isinstance(worker_crew_output, str): # 直接返回字符串
            actual_worker_task_output = worker_crew_output
        else: # 其他意外情况
            actual_worker_task_output = str(worker_crew_output)

        # 从 actual_worker_task_output 中提取最终的字符串结果
        if hasattr(actual_worker_task_output, 'raw') and isinstance(actual_worker_task_output.raw, str):
            worker_final_result = actual_worker_task_output.raw.strip()
        elif isinstance(actual_worker_task_output, str):
            worker_final_result = actual_worker_task_output.strip()
        else: # Fallback
            worker_final_result = str(actual_worker_task_output)
        
        print(f"Worker Task executed. Result: {worker_final_result}")
        
        # 获取 Worker Crew 的 token usage
        if hasattr(worker_crew, 'usage_metrics'):
            worker_crew_usage_metrics = worker_crew.usage_metrics
            print(f"DEBUG AGENT_ORCH: Raw worker_crew.usage_metrics object: {worker_crew_usage_metrics}")
            print(f"DEBUG AGENT_ORCH: Type of worker_crew.usage_metrics: {type(worker_crew_usage_metrics)}")
            if hasattr(worker_crew_usage_metrics, 'model_dump'):
                print(f"DEBUG AGENT_ORCH: worker_crew.usage_metrics.model_dump(): {worker_crew_usage_metrics.model_dump()}")
            elif isinstance(worker_crew_usage_metrics, dict):
                    print(f"DEBUG AGENT_ORCH: worker_crew.usage_metrics (is dict): {worker_crew_usage_metrics}")
            else:
                print(f"DEBUG AGENT_ORCH: worker_crew.usage_metrics is not a dict and has no model_dump.")
        else:
            print("DEBUG AGENT_ORCH: worker_crew does not have usage_metrics attribute.")
            worker_crew_usage_metrics = None # 确保它被定义
            # --- [结束关键修改] ---

            # 准备 token_usage_for_response
            token_usage_for_response = None
            if worker_crew_usage_metrics:
                if hasattr(worker_crew_usage_metrics, 'model_dump'):
                    token_usage_for_response = worker_crew_usage_metrics.model_dump()
                elif isinstance(worker_crew_usage_metrics, dict):
                    token_usage_for_response = worker_crew_usage_metrics
                else:
                    # 如果不是 Pydantic 模型或字典，尝试转换为字符串记录，但不作为结构化数据返回
                    service_logger.warning(f"Unexpected type for worker_crew_usage_metrics: {type(worker_crew_usage_metrics)}. Will not be included in structured token_usage.")

        return AgentTaskResponse(
                answer=worker_final_result,
                status="success",
                debug_info={
                    "manager_plan": manager_plan_object.model_dump(),
                    "worker_tool_used": selected_tool_name,
                    "worker_task_inputs": worker_task_inputs 
                },
                token_usage=token_usage_for_response # 使用处理后的 token_usage_for_response
            )

    except Exception as e:
    # 使用 traceback 打印详细错误
        print(f"Error executing Worker Task for tool {selected_tool_name}: {e}")
        traceback.print_exc() 
        return AgentTaskResponse(
            answer=f"执行工具 '{selected_tool_name}' 时发生错误。",
            status="error",
            error_message=str(e),
            debug_info={
                "manager_plan": manager_plan_object.model_dump(),
                "worker_tool_used": selected_tool_name,
                "worker_task_inputs": worker_task_inputs,
                "traceback": traceback.format_exc()
            }
        )

if __name__ == "__main__":
    print(f"--- Starting Agent Orchestrator FastAPI Service on {AGENT_SERVICE_HOST}:{AGENT_SERVICE_PORT} ---")
    uvicorn.run("agent_orchestrator_service:app", host=AGENT_SERVICE_HOST, port=AGENT_SERVICE_PORT, reload=True) # 确保模块名正确
    --- END OF FILE CONTENT ---

  File: batch_rename_and_replace.py
    --- START OF FILE CONTENT ---
import os
import shutil # 用于备份

# --- 配置 ---
ROOT_DIR = "/home/zhz/zhz_agent"  # 您项目的根目录
OLD_STRING = "sglang_wrapper"
NEW_STRING = "llm_interface"

# 需要处理的文件扩展名
TARGET_EXTENSIONS = ('.py', '.md', '.txt', '.json', '.yaml', '.yml') # 根据需要添加或修改

# 需要排除的目录名称 (精确匹配，小写)
EXCLUDE_DIRS = {
    '.git',
    '.venv',
    '__pycache__',
    'logs', # 假设您有日志目录不想扫描
    'stored_data', # 通常这里面是数据文件，不是代码引用
    'zhz_rag_pipeline_dagster_project.egg-info', # 构建产物
    'zhz_rag_core.egg-info', # 构建产物
    # 根据您的项目结构，可能还需要添加其他如 data, models, .vscode, node_modules 等
}

# 需要排除的文件名 (精确匹配，小写)
EXCLUDE_FILES = {
    'project_snapshot.txt', # 避免修改快照文件
    # 脚本自身会自动排除 (如果它在扫描路径下)
}

# --- 备份配置 ---
BACKUP_SUFFIX = ".bak_before_rename" # 备份文件的后缀

def should_process_file(filepath, filename_lower):
    """判断是否应该处理该文件"""
    if not filename_lower.endswith(TARGET_EXTENSIONS):
        return False
    if filename_lower in EXCLUDE_FILES:
        return False
    return True

def process_file_content(filepath):
    """读取文件内容，进行替换，如果发生改变则写回"""
    try:
        with open(filepath, 'r', encoding='utf-8') as f:
            content = f.read()
    except Exception as e:
        print(f"  - Error reading file {filepath}: {e}")
        return False

    if OLD_STRING in content:
        print(f"  - Found '{OLD_STRING}' in: {filepath}")
        
        # 创建备份
        backup_filepath = filepath + BACKUP_SUFFIX
        try:
            shutil.copy2(filepath, backup_filepath)
            print(f"    - Backup created: {backup_filepath}")
        except Exception as e_backup:
            print(f"    - ERROR creating backup for {filepath}: {e_backup}. Skipping modification.")
            return False

        new_content = content.replace(OLD_STRING, NEW_STRING)
        
        try:
            with open(filepath, 'w', encoding='utf-8') as f:
                f.write(new_content)
            print(f"    - Replaced in: {filepath}")
            return True
        except Exception as e_write:
            print(f"    - Error writing to file {filepath}: {e_write}")
            # 如果写回失败，可以考虑是否恢复备份，但通常保留备份让用户手动处理更好
            return False
    return False

def rename_files_and_dirs(root_path):
    """重命名包含 OLD_STRING 的文件和目录"""
    for dirpath, dirnames, filenames in os.walk(root_path, topdown=False): # topdown=False 确保先处理子目录
        # 排除特定目录
        dirnames[:] = [d for d in dirnames if d.lower() not in EXCLUDE_DIRS]

        # 重命名文件
        for filename in filenames:
            if OLD_STRING in filename:
                old_filepath = os.path.join(dirpath, filename)
                new_filename = filename.replace(OLD_STRING, NEW_STRING)
                new_filepath = os.path.join(dirpath, new_filename)
                if os.path.exists(new_filepath):
                    print(f"  - SKIPPING rename (file): Target '{new_filepath}' already exists.")
                    continue
                try:
                    os.rename(old_filepath, new_filepath)
                    print(f"  - Renamed file: '{old_filepath}' to '{new_filepath}'")
                except Exception as e:
                    print(f"  - ERROR renaming file '{old_filepath}': {e}")
        
        # 重命名目录 (在处理完文件名之后)
        # 注意：os.walk 的 dirnames 是副本，直接修改它不会影响遍历
        # 我们需要在下一次 walk 时，新的目录名才会生效，或者在一次遍历后重新执行这部分
        # 为了简单，这里只重命名当前级别的目录，如果深层嵌套目录名也包含OLD_STRING，可能需要多次运行或更复杂的逻辑
        # 但通常我们只关心顶层的那个 llm 目录下的 sglang_wrapper.py
        # 如果您有 sglang_wrapper_utils 这样的目录名也想改，这个逻辑能处理
        for dirname in dirnames: # dirnames 已经是被 EXCLUDE_DIRS 过滤后的
            if OLD_STRING in dirname:
                old_dirpath = os.path.join(dirpath, dirname)
                new_dirname = dirname.replace(OLD_STRING, NEW_STRING)
                new_dirpath = os.path.join(dirpath, new_dirname)
                if os.path.exists(new_dirpath):
                     print(f"  - SKIPPING rename (dir): Target '{new_dirpath}' already exists.")
                     continue
                try:
                    # shutil.move 更安全，可以处理跨文件系统的情况，但 os.rename 对于同文件系统内的重命名通常足够
                    os.rename(old_dirpath, new_dirpath)
                    print(f"  - Renamed directory: '{old_dirpath}' to '{new_dirpath}'")
                except Exception as e:
                    print(f"  - ERROR renaming directory '{old_dirpath}': {e}")


def main():
    print(f"Starting search and replace in directory: {ROOT_DIR}")
    print(f"Replacing all occurrences of '{OLD_STRING}' with '{NEW_STRING}'")
    print(f"Target file extensions: {TARGET_EXTENSIONS}")
    print(f"Excluded directories: {EXCLUDE_DIRS}")
    print(f"Excluded files: {EXCLUDE_FILES}")
    print("-" * 30)

    # 第一步：先重命名文件和目录
    # 特别是我们知道要将 zhz_rag/llm/sglang_wrapper.py 重命名为 zhz_rag/llm/llm_interface.py
    # 这个脚本的 rename_files_and_dirs 会处理所有匹配的文件名和目录名
    print("\n--- Phase 1: Renaming files and directories ---")
    # 为了确保我们只重命名目标文件，可以先手动执行这个，或者更精确地指定路径
    # 这里我们先尝试全局重命名（除了排除项）
    # rename_files_and_dirs(ROOT_DIR) # 如果您只想重命名特定的那个文件，建议手动操作

    # 手动重命名我们最关心的那个文件
    specific_old_file = os.path.join(ROOT_DIR, "zhz_rag", "llm", f"{OLD_STRING}.py")
    specific_new_file = os.path.join(ROOT_DIR, "zhz_rag", "llm", f"{NEW_STRING}.py")
    if os.path.exists(specific_old_file):
        if os.path.exists(specific_new_file):
            print(f"  - SKIPPING rename: Target '{specific_new_file}' already exists.")
        else:
            try:
                os.rename(specific_old_file, specific_new_file)
                print(f"  - Successfully renamed: '{specific_old_file}' to '{specific_new_file}'")
            except Exception as e:
                print(f"  - ERROR renaming '{specific_old_file}': {e}")
    else:
        print(f"  - INFO: Specific file '{specific_old_file}' not found, assuming already renamed or not present.")


    print("\n--- Phase 2: Replacing content in files ---")
    files_changed_count = 0
    files_scanned_count = 0

    for dirpath, dirnames, filenames in os.walk(ROOT_DIR):
        # 排除特定目录
        dirnames[:] = [d for d in dirnames if d.lower() not in EXCLUDE_DIRS and not d.endswith(BACKUP_SUFFIX)]
        
        for filename in filenames:
            if filename.endswith(BACKUP_SUFFIX): # 跳过备份文件
                continue

            filepath = os.path.join(dirpath, filename)
            filename_lower = filename.lower()
            
            # 排除脚本自身
            if os.path.abspath(filepath) == os.path.abspath(__file__):
                continue

            if should_process_file(filepath, filename_lower):
                files_scanned_count += 1
                if process_file_content(filepath):
                    files_changed_count += 1
            
    print("-" * 30)
    print(f"Scan complete. {files_scanned_count} files scanned.")
    print(f"{files_changed_count} files were modified (backups created with {BACKUP_SUFFIX} suffix).")
    if files_changed_count > 0:
        print("IMPORTANT: Please review the changes carefully, for example using 'git diff'.")
        print("You can restore individual files from their .bak_before_rename backups if needed.")

if __name__ == "__main__":
    # 再次确认
    confirm = input(f"This script will modify files in '{ROOT_DIR}'.\n"
                    f"It will rename '{OLD_STRING}.py' to '{NEW_STRING}.py' in 'zhz_rag/llm/' if it exists,\n"
                    f"and then replace all occurrences of '{OLD_STRING}' with '{NEW_STRING}' in file contents.\n"
                    f"Backups of modified files will be created with '{BACKUP_SUFFIX}' suffix.\n"
                    "Are you sure you want to continue? (yes/no): ")
    if confirm.lower() == 'yes':
        main()
    else:
        print("Operation cancelled by user.")

    --- END OF FILE CONTENT ---

  File: config.yaml
    --- START OF FILE CONTENT ---
# config.yaml

model_list:
  - model_name: gemini/gemini-1.5-flash-latest # 您已有的 Gemini 配置 (如果存在)
    litellm_params:
      model: gemini/gemini-1.5-flash-latest
      api_key: os.environ/GEMINI_API_KEY
      # api_base: http://localhost:YOUR_CLASH_PORT # 如果需要

  # --- 修改我们本地的 Qwen3 1.7B 模型配置 ---
  - model_name: local/qwen3-1.7b-gguf
    litellm_params:
      model: openai/my-local-qwen # 使用 "openai/" 前缀，后面可以跟一个自定义的描述符
      api_base: http://localhost:8088/v1
      api_key: nokey

# ... (其他 LiteLLM 配置) ...
    --- END OF FILE CONTENT ---

  File: debug_kuzu_visibility.py
    --- START OF FILE CONTENT ---
# debug_kuzu_visibility.py
import kuzu
import os
import shutil
import time
import pandas as pd

DB_PATH = "./zhz_rag/stored_data/kuzu_test_visibility_db" # 使用一个新的测试路径

def setup_and_verify_schema():
    print(f"--- Phase 1: Setup and Initial Verification ---")
    if os.path.exists(DB_PATH):
        print(f"Removing existing database at {DB_PATH}")
        shutil.rmtree(DB_PATH)
    
    db_setup = None
    conn_setup = None
    try:
        print(f"Creating new database at {DB_PATH}")
        db_setup = kuzu.Database(DB_PATH, read_only=False)
        conn_setup = kuzu.Connection(db_setup)
        print("Database and connection created for setup.")

        ddl_queries = [
            "CREATE NODE TABLE IF NOT EXISTS ExtractedEntity (id_prop STRING, text STRING, label STRING, PRIMARY KEY (id_prop))",
            "CREATE REL TABLE IF NOT EXISTS WorksAt (FROM ExtractedEntity TO ExtractedEntity)",
            "CREATE REL TABLE IF NOT EXISTS AssignedTo (FROM ExtractedEntity TO ExtractedEntity)"
        ]
        for query in ddl_queries:
            print(f"Executing DDL: {query}")
            conn_setup.execute(query)
        print("DDL execution complete.")

        print("Executing CHECKPOINT...")
        conn_setup.execute("CHECKPOINT;")
        print("CHECKPOINT complete.")

        # Phase 1 Verification
        print("Verifying tables immediately after creation (same session)...")
        try:
            # 尝试 SHOW TABLES;
            print("Attempting 'SHOW TABLES;' in Phase 1...")
            result = conn_setup.execute("SHOW TABLES;") # <--- 修改这里
            df = pd.DataFrame(result.get_as_df())
            print(f"Tables found using 'SHOW TABLES;' (Phase 1):\n{df}")
            # 检查 'name' 列是否存在，并且 'ExtractedEntity' 是否在其中
            if not df.empty and 'name' in df.columns and "ExtractedEntity" in df["name"].tolist():
                print("Phase 1 Verification (SHOW TABLES;): ExtractedEntity table FOUND.")
            elif not df.empty and 'name' not in df.columns:
                print("Phase 1 Verification (SHOW TABLES;): 'name' column not found in SHOW TABLES result.")
                print(f"Columns available in SHOW TABLES result: {df.columns.tolist()}")
            else: # df is empty or ExtractedEntity not in name column
                print("Phase 1 Verification (SHOW TABLES;): ExtractedEntity table NOT FOUND or result was empty.")
        except Exception as e:
            print(f"Error during Phase 1 verification with 'SHOW TABLES;': {e}")
            print("Falling back to direct query for ExtractedEntity in Phase 1...")
            try:
                result_direct = conn_setup.execute("MATCH (e:ExtractedEntity) RETURN count(e) AS entity_count;")
                df_direct = pd.DataFrame(result_direct.get_as_df())
                print(f"Direct query result (Phase 1):\n{df_direct}")
                count_val = df_direct['entity_count'].iloc[0] if not df_direct.empty else -1
                print(f"Phase 1 Verification (Direct Query): ExtractedEntity table SEEMS TO EXIST (count: {count_val}).")
            except Exception as e_direct_phase1:
                print(f"Error during Phase 1 direct query verification: {e_direct_phase1}")

    except Exception as e:
        print(f"Error during Phase 1 setup: {e}")
    finally:
        if conn_setup:
            print("Closing setup connection.")
            # conn_setup.close() # Kuzu Connection 没有显式 close
        if db_setup:
            print("Deleting setup database instance reference (will trigger close).")
            del db_setup # 依赖 __del__
        print("--- Phase 1 Complete ---")

def verify_in_new_session():
    print(f"\n--- Phase 2: Verification in a New Session ---")
    if not os.path.exists(DB_PATH):
        print(f"Database at {DB_PATH} does not exist. Cannot perform Phase 2.")
        return

    db_verify = None
    conn_verify = None
    try:
        print(f"Opening existing database at {DB_PATH} for verification.")
        db_verify = kuzu.Database(DB_PATH, read_only=False) # 打开同一个数据库
        conn_verify = kuzu.Connection(db_verify)
        print("Database and connection created for verification.")

        # Phase 2 Verification
        print("Verifying tables in new session...")
        try:
            # 尝试 SHOW TABLES;
            print("Attempting 'SHOW TABLES;' in Phase 2...")
            result = conn_verify.execute("SHOW TABLES;") # <--- 修改这里
            df = pd.DataFrame(result.get_as_df())
            print(f"Tables found using 'SHOW TABLES;' (Phase 2):\n{df}")
            if not df.empty and 'name' in df.columns and "ExtractedEntity" in df["name"].tolist():
                print("Phase 2 Verification (SHOW TABLES;): ExtractedEntity table FOUND.")
            elif not df.empty and 'name' not in df.columns:
                print("Phase 2 Verification (SHOW TABLES;): 'name' column not found in SHOW TABLES result.")
                print(f"Columns available in SHOW TABLES result: {df.columns.tolist()}")
            else: # df is empty or ExtractedEntity not in name column
                print("Phase 2 Verification (SHOW TABLES;): ExtractedEntity table NOT FOUND or result was empty.")
        except Exception as e:
            print(f"Error during Phase 2 verification with 'SHOW TABLES;': {e}")
            print("Falling back to direct query for ExtractedEntity in Phase 2...")
            try:
                result_direct = conn_verify.execute("MATCH (e:ExtractedEntity) RETURN count(e) AS entity_count;")
                df_direct = pd.DataFrame(result_direct.get_as_df())
                print(f"Direct query result (Phase 2):\n{df_direct}")
                count_val = df_direct['entity_count'].iloc[0] if not df_direct.empty else -1
                print(f"Phase 2 Verification (Direct Query): ExtractedEntity table SEEMS TO EXIST (count: {count_val}).")
            except Exception as e_direct_phase2:
                print(f"Error during Phase 2 direct query verification: {e_direct_phase2}")

    except Exception as e:
        print(f"Error during Phase 2 setup: {e}")
    finally:
        if conn_verify:
            print("Closing verification connection.")
        if db_verify:
            print("Deleting verification database instance reference.")
            del db_verify
        print("--- Phase 2 Complete ---")

if __name__ == "__main__":
    print(f"Kuzu Python client version: {kuzu.__version__}")
    setup_and_verify_schema()
    print("\nWaiting a moment before trying to open in a new session (simulating process switch)...")
    time.sleep(2) # 短暂等待，模拟进程切换的间隙
    verify_in_new_session()
    --- END OF FILE CONTENT ---

  File: llm_setup.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/llm_setup.py

import os
from typing import Any, Optional, List, Dict

# LiteLLM 本身通常是直接调用其 API，例如 litellm.completion
# 如果要将其适配给 CrewAI 的 Agent，我们需要将其包装成一个符合 CrewAI BaseLLM 接口的类。
# CrewAI 自身也提供了通过 LiteLLM 使用各种模型的集成，我们也可以研究直接使用 CrewAI 的方式。
# 为简化起见，我们先创建一个函数，返回配置好的 LiteLLM 调用参数，或者一个简单的包装器。

# --- 从 agent_orchestrator_service.py 中获取的配置常量 ---
# 本地 LLM 服务地址 (Qwen3)
LOCAL_QWEN_API_BASE = os.getenv("LOCAL_LLM_API_BASE", "http://localhost:8088/v1") # 确保与 local_llm_service.py 的端口一致
LOCAL_QWEN_MODEL_NAME_FOR_LITELLM = os.getenv("LOCAL_LLM_MODEL_NAME", "local/qwen3-1.7b-gguf") # 这个是给 litellm 的 model 参数，需要与 LiteLLM 配置或调用方式对应

# 云端 LiteLLM 网关地址 (用于 Gemini 等)
CLOUD_LITELLM_GW_API_BASE = os.getenv("CLOUD_LITELLM_GW_API_BASE", "YOUR_CLOUD_LITELLM_GATEWAY_URL_HERE/v1") # 假设您的网关也提供 /v1 路径
GEMINI_MODEL_NAME_FOR_LITELLM = os.getenv("CLOUD_LLM_MODEL_NAME", "gemini/gemini-1.5-flash-latest") # 或您希望通过网关调用的模型

# API Keys (通常 LiteLLM 会在其自己的配置或环境变量中处理，但这里可以作为参考)
# GEMINI_API_KEY = os.getenv("GEMINI_API_KEY") # 如果直接调用 Gemini 而非通过网关
# 注意：本地模型的 API Key 通常是 "nokey" 或在服务中不强制

# --- CrewAI LLM 包装器 (如果需要直接传递给 CrewAI Agent) ---
# CrewAI 期望一个 LLM 对象，我们可以创建一个简单的包装器，或者使用 CrewAI 提供的 LiteLLM 集成
# from crewai.llms.base_llm import BaseLLM as CrewAIBaseLLM
# import litellm
#
# class LiteLLMWrapperForCrewAI(CrewAIBaseLLM):
#     model: str
#     api_base: Optional[str] = None
#     api_key: Optional[str] = None
#     custom_llm_provider: Optional[str] = None # 例如 "openai" for OpenAI-compatible endpoints
#     litellm_kwargs: Dict[str, Any] = {}
#
#     def __init__(self, model: str, api_base: Optional[str] = None, api_key: Optional[str] = None, custom_llm_provider: Optional[str] = None, **kwargs):
#         super().__init__(model=model) # CrewAI BaseLLM 需要 model 参数
#         self.model = model
#         self.api_base = api_base
#         self.api_key = api_key
#         self.custom_llm_provider = custom_llm_provider
#         self.litellm_kwargs = kwargs
#
#     def call(self, messages: List[Dict[str, str]], **kwargs: Any) -> str:
#         # CrewAI 的 BaseLLM.call 方法通常期望返回一个字符串
#         # LiteLLM 的 completion 返回一个 ModelResponse 对象
#         params = {
#             "model": self.model,
#             "messages": messages,
#             "api_base": self.api_base,
#             "api_key": self.api_key,
#             "custom_llm_provider": self.custom_llm_provider,
#             **self.litellm_kwargs, # 包含 temperature, max_tokens 等
#             **kwargs # 运行时可能传递的额外参数
#         }
#         # 移除值为 None 的参数，因为 litellm.completion 不喜欢 None 的 api_key 等
#         params_cleaned = {k: v for k, v in params.items() if v is not None}
#
#         response = litellm.completion(**params_cleaned)
#
#         # 从 LiteLLM 响应中提取内容
#         content = ""
#         if response.choices and response.choices[0].message and response.choices[0].message.content:
#             content = response.choices[0].message.content
#         return content
#
#     def get_token_ids(self, text: str) -> List[int]:
#         # LiteLLM 通常不直接暴露 tokenizer，但可以尝试通过 litellm.token_counter (如果它返回ids)
#         # 或者如果使用特定模型，可以尝试加载其 tokenizer
#         # 为简单起见，我们先返回空列表或引发 NotImplementedError
#         # return litellm.encode(model=self.model, text=text) # 如果 litellm.encode 可用
#         raise NotImplementedError("get_token_ids is not implemented for this LiteLLM wrapper.")


def get_local_qwen_llm_instance(
    temperature: float = 0.7,
    max_tokens: int = 2048,
    # crewai_wrapper: bool = False # 是否返回 CrewAI 兼容的包装器
) -> Any: # 返回 Any 以便后续决定是直接返回配置字典还是包装器实例
    """
    获取配置好的本地 Qwen3 LLM 实例（通过 LiteLLM 调用）。
    """
    print(f"LLM Setup: Configuring Local Qwen LLM via LiteLLM.")
    print(f"  Model: {LOCAL_QWEN_MODEL_NAME_FOR_LITELLM}")
    print(f"  API Base: {LOCAL_QWEN_API_BASE}")
    # if crewai_wrapper:
    #     return LiteLLMWrapperForCrewAI(
    #         model=LOCAL_QWEN_MODEL_NAME_FOR_LITELLM,
    #         api_base=LOCAL_QWEN_API_BASE,
    #         api_key="nokey", # 本地服务通常不需要 key
    #         custom_llm_provider="openai", # 因为我们的本地服务是 OpenAI 兼容的
    #         temperature=temperature,
    #         max_tokens=max_tokens
    #     )
    # else:
    # 返回一个配置字典，调用方可以使用 litellm.completion(**config, messages=...)
    return {
        "model": LOCAL_QWEN_MODEL_NAME_FOR_LITELLM,
        "api_base": LOCAL_QWEN_API_BASE,
        "api_key": "nokey",
        "custom_llm_provider": "openai",
        "temperature": temperature,
        "max_tokens": max_tokens
    }


def get_cloud_gemini_llm_instance(
    temperature: float = 0.5,
    max_tokens: int = 4096,
    # crewai_wrapper: bool = False
) -> Any:
    """
    获取配置好的云端 Gemini LLM 实例（通过云端 LiteLLM 网关调用）。
    """
    print(f"LLM Setup: Configuring Cloud Gemini LLM via LiteLLM Gateway.")
    print(f"  Model: {GEMINI_MODEL_NAME_FOR_LITELLM}")
    print(f"  API Base (Gateway): {CLOUD_LITELLM_GW_API_BASE}")

    if CLOUD_LITELLM_GW_API_BASE == "YOUR_CLOUD_LITELLM_GATEWAY_URL_HERE/v1":
        print("LLM Setup WARNING: CLOUD_LITELLM_GW_API_BASE is not configured. Cloud LLM calls will likely fail.")
        # 可以选择返回 None 或者一个无效的配置，让调用方处理
        return None

    # if crewai_wrapper:
    #     return LiteLLMWrapperForCrewAI(
    #         model=GEMINI_MODEL_NAME_FOR_LITELLM,
    #         api_base=CLOUD_LITELLM_GW_API_BASE,
    #         # API Key 通常由云端 LiteLLM 网关管理，客户端调用网关时可能不需要直接提供
    #         # 或者网关本身可能需要某种形式的认证 key
    #         api_key=os.getenv("CLOUD_LITELLM_GATEWAY_API_KEY"), # 假设网关可能需要一个 key
    #         # custom_llm_provider 可能不需要，因为网关会处理到具体云服务的转换
    #         temperature=temperature,
    #         max_tokens=max_tokens
    #     )
    # else:
    return {
        "model": GEMINI_MODEL_NAME_FOR_LITELLM,
        "api_base": CLOUD_LITELLM_GW_API_BASE,
        "api_key": os.getenv("CLOUD_LITELLM_GATEWAY_API_KEY"), # 网关本身的key
        "temperature": temperature,
        "max_tokens": max_tokens
    }

# --- 示例用法 (可选，用于测试此文件) ---
async def main_test_llm_setup():
    print("--- Testing LLM Setup ---")

    print("\n--- Getting Local Qwen Config ---")
    local_qwen_config = get_local_qwen_llm_instance()
    if local_qwen_config:
        print(f"Local Qwen Config: {local_qwen_config}")
        # 模拟调用
        try:
            messages = [{"role": "user", "content": "你好！"}]
            print(f"Simulating LiteLLM call with local Qwen config for: {messages}")
            # response = await litellm.acompletion(**local_qwen_config, messages=messages)
            # print(f"Simulated local Qwen response (first choice): {response.choices[0].message.content if response.choices else 'No response'}")
            print("Actual LiteLLM call commented out for setup test.")
        except Exception as e:
            print(f"Error simulating local Qwen call: {e}")
    else:
        print("Failed to get local Qwen config.")

    print("\n--- Getting Cloud Gemini Config ---")
    cloud_gemini_config = get_cloud_gemini_llm_instance()
    if cloud_gemini_config:
        print(f"Cloud Gemini Config: {cloud_gemini_config}")
        if cloud_gemini_config.get("api_base") == "YOUR_CLOUD_LITELLM_GATEWAY_URL_HERE/v1":
            print("Skipping simulated cloud Gemini call as gateway URL is a placeholder.")
        else:
            try:
                messages = [{"role": "user", "content": "Hello!"}]
                print(f"Simulating LiteLLM call with cloud Gemini config for: {messages}")
                # response = await litellm.acompletion(**cloud_gemini_config, messages=messages)
                # print(f"Simulated cloud Gemini response (first choice): {response.choices[0].message.content if response.choices else 'No response'}")
                print("Actual LiteLLM call commented out for setup test.")
            except Exception as e:
                print(f"Error simulating cloud Gemini call: {e}")
    else:
        print("Failed to get cloud Gemini config (likely due to placeholder URL).")

if __name__ == "__main__":
    # 为了运行异步的 main_test_llm_setup
    # import asyncio
    # asyncio.run(main_test_llm_setup())
    pass
    --- END OF FILE CONTENT ---

  File: local_llm_service.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/local_llm_service.py
import os
import time
import uuid
import traceback
from contextlib import asynccontextmanager
from typing import List, Dict, Any, Optional, Union
import json


from zhz_rag.config.pydantic_models import ExtractedEntitiesAndRelationIntent


import uvicorn
from fastapi import FastAPI, HTTPException, Request as FastAPIRequest
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from llama_cpp import Llama, LlamaGrammar
import re

# --- 配置 ---
MODEL_DIR = os.getenv("LOCAL_LLM_MODEL_DIR", "/home/zhz/models/Qwen3-1.7B-GGUF")
MODEL_FILENAME = os.getenv("LOCAL_LLM_MODEL_FILENAME")
N_GPU_LAYERS = int(os.getenv("LOCAL_LLM_N_GPU_LAYERS", 0))
N_CTX = int(os.getenv("LOCAL_LLM_N_CTX", 4096))
N_BATCH = int(os.getenv("LOCAL_LLM_N_BATCH", 512))
SERVICE_PORT = int(os.getenv("LOCAL_LLM_SERVICE_PORT", 8088))
SERVICE_HOST = "0.0.0.0"

GBNF_FILE_PATH = os.path.join(os.path.dirname(__file__), "core", "grammars", "cypher_or_unable_output.gbnf")

# --- 全局变量 ---
llama_model: Optional[Llama] = None
model_path_global: Optional[str] = None
# logit_bias 相关的全局变量 (在 lifespan 中初始化)
failure_phrase_token_ids: List[int] = []
logit_bias_for_failure_phrase: Optional[Dict[int, float]] = None

@asynccontextmanager
async def lifespan(app: FastAPI):
    global llama_model, model_path_global, failure_phrase_token_ids, logit_bias_for_failure_phrase
    print("--- Local LLM Service: Lifespan startup (with GBNF and logit_bias prep) ---")
    
    model_file_to_load = MODEL_FILENAME
    if not model_file_to_load:
        print(f"MODEL_FILENAME environment variable not set. Attempting to auto-detect GGUF file in {MODEL_DIR}...")
        try:
            gguf_files = [f for f in os.listdir(MODEL_DIR) if f.endswith(".gguf")]
            if not gguf_files:
                error_msg = f"No GGUF models found in directory: {MODEL_DIR}"
                print(f"ERROR: {error_msg}")
                app.state.cypher_path_grammar = None # Ensure state variable exists even on error
                raise RuntimeError(error_msg)
            if len(gguf_files) > 1:
                print(f"Warning: Multiple GGUF models found in {MODEL_DIR}. Using the first one: {gguf_files[0]}")
            model_file_to_load = gguf_files[0]
            print(f"Auto-detected GGUF file: {model_file_to_load}")
        except FileNotFoundError:
            error_msg = f"Model directory not found: {MODEL_DIR}"
            print(f"ERROR: {error_msg}")
            app.state.cypher_path_grammar = None
            raise RuntimeError(error_msg)
        except Exception as e_find_model:
            error_msg = f"Error auto-detecting GGUF file: {e_find_model}"
            print(f"ERROR: {error_msg}")
            app.state.cypher_path_grammar = None
            raise RuntimeError(error_msg)

    model_path_global = os.path.join(MODEL_DIR, model_file_to_load)
    print(f"Attempting to load Llama model from: {model_path_global}")
    print(f"Parameters: n_gpu_layers={N_GPU_LAYERS}, n_ctx={N_CTX}, n_batch={N_BATCH}")

    try:
        llama_model = Llama(
            model_path=model_path_global,
            n_gpu_layers=N_GPU_LAYERS,
            n_ctx=N_CTX,
            n_batch=N_BATCH,
            verbose=True
        )
        print("Llama model loaded successfully.")

        # --- 初始化 logit_bias 相关 ---
        failure_phrase_str = "无法生成Cypher查询."
        if llama_model:
            try:
                # add_bos=False, special=False (通常用于非起始的、纯文本的词元化)
                failure_phrase_token_ids = llama_model.tokenize(failure_phrase_str.encode("utf-8"), add_bos=False, special=False)
                # 为这些 token ID 设置正向偏置，例如 10.0 (可以调整)
                # 避免偏置 EOS token (如果它意外地出现在短语的词元化结果中)
                eos_token_id = llama_model.token_eos()
                logit_bias_for_failure_phrase = {
                    token_id: 10.0 for token_id in failure_phrase_token_ids if token_id != eos_token_id
                }
                print(f"Successfully tokenized failure phrase '{failure_phrase_str}' to IDs: {failure_phrase_token_ids}")
                print(f"Logit bias for failure phrase: {logit_bias_for_failure_phrase}")
            except Exception as e_tokenize:
                print(f"ERROR: Failed to tokenize failure phrase for logit_bias: {e_tokenize}")
                failure_phrase_token_ids = []
                logit_bias_for_failure_phrase = None
        # --- 结束 logit_bias 初始化 ---

        gbnf_grammar_instance: Optional[LlamaGrammar] = None
        print(f"Attempting to load GBNF grammar from: {GBNF_FILE_PATH}")
        if os.path.exists(GBNF_FILE_PATH):
            try:
                gbnf_grammar_instance = LlamaGrammar.from_file(GBNF_FILE_PATH)
                print("GBNF grammar (success/failure paths) loaded successfully into lifespan.")
            except Exception as e_gbnf:
                print(f"ERROR: Failed to load or parse GBNF grammar file '{GBNF_FILE_PATH}': {e_gbnf}")
                traceback.print_exc()
        else:
            print(f"ERROR: GBNF grammar file not found at '{GBNF_FILE_PATH}'.")
        
        app.state.cypher_path_grammar = gbnf_grammar_instance

    except Exception as e:
        print(f"FATAL: Failed to load Llama model or prepare GBNF/logit_bias: {e}")
        app.state.cypher_path_grammar = None
        logit_bias_for_failure_phrase = None # Ensure this is also cleared
    
    yield
    print("--- Local LLM Service: Lifespan shutdown ---")

app = FastAPI(
    title="Local LLM Service (OpenAI Compatible)",
    description="Uses GBNF with logit_bias for conditional JSON output.",
    version="0.1.7", 
    lifespan=lifespan
)

class ChatMessage(BaseModel):
    role: str
    content: str

class ChatCompletionRequest(BaseModel):
    model: str
    messages: List[ChatMessage]
    temperature: float = 0.7
    max_tokens: Optional[int] = 1024 # Default to 1024 or higher
    stream: bool = False
    stop: Optional[Union[str, List[str]]] = None

class ChatCompletionMessage(BaseModel):
    role: str
    content: Optional[str] = None

class ChatCompletionChoice(BaseModel):
    index: int
    message: ChatCompletionMessage
    finish_reason: Optional[str] = "stop"

class UsageInfo(BaseModel):
    prompt_tokens: int
    completion_tokens: int
    total_tokens: int

class ChatCompletionResponse(BaseModel):
    id: str = Field(default_factory=lambda: f"chatcmpl-{uuid.uuid4().hex}")
    object: str = "chat.completion"
    created: int = Field(default_factory=lambda: int(time.time()))
    model: str
    choices: List[ChatCompletionChoice]
    usage: UsageInfo
    system_fingerprint: Optional[str] = None

def post_process_llm_output(content: Optional[str], finish_reason: Optional[str]) -> Optional[str]:
    if content is None:
        return None
    processed_content = content
    # Remove <think>...</think> blocks
    think_block_pattern = re.compile(r"<think[^>]*>.*?</think>\s*", flags=re.DOTALL | re.IGNORECASE)
    processed_content = think_block_pattern.sub("", processed_content)

    # Handle potentially incomplete <think> tags if generation was cut short
    if finish_reason == "length" and \
       re.match(r"<think[^>]*>", processed_content.strip(), flags=re.IGNORECASE) and \
       not re.search(r"</think\s*>", processed_content, flags=re.IGNORECASE):
        print("DEBUG_POST_PROCESS: Incomplete think block due to length, attempting to remove partial tag.")
        # More aggressive removal of any leading <think...> tag if it's incomplete
        processed_content = re.sub(r"^<think[^>]*>","", processed_content.strip(), flags=re.IGNORECASE).strip()

    # Remove any remaining stray <think> or </think> tags
    stray_think_tag_pattern = re.compile(r"</?\s*think[^>]*?>\s*", flags=re.IGNORECASE)
    processed_content = stray_think_tag_pattern.sub("", processed_content)
    return processed_content.strip()

@app.post("/v1/chat/completions", response_model=ChatCompletionResponse)
async def create_chat_completion_endpoint(fastapi_req: FastAPIRequest, request: ChatCompletionRequest):
    global llama_model, model_path_global, logit_bias_for_failure_phrase
    
    # --- 添加调试日志 ---
    print("\n--- local_llm_service: Received /v1/chat/completions request ---")
    try:
        print(f"Request Body (raw): {await fastapi_req.body()}") # 打印原始请求体
        print(f"Request Model: {request.model}")
        print(f"Request Messages (count): {len(request.messages) if request.messages else 0}")
        print(f"Request Temperature: {request.temperature}")
        print(f"Request Max Tokens: {request.max_tokens}")
        print(f"Request Stop: {request.stop}")
    except Exception as e_req_log:
        print(f"Error logging request details: {e_req_log}")
    # --- 调试日志结束 ---


    loaded_cypher_path_grammar: Optional[LlamaGrammar] = getattr(fastapi_req.app.state, 'cypher_path_grammar', None)
    
    if llama_model is None:
        raise HTTPException(status_code=503, detail="Llama model is not loaded or failed to load.")
    if request.stream:
        raise HTTPException(status_code=400, detail="Streaming responses are not yet supported by this service.")

    response_content_raw: Optional[str] = None
    prompt_tokens = 0
    completion_tokens = 0
    total_tokens = 0
    finish_reason = "stop"
    request_model_name = request.model 
    final_json_output_str_for_client: Optional[str] = None

    if not hasattr(llama_model, "create_chat_completion"):
        print("CRITICAL_ERROR: llama_model.create_chat_completion method not found.")
        # Construct a proper JSON string for the error case directly
        error_response_obj = {"status": "unable_to_generate", "query": "LLM service misconfiguration."}
        final_json_output_str_for_client = json.dumps(error_response_obj, ensure_ascii=False)
        # Early return with the error structure if model is misconfigured
        return ChatCompletionResponse(
            id=f"chatcmpl-{uuid.uuid4().hex}",
            object="chat.completion",
            created=int(time.time()),
            model=request_model_name or (os.path.basename(model_path_global) if model_path_global else "local-llm-misconfigured"),
            choices=[ChatCompletionChoice(index=0, message=ChatCompletionMessage(role="assistant", content=final_json_output_str_for_client), finish_reason="error")],
            usage=UsageInfo(prompt_tokens=0, completion_tokens=0, total_tokens=0)
        )

    try:
        dict_messages = [msg.model_dump() for msg in request.messages]
        completion_params: Dict[str, Any] = {
            "messages": dict_messages,
            "temperature": request.temperature, 
            "max_tokens": request.max_tokens or 1024,
            "stop": request.stop,
        }

            # --- 新增：根据模型名称判断是否启用JSON模式 ---
        # 假设模型名称中包含 "kg_entity_extraction" 表示我们期望JSON输出
        if "kg_entity_extraction" in request.model.lower():
            try:
                json_schema_for_extraction = ExtractedEntitiesAndRelationIntent.model_json_schema()
                completion_params["response_format"] = {
                    "type": "json_object",
                    "schema": json_schema_for_extraction
                }
                print(f"DEBUG_FastAPI: Enabled JSON mode with schema for model {request.model}")
                # print(f"DEBUG_FastAPI: JSON Schema: {json.dumps(json_schema_for_extraction, indent=2)}") # 可选打印
            except Exception as e_schema:
                print(f"ERROR_FastAPI: Failed to generate JSON schema for ExtractedEntitiesAndRelationIntent: {e_schema}")
        # --- 结束新增 ---

        is_cypher_gen_task = False
        if dict_messages and dict_messages[0]["role"] == "system":
            system_content_for_check = dict_messages[0]["content"]
            keyword_to_check = "知识图谱结构 (KuzuDB) 与 Cypher 查询生成规则" # Ensure this matches constants.py
            processed_system_content = system_content_for_check.lower()
            processed_keyword = keyword_to_check.lower()
            print(f"DEBUG_FastAPI_CypherTaskCheck: System prompt content (LOWERCASED, first 300 chars): '{processed_system_content[:300]}...'")
            print(f"DEBUG_FastAPI_CypherTaskCheck: Keyword to check (LOWERCASED): '{processed_keyword}'")
            if processed_keyword in processed_system_content:
                is_cypher_gen_task = True
                print("DEBUG_FastAPI_CypherTaskCheck: Cypher generation task DETECTED (after lowercasing and keyword adjustment).")
            else:
                print("DEBUG_FastAPI_CypherTaskCheck: Cypher generation task NOT DETECTED (keyword missing after lowercasing).")
        else:
            print("DEBUG_FastAPI_CypherTaskCheck: No system message found or messages empty, not a Cypher task.")
         
        if is_cypher_gen_task:
            print("DEBUG_FastAPI: Cypher generation task DETECTED.")
            if loaded_cypher_path_grammar is not None:
                print("DEBUG_FastAPI: Applying GBNF grammar (success/failure paths) FROM APP.STATE.")
                completion_params["grammar"] = loaded_cypher_path_grammar
                
                if logit_bias_for_failure_phrase:
                    print(f"DEBUG_FastAPI: Applying logit_bias for failure phrase: {logit_bias_for_failure_phrase}")
                    completion_params["logit_bias"] = logit_bias_for_failure_phrase
            else:
                print("DEBUG_FastAPI: GBNF grammar FROM APP.STATE IS NONE. Proceeding without grammar for Cypher task.")
        else:
            print("DEBUG_FastAPI: Not a Cypher task. No grammar or specific logit_bias applied.")
        
        print(f"DEBUG_FastAPI: Calling llama_model.create_chat_completion with params (preview): "
          f"model={request.model}, temp={completion_params['temperature']}, "
          f"max_tokens={completion_params['max_tokens']}, stop={completion_params['stop']}, "
          f"json_mode_enabled={'response_format' in completion_params}")
    
        completion = llama_model.create_chat_completion(**completion_params)

        # --- 添加调试日志 ---
        print(f"DEBUG_FastAPI: Raw completion object from llama_model.create_chat_completion: {completion}")
        # --- 调试日志结束 ---

        response_content_raw = completion['choices'][0]['message']['content']
        prompt_tokens = completion['usage']['prompt_tokens']
        completion_tokens = completion['usage']['completion_tokens']
        total_tokens = completion['usage']['total_tokens']
        finish_reason = completion['choices'][0].get('finish_reason', 'stop')
        print(f"DEBUG_FastAPI: Raw content from llama_model: '{response_content_raw}'")

    except Exception as e:
        print(f"Error during llama_model.create_chat_completion: {e}")
        traceback.print_exc()
        # Ensure this is a JSON string
        final_json_output_str_for_client = json.dumps({"status": "unable_to_generate", "query": "LLM call failed during generation."})
        finish_reason = "error" # Indicate an error finish

    if final_json_output_str_for_client is None: # Only process if no error above set this
        processed_llm_text = post_process_llm_output(response_content_raw, finish_reason)
        
        if is_cypher_gen_task:
            standard_success_template = {"status": "success", "query": ""} 
            standard_unable_json_obj = {"status": "unable_to_generate", "query": "无法生成Cypher查询."}
            final_json_to_return_obj = standard_unable_json_obj # Default to unable

            if processed_llm_text:
                cleaned_text_for_json_parse = processed_llm_text.strip()
                # Remove Markdown code block fences if present
                if cleaned_text_for_json_parse.startswith("```json"):
                    cleaned_text_for_json_parse = cleaned_text_for_json_parse[len("```json"):].strip()
                if cleaned_text_for_json_parse.endswith("```"):
                    cleaned_text_for_json_parse = cleaned_text_for_json_parse[:-len("```")].strip()
                
                try:
                    data = json.loads(cleaned_text_for_json_parse)
                    if isinstance(data, dict) and "status" in data and "query" in data:
                        if data.get("status") == "success" and isinstance(data.get("query"), str) and data.get("query").strip():
                            final_json_to_return_obj = data
                            print(f"DEBUG_FastAPI: LLM output is a valid 'success' JSON (GBNF success path likely): {json.dumps(final_json_to_return_obj, ensure_ascii=False)}")
                        elif data.get("status") == "unable_to_generate" and data.get("query") == "无法生成Cypher查询.":
                            final_json_to_return_obj = data # Already standard
                            print(f"DEBUG_FastAPI: LLM output is a valid 'unable_to_generate' JSON (GBNF failure path likely): {json.dumps(final_json_to_return_obj, ensure_ascii=False)}")
                        else: # JSON has status/query but not matching expected values
                            print(f"DEBUG_FastAPI: LLM JSON has unexpected status/query content. Status: '{data.get('status')}', Query: '{str(data.get('query'))[:100]}'. Defaulting to standard 'unable_to_generate'.")
                            # final_json_to_return_obj remains standard_unable_json_obj
                    else: 
                        print(f"DEBUG_FastAPI: LLM output parsed as JSON, but not the expected dict with status/query: '{cleaned_text_for_json_parse}'. Defaulting to standard 'unable_to_generate'.")
                except json.JSONDecodeError:
                    print(f"DEBUG_FastAPI: LLM output was not valid JSON. Raw (after post_process): '{processed_llm_text}'. Defaulting to standard 'unable_to_generate'.")
                except Exception as e_parse:
                     print(f"DEBUG_FastAPI: Unexpected error parsing LLM output: {e_parse}. Raw: '{processed_llm_text}'. Defaulting to 'unable_to_generate'.")
            else:
                print("DEBUG_FastAPI: LLM output was empty after post_processing. Defaulting to standard 'unable_to_generate' JSON.")
            
            final_json_output_str_for_client = json.dumps(final_json_to_return_obj, ensure_ascii=False)
        else: 
            # For non-Cypher tasks, return the processed text directly
            final_json_output_str_for_client = processed_llm_text if processed_llm_text is not None else ""


    print(f"DEBUG_FastAPI: Final content string to be returned to client: '{final_json_output_str_for_client}'")

    response_id = f"chatcmpl-{uuid.uuid4().hex}"
    created_time = int(time.time())
    effective_model_name = request_model_name
    if not effective_model_name and model_path_global:
        effective_model_name = os.path.basename(model_path_global)
    elif not effective_model_name:
        effective_model_name = "local-llm-unknown"
    
    # Ensure final_json_output_str_for_client is a string, even if empty (for non-Cypher tasks)
    if final_json_output_str_for_client is None:
        final_json_output_str_for_client = "" # Or some other default string

    return ChatCompletionResponse(
        id=response_id,
        object="chat.completion",
        created=created_time,
        model=effective_model_name,
        choices=[
            ChatCompletionChoice(
                index=0,
                message=ChatCompletionMessage(role="assistant", content=final_json_output_str_for_client),
                finish_reason=finish_reason 
            )
        ],
        usage=UsageInfo(
            prompt_tokens=prompt_tokens,
            completion_tokens=completion_tokens,
            total_tokens=total_tokens
        )
    )

@app.get("/v1/models", response_model=Dict[str, Any])
async def list_models():
    global model_path_global
    model_id_for_clients = "qwen3local_gguf_gbnf_logit_bias"
    model_name_to_display = "Qwen3-1.7B-GGUF (GBNF+LogitBias)"
    
    if model_path_global and os.path.exists(model_path_global):
        model_name_to_display = os.path.basename(model_path_global)
        created_timestamp = int(os.path.getctime(model_path_global))
    else:
        # Fallback if model_path_global is not set or file doesn't exist
        model_name_to_display = "Qwen3-1.7B-GGUF (Model path not resolved)"
        created_timestamp = int(time.time())

    return {
        "object": "list",
        "data": [
            {
                "id": model_id_for_clients,
                "object": "model",
                "created": created_timestamp,
                "owned_by": "user",
                "description": f"Locally hosted GGUF model: {model_name_to_display}. Uses GBNF and logit_bias for Cypher tasks."
            }
        ]
    }

if __name__ == "__main__":
    print(f"--- Starting Local LLM FastAPI Service on {SERVICE_HOST}:{SERVICE_PORT} ---")
    print(f"--- Model will be loaded from DIR: {MODEL_DIR}, FILE: {MODEL_FILENAME or 'Auto-detected GGUF'} ---")
    print(f"--- GBNF Grammar for Cypher/Unable output will be loaded from: {GBNF_FILE_PATH} ---")
    uvicorn.run("local_llm_service:app", host=SERVICE_HOST, port=SERVICE_PORT, reload=False)
    --- END OF FILE CONTENT ---

  File: requirements.txt
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/requirements.txt

# --- Core Application & Agent Framework ---
pydantic==2.11.5
crewai==0.121.0  # 您当前使用的版本，假设它兼容 Pydantic 2.11.5
crewai-tools==0.0.1 # 您当前使用的版本

# --- LLM Interaction & Management ---
litellm==1.68.0
openai==1.75.0 # 通常与 LiteLLM 一起使用

# --- Web Framework ---
fastapi==0.115.12
uvicorn[standard]==0.34.2 # [standard] 包含 websockets 和 httptools

# --- Data Storage & RAG ---
chromadb==0.6.3
sentence-transformers==4.1.0 # 您 freeze 输出中的版本，如果希望更新可以调整
transformers==4.52.3         # 您 freeze 输出中的版本，如果希望固定为 4.38.2 可以改回
torch==2.7.0                 # 您 freeze 输出中的版本
numpy==2.2.6                 # 您 freeze 输出中的版本 (注意：如果导致问题，可能需要回退到 <2.0)
kuzu # 让 uv 选择最新兼容版，或根据 Dagster 需求指定
portalocker # KuzuDB 可能需要

# --- Dagster Workflow ---
dagster==1.10.17             # 您当前使用的版本，假设兼容 Pydantic V2
dagster-webserver==1.10.17
dagster-graphql==1.10.17
dagster-pipes==1.10.17
dagster_shared==0.26.17

# --- Other Important Libraries from your freeze ---
aiohttp==3.12.0
aiosqlite==0.21.0
APScheduler==3.11.0
databases[aiosqlite]==0.9.0 # 确保包含 extras
httpx==0.28.1
jieba==0.42.1
langchain==0.3.25 # 这是一个元包，可能拉取很多 langchain-* 组件
langchain-core==0.3.61
langchain-community==0.3.24
langchain-text-splitters==0.3.8
neo4j==5.28.1
pandas==2.2.3
protobuf==5.29.4 # 确保这个版本与 opentelemetry 兼容
python-dotenv==1.1.0
pytz==2025.2
SQLAlchemy==2.0.41
bm25s==0.2.13

# --- OpenTelemetry (让 crewai 自动拉取，如果还有问题再显式指定) ---
# opentelemetry-api
# opentelemetry-sdk
# opentelemetry-exporter-otlp-proto-http
# opentelemetry-exporter-otlp-proto-grpc
# opentelemetry-proto
# opentelemetry-instrumentation
# opentelemetry-instrumentation-asgi
# opentelemetry-instrumentation-fastapi
# opentelemetry-semantic-conventions
# opentelemetry-util-http

# --- Potentially problematic or less critical (review if needed) ---
# 以下是一些在您 freeze 输出中但可能不需要严格固定，或者可以由其他包间接依赖的
# 如果遇到问题，可以考虑注释掉它们，让 uv 重新解析
# annotated-types==0.7.0 # 通常由 pydantic 依赖
# anyio==4.9.0 # 通常由 fastapi/starlette 依赖
# ... (您可以根据需要审查和删减 freeze 输出中的其他包) ...

# --- Editable local packages (uv pip install -e <path> 来安装) ---
# -e ./zhz_rag
# -e ./zhz_rag_pipeline_dagster
    --- END OF FILE CONTENT ---

  File: workspace.yaml
    --- START OF FILE CONTENT ---
load_from:
  - python_module:
      module_name: zhz_rag_pipeline_dagster.zhz_rag_pipeline.definitions
      working_directory: /home/zhz/zhz_agent
    --- END OF FILE CONTENT ---


Directory: zhz_rag_pipeline_dagster/
  File (skipped, extension not included): 25-5-28进度说明书.jpg
  File (skipped, extension not included): pyproject.toml
  File: setup.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/setup.py
from setuptools import find_packages, setup

setup(
    name="zhz_rag_pipeline_dagster_project", # 给一个包名
    version="0.0.1",
    packages=find_packages(), # 会找到 zhz_rag_pipeline 这个包
    install_requires=[
        "dagster",
        "dagster-webserver",
        # 添加其他 zhz_rag_pipeline_dagster 项目直接依赖的库
        # 例如 kuzu, dagster-pydantic (如果之后还要用) 等
        # 但核心的 zhz_rag 包的依赖不在这里列出，它应该是独立安装的
    ],
)
    --- END OF FILE CONTENT ---


Directory: zhz_rag_pipeline_dagster/zhz_rag_pipeline/
  File: __init__.py
    --- START OF FILE CONTENT ---
# zhz_rag_pipeline/__init__.py
# This file makes Python treat the directory as a package.
# You can also define a __version__ or import key modules here if needed.
    --- END OF FILE CONTENT ---

  File: custom_io_managers.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/zhz_rag_pipeline/custom_io_managers.py
import json
import os
from typing import List, Type, Union, get_args, get_origin, Any, Optional 
from dagster import UPathIOManager, InputContext, OutputContext, DagsterInvariantViolationError
from pydantic import BaseModel as PydanticBaseModel
from upath import UPath

class PydanticListJsonIOManager(UPathIOManager):
    extension: str = ".jsonl"

    def __init__(self, base_dir: Optional[str] = None): # Changed base_path to base_dir for clarity
        resolved_base_dir: UPath
        if base_dir:
            resolved_base_dir = UPath(base_dir).resolve() # Resolve to absolute path
        else:
            # Default to <DAGSTER_HOME>/storage/pydantic_jsonl_io
            # DAGSTER_HOME defaults to ~/.dagster, but can be overridden by env var
            dagster_home_str = os.getenv("DAGSTER_HOME", os.path.join(os.path.expanduser("~"), ".dagster"))
            resolved_base_dir = UPath(dagster_home_str) / "storage" / "pydantic_jsonl_io"
        
        # Ensure the directory exists
        try:
            resolved_base_dir.mkdir(parents=True, exist_ok=True)
        except Exception as e:
            # Log this error appropriately, perhaps using a direct print if logger isn't set up yet
            # or re-raise as a Dagster-specific error.
            print(f"[PydanticListJsonIOManager __init__] ERROR: Could not create bafef __init__(self, base_dir: Optse directory {resolved_base_dir}: {e}")
            # Depending on Dagster's init sequence, context.log might not be available here.
            # It's safer to let UPathIOManager handle its own base_path or ensure dir exists before.
            # For now, we proceed, UPathIOManager might handle it or fail later.

        super().__init__(base_path=resolved_base_dir)
        # Log the final base path used by the UPathIOManager instance
        # self.log available after super().__init__() in ConfigurableIOManager context
        # For direct instantiation, we might need to pass a logger or use a global one.
        # print(f"[PydanticListJsonIOManager __init__] Initialized with resolved base_path: {self.base_path}")


    def dump_to_path(self, context: OutputContext, obj: List[PydanticBaseModel], path: UPath):
        context.log.info(f"[PydanticListJsonIOManager dump_to_path] Attempting to dump to resolved path: {path.resolve()}")
        
        if not isinstance(obj, list):
            msg = f"Expected a list of Pydantic models, got {type(obj)}"
            context.log.error(msg)
            raise TypeError(msg)
        
        # Optional: More robust type checking for list items if needed, using context.dagster_type
        # For now, assume obj is List[PydanticBaseModel] based on upstream asset's type hint.

        try:
            with path.open("w", encoding="utf-8") as f:
                for model_instance in obj:
                    if not isinstance(model_instance, PydanticBaseModel):
                        context.log.warning(f"Item in list is not a Pydantic model: {type(model_instance)}. Skipping.")
                        continue
                    json_str = model_instance.json() # Pydantic V1
                    f.write(json_str + "\n")
            context.log.info(f"[PydanticListJsonIOManager dump_to_path] Successfully dumped {len(obj)} items to {path.resolve()}")
        except Exception as e:
            context.log.error(f"[PydanticListJsonIOManager dump_to_path] Failed to dump object to {path.resolve()}: {e}", exc_info=True)
            raise

    def load_from_path(self, context: InputContext, path: UPath) -> List[PydanticBaseModel]:
        context.log.info(f"[PydanticListJsonIOManager load_from_path] Attempting to load from resolved path: {path.resolve()}")
        
        list_typing_type = context.dagster_type.typing_type
        origin = get_origin(list_typing_type)
        args = get_args(list_typing_type)

        if not (origin is list and args and issubclass(args[0], PydanticBaseModel)):
            msg = (
                f"PydanticListJsonIOManager can only handle inputs of type List[PydanticModel], "
                f"but got {list_typing_type} for input '{context.name}'."
            )
            context.log.error(msg)
            raise DagsterInvariantViolationError(msg) # Use Dagster specific error
        
        model_type: Type[PydanticBaseModel] = args[0]
        context.log.info(f"[PydanticListJsonIOManager load_from_path] Target model type for list items: {model_type.__name__}")

        loaded_models: List[PydanticBaseModel] = []
        if not path.exists():
            context.log.warning(f"[PydanticListJsonIOManager load_from_path] File not found at {path.resolve()}, returning empty list for input '{context.name}'.")
            return loaded_models

        try:
            with path.open("r", encoding="utf-8") as f:
                for line_number, line in enumerate(f, 1):
                    line_content = line.strip()
                    if not line_content:
                        continue
                    try:
                        model_instance = model_type.parse_raw(line_content) # Pydantic V1
                        loaded_models.append(model_instance)
                    except Exception as e_parse:
                        context.log.error(
                            f"[PydanticListJsonIOManager load_from_path] Failed to parse JSON line {line_number} "
                            f"into {model_type.__name__} from {path.resolve()}: {e_parse}. "
                            f"Line content (first 100 chars): '{line_content[:100]}...'",
                            exc_info=True
                        )
                        # Optionally re-raise or decide to skip problematic lines
                        # For now, we'll skip
            context.log.info(f"[PydanticListJsonIOManager load_from_path] Successfully loaded {len(loaded_models)} instances of {model_type.__name__} from {path.resolve()}")
        except Exception as e_read:
            context.log.error(f"[PydanticListJsonIOManager load_from_path] Failed to read or process file {path.resolve()}: {e_read}", exc_info=True)
            raise # Re-raise if file reading itself fails catastrophically
            
        return loaded_models
    --- END OF FILE CONTENT ---

  File: definitions.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/zhz_rag_pipeline/definitions.py
import dagster as dg
import os

from dagster import (
    define_asset_job,
    Definitions,
    in_process_executor,
    AssetSelection  # 保持导入以备后用
)

# 导入所有资产列表
from zhz_rag_pipeline_dagster.zhz_rag_pipeline.ingestion_assets import all_ingestion_assets
from zhz_rag_pipeline_dagster.zhz_rag_pipeline.processing_assets import all_processing_assets
from zhz_rag_pipeline_dagster.zhz_rag_pipeline.evaluation_assets import all_evaluation_assets

# 导入所有资源和 IO 管理器
from zhz_rag_pipeline_dagster.zhz_rag_pipeline.resources import (
    SentenceTransformerResource, SentenceTransformerResourceConfig,
    ChromaDBResource, ChromaDBResourceConfig,
    LocalLLMAPIResource, LocalLLMAPIResourceConfig,
    KuzuDBReadWriteResource,
    KuzuDBReadOnlyResource,
    GeminiAPIResource, GeminiAPIResourceConfig
)
from zhz_rag_pipeline_dagster.zhz_rag_pipeline.custom_io_managers import PydanticListJsonIOManager

# 1. 定义一个包含项目中所有资产的列表
all_defined_assets = all_ingestion_assets + all_processing_assets + all_evaluation_assets

# 2. [最终的、最简单的作业定义]
#    直接选择所有资产。因为我们使用了 in_process_executor，
#    并且 KuzuDB 资源管理现在是正确的，所以即使选择了所有资产，
#    它们也会被串行执行，不会有并发问题。
#    当您只想运行 KuzuDB 相关的部分时，可以在 UI 中选择一个子集。
kuzu_kg_write_job = define_asset_job(
    name="kuzu_kg_write_job",
    selection=AssetSelection.all(), # 选择所有已定义的资产
    executor_def=in_process_executor
)

# 3. 实例化 IO 管理器
pydantic_io_manager_instance = PydanticListJsonIOManager()

# 4. 将所有定义组合成最终的 Definitions 对象
defs = Definitions(
    assets=all_defined_assets,
    jobs=[kuzu_kg_write_job],
    resources={
        "embedder": SentenceTransformerResource(model_name_or_path=SentenceTransformerResourceConfig().model_name_or_path),
        "chroma_db": ChromaDBResource(collection_name=ChromaDBResourceConfig().collection_name, persist_directory=ChromaDBResourceConfig().persist_directory),
        "sglang_api": LocalLLMAPIResource(api_url=LocalLLMAPIResourceConfig().api_url, default_temperature=LocalLLMAPIResourceConfig().default_temperature, default_max_new_tokens=LocalLLMAPIResourceConfig().default_max_new_tokens),
        "kuzu_readwrite_db": KuzuDBReadWriteResource(),
        "kuzu_readonly_db": KuzuDBReadOnlyResource(),
        "gemini_api": GeminiAPIResource(model_name=GeminiAPIResourceConfig().model_name, proxy_url=GeminiAPIResourceConfig().proxy_url, default_temperature=GeminiAPIResourceConfig().default_temperature, default_max_tokens=GeminiAPIResourceConfig().default_max_tokens),
        "pydantic_json_io_manager": pydantic_io_manager_instance,
    }
)
    --- END OF FILE CONTENT ---

  File: evaluation_assets.py
    --- START OF FILE CONTENT ---
import dagster as dg
import os
from typing import Dict, List, Any # Optional 可能之后会用到

# 从项目中导入我们重构的批量评估函数和相关工具/常量
from zhz_rag.evaluation.batch_eval_cypher import run_cypher_batch_evaluation
from zhz_rag.evaluation.batch_eval_answer import run_answer_batch_evaluation
from zhz_rag.evaluation.analyze_cypher import perform_cypher_evaluation_analysis
from zhz_rag.evaluation.analyze_answer import perform_answer_evaluation_analysis
from zhz_rag.utils.common_utils import (
    find_latest_rag_interaction_log,
    RAG_INTERACTION_LOGS_DIR,
    EVALUATION_RESULTS_LOGS_DIR,
    get_evaluation_result_log_filepath
)
# 导入 GeminiAPIResource 以声明资源依赖
from zhz_rag_pipeline_dagster.zhz_rag_pipeline.resources import GeminiAPIResource

# --- 资产定义 ---

@dg.asset(
    name="latest_rag_interaction_log_for_evaluation",
    description="Provides the filepath of the latest RAG interaction log to be used for evaluation.",
    group_name="evaluation_pipeline",
    compute_kind="python" # 可选，指明计算类型
)
def latest_rag_interaction_log_for_evaluation_asset(context: dg.AssetExecutionContext) -> str:
    """
    Finds and returns the path to the latest RAG interaction log file.
    """
    log_filepath = find_latest_rag_interaction_log(RAG_INTERACTION_LOGS_DIR)
    if not log_filepath or not os.path.exists(log_filepath):
        error_msg = f"No RAG interaction log file found in directory: {RAG_INTERACTION_LOGS_DIR}"
        context.log.error(error_msg)
        raise dg.Failure(description=error_msg)
    
    context.log.info(f"Using RAG interaction log for evaluation: {log_filepath}")
    context.add_output_metadata({"log_filepath": log_filepath, "filename": os.path.basename(log_filepath)})
    return log_filepath

@dg.asset(
    name="batch_cypher_evaluations_log", # 资产名称最好能反映它产出的是日志文件
    description="Runs batch evaluation of Cypher queries and produces an evaluation log file.",
    group_name="evaluation_pipeline",
    compute_kind="python",
    # deps=[latest_rag_interaction_log_for_evaluation_asset] # 通过函数参数自动推断依赖
)
async def batch_cypher_evaluation_log_asset(
    context: dg.AssetExecutionContext,
    gemini_api: GeminiAPIResource,
    latest_rag_interaction_log_for_evaluation: str # <--- 修改参数名
) -> dg.Output[str]:
    context.log.info(f"Starting batch Cypher evaluation using log file: {latest_rag_interaction_log_for_evaluation}") # <--- 使用新参数名
    
    # 从 Dagster 配置中获取参数，或使用默认/环境变量
    # 这里我们先用之前脚本中的方式，未来可以转为 Dagster run_config
    app_version = os.getenv("APP_VERSION_TAG", "dagster_cypher_eval_0.2")
    # 对于 use_simulated_api，在 Dagster 中通常会通过资源配置或 op_config 来控制，
    # 而不是直接依赖环境变量，这样更灵活。但为了保持与脚本一致，暂时保留。
    use_simulated = os.getenv("USE_SIMULATED_GEMINI_CYPHER_EVAL", "false").lower() == "true"
    api_delay = float(os.getenv("GEMINI_API_CALL_DELAY_SECONDS", "4.1"))

    if use_simulated:
        context.log.warning("Cypher evaluation asset is using SIMULATED Gemini API calls.")

    # 调用我们重构的、现在接受 gemini_resource 的批量评估函数
    eval_stats = await run_cypher_batch_evaluation(
        gemini_resource_for_evaluator=gemini_api, # 传递注入的 Dagster 资源
        rag_interaction_log_filepath=latest_rag_interaction_log_for_evaluation,
        app_version=app_version,
        use_simulated_api=use_simulated, # 这个参数现在由 run_cypher_batch_evaluation 内部处理
        api_call_delay=api_delay
    )
    context.log.info(f"Batch Cypher evaluation completed. Statistics: {eval_stats}")

    # 确定输出的评估结果日志文件名 (与 evaluator.py 中一致)
    output_log_filepath = get_evaluation_result_log_filepath(evaluation_name="cypher_gemini_flash")
    
    # 确保目录存在 (get_evaluation_result_log_filepath 内部的 log_interaction_data 会处理)
    # 但这里我们也可以提前确保，或者依赖 log_interaction_data
    os.makedirs(os.path.dirname(output_log_filepath), exist_ok=True)
            
    metadata = {"evaluation_stats": eval_stats, "output_filepath": output_log_filepath}
    if eval_stats.get("cypher_queries_evaluated", 0) == 0:
        metadata["warning"] = "No Cypher queries were evaluated. Output log might be empty."
        context.log.warning(metadata["warning"])

    return dg.Output(output_log_filepath, metadata=metadata)


@dg.asset(
    name="batch_answer_evaluations_log", # 资产名称
    description="Runs batch evaluation of generated answers from RAG logs using Gemini.",
    group_name="evaluation_pipeline",
    compute_kind="python",
    # deps=[latest_rag_interaction_log_for_evaluation_asset] # 通过函数参数自动推断依赖
)
async def batch_answer_evaluation_log_asset(
    context: dg.AssetExecutionContext,
    gemini_api: GeminiAPIResource,
    latest_rag_interaction_log_for_evaluation: str # <--- 修改参数名
) -> dg.Output[str]:
    context.log.info(f"Starting batch Answer evaluation using log file: {latest_rag_interaction_log_for_evaluation}") # <--- 使用新参数名
    app_version = os.getenv("APP_VERSION_TAG", "dagster_answer_eval_0.2")
    use_simulated = os.getenv("USE_SIMULATED_GEMINI_ANSWER_EVAL", "false").lower() == "true"
    api_delay = float(os.getenv("GEMINI_API_CALL_DELAY_SECONDS", "4.1"))

    if use_simulated:
        context.log.warning("Answer evaluation asset is using SIMULATED Gemini API calls.")

    eval_stats = await run_answer_batch_evaluation(
        gemini_resource_for_evaluator=gemini_api, # 传递注入的 Dagster 资源
        rag_interaction_log_filepath=latest_rag_interaction_log_for_evaluation,
        app_version=app_version,
        use_simulated_api=use_simulated, # 这个参数现在由 run_answer_batch_evaluation 内部处理
        api_call_delay=api_delay
    )
    context.log.info(f"Batch Answer evaluation completed. Statistics: {eval_stats}")

    output_log_filepath = get_evaluation_result_log_filepath(evaluation_name="answer_gemini_flash")
    os.makedirs(os.path.dirname(output_log_filepath), exist_ok=True)

    metadata = {"evaluation_stats": eval_stats, "output_filepath": output_log_filepath}
    if eval_stats.get("answers_evaluated", 0) == 0:
        metadata["warning"] = "No answers were evaluated. Output log might be empty."
        context.log.warning(metadata["warning"])
        
    return dg.Output(output_log_filepath, metadata=metadata)

@dg.asset(
    name="cypher_evaluation_analysis_report", # 资产名称
    description="Generates a CSV analysis report from Cypher evaluation results.",
    group_name="evaluation_pipeline",
    compute_kind="python",
    # deps=[batch_cypher_evaluation_log_asset] # 通过函数参数自动推断依赖
)
def cypher_analysis_report_asset(
    context: dg.AssetExecutionContext,
    batch_cypher_evaluations_log: str # 上游资产的输出 (即 cypher 评估日志文件的路径)
) -> dg.Output[str]: # 输出 CSV 报告文件的路径
    """
    Analyzes Cypher evaluation logs and produces a CSV report.
    """
    context.log.info(f"Starting Cypher evaluation analysis using log file: {batch_cypher_evaluations_log}")

    if not os.path.exists(batch_cypher_evaluations_log):
        error_msg = f"Input Cypher evaluation log file not found: {batch_cypher_evaluations_log}"
        context.log.error(error_msg)
        raise dg.Failure(description=error_msg)

    # 构建输出CSV文件的路径
    # 我们希望CSV文件也存储在 EVALUATION_RESULTS_LOGS_DIR 目录下
    # 文件名可以基于输入日志名或固定一个模式
    base_input_log_name = os.path.basename(batch_cypher_evaluations_log)
    # 从 "eval_results_cypher_gemini_flash_YYYYMMDD.jsonl" 生成 "analysis_cypher_gemini_flash_YYYYMMDD.csv"
    if base_input_log_name.startswith("eval_results_") and base_input_log_name.endswith(".jsonl"):
        analysis_file_name = "analysis_" + base_input_log_name[len("eval_results_"):-len(".jsonl")] + ".csv"
    else: # Fallback naming
        analysis_file_name = f"analysis_cypher_report_{context.run_id[:8]}.csv"
    
    output_csv_filepath = os.path.join(EVALUATION_RESULTS_LOGS_DIR, analysis_file_name)
    
    success = perform_cypher_evaluation_analysis(
        evaluation_log_filepath=batch_cypher_evaluations_log,
        output_csv_filepath=output_csv_filepath
    )

    if success:
        context.log.info(f"Cypher evaluation analysis report generated: {output_csv_filepath}")
        return dg.Output(output_csv_filepath, metadata={"output_csv_filepath": output_csv_filepath, "source_log": base_input_log_name})
    else:
        error_msg = f"Cypher evaluation analysis failed for log file: {batch_cypher_evaluations_log}"
        context.log.error(error_msg)
        raise dg.Failure(description=error_msg)


@dg.asset(
    name="answer_evaluation_analysis_report", # 资产名称
    description="Generates a CSV analysis report from Answer evaluation results.",
    group_name="evaluation_pipeline",
    compute_kind="python",
    # deps=[batch_answer_evaluations_log_asset] # 通过函数参数自动推断依赖
)
def answer_analysis_report_asset(
    context: dg.AssetExecutionContext,
    batch_answer_evaluations_log: str # 上游资产的输出 (即 answer 评估日志文件的路径)
) -> dg.Output[str]: # 输出 CSV 报告文件的路径
    """
    Analyzes Answer evaluation logs and produces a CSV report.
    """
    context.log.info(f"Starting Answer evaluation analysis using log file: {batch_answer_evaluations_log}")

    if not os.path.exists(batch_answer_evaluations_log):
        error_msg = f"Input Answer evaluation log file not found: {batch_answer_evaluations_log}"
        context.log.error(error_msg)
        raise dg.Failure(description=error_msg)

    base_input_log_name = os.path.basename(batch_answer_evaluations_log)
    if base_input_log_name.startswith("eval_results_") and base_input_log_name.endswith(".jsonl"):
        analysis_file_name = "analysis_" + base_input_log_name[len("eval_results_"):-len(".jsonl")] + ".csv"
    else: # Fallback naming
        analysis_file_name = f"analysis_answer_report_{context.run_id[:8]}.csv"
        
    output_csv_filepath = os.path.join(EVALUATION_RESULTS_LOGS_DIR, analysis_file_name)

    success = perform_answer_evaluation_analysis(
        evaluation_log_filepath=batch_answer_evaluations_log,
        output_csv_filepath=output_csv_filepath
    )

    if success:
        context.log.info(f"Answer evaluation analysis report generated: {output_csv_filepath}")
        return dg.Output(output_csv_filepath, metadata={"output_csv_filepath": output_csv_filepath, "source_log": base_input_log_name})
    else:
        error_msg = f"Answer evaluation analysis failed for log file: {batch_answer_evaluations_log}"
        context.log.error(error_msg)
        raise dg.Failure(description=error_msg)

# 将所有评估相关的资产收集到一个列表中，方便在 definitions.py 中引用
all_evaluation_assets = [
    latest_rag_interaction_log_for_evaluation_asset,
    batch_cypher_evaluation_log_asset,
    batch_answer_evaluation_log_asset,
    cypher_analysis_report_asset, # <--- 新增
    answer_analysis_report_asset, # <--- 新增
]
    --- END OF FILE CONTENT ---

  File: ingestion_assets.py
    --- START OF FILE CONTENT ---
# zhz_rag_pipeline/ingestion_assets.py
import dagster as dg
import os
from typing import List, Dict, Any, Union

# 从我们新建的pydantic模型文件中导入
from .pydantic_models_dagster import LoadedDocumentOutput, ParsedDocumentOutput

class LoadDocumentsConfig(dg.Config):
    documents_directory: str = "/home/zhz/zhz_agent/data/raw_documents/" # 更新后的原始文档目录
    allowed_extensions: List[str] = [".txt"]

@dg.asset(
    name="raw_documents",
    description="Loads raw documents from a specified directory.",
    group_name="ingestion" # 给资产分组
)
def load_documents_asset(
    context: dg.AssetExecutionContext, 
    config: LoadDocumentsConfig
) -> List[LoadedDocumentOutput]:
    
    loaded_docs: List[LoadedDocumentOutput] = []
    target_directory = config.documents_directory
    allowed_exts = tuple(config.allowed_extensions) 

    context.log.info(f"Scanning directory: {target_directory} for files with extensions: {allowed_exts}")

    if not os.path.isdir(target_directory):
        context.log.error(f"Directory not found: {target_directory}")
        return loaded_docs

    for filename in os.listdir(target_directory):
        file_path = os.path.join(target_directory, filename)
        if os.path.isfile(file_path):
            file_name_lower = filename.lower()
            file_extension = os.path.splitext(file_name_lower)[1]

            if file_extension in allowed_exts:
                context.log.info(f"Found matching file: {file_path}")
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    
                    doc_output = LoadedDocumentOutput(
                        document_path=file_path,
                        file_type=file_extension,
                        raw_content=content,
                        metadata={
                            "filename": filename,
                            "source_directory": target_directory,
                            "size_bytes": os.path.getsize(file_path)
                        }
                    )
                    loaded_docs.append(doc_output)
                    context.log.info(f"Successfully loaded and created output for: {file_path}")
                except Exception as e:
                    context.log.error(f"Failed to read or process file {file_path}: {e}")
            else:
                context.log.debug(f"Skipping file with non-allowed extension: {file_path}")
        else:
            context.log.debug(f"Skipping non-file item: {file_path}")
            
    if not loaded_docs:
        context.log.warning(f"No matching documents found in {target_directory}")

    if loaded_docs:
        context.add_output_metadata(
            metadata={
                "num_documents_loaded": len(loaded_docs),
                "first_document_path": loaded_docs[0].document_path if loaded_docs else "N/A"
            }
        )
    return loaded_docs


@dg.asset(
    name="parsed_documents",
    description="Parses loaded documents into text and extracts basic structure.",
    group_name="ingestion"
    # deps=[load_documents_asset] # <--- 删除这一行，因为依赖已通过函数参数 raw_documents 声明
)
def parse_document_asset(
    context: dg.AssetExecutionContext, 
    raw_documents: List[LoadedDocumentOutput] 
) -> List[ParsedDocumentOutput]:
    
    parsed_docs: List[ParsedDocumentOutput] = []
    context.log.info(f"Received {len(raw_documents)} documents to parse.")

    for doc_input in raw_documents:
        context.log.info(f"Parsing document: {doc_input.document_path} (Type: {doc_input.file_type})")
        parsed_text_content = ""
        
        try:
            if doc_input.file_type == ".txt":
                if isinstance(doc_input.raw_content, bytes):
                    parsed_text_content = doc_input.raw_content.decode('utf-8')
                elif isinstance(doc_input.raw_content, str):
                    parsed_text_content = doc_input.raw_content
                else:
                    # 抛出更具体的错误或记录并跳过
                    context.log.error(f"Unexpected raw_content type for .txt file: {type(doc_input.raw_content)} in {doc_input.document_path}")
                    parsed_text_content = f"[Error: Unexpected content type {type(doc_input.raw_content)}]"

            # TODO: Add parsers for other file types like .pdf, .docx here
            # elif doc_input.file_type == ".pdf":
            #     parsed_text_content = "[PDF parsing not yet implemented]"
            #     context.log.warning(f"PDF parsing not yet implemented for {doc_input.document_path}")
            else:
                parsed_text_content = f"[Unsupported file type: {doc_input.file_type}]"
                context.log.warning(f"Unsupported file type '{doc_input.file_type}' for parsing: {doc_input.document_path}")

            parsed_output = ParsedDocumentOutput(
                parsed_text=parsed_text_content,
                # document_structure is None by default
                original_metadata=doc_input.metadata 
            )
            parsed_docs.append(parsed_output)
            context.log.info(f"Successfully (or with placeholder) parsed: {doc_input.document_path}")

        except Exception as e:
            context.log.error(f"Failed to parse document {doc_input.document_path}: {e}")
            parsed_output = ParsedDocumentOutput(
                parsed_text=f"[Error parsing document: {str(e)}]",
                original_metadata=doc_input.metadata
            )
            parsed_docs.append(parsed_output)

    if parsed_docs:
        context.add_output_metadata(
            metadata={
                "num_documents_parsed": len(parsed_docs),
                "first_parsed_doc_filename": parsed_docs[0].original_metadata.get("filename", "N/A") if parsed_docs else "N/A"
            }
        )
    return parsed_docs

all_ingestion_assets = [load_documents_asset, parse_document_asset]
    --- END OF FILE CONTENT ---

  File: processing_assets.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/zhz_rag_pipeline/processing_assets.py
import dagster as dg
from typing import List, Dict, Any, Optional
import uuid
from langchain_text_splitters import RecursiveCharacterTextSplitter
import hashlib
import pandas as pd 
from zhz_rag.utils.common_utils import normalize_text_for_id
from zhz_rag_pipeline_dagster.zhz_rag_pipeline.pydantic_models_dagster import (
    ChunkOutput,
    ParsedDocumentOutput,
    EmbeddingOutput,
    KGTripleSetOutput, 
    ExtractedEntity,
    ExtractedRelation
)
from zhz_rag_pipeline_dagster.zhz_rag_pipeline.resources import (
    SentenceTransformerResource,
    ChromaDBResource,
    LocalLLMAPIResource, 
    KuzuDBReadWriteResource,
    KuzuDBReadOnlyResource
)
import jieba
import bm25s
import pickle
import numpy as np
import os
import time

class TextChunkerConfig(dg.Config):
    chunk_size: int = 500
    chunk_overlap: int = 50
    # separators: Optional[List[str]] = None # 可选的自定义分隔符

@dg.asset(
    name="text_chunks",
    description="Cleans and chunks parsed documents into smaller text segments.",
    group_name="processing", # 属于处理组
    # deps=["parsed_documents"] # <--- 删除或注释掉这一行
)
def clean_chunk_text_asset(
    context: dg.AssetExecutionContext,
    config: TextChunkerConfig,
    parsed_documents: List[ParsedDocumentOutput] 
) -> List[ChunkOutput]: 
    
    all_chunks: List[ChunkOutput] = []
    context.log.info(f"Received {len(parsed_documents)} parsed documents to clean and chunk.")

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=config.chunk_size,
        chunk_overlap=config.chunk_overlap,
        # length_function=len, # 默认
        # add_start_index=True, # 如果需要块的起始索引
        # separators=config.separators if config.separators else None # 使用配置的分隔符
    )

    for parsed_doc in parsed_documents:
        # 使用原始文件名作为文档ID，如果不存在则生成UUID
        doc_id_from_meta = parsed_doc.original_metadata.get("filename")
        if not doc_id_from_meta:
            doc_id_from_meta = parsed_doc.original_metadata.get("document_path", str(uuid.uuid4()))
            context.log.warning(f"Filename not found in metadata for a document, using path or UUID: {doc_id_from_meta}")
        
        source_dir = parsed_doc.original_metadata.get('source_directory', 'Unknown_Source_Dir')
        context.log.info(f"Processing document: {doc_id_from_meta} (from {source_dir})")

        cleaned_text = parsed_doc.parsed_text.strip()

        if not cleaned_text or cleaned_text.startswith("[Unsupported file type:") or cleaned_text.startswith("[Error parsing document:"):
            context.log.warning(f"Document {doc_id_from_meta} has no valid content or was unsupported/errored in parsing, skipping chunking.")
            continue
        
        try:
            chunks_text_list = text_splitter.split_text(cleaned_text)
            context.log.info(f"Document {doc_id_from_meta} split into {len(chunks_text_list)} chunks.")

            for i, chunk_text_content in enumerate(chunks_text_list):
                chunk_meta = parsed_doc.original_metadata.copy() 
                chunk_meta.update({
                    "chunk_number": i + 1,
                    "total_chunks_for_doc": len(chunks_text_list),
                    "chunk_char_length": len(chunk_text_content),
                })

                chunk_output = ChunkOutput( # chunk_id 会在 ChunkOutput 的 __init__ 中自动生成
                    chunk_text=chunk_text_content,
                    source_document_id=doc_id_from_meta, # 使用从元数据获取的文档ID
                    chunk_metadata=chunk_meta
                )
                all_chunks.append(chunk_output)
        
        except Exception as e:
            context.log.error(f"Failed to chunk document {doc_id_from_meta}: {e}")
            
    if all_chunks:
        context.add_output_metadata(
            metadata={
                "total_chunks_generated": len(all_chunks),
                "first_chunk_doc_id": all_chunks[0].source_document_id if all_chunks else "N/A"
            }
        )
    else:
        context.log.warning("No chunks were generated from the input documents.")
        
    return all_chunks

# --- 新增：EmbeddingGenerationAgent (实现为 Asset) ---
@dg.asset(
    name="text_embeddings",
    description="Generates vector embeddings for text chunks using a SentenceTransformer model.",
    group_name="processing",
    # deps=["text_chunks"] # <--- 删除或注释掉这一行
)
def generate_embeddings_asset(
    context: dg.AssetExecutionContext,
    text_chunks: List[ChunkOutput], # 输入是上游资产的输出列表
    embedder: SentenceTransformerResource # <--- 注入我们定义的Resource
) -> List[EmbeddingOutput]:
    
    all_embeddings: List[EmbeddingOutput] = []
    context.log.info(f"Received {len(text_chunks)} text chunks to generate embeddings for.")

    if not text_chunks:
        context.log.warning("No text chunks received, skipping embedding generation.")
        return all_embeddings

    # 提取所有块的文本内容进行批量编码
    chunk_texts_to_encode = [chunk.chunk_text for chunk in text_chunks]
    
    try:
        context.log.info(f"Starting batch embedding generation for {len(chunk_texts_to_encode)} texts...")
        # 使用Resource的encode方法
        vectors = embedder.encode(chunk_texts_to_encode) 
        context.log.info(f"Successfully generated {len(vectors)} embedding vectors.")

        if len(vectors) != len(text_chunks):
            # 这是一个预期外的情况，应该记录严重错误
            context.log.error(f"Mismatch in number of chunks ({len(text_chunks)}) and generated vectors ({len(vectors)}). Aborting.")
            # 可以在这里抛出异常来使资产失败
            raise ValueError("Embedding generation resulted in a mismatched number of vectors.")

        for i, chunk_input in enumerate(text_chunks):
            embedding_output = EmbeddingOutput(
                chunk_id=chunk_input.chunk_id,
                chunk_text=chunk_input.chunk_text,
                embedding_vector=vectors[i],
                embedding_model_name=embedder.model_name_or_path, # 从Resource获取模型名
                original_chunk_metadata=chunk_input.chunk_metadata
            )
            all_embeddings.append(embedding_output)
        
        context.log.info(f"All {len(all_embeddings)} embeddings prepared.")

    except Exception as e:
        context.log.error(f"Failed to generate embeddings: {e}")
        # 根据策略，可以选择让资产失败，或者返回空列表/部分结果
        # 这里我们选择抛出异常，让资产运行失败，以便调查
        raise

    if all_embeddings:
        context.add_output_metadata(
            metadata={
                "total_embeddings_generated": len(all_embeddings),
                "embedding_model_used": embedder.model_name_or_path,
                "first_chunk_id_embedded": all_embeddings[0].chunk_id if all_embeddings else "N/A"
            }
        )
    return all_embeddings

# --- 新增：VectorStorageAgent (实现为 Asset) ---
@dg.asset(
    name="vector_store_embeddings",
    description="Stores text embeddings into a ChromaDB vector store.",
    group_name="indexing", # 新的分组
    # deps=["text_embeddings"] # 依赖上一个资产的name
)
def vector_storage_asset(
    context: dg.AssetExecutionContext,
    text_embeddings: List[EmbeddingOutput], # 输入是上游资产的输出列表
    chroma_db: ChromaDBResource # <--- 注入ChromaDB Resource
) -> None: # 这个资产通常只执行操作，不产生新的可传递数据资产，所以返回None
    
    context.log.info(f"Received {len(text_embeddings)} embeddings to store in ChromaDB.")

    if not text_embeddings:
        context.log.warning("No embeddings received, nothing to store.")
        # 可以在这里添加一个 AssetMaterialization 来记录这个空操作
        context.add_output_metadata(metadata={"num_embeddings_stored": 0, "status": "No data to store"})
        return

    ids_to_store: List[str] = []
    embeddings_to_store: List[List[float]] = []
    metadatas_to_store: List[Dict[str, Any]] = []

    for emb_output in text_embeddings: # emb_output 是 EmbeddingOutput 类型
        ids_to_store.append(emb_output.chunk_id) 
        embeddings_to_store.append(emb_output.embedding_vector)
        
        simple_metadata = {}
        for key, value in emb_output.original_chunk_metadata.items(): # 从原始块元数据开始
            if isinstance(value, (str, int, float, bool)):
                simple_metadata[key] = value
            else:
                simple_metadata[key] = str(value) 
        
        simple_metadata["chunk_text"] = emb_output.chunk_text # <--- 直接从emb_output获取

        metadatas_to_store.append(simple_metadata)

    try:
        chroma_db.add_embeddings(
            ids=ids_to_store,
            embeddings=embeddings_to_store,
            metadatas=metadatas_to_store
        )
        context.log.info(f"Successfully stored/updated {len(ids_to_store)} embeddings in ChromaDB.")
        
        # 记录物化信息
        context.add_output_metadata(
            metadata={
                "num_embeddings_stored": len(ids_to_store),
                "collection_name": chroma_db.collection_name,
                "status": "Success"
            }
        )
    except Exception as e:
        context.log.error(f"Failed to store embeddings in ChromaDB: {e}")
        context.add_output_metadata(
            metadata={
                "num_embeddings_stored": 0,
                "collection_name": chroma_db.collection_name,
                "status": f"Failed: {str(e)}"
            }
        )
        raise # 让资产失败


# --- 新增：定义 KeywordIndexAgent 的配置 Pydantic 模型 ---
class BM25IndexConfig(dg.Config):
    index_file_path: str = "/home/zhz/zhz_agent/zhz_rag/stored_data/bm25_index/"

# --- 修改：KeywordIndexAgent (实现为 Asset) ---
@dg.asset(
    name="keyword_index",
    description="Builds and persists a BM25 keyword index from text chunks.",
    group_name="indexing",
    # deps=["text_chunks"] 
)
def keyword_index_asset(
    context: dg.AssetExecutionContext,
    config: BM25IndexConfig,
    text_chunks: List[ChunkOutput] 
) -> None:
    
    context.log.info(f"Received {len(text_chunks)} text chunks to build BM25 index.")

    if not text_chunks:
        # ... (无数据处理不变) ...
        context.log.warning("No text chunks received, skipping BM25 index building.")
        context.add_output_metadata(metadata={"num_documents_indexed": 0, "status": "No data"})
        return

    corpus_texts: List[str] = [chunk.chunk_text for chunk in text_chunks]
    document_ids: List[str] = [chunk.chunk_id for chunk in text_chunks] # 我们仍然需要保存这个映射

    context.log.info("Tokenizing corpus using jieba...")
    # 使用jieba分词，但bm25s有自己的tokenize函数，可以接受自定义分词器，或者直接处理分词后的列表
    # 为了与bm25s的tokenize函数配合，我们可以先用jieba分好，再传给bm25s的tokenizer
    # 或者，如果bm25s的默认分词或其Tokenizer类能满足中文需求，可以直接用。
    # 为了简单且利用jieba，我们先分词
    corpus_tokenized_jieba = [list(jieba.cut_for_search(text)) for text in corpus_texts]
    context.log.info(f"Jieba tokenization complete. Example: {corpus_tokenized_jieba[0][:10] if corpus_tokenized_jieba else 'N/A'}")

    context.log.info("Initializing BM25s model and indexing corpus...")
    try:
        # 根据bm25s文档，先创建BM25对象，然后调用index方法
        bm25_model = bm25s.BM25() # 可以传入k1, b等参数
        # .index() 方法接受已经分词的语料库 (list of list of str)
        bm25_model.index(corpus_tokenized_jieba)
        context.log.info("BM25s model indexed successfully.")
    except Exception as e:
        context.log.error(f"Failed to initialize or index with BM25s model: {e}")
        raise

    index_directory = config.index_file_path # 这应该是一个目录路径

    if not os.path.exists(index_directory):
        os.makedirs(index_directory, exist_ok=True)
        context.log.info(f"Created directory for BM25 index: {index_directory}")

    try:
        context.log.info(f"Saving BM25 model to directory: {index_directory}")
        # 使用bm25s的save方法，它会将多个文件保存到该目录下
        # 它会自动保存词汇表 (vocab.index.json) 和其他必要文件
        bm25_model.save(
            index_directory,
            # 文件名参数是可选的，bm25s有默认文件名，例如：
            # data_name="data.csc.index.npy",
            # indices_name="indices.csc.index.npy",
            # indptr_name="indptr.csc.index.npy",
            # vocab_name="vocab.index.json", 
            # params_name="params.index.json"
        )
        context.log.info(f"BM25 model saved successfully to {index_directory}")

        # 单独保存我们的 document_ids 列表，因为bm25s内部索引是基于0,1,2...
        # 而我们需要映射回原始的chunk_id
        doc_ids_path = os.path.join(index_directory, "doc_ids.pkl") # 保持这个文件名
        with open(doc_ids_path, 'wb') as f_out:
            pickle.dump(document_ids, f_out)
        context.log.info(f"Document IDs saved successfully to {doc_ids_path}")
        
        context.add_output_metadata(
            metadata={
                "num_documents_indexed": len(corpus_texts),
                "index_directory_path": index_directory, # <--- 修改：现在是目录路径
                "status": "Success"
            }
        )
    except Exception as e:
        # ... (错误处理不变) ...
        context.log.error(f"Failed to save BM25 model or document IDs: {e}")
        context.add_output_metadata(
            metadata={
                "num_documents_indexed": 0,
                "index_directory_path": index_directory,
                "status": f"Failed to save index: {str(e)}"
            }
        )
        raise

# 知识图谱
DEFAULT_KG_EXTRACTION_SCHEMA = { # <--- 覆盖这里的整个字典
    "type": "object",
    "properties": {
        "entities": {
            "type": "array",
            "items": {
                "type": "object",
                "properties": {
                    "text": {"type": "string", "description": "提取到的实体原文"},
                    "label": {"type": "string", "description": "实体类型 (例如: PERSON, ORGANIZATION, TASK)"}
                },
                "required": ["text", "label"]
            },
            "description": "从文本中提取出的实体列表。"
        },
        "relations": { # <--- 新增/确保这部分存在且正确
            "type": "array",
            "items": {
                "type": "object",
                "properties": {
                    "head_entity_text": {"type": "string", "description": "头实体的文本"},
                    "head_entity_label": {"type": "string", "description": "头实体的类型 (例如: PERSON, TASK)"},
                    "relation_type": {"type": "string", "description": "关系类型 (例如: WORKS_AT, ASSIGNED_TO)"},
                    "tail_entity_text": {"type": "string", "description": "尾实体的文本"},
                    "tail_entity_label": {"type": "string", "description": "尾实体的类型 (例如: ORGANIZATION, PERSON)"}
                },
                "required": ["head_entity_text", "head_entity_label", "relation_type", "tail_entity_text", "tail_entity_label"]
            },
            "description": "从文本中提取出的关系三元组列表。"
        }
    },
    "required": ["entities", "relations"] # <--- 确保 "relations" 也在这里
}

class KGExtractionConfig(dg.Config):
    extraction_prompt_template: str = ( # <--- 覆盖这里的整个多行字符串
        "你是一个信息抽取助手。请从以下提供的文本中抽取出所有的人名(PERSON)、组织机构名(ORGANIZATION)和任务(TASK)实体。\n"
        "同时，请抽取出以下两种关系：\n"
        "1. WORKS_AT (当一个人在一个组织工作时，例如：PERSON WORKS_AT ORGANIZATION)\n"
        "2. ASSIGNED_TO (当一个任务分配给一个人时，例如：TASK ASSIGNED_TO PERSON)\n\n"
        "请严格按照以下JSON格式进行输出，不要包含任何额外的解释或Markdown标记：\n"
        "{{\n"
        "  \"entities\": [\n"
        "    {{\"text\": \"实体1原文\", \"label\": \"实体1类型\"}},\n"
        "    ...\n"
        "  ],\n"
        "  \"relations\": [\n"
        "    {{\"head_entity_text\": \"头实体文本\", \"head_entity_label\": \"头实体类型\", \"relation_type\": \"关系类型\", \"tail_entity_text\": \"尾实体文本\", \"tail_entity_label\": \"尾实体类型\"}},\n"
        "    ...\n"
        "  ]\n"
        "}}\n"
        "如果文本中没有可抽取的实体或关系，请返回一个空的对应列表 (例如 {{\"entities\": [], \"relations\": []}})。\n\n"
        "示例文本1：'项目Alpha的文档编写任务分配给了张三。张三在谷歌工作。'\n"
        "期望JSON输出1：\n"
        "{{\n"
        "  \"entities\": [\n"
        "    {{\"text\": \"项目Alpha的文档编写任务\", \"label\": \"TASK\"}},\n"
        "    {{\"text\": \"张三\", \"label\": \"PERSON\"}},\n"
        "    {{\"text\": \"谷歌\", \"label\": \"ORGANIZATION\"}}\n"
        "  ],\n"
        "  \"relations\": [\n"
        "    {{\"head_entity_text\": \"项目Alpha的文档编写任务\", \"head_entity_label\": \"TASK\", \"relation_type\": \"ASSIGNED_TO\", \"tail_entity_text\": \"张三\", \"tail_entity_label\": \"PERSON\"}},\n"
        "    {{\"head_entity_text\": \"张三\", \"head_entity_label\": \"PERSON\", \"relation_type\": \"WORKS_AT\", \"tail_entity_text\": \"谷歌\", \"tail_entity_label\": \"ORGANIZATION\"}}\n"
        "  ]\n"
        "}}\n\n"
        "文本：\n"
        "\"{text_to_extract}\"\n\n"
        "JSON输出："
    )
    sglang_model_name: str = "Qwen2.5-3B-Instruct_via_SGLang" # 保持不变

# --- 新增：KGExtractionAgent (实现为 Asset) ---
@dg.asset(
    name="kg_extractions",
    description="Extracts entities (and potentially relations) from text chunks for knowledge graph construction.",
    group_name="kg_building",
    io_manager_key="pydantic_json_io_manager" 
)
async def kg_extraction_asset(
    context: dg.AssetExecutionContext,
    text_chunks: List[ChunkOutput], 
    config: KGExtractionConfig,     
    sglang_api: LocalLLMAPIResource 
) -> List[KGTripleSetOutput]:
    
    all_kg_outputs: List[KGTripleSetOutput] = []
    context.log.info(f"Received {len(text_chunks)} text chunks for KG extraction.")

    if not text_chunks:
        context.log.warning("No text chunks received, skipping KG extraction.")
        context.add_output_metadata(metadata={"num_chunks_processed": 0, "total_entities_extracted": 0, "total_relations_extracted": 0})
        return all_kg_outputs

    total_entities_count = 0
    total_relations_count = 0
    for i, chunk in enumerate(text_chunks):
        context.log.info(f"Extracting KG from chunk {i+1}/{len(text_chunks)} (ID: {chunk.chunk_id})")
        
        prompt = config.extraction_prompt_template.format(text_to_extract=chunk.chunk_text)
        
        try:
            structured_response = await sglang_api.generate_structured_output(
                prompt=prompt,
                json_schema=DEFAULT_KG_EXTRACTION_SCHEMA 
            )
             # --- 核心修改：在这一步就进行规范化 ---
            entities_data = structured_response.get("entities", [])
            extracted_entities_list: List[ExtractedEntity] = []
            if isinstance(entities_data, list):
                for entity_dict in entities_data:
                    if isinstance(entity_dict, dict) and "text" in entity_dict and "label" in entity_dict:
                        # 对 LLM 返回的文本和标签立即进行规范化
                        normalized_text = normalize_text_for_id(entity_dict["text"])
                        normalized_label = entity_dict["label"].upper()
                        extracted_entities_list.append(ExtractedEntity(
                            text=normalized_text, 
                            label=normalized_label
                        ))
            
            relations_data = structured_response.get("relations", [])
            extracted_relations_list: List[ExtractedRelation] = []
            if isinstance(relations_data, list):
                for rel_dict in relations_data:
                    if (isinstance(rel_dict, dict) and
                        all(key in rel_dict for key in ["head_entity_text", "head_entity_label", 
                                                        "relation_type", "tail_entity_text", "tail_entity_label"])):
                        # 对关系中的所有文本和标签也进行规范化
                        extracted_relations_list.append(ExtractedRelation(
                            head_entity_text=normalize_text_for_id(rel_dict["head_entity_text"]),
                            head_entity_label=rel_dict["head_entity_label"].upper(),
                            relation_type=rel_dict["relation_type"].upper(),
                            tail_entity_text=normalize_text_for_id(rel_dict["tail_entity_text"]),
                            tail_entity_label=rel_dict["tail_entity_label"].upper()
                        ))
            # --- 修改结束 --- 
            total_entities_count += len(extracted_entities_list)
            total_relations_count += len(extracted_relations_list)

            kg_output = KGTripleSetOutput(
                chunk_id=chunk.chunk_id,
                extracted_entities=extracted_entities_list,
                extracted_relations=extracted_relations_list,
                extraction_model_name=config.sglang_model_name,
                original_chunk_metadata=chunk.chunk_metadata
            )
            all_kg_outputs.append(kg_output)

        except Exception as e:
            context.log.error(f"Failed KG extraction for chunk {chunk.chunk_id}: {e}", exc_info=True) # 添加 exc_info=True
            all_kg_outputs.append(KGTripleSetOutput(
                chunk_id=chunk.chunk_id,
                extraction_model_name=config.sglang_model_name,
                original_chunk_metadata={"error": str(e), **chunk.chunk_metadata}
            ))
            
    context.add_output_metadata(
        metadata={
            "num_chunks_processed": len(text_chunks),
            "total_entities_extracted": total_entities_count,
            "total_relations_extracted": total_relations_count,
            "status": "Success" if len(all_kg_outputs) == len(text_chunks) else "Partial Success"
        }
    )
    return all_kg_outputs

KUZU_WRITE_CONCURRENCY_KEY = "kuzu_write_access"


@dg.asset(
    name="kuzu_graph_construction",
    description="Creates schema, loads data, and creates vector index in KuzuDB.",
    group_name="kg_building",
    op_tags={"dagster/concurrency_key": "kuzu_write_pool"},
    deps=[kg_extraction_asset],
    tags={dg.MAX_RUNTIME_SECONDS_TAG: "900"}
)
def kuzu_graph_construction_asset(
    context: dg.AssetExecutionContext,
    kg_extractions: List[KGTripleSetOutput],
    kuzu_readwrite_db: KuzuDBReadWriteResource,
    embedder: SentenceTransformerResource
):
    context.log.info("--- Starting Unified KuzuDB Graph Construction Asset ---")
    if not kg_extractions:
        context.log.warning("No KG extractions received. Skipping.")
        return

    with kuzu_readwrite_db.get_connection() as conn:
        try:
            # 步骤 1: 创建 Schema (在自动事务中)
            context.log.info("Step 1: Creating Schema...")
            EMBEDDING_DIM = embedder.get_embedding_dimension()
            conn.execute(
                f"CREATE NODE TABLE IF NOT EXISTS ExtractedEntity (id_prop STRING, text STRING, label STRING, embedding FLOAT[{EMBEDDING_DIM}], PRIMARY KEY (id_prop))"
            )
            conn.execute("CREATE REL TABLE IF NOT EXISTS WorksAt (FROM ExtractedEntity TO ExtractedEntity)")
            conn.execute("CREATE REL TABLE IF NOT EXISTS AssignedTo (FROM ExtractedEntity TO ExtractedEntity)")
            context.log.info("Schema DDL commands executed.")

            # 步骤 2: 加载节点数据 (在自动事务中)
            context.log.info("Step 2: Loading node data...")
            node_data = [
                {
                    "id_prop": hashlib.md5((normalize_text_for_id(e.text) + e.label.upper()).encode('utf-8')).hexdigest(),
                    "text": e.text, "label": e.label.upper(), "embedding": embedder.encode([e.text])[0]
                }
                for kg_set in kg_extractions for e in kg_set.extracted_entities
            ]
            if node_data:
                nodes_df = pd.DataFrame(node_data)
                conn.execute("COPY ExtractedEntity FROM nodes_df (ignore_errors=true);")
                context.log.info(f"Successfully copied {len(nodes_df)} nodes.")

            # 步骤 3: 加载关系数据 (在自动事务中)
            context.log.info("Step 3: Loading relation data...")
            relation_data = [
                {"from_id": hashlib.md5((normalize_text_for_id(r.head_entity_text) + r.head_entity_label.upper()).encode('utf-8')).hexdigest(),
                 "to_id": hashlib.md5((normalize_text_for_id(r.tail_entity_text) + r.tail_entity_label.upper()).encode('utf-8')).hexdigest(),
                 "relation_type": r.relation_type.upper()}
                for kg_set in kg_extractions for r in kg_set.extracted_relations
            ]
            if relation_data:
                all_rels_df = pd.DataFrame(relation_data)
                for rel_type in all_rels_df['relation_type'].unique():
                    rels_df_for_copy = all_rels_df[all_rels_df['relation_type'] == rel_type][["from_id", "to_id"]]
                    if not rels_df_for_copy.empty:
                        conn.execute(f"COPY {rel_type} FROM (LOAD FROM rels_df_for_copy RETURN from_id, to_id);")
                context.log.info(f"Successfully copied {len(relation_data)} relations.")

            # 步骤 4: CHECKPOINT (可选但推荐)
            context.log.info("Step 4: Executing CHECKPOINT...")
            conn.execute("CHECKPOINT;")
            context.log.info("CHECKPOINT successful.")

            # 步骤 5: 创建向量索引 (在自动事务中)
            context.log.info("Step 5: Creating Vector Index...")
            conn.execute("LOAD VECTOR;")
            conn.execute("CALL CREATE_VECTOR_INDEX('ExtractedEntity', 'entity_embedding_idx', 'embedding', metric := 'cosine')")
            context.log.info("Vector index creation command successfully executed.")

        except Exception as e:
            raise dg.Failure(f"An error occurred within the KuzuDB asset: {e}") from e

    context.log.info("--- KuzuDB Graph Construction Asset Finished Successfully ---")


# 更新 all_processing_assets 列表
all_processing_assets = [
    clean_chunk_text_asset,
    generate_embeddings_asset,
    vector_storage_asset,
    keyword_index_asset,
    kg_extraction_asset,
    kuzu_graph_construction_asset
]

    --- END OF FILE CONTENT ---

  File: pydantic_models_dagster.py
    --- START OF FILE CONTENT ---
# zhz_rag_pipeline/pydantic_models_dagster.py
from typing import List, Dict, Any, Union, Optional
from pydantic import BaseModel
import uuid
from typing import List # 确保 List 已导入


class LoadedDocumentOutput(BaseModel):
    document_path: str
    file_type: str
    raw_content: Union[str, bytes]
    metadata: Dict[str, Any]

class ParsedDocumentOutput(BaseModel):
    parsed_text: str
    document_structure: Optional[Dict[str, Any]] = None
    original_metadata: Dict[str, Any]

class ChunkOutput(BaseModel):
    chunk_id: str = "" 
    chunk_text: str
    source_document_id: str 
    chunk_metadata: Dict[str, Any]

    def __init__(self, **data: Any):
        if 'chunk_id' not in data or not data['chunk_id']:
            data['chunk_id'] = str(uuid.uuid4())
        super().__init__(**data)

class EmbeddingOutput(BaseModel):
    chunk_id: str 
    chunk_text: str 
    embedding_vector: List[float]
    embedding_model_name: str 
    original_chunk_metadata: Dict[str, Any]

class ExtractedEntity(BaseModel):
    text: str 
    label: str 

class ExtractedRelation(BaseModel):
    head_entity_text: str
    head_entity_label: str # 例如 "PERSON"
    relation_type: str    # 例如 "WORKS_AT"
    tail_entity_text: str
    tail_entity_label: str # 例如 "ORGANIZATION"

class KGTripleSetOutput(BaseModel):
    chunk_id: str 
    extracted_entities: List[ExtractedEntity] = []
    extracted_relations: List[ExtractedRelation] = [] 
    extraction_model_name: str 
    original_chunk_metadata: Dict[str, Any]

    --- END OF FILE CONTENT ---

  File: resources.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/zhz_rag_pipeline/resources.py
import dagster as dg
from sentence_transformers import SentenceTransformer
import chromadb
from chromadb.config import Settings
from typing import List, Dict, Any, Union, Optional, ContextManager, Iterator,Tuple
import logging
import httpx
import asyncio
import json
# from neo4j import GraphDatabase, Driver, Result # Neo4j不再直接用于此资源
import litellm
import os
import kuzu # 确保导入 kuzu
import shutil
from pydantic import PrivateAttr, Field as PydanticField
from contextlib import contextmanager
import portalocker # <--- 重新导入 portalocker
import time # <--- 导入 time，可能用于短暂等待

# --- SentenceTransformerResource ---
class SentenceTransformerResourceConfig(dg.Config):
    model_name_or_path: str = "/home/zhz/models/bge-small-zh-v1.5"

class SentenceTransformerResource(dg.ConfigurableResource):
    model_name_or_path: str
    _model: SentenceTransformer = PrivateAttr(default=None)
    _logger: Optional[dg.DagsterLogManager] = PrivateAttr(default=None)

    def setup_for_execution(self, context: dg.InitResourceContext) -> None:
        self._logger = context.log
        self._logger.info(f"Initializing SentenceTransformer model from: {self.model_name_or_path}")
        try:
            self._model = SentenceTransformer(self.model_name_or_path)
            self._logger.info("SentenceTransformer model initialized successfully.")
        except Exception as e:
            self._logger.error(f"Failed to initialize SentenceTransformer model: {e}", exc_info=True)
            raise

    def encode(self, texts: List[str], batch_size: int = 32, normalize_embeddings: bool = True) -> List[List[float]]:
        if self._model is None:
            if self._logger:
                self._logger.error("SentenceTransformer model is not initialized in encode method.")
            raise RuntimeError("SentenceTransformer model is not initialized.")
        
        logger_instance = self._logger if self._logger else dg.get_dagster_logger()
        logger_instance.debug(f"Encoding {len(texts)} texts. Normalize embeddings: {normalize_embeddings}")
        
        embeddings_np = self._model.encode(
            texts, 
            batch_size=batch_size, 
            convert_to_tensor=False, 
            normalize_embeddings=normalize_embeddings
        )
        return [emb.tolist() for emb in embeddings_np]
    
    # --- [添加开始] ---
    def get_embedding_dimension(self) -> int:
        """返回嵌入模型的维度大小。"""
        if self._model and hasattr(self._model, 'get_sentence_embedding_dimension'):
            return self._model.get_sentence_embedding_dimension()
        return 512

# --- ChromaDBResource ---
class ChromaDBResourceConfig(dg.Config):
    collection_name: str = "rag_documents"
    persist_directory: str = "/home/zhz/zhz_agent/zhz_rag/stored_data/chromadb_index/"

class ChromaDBResource(dg.ConfigurableResource):
    collection_name: str
    persist_directory: str

    _client: chromadb.Client = PrivateAttr(default=None)
    _collection: chromadb.Collection = PrivateAttr(default=None)
    _logger: Optional[dg.DagsterLogManager] = PrivateAttr(default=None)

    def setup_for_execution(self, context: dg.InitResourceContext) -> None:
        self._logger = context.log
        self._logger.info(f"Initializing ChromaDB client and collection '{self.collection_name}'...")
        self._logger.info(f"ChromaDB data will be persisted to: {self.persist_directory}")
        try:
            os.makedirs(self.persist_directory, exist_ok=True)
            self._client = chromadb.PersistentClient(path=self.persist_directory)
            self._collection = self._client.get_or_create_collection(
                name=self.collection_name,
                metadata={"hnsw:space": "cosine"}
            )
            self._logger.info(f"ChromaDB collection '{self.collection_name}' initialized/loaded. Count: {self._collection.count()}")
        except Exception as e:
            self._logger.error(f"Failed to initialize ChromaDB: {e}", exc_info=True)
            raise

    def add_embeddings(self, ids: List[str], embeddings: List[List[float]], metadatas: List[Dict[str, Any]] = None):
        logger_instance = self._logger if self._logger else dg.get_dagster_logger()
        if self._collection is None:
            logger_instance.error("ChromaDB collection is not initialized. Cannot add embeddings.")
            raise RuntimeError("ChromaDB collection is not initialized.")
        
        if not (len(ids) == len(embeddings) and (metadatas is None or len(ids) == len(metadatas))):
            logger_instance.error("Length mismatch for ids, embeddings, or metadatas.")
            raise ValueError("Length of ids, embeddings, and metadatas (if provided) must be the same.")

        if not ids:
            logger_instance.info("No ids provided to add_embeddings, skipping.")
            return

        logger_instance.info(f"Adding/updating {len(ids)} embeddings to ChromaDB collection '{self.collection_name}'...")
        self._collection.add(ids=ids, embeddings=embeddings, metadatas=metadatas)
        logger_instance.info(f"Embeddings added/updated. Collection count now: {self._collection.count()}")

    def query_embeddings(self, query_embeddings: List[List[float]], n_results: int = 5) -> chromadb.QueryResult:
        logger_instance = self._logger if self._logger else dg.get_dagster_logger()
        if self._collection is None:
            logger_instance.error("ChromaDB collection is not initialized. Cannot query embeddings.")
            raise RuntimeError("ChromaDB collection is not initialized.")
        logger_instance.debug(f"Querying ChromaDB collection '{self.collection_name}' with {len(query_embeddings)} vectors, n_results={n_results}.")
        return self._collection.query(query_embeddings=query_embeddings, n_results=n_results)

# --- LocalLLMAPIResource ---
class LocalLLMAPIResourceConfig(dg.Config):
    api_url: str = "http://127.0.0.1:8088/v1/chat/completions" # <--- 修改
    default_temperature: float = 0.1
    default_max_new_tokens: int = 2048

class LocalLLMAPIResource(dg.ConfigurableResource):
    # 我们将 SGLangAPIResource 重命名为 LocalLLMAPIResource
    api_url: str
    default_temperature: float
    default_max_new_tokens: int
    _logger: Optional[dg.DagsterLogManager] = PrivateAttr(default=None)

    def setup_for_execution(self, context: dg.InitResourceContext) -> None:
        self._logger = context.log
        self._logger.info(f"LocalLLMAPIResource configured with API URL: {self.api_url}") # <--- 修改日志信息

    # generate_structured_output 方法的逻辑需要调整以适配 OpenAI 兼容的 API
    async def generate_structured_output(
        self, prompt: str, json_schema: Dict[str, Any],
        temperature: Optional[float] = None, max_new_tokens: Optional[int] = None
    ) -> Dict[str, Any]:
        logger_instance = self._logger if self._logger else dg.get_dagster_logger()
        temp_to_use = temperature if temperature is not None else self.default_temperature
        tokens_to_use = max_new_tokens if max_new_tokens is not None else self.default_max_new_tokens

        messages = [{"role": "user", "content": prompt}] # 简化处理

        payload = {
            "model": "local_kg_extraction_model", # 模型名对于本地服务不重要，但需要有
            "messages": messages,
            "temperature": temp_to_use,
            "max_tokens": tokens_to_use,
            "response_format": { # 我们的 local_llm_service.py 支持这个
                "type": "json_object",
                "schema": json_schema
            }
        }
        logger_instance.debug(f"Sending request to Local LLM Service. Prompt (start): {prompt[:100]}...")

        try:
            async with httpx.AsyncClient(timeout=120.0) as client:
                response = await client.post(self.api_url, json=payload) # 使用 self.api_url
                response.raise_for_status()
                response_json = response.json()
                
                # OpenAI 兼容 API 的响应格式是 {"choices": [{"message": {"content": "..."}}]}
                if response_json.get("choices") and response_json["choices"][0].get("message"):
                    generated_text = response_json["choices"][0]["message"].get("content", "")
                    logger_instance.debug(f"Local LLM raw response text: {generated_text}")
                    try:
                        parsed_output = json.loads(generated_text)
                        return parsed_output
                    except json.JSONDecodeError as e:
                        logger_instance.error(f"Failed to decode JSON from Local LLM output: {generated_text}. Error: {e}", exc_info=True)
                        raise ValueError(f"Local LLM output was not valid JSON: {generated_text}") from e
                else:
                    raise ValueError(f"Local LLM response format is incorrect: {response_json}")
        except httpx.HTTPStatusError as e:
            logger_instance.error(f"Local LLM API HTTP error: {e.response.status_code} - {e.response.text}", exc_info=True)
            raise
        except httpx.RequestError as e:
            logger_instance.error(f"Local LLM API request error: {e}", exc_info=True)
            raise
        except Exception as e:
            logger_instance.error(f"Unexpected error during Local LLM call: {e}", exc_info=True)
            raise

# --- GeminiAPIResource ---
class GeminiAPIResourceConfig(dg.Config):
    model_name: str = PydanticField(default="gemini/gemini-1.5-flash-latest", description="Name of the Gemini model.")
    proxy_url: Optional[str] = PydanticField(default_factory=lambda: os.getenv("LITELLM_PROXY_URL"), description="Optional proxy URL for LiteLLM.")
    default_temperature: float = 0.1
    default_max_tokens: int = 2048
    
class GeminiAPIResource(dg.ConfigurableResource):
    model_name: str
    proxy_url: Optional[str]
    default_temperature: float
    default_max_tokens: int
    _api_key: Optional[str] = PrivateAttr(default=None)
    _logger: Optional[dg.DagsterLogManager] = PrivateAttr(default=None)

    def setup_for_execution(self, context: dg.InitResourceContext) -> None:
        self._logger = context.log
        self._api_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
        if self.model_name and not self.model_name.startswith("gemini/"):
            if "gemini" in self.model_name.lower():
                self._logger.info(f"Model name '{self.model_name}' auto-prefixed to 'gemini/'.")
                self.model_name = f"gemini/{self.model_name.split('/')[-1]}"
            else:
                self._logger.warning(f"Model name '{self.model_name}' does not start with 'gemini/'.")
        if not self._api_key:
            self._logger.warning("Gemini API key not found. API calls will likely fail.")
        else:
            self._logger.info(f"GeminiAPIResource initialized. Model: {self.model_name}, Proxy: {self.proxy_url or 'Not set'}")

    async def call_completion(
        self, messages: List[Dict[str, str]],
        temperature: Optional[float] = None, max_tokens: Optional[int] = None,
    ) -> Optional[str]:
        logger_instance = self._logger if self._logger else dg.get_dagster_logger()
        if not self._api_key:
            logger_instance.error("Gemini API key is not configured.")
            return None
        temp_to_use = temperature if temperature is not None else self.default_temperature
        tokens_to_use = max_tokens if max_tokens is not None else self.default_max_tokens
        litellm_params = {
            "model": self.model_name, "messages": messages, "api_key": self._api_key,
            "temperature": temp_to_use, "max_tokens": tokens_to_use,
        }
        if self.proxy_url:
            litellm_params["proxy"] = {"http": self.proxy_url, "https": self.proxy_url} # type: ignore
        logger_instance.debug(f"Calling LiteLLM (Gemini) with params (excluding messages): { {k:v for k,v in litellm_params.items() if k != 'messages'} }")
        raw_output_text: Optional[str] = None
        try:
            response = await litellm.acompletion(**litellm_params) # type: ignore
            if response and response.choices and response.choices[0].message and response.choices[0].message.content:
                raw_output_text = response.choices[0].message.content
                logger_instance.debug(f"LiteLLM (Gemini) raw response (first 300 chars): {raw_output_text[:300]}...")
            else:
                logger_instance.warning(f"LiteLLM (Gemini) returned empty/malformed response: {response}")
        except Exception as e_generic:
            logger_instance.error(f"Error calling Gemini via LiteLLM: {e_generic}", exc_info=True)
        return raw_output_text


class KuzuDBReadWriteResource(dg.ConfigurableResource):
    db_path_str: str = PydanticField(
        default=os.path.join("zhz_rag", "stored_data", "kuzu_default_db"),
        description="Path to the KuzuDB database directory."
    )

    _logger: Optional[dg.DagsterLogManager] = PrivateAttr(default=None)
    _resolved_db_path: str = PrivateAttr()
    _db: Optional[kuzu.Database] = PrivateAttr(default=None) # 这个 db 对象将在整个作业运行期间保持

    def setup_for_execution(self, context: dg.InitResourceContext) -> None:
        """
        在作业开始时被调用一次。创建并持有 kuzu.Database 对象。
        """
        self._logger = context.log
        project_root = os.environ.get("DAGSTER_PROJECT_ROOT", os.getcwd())
        self._resolved_db_path = os.path.join(project_root, self.db_path_str)
        
        self._logger.info(
            f"KuzuDBReadWriteResource setup: DB path is '{self._resolved_db_path}'."
        )
        
        # 清理旧数据库，确保从干净状态开始
        if os.path.exists(self._resolved_db_path):
            shutil.rmtree(self._resolved_db_path)
            self._logger.info(f"Cleaned up existing database directory: {self._resolved_db_path}")

        try:
            os.makedirs(os.path.dirname(self._resolved_db_path), exist_ok=True)
            self._db = kuzu.Database(self._resolved_db_path)
            self._logger.info(f"Shared kuzu.Database object CREATED successfully for the run.")
        except Exception as e:
            self._logger.error(f"Failed to initialize shared kuzu.Database object: {e}", exc_info=True)
            raise

    def teardown_for_execution(self, context: dg.InitResourceContext) -> None:
        """
        在作业结束时被调用一次。销毁 kuzu.Database 对象以释放所有资源。
        """
        self._logger.info(f"Tearing down KuzuDBReadWriteResource...")
        if self._db is not None:
            del self._db 
            self._db = None
            self._logger.info("Shared kuzu.Database object destroyed, releasing file lock.")
        self._logger.info("KuzuDBReadWriteResource teardown complete.")

    @contextmanager
    def get_connection(self) -> Iterator[kuzu.Connection]:
        """
        一个上下文管理器，从共享的 DB 对象获取一个新连接。
        """
        if self._db is None:
            raise ConnectionError("Shared KuzuDB Database object is not initialized.")
        
        conn = None
        try:
            # --- [最终修复] 使用正确的语法创建连接 ---
            conn = kuzu.Connection(self._db) 
            # --- [最终修复] ---
            self._logger.debug("New kuzu.Connection obtained from shared Database object.")
            yield conn
        finally:
            if conn is not None:
                del conn
                self._logger.debug("kuzu.Connection object destroyed.")


class KuzuDBReadOnlyResource(dg.ConfigurableResource):
    db_path_str: str = PydanticField(
        default=os.path.join("zhz_rag", "stored_data", "kuzu_default_db"),
        description=(
            "Path to the KuzuDB database directory for read-only access. "
            "Can be relative to the project root (if not starting with '/') or absolute."
        )
    )
    _logger: Optional[dg.DagsterLogManager] = PrivateAttr(default=None)
    _resolved_db_path: str = PrivateAttr()

    def setup_for_execution(self, context: dg.InitResourceContext) -> None:
        self._logger = context.log
        if os.path.isabs(self.db_path_str):
            self._resolved_db_path = self.db_path_str
        else:
            self._resolved_db_path = os.path.abspath(self.db_path_str)

        self._logger.info(f"KuzuDBReadOnlyResource setup: resolved_path='{self._resolved_db_path}'")
        if not os.path.exists(self._resolved_db_path):
            self._logger.error(f"KuzuDB path {self._resolved_db_path} does not exist for ReadOnly access. Operations will likely fail.")

    def teardown_for_execution(self, context: dg.InitResourceContext) -> None:
        logger_instance = self._logger if self._logger else dg.get_dagster_logger()
        logger_instance.info("KuzuDBReadOnlyResource teardown complete.")

    @contextmanager
    def get_readonly_connection(self) -> Iterator[kuzu.Connection]:
        logger_instance = self._logger if self._logger else dg.get_dagster_logger()
        db_instance: Optional[kuzu.Database] = None
        logger_instance.info(f"Attempting to open KuzuDB(RO) at {self._resolved_db_path} for readonly session.")
        
        if not os.path.exists(self._resolved_db_path):
            logger_instance.error(f"KuzuDB directory {self._resolved_db_path} not found for read-only access.")
            raise FileNotFoundError(f"KuzuDB directory {self._resolved_db_path} not found for read-only access.")

        try:
            db_instance = kuzu.Database(self._resolved_db_path, read_only=True)
            logger_instance.info(f"KuzuDB(RO) session opened at {self._resolved_db_path}")
            conn = kuzu.Connection(db_instance)
            yield conn
        except Exception as e:
            logger_instance.error(f"Error during KuzuDB(RO) session: {e}", exc_info=True)
            raise
        finally:
            if db_instance:
                del db_instance
                logger_instance.info(f"KuzuDB(RO) Database object for session at {self._resolved_db_path} dereferenced (closed).")
    --- END OF FILE CONTENT ---

  File: 获取当前目录下的所有py格式文件.py
    --- START OF FILE CONTENT ---
import os
import sys
import fnmatch

# --- 配置 ---
# 1. 输出文件的名称
OUTPUT_FILENAME = '现阶段核心代码.txt' # <--- 输出文件名

# 2. 需要包含的文件扩展名 (小写) 或完整文件名
TARGET_ITEMS = (
    '.py',      # Python 脚本
)

# 3. 需要排除的目录名称 (主要用于递归扫描，此处作用较小，但保留以防万一)
EXCLUDE_DIRS = {
    '.git',
    '__pycache__',
    'venv', '.venv', # 虚拟环境
    'tests', 'test',
    '.vscode',
    'node_modules',
    'dist', 'build',
}

# 4. 需要排除的文件名或模式 (使用 fnmatch，不区分大小写)
EXCLUDE_FILES = {
    '.DS_Store',
    '*.log',
    # 脚本自身和输出文件会自动排除
}
# ---

script_dir = os.path.dirname(os.path.abspath(__file__))
# 项目路径现在就是脚本所在的目录
project_path = script_dir
output_file_path = os.path.join(script_dir, OUTPUT_FILENAME)

combined_content = []
processed_files_count = 0

print(f"正在读取 '{project_path}' 目录下的核心 Python 文件...")

# 自动排除脚本自身和输出文件
exclude_files_lower_patterns = {f.lower() for f in EXCLUDE_FILES}
exclude_files_lower_patterns.add(os.path.basename(__file__).lower())
exclude_files_lower_patterns.add(OUTPUT_FILENAME.lower())

exclude_dirs_lower = {d.lower() for d in EXCLUDE_DIRS} # 保留，但在此版本中作用不大

def should_exclude_dir(dir_name_full_path):
    # 此函数在此版本中基本不会被积极使用，因为我们不递归进入子目录
    dir_name = os.path.basename(dir_name_full_path)
    return dir_name.lower() in exclude_dirs_lower

def should_include_file(file_name_full_path):
    file_name = os.path.basename(file_name_full_path)
    file_name_lower = file_name.lower()

    # 检查是否在排除文件列表
    for pattern in exclude_files_lower_patterns:
        if fnmatch.fnmatchcase(file_name_lower, pattern):
            return False # 排除

    # 检查是否匹配目标扩展名或完整文件名
    for target in TARGET_ITEMS:
        if target.startswith('.'): # 是扩展名
            if file_name_lower.endswith(target):
                return True
        else: # 是完整文件名 (虽然我们现在只用扩展名)
            if file_name_lower == target.lower():
                return True
    return False # 不包含

def collect_files_in_current_dir(current_path_abs):
    global processed_files_count
    try:
        items = sorted(os.listdir(current_path_abs))
    except Exception as e:
        print(f"错误：无法读取目录 '{current_path_abs}' 的内容: {e}")
        return

    for item_name in items:
        item_path_abs = os.path.join(current_path_abs, item_name)
        # 对于当前目录的文件，其相对于 project_path (即 current_path_abs) 的路径就是文件名本身
        relative_path_to_project_root = item_name

        if os.path.isfile(item_path_abs): # 只处理文件
            if should_include_file(item_path_abs):
                print(f"  正在添加: {relative_path_to_project_root}")
                processed_files_count += 1
                combined_content.append(f"--- START OF FILE {relative_path_to_project_root.replace(os.sep, '/')} ---")
                try:
                    with open(item_path_abs, 'r', encoding='utf-8', errors='ignore') as f:
                        content = f.read()
                        combined_content.append(content)
                except Exception as e:
                    print(f"  *** 警告：读取文件 {relative_path_to_project_root} 时出错: {e} ***")
                    combined_content.append(f"*** ERROR READING FILE {relative_path_to_project_root}: {e} ***")
                combined_content.append(f"--- END OF FILE {relative_path_to_project_root.replace(os.sep, '/')} ---")
                combined_content.append("\n\n")
        # 我们不再递归进入子目录，所以移除了 os.path.isdir 的检查和递归调用

# 执行收集
collect_files_in_current_dir(project_path) # project_path 就是 script_dir

# --- 保存合并后的内容到文件 ---
final_output = "".join(combined_content)

if processed_files_count > 0:
    try:
        with open(output_file_path, 'w', encoding='utf-8') as outfile:
            outfile.write(final_output)
        print(f"\n成功！已将 {processed_files_count} 个 Python 文件的合并内容保存到文件: {output_file_path}")
    except Exception as e:
        print(f"\n错误：无法将内容写入文件 '{output_file_path}': {e}")
        sys.exit(1)
else:
    print(f"\n在当前目录 '{project_path}' 中未找到任何符合条件的 Python 文件。请检查 TARGET_ITEMS 设置。")
    --- END OF FILE CONTENT ---


Directory: zhz_rag_pipeline_dagster/zhz_rag_pipeline_dagster_project.egg-info/
  File (skipped, extension not included): PKG-INFO
  File: SOURCES.txt
    --- START OF FILE CONTENT ---
pyproject.toml
setup.py
zhz_rag_pipeline/__init__.py
zhz_rag_pipeline/custom_io_managers.py
zhz_rag_pipeline/definitions.py
zhz_rag_pipeline/evaluation_assets.py
zhz_rag_pipeline/ingestion_assets.py
zhz_rag_pipeline/processing_assets.py
zhz_rag_pipeline/pydantic_models_dagster.py
zhz_rag_pipeline/resources.py
zhz_rag_pipeline/获取当前目录下的所有py格式文件.py
zhz_rag_pipeline_dagster_project.egg-info/PKG-INFO
zhz_rag_pipeline_dagster_project.egg-info/SOURCES.txt
zhz_rag_pipeline_dagster_project.egg-info/dependency_links.txt
zhz_rag_pipeline_dagster_project.egg-info/requires.txt
zhz_rag_pipeline_dagster_project.egg-info/top_level.txt
    --- END OF FILE CONTENT ---

  File: dependency_links.txt
    --- START OF FILE CONTENT ---


    --- END OF FILE CONTENT ---

  File: requires.txt
    --- START OF FILE CONTENT ---
dagster
dagster-webserver

    --- END OF FILE CONTENT ---

  File: top_level.txt
    --- START OF FILE CONTENT ---
zhz_rag_pipeline

    --- END OF FILE CONTENT ---


Directory: zhz_rag/
  File: __init__.py
    --- File is empty ---

  File (skipped, extension not included): setup - 副本.py:Zone.Identifier
  File: setup.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/zhz_rag/setup.py
from setuptools import find_packages, setup

setup(
    name="zhz_rag_core",
    version="0.1.0",
    packages=find_packages(),
    install_requires=[
        # Pydantic 版本由主 requirements.txt 控制
        # LiteLLM 版本由主 requirements.txt 控制
        # ChromaDB 版本由主 requirements.txt 控制

        "protobuf>=4.25.0,<5.30.0", # 放宽 protobuf 上限，因为 pydantic 2.11.5 可能需要较新的
        "packaging>=23.2,<25.0",
        "rich>=13.7.0,<14.0.0",
        
        "fastapi>=0.110.0,<0.116.0", # 保持较新
        "starlette>=0.35.0,<0.47.0", # 保持较新

        "langchain-core>=0.1.50,<0.4.0", # 较新 langchain 可能更好兼容
        "langchain-text-splitters>=0.0.1,<0.3.0",

        "httpx>=0.27.0", # 使用较新 httpx
        "python-dotenv>=1.0.0",
        "neo4j>=5.0.0", # neo4j 驱动
        "sentence-transformers>=2.2.0", # sentence-transformers
        "transformers>=4.38.0,<4.39.0", # 固定您之前的版本或小幅更新
        "torch>=2.0.0",
        "numpy<2.0", # 保持 Numpy < 2.0
        "bm25s",
        "jieba",
        "uvicorn[standard]", # 添加 standard extras
        "pandas>=2.0.0",
        "sqlalchemy>=2.0.0",
        "databases[aiosqlite]>=0.9.0", # for async sqlite
        "apscheduler>=3.10.0",
        "pytz",
    ],
)
    --- END OF FILE CONTENT ---


Directory: zhz_rag/crewai_integration/
  File: __init__.py
    --- File is empty ---

  File: run_crew.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/run_agent.py

import os
import json
import datetime

from crewai import Agent, Task, Crew, Process

# --- 导入我们自己的项目模块 (使用绝对导入) ---
from zhz_rag.crewai_integration.tools import HybridRAGTool, BaseMCPTool
from zhz_rag.config.pydantic_models import QueryRequest # 用于 RAG 工具的输入
from zhz_rag.utils.common_utils import call_mcpo_tool
from zhz_rag.llm.custom_crewai_llms import CustomGeminiLLM

# --- 环境配置 ---
from dotenv import load_dotenv
load_dotenv()

# --- CrewAI 基类和事件系统 ---
from crewai.tools import BaseTool
from pydantic import BaseModel, Field
from typing import Any, Dict, List, Optional, Type
from crewai.llms.base_llm import BaseLLM as CrewAIBaseLLM
try:
    from crewai.utilities.events.base_event_listener import BaseEventListener as CrewAIBaseCallbackHandler
    from crewai.utilities.events import LLMCallStartedEvent, LLMCallCompletedEvent
    print("Successfully imported BaseEventListener and Event Types")
except ImportError:
    print("Failed to import BaseEventListener or Event Types, using dummy classes.")
    class CrewAIBaseCallbackHandler: pass
    class LLMCallStartedEvent: pass
    class LLMCallCompletedEvent: pass

# --- LiteLLM ---
import litellm

# --- 定义简单工具以供测试 ---
class SimpleToolInput(BaseModel):
    message: str = Field(description="A simple message string for the tool.")

class MySimpleTestTool(BaseTool):
    name: str = "MySimpleTestTool"
    description: str = "A very simple test tool that takes a message and returns it."
    args_schema: Type[BaseModel] = SimpleToolInput

    def _run(self, message: str) -> str:
        print(f"MySimpleTestTool received: {message}")
        return f"MySimpleTestTool processed: {message}"

# --- 配置 Agent 使用的 LLM 实例 ---
GEMINI_MODEL_NAME = "gemini/gemini-1.5-flash-latest"
GEMINI_API_KEY = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")

if not GEMINI_API_KEY:
    print("CRITICAL ERROR: GOOGLE_API_KEY or GEMINI_API_KEY not set.")
    exit(1)

# --- 定义详细的事件监听器 ---
class MyDetailedLogger(CrewAIBaseCallbackHandler):
    def __init__(self):
        super().__init__()
        print(f"[{datetime.datetime.now()}] MyDetailedLogger: 已初始化。")

    def setup_listeners(self, crewai_event_bus):
        print(f"[{datetime.datetime.now()}] MyDetailedLogger: 正在设置监听器...")

        @crewai_event_bus.on(LLMCallStartedEvent)
        def handle_llm_start(source, event: LLMCallStartedEvent):
            self.on_llm_start_logic(source, event)

        @crewai_event_bus.on(LLMCallCompletedEvent)
        def handle_llm_completed(source, event: LLMCallCompletedEvent):
            self.on_llm_end_logic(source, event)

        print(f"[{datetime.datetime.now()}] MyDetailedLogger: 监听器设置完成。")

    def on_llm_start_logic(self, source, event: LLMCallStartedEvent):
        print(f"\n>>>> LLM 调用开始 (Event Logic) <<<<")
        llm_inputs = getattr(event, 'llm_inputs', {})
        messages = llm_inputs.get('messages')
        tools = llm_inputs.get('tools')
        print(f"来源 (Source): {source}")
        if messages:
            print("消息 (来自 event.llm_inputs):")
            if isinstance(messages, list) and len(messages) > 0:
                first_message = messages[0]
                if isinstance(first_message, dict) and 'content' in first_message:
                    content_snippet = str(first_message.get('content', ''))[:300]
                    print(f"   Role: {first_message.get('role')}, Content Snippet: {content_snippet}...")
                else:
                     print(f"  First message (raw): {first_message}")
            else:
                 print(f"  Messages (raw): {messages}")
        else:
            print("消息 (来自 event.llm_inputs): 无")
        if tools:
            print("工具 (来自 event.llm_inputs):")
            try:
                print(f"  {json.dumps(tools, indent=2, ensure_ascii=False)}")
            except Exception as e:
                print(f"  无法序列化工具为 JSON: {e}. 工具: {tools}")
        else:
            print("工具 (来自 event.llm_inputs): 无")
        print("----------------------------------")

    def on_llm_end_logic(self, source, event: LLMCallCompletedEvent):
        print(f"\n>>>> LLM 调用结束 (Event Logic) <<<<")
        response = getattr(event, 'llm_output', None)
        print(f"来源 (Source): {source}")
        if response:
            if hasattr(response, 'choices') and response.choices:
                choice = response.choices[0]
                if hasattr(choice, 'message') and choice.message:
                    print(f"  消息内容: {choice.message.content}")
                    if hasattr(choice.message, 'tool_calls') and choice.message.tool_calls:
                        print(f"  工具调用: {choice.message.tool_calls}")
                    else:
                        print(f"  工具调用: 无")
            elif hasattr(response, 'content'):
                print(f"  响应内容: {response.content}")
            else:
                print(f"  LLM 响应 (来自 event.llm_output): {str(response)[:500]}...")
        else:
            print("  在 event.llm_output 中未找到响应对象。")
        print("----------------------------------")

# --- 实例化 CustomGeminiLLM ---
custom_llm_tool_config = {"function_calling_config": {"mode": "AUTO"}}
zhz_agent_tool = HybridRAGTool()
researcher_tools = [zhz_agent_tool]

llm_for_agent = CustomGeminiLLM(
    model=GEMINI_MODEL_NAME,
    api_key=GEMINI_API_KEY,
    temperature=0.1,
    max_tokens=2048,
    tool_config=custom_llm_tool_config,
    agent_tools=researcher_tools # 传递工具列表以供缓存
)
print(f"Custom Agent LLM configured: {GEMINI_MODEL_NAME} with custom tool_config")

# --- 设置 BaseMCPTool 的调用器 ---
BaseMCPTool.set_mcpo_caller(call_mcpo_tool)

# --- 定义 Agents ---
researcher_agent = Agent(
    role='信息检索专家',
    goal='准确地回答用户查询，并且只使用提供的工具。',
    backstory=(
        "你是一位高级AI助手，专注于信息检索。"
        "你的专长在于高效地利用工具来查找最相关和最精确的答案来回应用户的查询。"
    ),
    llm=llm_for_agent,
    tools=researcher_tools,
    verbose=True,
    allow_delegation=False,
)

writer_agent = Agent(
    role='报告撰写专家',
    goal='根据提供的信息，撰写清晰、结构良好且富有洞察力的报告。',
    backstory=(
        "您是一位资深的报告撰写专家，拥有出色的分析和写作能力。"
        "您擅长将复杂的信息提炼成易于理解的报告，并能根据不同的输出状态（答案、澄清、错误）"
        "灵活调整报告内容和格式。"
    ),
    llm=llm_for_agent,
    verbose=True,
    allow_delegation=False,
)

# --- 定义 Tasks (包含上下文传递修复) ---
research_task_description = """你收到了来自用户的以下查询：

'{query}'

你应该使用提供的 `HybridRAGQueryTool` 工具来处理这个查询。
如果这个工具需要 `top_k_vector`, `top_k_kg`, 或 `top_k_bm25` 这些参数，请使用以下建议值：
top_k_vector: 5, top_k_kg: 3, top_k_bm25: 5。
在使用完必要的工具后，你的最终输出应该是（使用中文）：'我的最终答案是：' 
后跟你使用的工具返回的精确、完整的JSON字符串输出内容。"""

research_task_expected_output = "短语 '我的最终答案是：' 后跟你使用的工具返回的精确、完整的JSON字符串输出内容。"

research_task = Task(
    description=research_task_description,
    expected_output=research_task_expected_output,
    agent=researcher_agent,
)

report_writing_task = Task(
    description="""根据【前一个任务】（信息检索专家）提供的RAG工具输出（它是一个JSON字符串），生成一份报告或响应。
请仔细分析这个JSON字符串输出，它应该包含一个 'status' 字段。
1. 如果 'status' 是 'success'，则提取 'final_answer' 字段的内容，并基于此答案撰写一份简洁的报告。
2. 如果 'status' 是 'clarification_needed'，则提取 'clarification_question' 字段的内容，并向用户明确指出需要澄清的问题，例如：'系统需要澄清：[澄清问题]'。
3. 如果 'status' 是 'error'，则提取 'error_message' (或 'error') 字段的内容，并向用户报告错误，例如：'RAG服务发生错误：[错误信息]'。
你的最终输出必须是清晰、专业且符合上述情况的报告或响应。""",
    expected_output="一份清晰的报告，或者一个明确的澄清请求，或者一个错误报告。",
    agent=writer_agent,
    context=[research_task],
)

# --- 实例化监听器 ---
my_event_logger = MyDetailedLogger()

# --- 定义 Crew (添加 event_listeners) ---
office_brain_crew = Crew(
    agents=[researcher_agent, writer_agent],
    tasks=[research_task, report_writing_task],
    process=Process.sequential,
    verbose=True,
    event_listeners=[my_event_logger] # <<< --- 激活事件监听器 ---
)

# --- 启动 Crew ---
if __name__ == "__main__":
    print("--- 启动智能助手终端大脑 Crew (使用 CustomGeminiLLM 和事件监听器) ---")
    user_query_input = "公司2024年第一季度在华东和华北的总销售额一共是多少？"
    # --- 修复：kickoff inputs 只包含 query ---
    inputs = {'query': user_query_input}
    result = office_brain_crew.kickoff(inputs=inputs)
    print("\n\n=== 最终报告 ===\n")
    if hasattr(result, 'raw'):
        print(result.raw)
    else:
        print(result)
    print("\n--- Crew 任务完成 ---")
    --- END OF FILE CONTENT ---

  File: tools.py
    --- START OF FILE CONTENT ---
# zhz_agent/custom_crewai_tools.py

import os
import json
import asyncio
import traceback
from typing import Type, List, Dict, Any, Optional, ClassVar
from pydantic import BaseModel, Field
from crewai.tools import BaseTool
import httpx

# 从 zhz_agent.pydantic_models 导入 QueryRequest
from zhz_rag.config.pydantic_models import QueryRequest # 用于 RAG 工具的输入

# MCPO 代理的基地址
MCPO_BASE_URL = os.getenv("MCPO_BASE_URL", "http://localhost:8006")

class BaseMCPTool(BaseTool):
    mcpo_base_url: str = MCPO_BASE_URL
    _call_mcpo_func: ClassVar[callable] = None

    @classmethod
    def set_mcpo_caller(cls, caller: callable):
        cls._call_mcpo_func = caller

    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    async def _call_mcpo_endpoint(self, service_and_tool_path: str, payload: dict) -> dict | str:
        api_url = f"{self.mcpo_base_url}/{service_and_tool_path}"
        cleaned_payload = {k: v for k, v in payload.items() if v is not None}
        print(f"BaseMCPTool: Calling mcpo endpoint: POST {api_url} with payload: {json.dumps(cleaned_payload, ensure_ascii=False)}")
        
        # --- [修改] 移除 proxies=None 参数 ---
        async with httpx.AsyncClient(trust_env=False) as client:
            response = None
            try:
                headers = {"Content-Type": "application/json"}
                response = await client.post(api_url, json=cleaned_payload, headers=headers, timeout=300.0)
                print(f"BaseMCPTool: mcpo status code for {service_and_tool_path}: {response.status_code}")
                print(f"BaseMCPTool: mcpo response headers for {service_and_tool_path}: {response.headers}") # <--- 新增日志
                # 尝试分块读取响应或提前获取少量内容进行日志记录，以防响应过大卡住 .text 或 .json()
                try:
                    response_text_snippet = await response.aread(num_bytes=1024) # 读取前1KB
                    print(f"BaseMCPTool: mcpo response text snippet (first 1KB) for {service_and_tool_path}: {response_text_snippet.decode(errors='ignore')}")
                except Exception as e_read:
                    print(f"BaseMCPTool: Error reading response snippet: {e_read}")

                if response.status_code == 200:
                    try:
                        # print(f"BaseMCPTool: mcpo raw response text for {service_and_tool_path}: {response.text}") # 如果怀疑内容问题，可以取消注释，但小心大响应
                        return response.json()
                    except json.JSONDecodeError:
                        print(f"BaseMCPTool Warning: mcpo returned status 200 but response is not JSON for '{service_and_tool_path}'. Returning raw text.")
                        return response.text
                else:
                    error_text = f"mcpo call to '{service_and_tool_path}' failed with status {response.status_code}. Response: {response.text[:500]}..."
                    print(f"BaseMCPTool Error: {error_text}")
                    return {"error": error_text, "status_code": response.status_code}
            except httpx.RequestError as exc:
                error_msg = f"BaseMCPTool HTTP RequestError calling mcpo tool '{service_and_tool_path}': {type(exc).__name__} - {exc}"
                print(f"BaseMCPTool Error: {error_msg}"); traceback.print_exc()
                return {"error": error_msg, "exception_type": type(exc).__name__}
            except Exception as exc:
                error_msg = f"BaseMCPTool Unexpected error calling mcpo tool '{service_and_tool_path}': {type(exc).__name__} - {exc}"
                response_text_snippet = response.text[:500] if response and hasattr(response, 'text') else "Response object not available or no text."
                print(f"BaseMCPTool Error: {error_msg}. Response snippet: {response_text_snippet}"); traceback.print_exc()
                return {"error": error_msg, "exception_type": type(exc).__name__}

    def _handle_tool_result(self, result: dict | str, tool_name_for_log: str) -> str:
        print(f"BaseMCPTool DEBUG: {tool_name_for_log} result from mcpo: {str(result)[:500]}...")
        parsed_result = result
        if isinstance(result, str):
            try:
                parsed_result = json.loads(result)
            except json.JSONDecodeError:
                if "error" in result.lower() or "failed" in result.lower() or "traceback" in result.lower():
                    return f"调用 {tool_name_for_log} 失败，返回非JSON错误文本: {result}"
                print(f"BaseMCPTool Info: Result for {tool_name_for_log} is a non-JSON string, returning as is.")
                return result
        if isinstance(parsed_result, dict):
            if "error" in parsed_result and "status_code" in parsed_result:
                return f"调用 {tool_name_for_log} 时发生HTTP错误：{parsed_result.get('error')}"
            if parsed_result.get("status") == "error":
                error_msg = parsed_result.get("error_message", "未知错误")
                error_code = parsed_result.get("error_code", "NO_CODE")
                return f"工具 {tool_name_for_log} 执行失败 (错误码: {error_code})：{error_msg}"
            try:
                return json.dumps(parsed_result, ensure_ascii=False, indent=2)
            except Exception as e:
                print(f"BaseMCPTool Error formatting successful dict result for {tool_name_for_log}: {e}")
                return str(parsed_result)
        print(f"BaseMCPTool Warning: Unexpected result format from {tool_name_for_log} mcpo call: {type(result)}, content: {str(result)[:200]}")
        return f"从 {tool_name_for_log} 服务收到的结果格式不正确或无法处理: {str(result)[:500]}"

    def _run_default_sync_wrapper(self, **kwargs) -> str:
        tool_name = getattr(self, 'name', self.__class__.__name__)
        print(f"BaseMCPTool INFO: Synchronous _run called for {tool_name} with args: {kwargs}.")
        result_str = ""
        try:
            # --- 改进的 asyncio.run 处理 ---
            try:
                loop = asyncio.get_running_loop()
            except RuntimeError:
                loop = None

            async def async_runner():
                return await self._arun(**kwargs)

            if loop and loop.is_running():
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                    future = executor.submit(asyncio.run, async_runner())
                    result = future.result(timeout=120)
            else:
                result = asyncio.run(async_runner())
            result_str = str(result)
        except asyncio.TimeoutError:
            error_message = f"Tool {tool_name} execution timed out after 120 seconds."
            print(f"BaseMCPTool ERROR_TRACE: {error_message}"); result_str = error_message
        except RuntimeError as e:
            if "cannot run event loop while another loop is running" in str(e).lower() or "event loop is already running" in str(e).lower():
                error_message = (f"BaseMCPTool Error in {tool_name} _run: Nested asyncio event loop conflict. Original error: {e}")
            else:
                error_message = f"BaseMCPTool RuntimeError in {tool_name} _run: {type(e).__name__} - {e}"
            print(f"BaseMCPTool ERROR_TRACE: {error_message}"); traceback.print_exc();
            result_str = error_message
        except Exception as e:
            error_message = f"BaseMCPTool General Exception in {tool_name} _run: {type(e).__name__} - {e}"
            print(f"BaseMCPTool ERROR_TRACE: {error_message}"); traceback.print_exc(); result_str = error_message
        return result_str

class HybridRAGTool(BaseMCPTool):
    name: str = "HybridRAGQueryTool"
    description: str = (
        "【核心RAG工具】用于通过执行混合检索增强生成 (RAG) 搜索来回答用户问题。 "
        "该工具整合了向量检索、知识图谱检索和关键词检索，并进行智能融合和重排序。 "
        "当用户需要从知识库中获取信息、回答复杂问题或生成报告时，应调用此工具。"
    )
    args_schema: Type[BaseModel] = QueryRequest
    target_mcp_service_path: str = "zhz_agent_rag_service/query_rag_v2"

    async def _arun(self, query: str, top_k_vector: int, top_k_kg: int, top_k_bm25: int, **kwargs: Any) -> str: # --- 确认有 **kwargs ---
        tool_name_for_log = getattr(self, 'name', "HybridRAGTool")
        print(f"CrewAI Tool DEBUG: {tool_name_for_log}._arun called with query='{query}', top_k_vector={top_k_vector}, top_k_kg={top_k_kg}, top_k_bm25={top_k_bm25}, additional_kwargs={kwargs}")

        security_context = kwargs.get('security_context')
        if security_context:
            print(f"CrewAI Tool INFO: Received security_context (in HybridRAGTool): {str(security_context)[:200]}...")

        payload = {
            "query": query,
            "top_k_vector": top_k_vector,
            "top_k_kg": top_k_kg,
            "top_k_bm25": top_k_bm25
        }
        result = await self._call_mcpo_endpoint(self.target_mcp_service_path, payload)
        return self._handle_tool_result(result, self.name)

    def _run(self, query: str, top_k_vector: int, top_k_kg: int, top_k_bm25: int, **kwargs: Any) -> str: # --- 确认有 **kwargs ---
        return self._run_default_sync_wrapper(query=query, top_k_vector=top_k_vector, top_k_kg=top_k_kg, top_k_bm25=top_k_bm25, **kwargs)
    --- END OF FILE CONTENT ---


Directory: zhz_rag/config/
  File: constants.py
    --- START OF FILE CONTENT ---
# zhz_agent/zhz_rag/config/constants.py

NEW_KG_SCHEMA_DESCRIPTION = """
# 知识图谱结构 (KuzuDB) 与 Cypher 查询生成规则

## 1. 节点定义:
- 节点标签: `:ExtractedEntity` (你必须且只能使用此节点标签)
- 节点属性:
    - `id_prop`: STRING (主键，实体的唯一标识)
    - `text`: STRING (实体的名称或文本内容)
    - `label`: STRING (实体类型。允许的值: "PERSON", "ORGANIZATION", "TASK")

## 2. 关系定义:
- 关系类型: `:WorksAt`
    - 结构: `(:ExtractedEntity {label:"PERSON"}) -[:WorksAt]-> (:ExtractedEntity {label:"ORGANIZATION"})`
    - 含义: 一个人 (PERSON) 在一个组织 (ORGANIZATION) 工作。
- 关系类型: `:AssignedTo`
    - 结构: `(:ExtractedEntity {label:"TASK"}) -[:AssignedTo]-> (:ExtractedEntity {label:"PERSON"})`
    - 含义: 一个任务 (TASK) 被分配给一个人 (PERSON)。

## 3. Cypher 查询生成 - 输出为 JSON 对象:

    你的【完整且唯一】的回答，必须是一个包含 "status" 和 "query" 字段的JSON对象。
    - 如果你能根据用户问题和Schema生成一个有效的Cypher查询：
        - "status" 字段应为 "success"。
        - "query" 字段应为该Cypher查询字符串。
    - 如果你无法生成有效的Cypher查询：
        - "status" 字段应为 "unable_to_generate"。
        - "query" 字段应为 "无法生成Cypher查询."。
    【不要在JSON之外或query字段内（当status为success时）包含任何解释或额外文本。】

## 4. JSON 输出格式示例:

### 示例 1 (能够生成查询):
用户问题: "任务'FixBug123'分配给了谁？"
你的【完整且唯一】的 JSON 回答:
```json
{
  "status": "success",
  "query": "MATCH (t:ExtractedEntity {text: 'FixBug123', label: 'TASK'})-[:AssignedTo]->(p:ExtractedEntity {label: 'PERSON'}) RETURN p.text AS Assignee"
}
示例 2 (无法根据Schema回答):
用户问题: "法国的首都是哪里？"
你的【完整且唯一】的 JSON 回答:
{
  "status": "unable_to_generate",
  "query": "无法生成Cypher查询."
}
"""
    --- END OF FILE CONTENT ---

  File: pydantic_models.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/zhz_rag/config/pydantic_models.py
from pydantic import BaseModel, Field, root_validator
from typing import List, Dict, Any, Optional
from enum import Enum
from datetime import datetime
import uuid

# --- RAG Models ---
class QueryRequest(BaseModel):
    query: str = Field(description="用户提出的原始查询文本。", json_schema_extra=lambda schema: schema.pop('default', None) if schema.get('default') is None else None)
    top_k_vector: int = Field(description="期望检索的向量搜索结果数量。", json_schema_extra=lambda schema: schema.pop('default', None) if schema.get('default') is None else None)
    top_k_kg: int = Field(description="期望检索的知识图谱结果数量。", json_schema_extra=lambda schema: schema.pop('default', None) if schema.get('default') is None else None)
    top_k_bm25: int = Field(description="期望检索的 BM25 关键词搜索结果数量。", json_schema_extra=lambda schema: schema.pop('default', None) if schema.get('default') is None else None)

    @root_validator(pre=True)
    @classmethod
    def remove_internal_params(cls, values: Dict[str, Any]) -> Dict[str, Any]:
        if isinstance(values, dict):
            values.pop('security_context', None)
            values.pop('agent_fingerprint', None)
        return values

    class Config:
        extra = 'forbid'

class RetrievedDocument(BaseModel):
    source_type: str
    content: str
    score: Optional[float] = Field(default=None, json_schema_extra=lambda schema: schema.pop('default', None) if schema.get('default') is None else None)
    metadata: Optional[Dict[str, Any]] = Field(default=None, json_schema_extra=lambda schema: schema.pop('default', None) if schema.get('default') is None else None)

class HybridRAGResponse(BaseModel):
    original_query: str
    answer: str
    retrieved_sources: List[RetrievedDocument]
    debug_info: Optional[Dict[str, Any]] = Field(default=None, json_schema_extra=lambda schema: schema.pop('default', None) if schema.get('default') is None else None)


# --- Task Management Models ---
class TaskStatus(str, Enum):
    PENDING = "pending"
    ACTIVE = "active"
    COMPLETED = "completed"
    CANCELLED = "cancelled"
    FAILED = "failed"
    REMINDING = "reminding"

class ReminderMethod(str, Enum):
    NOTIFICATION = "notification"

class TaskModel(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid.uuid4()), description="任务的唯一ID (自动生成)")
    title: str = Field(description="任务标题")
    description: Optional[str] = Field(None, description="任务的详细描述")
    status: TaskStatus = Field(default=TaskStatus.PENDING, description="任务当前状态")
    created_at: datetime = Field(default_factory=datetime.utcnow, description="任务创建时间 (UTC)")
    updated_at: datetime = Field(default_factory=datetime.utcnow, description="任务最后更新时间 (UTC)")
    due_date: Optional[datetime] = Field(None, description="任务截止日期或计划执行时间 (UTC)")
    reminder_time: Optional[datetime] = Field(None, description="任务提醒时间 (UTC)")
    reminder_offset_minutes: Optional[int] = Field(None, description="提醒时间相对于due_date的提前分钟数 (例如10分钟前)")
    reminder_methods: List[ReminderMethod] = Field(default=[ReminderMethod.NOTIFICATION], description="提醒方式列表")
    priority: int = Field(default=0, description="任务优先级 (例如 0:普通, 1:重要, 2:紧急)")
    tags: List[str] = Field(default_factory=list, description="任务标签")
    action_type: Optional[str] = Field(None, description="任务到期时需要执行的动作类型 (例如 'navigate', 'send_message', 'run_report')")
    action_payload: Dict[str, Any] = Field(default_factory=dict, description="执行动作时需要的参数 (例如导航的目的地)")
    execution_result: Optional[str] = Field(None, description="任务执行后的结果或错误信息")
    last_executed_at: Optional[datetime] = Field(None, description="上次执行时间 (UTC)")

    class Config:
        use_enum_values = True
        # Pydantic V2 uses model_config, or just rely on default behavior if orm_mode was for SQLAlchemy V1 style
        # For Pydantic V2, if you need ORM mode (e.g. for SQLAlchemy integration):
        # model_config = {"from_attributes": True} 
        # However, if you are not directly using it with an ORM that way, orm_mode=True might be a V1 relic.
        # For now, I will keep it as orm_mode = True as per your original file,
        # but be aware it might need adjustment if you are on Pydantic V2 and using its ORM features.
        orm_mode = True


class CreateTaskRequest(BaseModel):
    title: str
    description: Optional[str] = None
    due_date: Optional[datetime] = None
    reminder_offset_minutes: Optional[int] = None
    reminder_methods: Optional[List[ReminderMethod]] = [ReminderMethod.NOTIFICATION]
    priority: Optional[int] = 0
    tags: Optional[List[str]] = None
    action_type: Optional[str] = None
    action_payload: Optional[Dict[str, Any]] = None
    
    class Config:
        extra = 'forbid'

class UpdateTaskRequest(BaseModel):
    title: Optional[str] = None
    description: Optional[str] = None
    status: Optional[TaskStatus] = None
    due_date: Optional[datetime] = None
    reminder_offset_minutes: Optional[int] = None
    reminder_methods: Optional[List[ReminderMethod]] = None
    priority: Optional[int] = None
    tags: Optional[List[str]] = None
    action_type: Optional[str] = None
    action_payload: Optional[Dict[str, Any]] = None

    class Config:
        extra = 'forbid'


class IdentifiedEntity(BaseModel):
    text: str = Field(description="识别出的实体文本。")
    label: Optional[str] = Field(None, description="推断的实体类型 (例如 PERSON, ORGANIZATION, TASK)。")

class ExtractedEntitiesAndRelationIntent(BaseModel):
    entities: List[IdentifiedEntity] = Field(default_factory=list, description="从用户查询中识别出的核心实体列表（通常1-2个）。")
    relation_hint: Optional[str] = Field(None, description="如果用户查询暗示了实体间的特定关系，这里是关系的文本描述或关键词（例如 “工作于”, “负责”, “销售额”）。")
    --- END OF FILE CONTENT ---


Directory: zhz_rag/zhz_rag_core.egg-info/
  File (skipped, extension not included): PKG-INFO
  File: SOURCES.txt
    --- START OF FILE CONTENT ---
setup.py
api/__init__.py
api/main_api.py
api/rag_mcp_service.py
api/task_manager_api.py
core_rag/__init__.py
core_rag/fusion_engine.py
core_rag/kg_retriever.py
core_rag/retrievers/__init__.py
core_rag/retrievers/chromadb_retriever.py
core_rag/retrievers/file_bm25_retriever.py
crewai_integration/__init__.py
crewai_integration/run_crew.py
crewai_integration/tools.py
evaluation/__init__.py
evaluation/analyze_answer.py
evaluation/analyze_cypher.py
evaluation/batch_eval_answer.py
evaluation/batch_eval_cypher.py
evaluation/evaluator.py
finetuning/__init__.py
finetuning/refine_answer_data.py
finetuning/refine_cypher_data.py
llm/__init__.py
llm/custom_crewai_llms.py
llm/sglang_wrapper.py
stored_data/__init__.py
task_management/__init__.py
task_management/db_models.py
task_management/jobs.py
zhz_rag_core.egg-info/PKG-INFO
zhz_rag_core.egg-info/SOURCES.txt
zhz_rag_core.egg-info/dependency_links.txt
zhz_rag_core.egg-info/requires.txt
zhz_rag_core.egg-info/top_level.txt
    --- END OF FILE CONTENT ---

  File: dependency_links.txt
    --- START OF FILE CONTENT ---


    --- END OF FILE CONTENT ---

  File: requires.txt
    --- START OF FILE CONTENT ---
protobuf<5.30.0,>=4.25.0
packaging<25.0,>=23.2
rich<14.0.0,>=13.7.0
fastapi<0.116.0,>=0.110.0
starlette<0.47.0,>=0.35.0
langchain-core<0.4.0,>=0.1.50
langchain-text-splitters<0.3.0,>=0.0.1
httpx>=0.27.0
python-dotenv>=1.0.0
neo4j>=5.0.0
sentence-transformers>=2.2.0
transformers<4.39.0,>=4.38.0
torch>=2.0.0
numpy<2.0
bm25s
jieba
uvicorn[standard]
pandas>=2.0.0
sqlalchemy>=2.0.0
databases[aiosqlite]>=0.9.0
apscheduler>=3.10.0
pytz

    --- END OF FILE CONTENT ---

  File: top_level.txt
    --- START OF FILE CONTENT ---
api
core_rag
crewai_integration
evaluation
finetuning
llm
stored_data
task_management

    --- END OF FILE CONTENT ---


Directory: zhz_rag/utils/
  File (skipped, extension not included): ZHZ_AGENT_tasks.db
  File: common_utils.py
    --- START OF FILE CONTENT ---
# zhz_rag/utils/common_utils.py

import httpx
import json
import traceback
import os
import glob
from dotenv import load_dotenv
from datetime import datetime, timezone
import uuid
import logging
import asyncio #确保 asyncio 被导入
from typing import List, Dict, Any, Optional
import re
import unicodedata
import logging

load_dotenv()

# --- Logger Configuration ---
utils_logger = logging.getLogger("UtilsLogger")
utils_logger.setLevel(logging.INFO)
if not utils_logger.hasHandlers():
    _utils_console_handler = logging.StreamHandler()
    _utils_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
    _utils_console_handler.setFormatter(_utils_formatter)
    utils_logger.addHandler(_utils_console_handler)
    utils_logger.propagate = False
utils_logger.info("--- UtilsLogger configured ---")

# --- MCP Configuration ---
MCPO_BASE_URL = os.getenv("MCPO_BASE_URL", "http://localhost:8006")

_CURRENT_FILE_DIR = os.path.dirname(os.path.abspath(__file__))
_ZHZ_RAG_PACKAGE_DIR = os.path.dirname(_CURRENT_FILE_DIR)

STORED_DATA_ROOT_DIR = os.path.join(_ZHZ_RAG_PACKAGE_DIR, 'stored_data')

RAG_INTERACTION_LOGS_DIR = os.path.join(STORED_DATA_ROOT_DIR, 'rag_interaction_logs')
EVALUATION_RESULTS_LOGS_DIR = os.path.join(STORED_DATA_ROOT_DIR, 'evaluation_results_logs')

FINETUNING_GENERATED_DATA_DIR = os.path.join(_ZHZ_RAG_PACKAGE_DIR, 'finetuning', 'generated_data')

# Ensure these directories exist
_DIRECTORIES_TO_CREATE = [
    STORED_DATA_ROOT_DIR,
    RAG_INTERACTION_LOGS_DIR,
    EVALUATION_RESULTS_LOGS_DIR,
    FINETUNING_GENERATED_DATA_DIR
]
for dir_path in _DIRECTORIES_TO_CREATE:
    if not os.path.exists(dir_path):
        try:
            os.makedirs(dir_path, exist_ok=True)
            utils_logger.info(f"Successfully created directory: {dir_path}")
        except Exception as e:
            utils_logger.error(f"Error creating directory {dir_path}: {e}. Consider creating it manually.")

# --- Log File Path Getters ---

def get_interaction_log_filepath() -> str:
    """Gets the full path for the current RAG interaction log file (daily rotation)."""
    today_str = datetime.now(timezone.utc).strftime("%Y%m%d")
    return os.path.join(RAG_INTERACTION_LOGS_DIR, f"rag_interactions_{today_str}.jsonl")

def get_evaluation_result_log_filepath(evaluation_name: str) -> str:
    """Gets the full path for an evaluation result log file (daily rotation, by evaluation name)."""
    today_str = datetime.now(timezone.utc).strftime("%Y%m%d")
    return os.path.join(EVALUATION_RESULTS_LOGS_DIR, f"eval_results_{evaluation_name}_{today_str}.jsonl")

def find_latest_rag_interaction_log(log_dir: str = RAG_INTERACTION_LOGS_DIR) -> Optional[str]:
    """
    Finds the latest RAG interaction log file (rag_interactions_*.jsonl) in the specified directory.
    Defaults to RAG_INTERACTION_LOGS_DIR.
    """
    utils_logger.debug(f"Searching for RAG interaction logs in: {log_dir}")
    rag_log_pattern = os.path.join(log_dir, "rag_interactions_*.jsonl")
    candidate_rag_logs = glob.glob(rag_log_pattern)

    if candidate_rag_logs:
        candidate_rag_logs.sort(key=os.path.getmtime, reverse=True)
        utils_logger.info(f"Automatically selected RAG interaction log: {candidate_rag_logs[0]}")
        return candidate_rag_logs[0]
    else:
        utils_logger.warning(f"No RAG interaction log files found matching pattern: {rag_log_pattern} in directory {log_dir}")
        return None

# --- Logging Function ---

async def log_interaction_data(
    interaction_data: Dict[str, Any],
    is_evaluation_result: bool = False,
    evaluation_name_for_file: Optional[str] = None
):
    """
    Asynchronously appends a single interaction data or evaluation result to a JSONL file.
    """
    if is_evaluation_result:
        if not evaluation_name_for_file:
            evaluation_name_for_file = interaction_data.get("task_type", "general_eval_result") # More specific default
        filepath = get_evaluation_result_log_filepath(evaluation_name=evaluation_name_for_file)
    else:
        filepath = get_interaction_log_filepath()

    if "timestamp_utc" not in interaction_data:
        interaction_data["timestamp_utc"] = datetime.now(timezone.utc).isoformat()
    if "interaction_id" not in interaction_data and not is_evaluation_result: # Eval results use original_interaction_id_ref
        interaction_data["interaction_id"] = str(uuid.uuid4())
    elif "interaction_id" not in interaction_data and is_evaluation_result and "original_interaction_id_ref" in interaction_data:
        # For eval results, ensure there's an ID, can be a new one for the eval log entry itself
         interaction_data["interaction_id"] = str(uuid.uuid4())

    try:
        data_to_log = interaction_data.copy()
        if 'llm_input_messages' in data_to_log and isinstance(data_to_log['llm_input_messages'], list):
            processed_messages = []
            for msg in data_to_log['llm_input_messages']:
                if isinstance(msg, dict) and msg.get('role') == 'system':
                    processed_msg = msg.copy() # 复制消息字典
                    original_content = processed_msg.get('content', '')
                    processed_msg['content'] = f"System Prompt (length: {len(original_content)}, starts with: {original_content[:70]}...)"
                    processed_messages.append(processed_msg)
                else:
                    processed_messages.append(msg)
            data_to_log['llm_input_messages'] = processed_messages # 用处理过的消息列表替换原来的
        if 'llm_input_original_prompt_if_string' in data_to_log and isinstance(data_to_log['llm_input_original_prompt_if_string'], str):
            original_prompt_str = data_to_log['llm_input_original_prompt_if_string']
            if len(original_prompt_str) > 500: # 如果原始prompt字符串太长
                data_to_log['llm_input_original_prompt_if_string'] = f"Original Prompt String (length: {len(original_prompt_str)}, starts with: {original_prompt_str[:200]}...)"
        # --- 添加结束 ---

        def _write_sync():
            log_file_dir = os.path.dirname(filepath)
            if not os.path.exists(log_file_dir):
                try:
                    os.makedirs(log_file_dir, exist_ok=True)
                    # utils_logger.info(f"Created directory for log file: {log_file_dir}") # 使用print替代，避免日志级别问题
                    print(f"COMMON_UTILS_LOG_DATA: Created directory for log file: {log_file_dir}")
                except Exception as e_mkdir:
                    # utils_logger.error(f"Error creating directory {log_file_dir} for log file: {e_mkdir}")
                    print(f"COMMON_UTILS_LOG_DATA: Error creating directory {log_file_dir} for log file: {e_mkdir}")
                    return 

            with open(filepath, 'a', encoding='utf-8') as f:
                json_string_to_write = json.dumps(data_to_log, ensure_ascii=False, default=str)
                f.write(json_string_to_write + "\n") 
        await asyncio.to_thread(_write_sync)
    except Exception as e:
        utils_logger.error(f"Failed to log interaction data to {filepath}: {e}", exc_info=True)

# --- MCP Tool Calling Utility ---

async def call_mcpo_tool(tool_name_with_prefix: str, payload: Dict[str, Any]) -> Dict[str, Any]:
    """
    异步调用MCP工具服务，并返回结构化的成功或错误响应。
    """
    api_url = f"{MCPO_BASE_URL}/{tool_name_with_prefix}"
    cleaned_payload = {k: v for k, v in (payload or {}).items() if v is not None}

    utils_logger.info(f"CALL_MCPO_TOOL: Attempting to call {api_url}") # 使用 utils_logger
    utils_logger.debug(f"CALL_MCPO_TOOL: Payload: {json.dumps(cleaned_payload, ensure_ascii=False)}") # 使用 utils_logger

    timeout_config = httpx.Timeout(120.0, connect=10.0, read=120.0, write=10.0) 
    
    async with httpx.AsyncClient(timeout=timeout_config) as client:
        response: Optional[httpx.Response] = None 
        try:
            headers = {
                "Content-Type": "application/json",
                "Accept": "application/json",
                "User-Agent": "ZhzAgent/1.0 (call_mcpo_tool)"
            }
            utils_logger.debug(f"CALL_MCPO_TOOL: Sending POST request to {api_url} with headers: {headers}") # 使用 utils_logger
            response = await client.post(api_url, json=cleaned_payload, headers=headers)
            
            utils_logger.info(f"CALL_MCPO_TOOL: Response from {api_url} - Status: {response.status_code}") # 使用 utils_logger
            utils_logger.debug(f"CALL_MCPO_TOOL: Response Headers: {response.headers}") # 使用 utils_logger
            
            try:
                response_text_snippet = response.text[:500] 
                utils_logger.debug(f"CALL_MCPO_TOOL: Response Text Snippet (first 500 chars): {response_text_snippet}") # 使用 utils_logger
            except Exception as e_read_snippet:
                utils_logger.warning(f"CALL_MCPO_TOOL: Could not read response text snippet: {e_read_snippet}") # 使用 utils_logger

            response.raise_for_status() 

            try:
                result_data = response.json()
                utils_logger.info(f"CALL_MCPO_TOOL: Successfully received and parsed JSON response from {api_url}.") # 使用 utils_logger
                if isinstance(result_data, dict) and result_data.get("isError"):
                    error_content_list = result_data.get("content", [{"type": "text", "text": "Unknown error from MCP tool"}])
                    error_text_from_mcp_payload = "Unknown error from MCP tool"
                    for item in error_content_list:
                        if item.get("type") == "text":
                            error_text_from_mcp_payload = item.get("text", error_text_from_mcp_payload)
                            break
                    utils_logger.error(f"CALL_MCPO_TOOL: MCP Tool '{tool_name_with_prefix}' reported an application-level error (isError=true): {error_text_from_mcp_payload}") # 使用 utils_logger
                    return {
                        "success": False,
                        "error": f"MCP tool '{tool_name_with_prefix}' reported failure: {error_text_from_mcp_payload}",
                        "error_type": "MCP_APPLICATION_ERROR",
                        "status_code": response.status_code
                    }
                return {
                        "success": True, 
                        "data": result_data 
                }
            except json.JSONDecodeError as e_json_decode:
                utils_logger.error(f"CALL_MCPO_TOOL: Response from {api_url} was 2xx but not valid JSON. Error: {e_json_decode}", exc_info=True) # 使用 utils_logger
                return {
                    "success": False,
                    "error": "MCP service returned a 2xx status but the response was not valid JSON.",
                    "error_type": "JSON_DECODE_ERROR",
                    "status_code": response.status_code,
                    "raw_response_snippet": response.text[:500] if response else "N/A"
                }

        except httpx.HTTPStatusError as exc_http_status:
            error_message = f"HTTP Error {exc_http_status.response.status_code} when calling {api_url}."
            utils_logger.error(f"CALL_MCPO_TOOL: {error_message} Response: {exc_http_status.response.text[:500]}", exc_info=True) # 使用 utils_logger
            error_detail_from_response = exc_http_status.response.text
            try:
                parsed_error_json = exc_http_status.response.json()
                if isinstance(parsed_error_json, dict) and "detail" in parsed_error_json:
                    error_detail_from_response = parsed_error_json["detail"]
                elif isinstance(parsed_error_json, dict) and "error" in parsed_error_json: 
                    error_detail_from_response = parsed_error_json["error"]
            except json.JSONDecodeError:
                pass 
            return {
                "success": False,
                "error": f"HTTP error from MCP service: {error_detail_from_response}",
                "error_type": "HTTP_STATUS_ERROR",
                "status_code": exc_http_status.response.status_code,
                "raw_response_snippet": exc_http_status.response.text[:500] if exc_http_status.response else "N/A"
            }
        except httpx.TimeoutException as exc_timeout:
            utils_logger.error(f"CALL_MCPO_TOOL: Timeout when calling {api_url}. Error: {exc_timeout}", exc_info=True) # 使用 utils_logger
            return {
                "success": False,
                "error": f"Request to MCP service timed out after {timeout_config.read if timeout_config else 'default'}s.",
                "error_type": "TIMEOUT_ERROR",
                "status_code": None 
            }
        except httpx.ConnectError as exc_connect:
            utils_logger.error(f"CALL_MCPO_TOOL: Connection error when calling {api_url}. Is the MCP service running at {MCPO_BASE_URL}? Error: {exc_connect}", exc_info=True) # 使用 utils_logger
            return {
                "success": False,
                "error": f"Could not connect to MCP service at {MCPO_BASE_URL}.",
                "error_type": "CONNECTION_ERROR",
                "status_code": None
            }
        except httpx.RequestError as exc_request_other: 
            utils_logger.error(f"CALL_MCPO_TOOL: Network request error when calling {api_url}. Error: {exc_request_other}", exc_info=True) # 使用 utils_logger
            return {
                "success": False,
                "error": f"A network request error occurred: {str(exc_request_other)}",
                "error_type": type(exc_request_other).__name__,
                "status_code": None
            }
        except Exception as exc_unexpected:
            utils_logger.error(f"CALL_MCPO_TOOL: Unexpected error when calling {api_url}. Error: {exc_unexpected}", exc_info=True) # 使用 utils_logger
            return {
                "success": False,
                "error": f"An unexpected error occurred during MCP call: {str(exc_unexpected)}",
                "error_type": type(exc_unexpected).__name__,
                "status_code": response.status_code if response else None, 
                "traceback": traceback.format_exc() 
            }

def load_jsonl_file(filepath: str, encoding: str = 'utf-8') -> List[Dict[str, Any]]:
    """
    从 JSONL 文件加载数据。

    Args:
        filepath (str): JSONL 文件的路径。
        encoding (str): 文件编码，默认为 'utf-8'。

    Returns:
        List[Dict[str, Any]]: 从文件中加载的字典列表。如果文件不存在或解析出错，
                              会记录错误并返回空列表。
    """
    data_list: List[Dict[str, Any]] = []
    if not os.path.exists(filepath):
        utils_logger.error(f"File not found: {filepath}") # 使用已有的 utils_logger
        return data_list

    try:
        with open(filepath, 'r', encoding=encoding) as f:
            for line_number, line in enumerate(f, 1):
                try:
                    if line.strip(): # 确保行不是空的
                        data_list.append(json.loads(line.strip()))
                except json.JSONDecodeError as e_json:
                    utils_logger.warning(f"Skipping malformed JSON line {line_number} in {filepath}: {e_json}")
                except Exception as e_line:
                    utils_logger.warning(f"Error processing line {line_number} in {filepath}: {e_line}")
    except FileNotFoundError: # 再次捕获以防万一，虽然上面已经检查了
        utils_logger.error(f"File not found during open: {filepath}")
    except Exception as e_file:
        utils_logger.error(f"Error reading or processing file {filepath}: {e_file}", exc_info=True)
        return [] # 如果文件读取层面发生严重错误，返回空列表

    utils_logger.info(f"Successfully loaded {len(data_list)} entries from {filepath}")
    return data_list


def normalize_text_for_id(text: str) -> str:
    if not isinstance(text, str):
        return str(text) 
    
    try:
        normalized_text = unicodedata.normalize('NFKD', text)
        normalized_text = normalized_text.lower()
        normalized_text = normalized_text.strip()
        normalized_text = re.sub(r'\s+', ' ', normalized_text)
        return normalized_text
    except Exception as e:
        return text
    --- END OF FILE CONTENT ---

  File: db_utils.py
    --- START OF FILE CONTENT ---
# ZHZ_AGENT/database.py
import os
from databases import Database
from sqlalchemy import create_engine
from sqlalchemy.orm import declarative_base
from typing import Optional

# --- APScheduler 相关导入 ---
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.jobstores.sqlalchemy import SQLAlchemyJobStore
# --- [修改] 明确导入并使用 pytz ---
import pytz #

# --- 数据库配置 ---
ZHZ_AGENT_DIR = os.path.dirname(os.path.abspath(__file__))
DB_NAME = "ZHZ_AGENT_tasks.db"
DATABASE_FILE_PATH = os.path.join(ZHZ_AGENT_DIR, DB_NAME)
DATABASE_URL = f"sqlite+aiosqlite:///{DATABASE_FILE_PATH}"

database = Database(DATABASE_URL)
sqlalchemy_engine = create_engine(DATABASE_URL.replace("+aiosqlite", ""))
Base = declarative_base() #

# --- 全局调度器实例定义 ---
scheduler: Optional[AsyncIOScheduler] = None

def get_scheduler() -> AsyncIOScheduler:
    """获取或创建调度器实例，并配置作业存储和 UTC 时区。"""
    global scheduler
    if scheduler is None:
        jobstore_url = f"sqlite:///{DATABASE_FILE_PATH}"
        jobstores = {
            'default': SQLAlchemyJobStore(url=jobstore_url, tablename='apscheduler_jobs_v2') #
        }
        # --- [修复] 明确使用 pytz.utc 设置时区 ---
        scheduler = AsyncIOScheduler(
            jobstores=jobstores,
            timezone=pytz.utc # <--- 强制使用 pytz.utc #
        )
        import logging
        logging.getLogger('apscheduler').setLevel(logging.DEBUG)
        print(f"APScheduler initialized with timezone: {pytz.utc}") # 确认使用 pytz.utc #
    return scheduler #
    --- END OF FILE CONTENT ---

  File: load_neo4j_data.py
    --- START OF FILE CONTENT ---
# zhz_agent/load_neo4j_data.py
import json
import os
from neo4j import GraphDatabase, basic_auth
from dotenv import load_dotenv
import traceback

load_dotenv() # 确保加载 .env 文件中的NEO4J凭证

NEO4J_URI = os.getenv("NEO4J_URI", "bolt://localhost:7687")
NEO4J_USER = os.getenv("NEO4J_USER", "neo4j")
NEO4J_PASSWORD = os.getenv("NEO4J_PASSWORD") # 您需要确保这个密码是正确的

DATA_PATH = os.path.join(os.path.dirname(__file__), "data")
SAMPLE_KG_PATH = os.path.join(DATA_PATH, "sample_kg.json")

def clear_database(driver):
    """清除数据库中的所有节点和关系"""
    with driver.session(database="neo4j") as session:
        session.run("MATCH (n) DETACH DELETE n")
        print("Cleared all nodes and relationships from the database.")

def create_constraints(driver):
    """创建一些基本约束，确保节点属性的唯一性（如果适用）"""
    with driver.session(database="neo4j") as session:
        try:
            session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (p:Person) REQUIRE p.name IS UNIQUE")
            session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (pr:Project) REQUIRE pr.name IS UNIQUE")
            session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (r:Region) REQUIRE r.name IS UNIQUE")
            session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (prod:Product) REQUIRE prod.name IS UNIQUE")
            session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (d:Document) REQUIRE d.id IS UNIQUE")
            # SalesAmount 通常不需要唯一约束，因为它可能重复（例如不同区域同一时期的销售）
            print("Ensured constraints are created (or already exist).")
        except Exception as e:
            print(f"Error creating constraints: {e}")


def load_data(driver, kg_data):
    """根据kg_data中的facts加载数据到Neo4j"""
    facts = kg_data.get("facts", [])
    
    with driver.session(database="neo4j") as session:
        entities_to_create = set()
        node_types_from_schema = { # 定义了主要实体的标签和它们的主要标识属性
            "Person": "name", "Project": "name", "Region": "name", 
            "Product": "name", "Document": "id", "Idea": "name" # 新增Idea类型
        }

        for fact in facts:
            subject_name = fact.get("subject")
            object_name = fact.get("object")
            fact_type = fact.get("type", "")

            subject_label = None
            # 基于fact_type或其他逻辑推断subject_label
            if "person_" in fact_type: subject_label = "Person"
            elif "region_" in fact_type: subject_label = "Region"
            elif "product_" in fact_type: subject_label = "Product"
            # ... 其他类型的映射 ...
            
            if subject_label and subject_name:
                prop_name = node_types_from_schema.get(subject_label, "name")
                entities_to_create.add((subject_label, prop_name, subject_name))

            object_label = None
            if not fact_type.endswith("_amount"): # 不是直接的销售额事实
                if "_project" in fact_type: object_label = "Project"
                elif "_product" in fact_type: object_label = "Product"
                elif "_document" in fact_type: object_label = "Document"
                elif "_idea" in fact_type: object_label = "Idea" # 新增对Idea类型的处理
                # ... 其他类型的映射 ...

                if object_label and object_name:
                    prop_name = node_types_from_schema.get(object_label, "name") # Document会用id, Idea会用name
                    entities_to_create.add((object_label, prop_name, object_name))
        
        for label, prop, value in entities_to_create:
            if value is not None:
                query = f"MERGE (n:{label} {{{prop}: $value}})"
                session.run(query, value=value)
                print(f"Merged node: ({label} {{{prop}: '{value}'}})")

        for fact in facts:
            s_name = fact.get("subject")
            rel = fact.get("relation")
            o_name = fact.get("object")
            fact_type = fact.get("type", "")
            period = fact.get("period")

            if fact_type == "region_sales_amount" and period:
                session.run("MERGE (r:Region {name: $s_name})", s_name=s_name)
                try:
                    # ... (销售额解析逻辑不变) ...
                    if isinstance(o_name, str) and '万元' in o_name:
                        numeric_val_str = o_name.replace('万元', '').strip()
                        numeric_val = float(numeric_val_str)
                        unit_val = '万元'
                    # ... (其他单位解析) ...
                    else:
                        numeric_val = float(o_name) # 尝试直接转换
                        unit_val = None 
                    
                    query = """
                    MATCH (r:Region {name: $s_name})
                    CREATE (sa:SalesAmount {numeric_amount: $num_val, period: $period, unit: $unit_val})
                    CREATE (r)-[:HAS_SALES_AMOUNT]->(sa)
                    """
                    session.run(query, s_name=s_name, num_val=numeric_val, period=period, unit_val=unit_val)
                    print(f"Created SalesAmount for {s_name}, {period}: {numeric_val} {unit_val or ''}")
                except ValueError:
                    print(f"Could not parse sales amount: {o_name} for {s_name}, {period}. Skipping this SalesAmount fact.")
                
            elif s_name and rel and o_name: 
                s_label, o_label = None, None
                s_prop, o_prop = "name", "name" 

                # --- 更精确的标签和属性推断 ---
                if fact_type == "person_project" and rel == "WORKS_ON":
                    s_label, o_label = "Person", "Project"
                elif fact_type == "person_idea" and rel == "PROPOSED_IDEA": # 新增
                    s_label, o_label = "Person", "Idea"
                elif fact_type == "region_product" and rel == "HAS_SALES_PRODUCT": # 假设type是 region_product
                    s_label, o_label = "Region", "Product"
                elif fact_type == "product_document" and rel == "RELATED_TO":
                    s_label, o_label = "Product", "Document"
                    o_prop = "id" # Document用id匹配
                # 您可以根据您的fact_type添加更多精确的映射规则

                if s_label and o_label:
                    query = f"""
                    MATCH (s:{s_label} {{{s_prop}: $s_name}})
                    MATCH (o:{o_label} {{{o_prop}: $o_name}})
                    MERGE (s)-[:{rel}]->(o)
                    """
                    session.run(query, s_name=s_name, o_name=o_name)
                    print(f"Merged relationship: ({s_label} {{{s_prop}:'{s_name}'}})-[:{rel}]->({o_label} {{{o_prop}:'{o_name}'}})")
                else:
                    print(f"Could not determine labels for fact: {fact} (s_label: {s_label}, o_label: {o_label}). Relationship not created.")
            else:
                print(f"Skipping incomplete fact: {fact}")


if __name__ == "__main__":
    driver = None
    try:
        driver = GraphDatabase.driver(NEO4J_URI, auth=basic_auth(NEO4J_USER, NEO4J_PASSWORD))
        driver.verify_connectivity()
        print("Successfully connected to Neo4j.")
        
        clear_database(driver) # 清空数据库
        create_constraints(driver) # 创建约束

        with open(SAMPLE_KG_PATH, 'r', encoding='utf-8') as f:
            kg_data_to_load = json.load(f)
        
        load_data(driver, kg_data_to_load)
        
        print("\nData loading process completed.")
        print("You can now verify the data in Neo4j Browser (http://localhost:7474).")
        print("Example query to check SalesAmount:")
        print("MATCH (r:Region)-[:HAS_SALES_AMOUNT]->(sa:SalesAmount) RETURN r.name, sa.numeric_amount, sa.unit, sa.period")
        print("Example query to check Person-Project:")
        print("MATCH (p:Person)-[:WORKS_ON]->(proj:Project) RETURN p.name, proj.name")

    except Exception as e:
        print(f"An error occurred: {e}")
        traceback.print_exc()
    finally:
        if driver:
            driver.close()
            print("Neo4j connection closed.")
    --- END OF FILE CONTENT ---


Directory: zhz_rag/core_rag/
  File: __init__.py
    --- START OF FILE CONTENT ---
# zhz_agent/core_rag/__init__.py
from .kg_retriever import KGRetriever
from .fusion_engine import FusionEngine
# 如果上面 retrievers/__init__.py 也做了导出，这里也可以考虑是否进一步导出
    --- END OF FILE CONTENT ---

  File: fusion_engine.py
    --- START OF FILE CONTENT ---
# zhz_agent/fusion.py
import hashlib
import jieba
import torch
import asyncio
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from typing import List, Dict, Any, Optional
import logging 
import os

# 从项目内部导入pydantic_models
from zhz_rag.config.pydantic_models import RetrievedDocument

class FusionEngine:
    _current_script_path = os.path.abspath(__file__)
    _script_directory = os.path.dirname(_current_script_path)

    # 确保您的模型路径指向正确的位置，如果不在 local_models/bge-reranker-base
    LOCAL_RERANKER_MODEL_PATH = os.getenv(
        "RERANKER_MODEL_PATH", 
        "/home/zhz/models/bge-reranker-base" # <--- 直接指定新的、统一管理后的模型路径
    )

    def __init__(self, logger: Optional[logging.Logger] = None):
        if logger:
            self.logger = logger
        else:
            self.logger = logging.getLogger("FusionEngineLogger")
            if not self.logger.hasHandlers():
                self.logger.setLevel(logging.INFO)
                self.logger.info("FusionEngine initialized with its own basic logger (no handlers configured by default).")
            else:
                self.logger.info("FusionEngine initialized, re-using existing logger configuration for FusionEngineLogger.")

        self.reranker_tokenizer: Optional[AutoTokenizer] = None
        self.reranker_model: Optional[AutoModelForSequenceClassification] = None
        self.reranker_device = "cuda" if torch.cuda.is_available() else "cpu"
        self._load_reranker_model()

    def _load_reranker_model(self):
        self.logger.info(f"FusionEngine: Loading reranker model from: {self.LOCAL_RERANKER_MODEL_PATH} to {self.reranker_device}...")
        
        if not os.path.isdir(self.LOCAL_RERANKER_MODEL_PATH): # 检查是否是目录
            _error_msg_model_path = f"Error: Reranker model local path does not exist or is not a directory: {self.LOCAL_RERANKER_MODEL_PATH}."
            self.logger.error(_error_msg_model_path)
            # 在实际应用中，这里可能应该抛出异常，或者让服务无法启动
            # 为了测试，我们先允许模型为空，后续调用会检查
            self.reranker_model = None
            self.reranker_tokenizer = None
            return # 提前返回

        try:
            self.reranker_tokenizer = AutoTokenizer.from_pretrained(self.LOCAL_RERANKER_MODEL_PATH)
            self.reranker_model = AutoModelForSequenceClassification.from_pretrained(self.LOCAL_RERANKER_MODEL_PATH)
            self.reranker_model.to(self.reranker_device)

            if self.reranker_device == 'cuda' and hasattr(self.reranker_model, 'half'): # 检查是否有half方法
                self.reranker_model.half()
                self.logger.info("FusionEngine: Reranker model loaded to GPU and using FP16.")
            else:
                self.logger.info(f"FusionEngine: Reranker model loaded to {self.reranker_device}.")
            
            self.reranker_model.eval()
            self.logger.info("FusionEngine: Reranker model loading successful!")
        except Exception as e:
            self.logger.error(f"Error: Reranker model loading failed: {e}", exc_info=True)
            self.reranker_tokenizer = None
            self.reranker_model = None

    def _rerank_documents_sync(self, query: str, documents: List[RetrievedDocument]) -> List[RetrievedDocument]:
        if not self.reranker_model or not self.reranker_tokenizer:
            self.logger.warning("FusionEngine: Reranker model not loaded. Cannot perform fine-grained reranking. Returning documents as is (or after basic sort if any).")
            # 可以选择返回原始顺序，或者按原始分数（如果可比）排序
            return sorted(documents, key=lambda doc: doc.score if doc.score is not None else -float('inf'), reverse=True)


        if not documents:
            self.logger.info("FusionEngine: No documents to rerank.")
            return []

        # 确保文档内容是字符串
        pairs = []
        valid_documents_for_reranking = []
        for doc in documents:
            if isinstance(doc.content, str):
                pairs.append([query, doc.content])
                valid_documents_for_reranking.append(doc)
            else:
                self.logger.warning(f"FusionEngine: Document with non-string content skipped for reranking. ID: {doc.metadata.get('chunk_id', 'N/A')}, Type: {type(doc.content)}")
        
        if not pairs:
            self.logger.info("FusionEngine: No valid document pairs for reranking after content check.")
            return []

        self.logger.info(f"FusionEngine: Reranking {len(valid_documents_for_reranking)} documents with CrossEncoder...")
        
        try:
            with torch.no_grad():
                inputs = self.reranker_tokenizer(
                    pairs, 
                    padding=True, 
                    truncation=True, 
                    return_tensors='pt', 
                    max_length=512 # BGE Reranker通常是512
                ).to(self.reranker_device)
                
                logits = self.reranker_model(**inputs).logits
                # CrossEncoder通常直接输出一个分数，而不是需要sigmoid/softmax
                # BGE-Reranker输出的是logit，可以直接用作分数，或者通过sigmoid转为概率（但不必要）
                scores = logits.view(-1).float().cpu().numpy()

            for i, doc in enumerate(valid_documents_for_reranking):
                doc.score = float(scores[i]) # 更新文档的score为reranker的打分

            reranked_docs = sorted(valid_documents_for_reranking, key=lambda doc: doc.score, reverse=True)
            self.logger.info(f"FusionEngine: Reranking complete. {len(reranked_docs)} documents sorted.")
            return reranked_docs
        except Exception as e_rerank_detail:
            self.logger.error(f"FusionEngine: Detailed error during reranking with CrossEncoder: {e_rerank_detail}", exc_info=True)
            # 如果重排序失败，返回按原始分数（如果可比）排序的文档，或者简单返回valid_documents_for_reranking
            return sorted(valid_documents_for_reranking, key=lambda d: d.score if d.score is not None else -float('inf'), reverse=True)


    def _tokenize_text(self, text: str) -> set[str]:
        if not isinstance(text, str): # 添加类型检查
            self.logger.warning(f"FusionEngine: _tokenize_text received non-string input: {type(text)}. Returning empty set.")
            return set()
        return set(jieba.cut(text))

    def _calculate_jaccard_similarity(self, query_tokens: set[str], doc_tokens: set[str]) -> float:
        if not query_tokens or not doc_tokens: # 处理空集合的情况
            return 0.0
        intersection = len(query_tokens.intersection(doc_tokens))
        union = len(query_tokens.union(doc_tokens))
        return intersection / union if union > 0 else 0.0
        
    async def fuse_results(
        self,
        all_raw_retrievals: List[RetrievedDocument],
        user_query: str,
        top_n_final: int = 3
    ) -> List[RetrievedDocument]:

        self.logger.info(f"FusionEngine: Fusing {len(all_raw_retrievals)} raw retrieved documents for query: '{user_query}'. Target top_n_final: {top_n_final}")

        if not all_raw_retrievals:
            self.logger.info("FusionEngine: No documents to fuse.")
            return []

        # 1. 去重 (基于内容的哈希值)
        unique_docs_map: Dict[str, RetrievedDocument] = {}
        for doc in all_raw_retrievals:
            if not isinstance(doc.content, str) or not doc.content.strip():
                self.logger.debug(f"FusionEngine: Skipping doc with invalid content for hashing: {doc.metadata.get('chunk_id', 'N/A') if doc.metadata else 'N/A'}")
                continue
            content_hash = hashlib.md5(doc.content.encode('utf-8')).hexdigest()
            if content_hash not in unique_docs_map:
                unique_docs_map[content_hash] = doc
            else:
                # 如果内容重复，可以保留分数较高的一个（如果分数可比且来自不同召回源）
                # 这里简化处理，保留第一个遇到的，或者可以根据source_type和score进行更复杂的选择
                if doc.score is not None and unique_docs_map[content_hash].score is not None:
                    if doc.score > unique_docs_map[content_hash].score: # type: ignore
                        unique_docs_map[content_hash] = doc # 保留分数更高的
                elif doc.score is not None: # 当前文档有分数，已存的没有
                     unique_docs_map[content_hash] = doc
                self.logger.debug(f"FusionEngine: Duplicate content hash found. Doc with score {doc.score} vs existing {unique_docs_map[content_hash].score}. Content: {doc.content[:50]}...")
        
        unique_docs = list(unique_docs_map.values())
        self.logger.info(f"FusionEngine: After deduplication (content hash): {len(unique_docs)} documents.")

        if not unique_docs:
            return []

        # 2. 初步筛选 (基于长度和Jaccard相似度)
        # 定义阈值
        JACCARD_THRESHOLD = 0.05  # Jaccard相似度阈值，低于此则可能被过滤
        MIN_DOC_LENGTH_CHARS_KG = 10    # 知识图谱结果的最小字符长度
        MIN_DOC_LENGTH_CHARS_OTHER = 10 # 其他来源（向量、BM25）的最小字符长度
        # Reranker (如BGE-Reranker) 通常处理的token上限是512。
        # 一个中文字符大致对应1-3个token，英文单词大致对应1个token。
        # 为安全起见，可以设置一个字符上限，例如 1000-1500 字符，避免超长输入给Reranker。
        # 如果Reranker的tokenizer有max_length参数，它会自动截断，但预先过滤可以减少不必要的处理。
        MAX_DOC_LENGTH_CHARS = 1500 # 文档的最大字符长度，防止过长输入给reranker

        query_tokens_set = self._tokenize_text(user_query)
        screened_results: List[RetrievedDocument] = []
        
        self.logger.info(f"FusionEngine: Starting light screening for {len(unique_docs)} unique documents.")
        for doc_idx, doc in enumerate(unique_docs):
            doc_content_str = str(doc.content) # 确保是字符串
            doc_length = len(doc_content_str)
            
            # 长度筛选
            min_len_chars = MIN_DOC_LENGTH_CHARS_KG if doc.source_type == "knowledge_graph" else MIN_DOC_LENGTH_CHARS_OTHER
            if not (min_len_chars <= doc_length <= MAX_DOC_LENGTH_CHARS):
                self.logger.debug(f"  Screening: Doc {doc_idx} (ID: {doc.metadata.get('chunk_id', 'N/A') if doc.metadata else 'N/A'}) failed length check. Length: {doc_length}, Expected: [{min_len_chars}-{MAX_DOC_LENGTH_CHARS}], Type: {doc.source_type}. Content: '{doc_content_str[:50]}...'")
                continue

            # Jaccard相似度筛选 (可选，如果query_tokens_set为空则跳过)
            if query_tokens_set: # 只有当查询分词结果非空时才进行Jaccard计算
                doc_tokens_set = self._tokenize_text(doc_content_str)
                if not doc_tokens_set: # 如果文档分词结果为空，Jaccard为0
                    jaccard_sim = 0.0
                else:
                    jaccard_sim = self._calculate_jaccard_similarity(query_tokens_set, doc_tokens_set)
                
                if jaccard_sim < JACCARD_THRESHOLD:
                    self.logger.debug(f"  Screening: Doc {doc_idx} (ID: {doc.metadata.get('chunk_id', 'N/A') if doc.metadata else 'N/A'}) failed Jaccard check. Similarity: {jaccard_sim:.4f} < {JACCARD_THRESHOLD}. Content: '{doc_content_str[:50]}...'")
                    continue
            else:
                self.logger.debug(f"  Screening: Doc {doc_idx} (ID: {doc.metadata.get('chunk_id', 'N/A') if doc.metadata else 'N/A'}) - Query tokens empty, skipping Jaccard check.")


            screened_results.append(doc)
            self.logger.debug(f"  Screening: Doc {doc_idx} (ID: {doc.metadata.get('chunk_id', 'N/A') if doc.metadata else 'N/A'}) passed light screening.")
        
        self.logger.info(f"FusionEngine: After light screening: {len(screened_results)} documents remain.")
        
        if not screened_results:
            self.logger.info("FusionEngine: No documents remain after light screening. Returning empty list.")
            return []
        
        # 如果初筛后文档数量仍然很多，可以考虑再根据原始分数进行一次粗排和截断
        # 例如，如果 screened_results 数量 > top_n_final * 10，则取分数最高的前 top_n_final * 10 个
        # 这需要确保原始分数具有一定的可比性，或者对不同来源的分数进行大致的归一化
        # 当前我们先不加这一步，假设上游召回和初步筛选已将数量控制在合理范围
        docs_for_reranking = screened_results

        # 3. 使用Cross-Encoder进行精细重排序
        # _rerank_documents_sync 是同步函数，在异步函数中调用需要用 asyncio.to_thread
        final_fused_and_reranked_results = await asyncio.to_thread(
            self._rerank_documents_sync,
            query=user_query,
            documents=docs_for_reranking # 使用筛选后的文档
        )

        self.logger.info(f"FusionEngine: After reranking: {len(final_fused_and_reranked_results)} documents.")
        for i_doc, doc_reranked in enumerate(final_fused_and_reranked_results[:top_n_final+5]): # 日志多打几条看看分数
            self.logger.debug(f"  Reranked Doc {i_doc}: type={doc_reranked.source_type}, new_score={doc_reranked.score:.4f}, content='{str(doc_reranked.content)[:100]}...'")

        # 4. 根据 top_n_final 截取最终结果
        final_output_documents = final_fused_and_reranked_results[:top_n_final]

        self.logger.info(f"FusionEngine: Returning final top {len(final_output_documents)} documents.")
        return final_output_documents
    --- END OF FILE CONTENT ---

  File: kg_retriever.py
    --- START OF FILE CONTENT ---
# zhz_rag/core_rag/kg_retriever.py
import os
import json
import kuzu
import pandas as pd
from sentence_transformers import SentenceTransformer
from typing import List, Dict, Any, Optional, Callable, Iterator
import logging
from contextlib import contextmanager
import hashlib # 确保导入 hashlib

# --- 从项目中导入必要的模块 ---
from zhz_rag.llm.llm_interface import extract_entities_for_kg_query
from zhz_rag.config.pydantic_models import ExtractedEntitiesAndRelationIntent, IdentifiedEntity

# 日志配置
kg_logger = logging.getLogger(__name__)
if not kg_logger.hasHandlers():
    kg_logger.setLevel(logging.DEBUG) # 可以设置为 INFO 或 DEBUG
    ch = logging.StreamHandler()
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s')
    ch.setFormatter(formatter)
    kg_logger.addHandler(ch)
    kg_logger.propagate = False
kg_logger.info("KGRetriever (KuzuDB) logger initialized/reconfirmed.")


class KGRetriever:
    KUZU_DB_PATH_ENV = os.getenv("KUZU_DB_PATH", "/home/zhz/zhz_agent/zhz_rag/stored_data/kuzu_default_db")

    def __init__(self, db_path: Optional[str] = None, embedder: Optional[SentenceTransformer] = None):
        self.db_path = db_path if db_path else self.KUZU_DB_PATH_ENV
        self._db: Optional[kuzu.Database] = None
        self._embedder = embedder
        kg_logger.info(f"KGRetriever (KuzuDB) __init__ called. DB path: {self.db_path}")
        if self._embedder:
            kg_logger.info(f"KGRetriever initialized with embedder: {type(self._embedder)}")
        else:
            kg_logger.warning("KGRetriever initialized WITHOUT an embedder. Vector search will not be available.")
        self._connect_to_kuzu()

    def _connect_to_kuzu(self):
        kg_logger.info(f"Attempting to load KuzuDB from path: {self.db_path}")
        try:
            if not os.path.exists(self.db_path):
                kg_logger.error(f"KuzuDB path does not exist: {self.db_path}. KGRetriever cannot connect.")
                self._db = None
                return
            
            self._db = kuzu.Database(self.db_path, read_only=True)
            kg_logger.info(f"Successfully loaded KuzuDB from {self.db_path}.")

            # --- 添加开始：为 KGRetriever 的连接加载 VECTOR 扩展 ---
            try:
                kg_logger.info("KGRetriever: Attempting to LOAD VECTOR extension for this session...")
                with self._get_connection() as conn:
                    conn.execute("LOAD VECTOR;")
                kg_logger.info("KGRetriever: VECTOR extension loaded successfully for this retriever instance.")
            except Exception as e_load:
                kg_logger.error(f"KGRetriever: CRITICAL - Failed to LOAD VECTOR extension during initialization: {e_load}", exc_info=True)
                raise ConnectionError(f"Failed to load KuzuDB VECTOR extension for KGRetriever: {e_load}") from e_load

        except Exception as e:
            kg_logger.error(f"Failed to connect to KuzuDB at {self.db_path}: {e}", exc_info=True)
            self._db = None

    @contextmanager
    def _get_connection(self) -> Iterator[kuzu.Connection]:
        if not self._db:
            kg_logger.warning("KuzuDB database object is None in _get_connection. Attempting to reconnect...")
            self._connect_to_kuzu()
            if not self._db:
                kg_logger.error("KuzuDB reconnection failed. Cannot get a connection.")
                raise ConnectionError("KuzuDB is not connected or failed to reconnect. Cannot get a connection.")
        conn = None
        try:
            conn = kuzu.Connection(self._db)
            kg_logger.debug("KuzuDB connection obtained.")
            yield conn
        except Exception as e_conn:
            kg_logger.error(f"Failed to create KuzuDB connection object: {e_conn}", exc_info=True)
            raise ConnectionError(f"Failed to create KuzuDB connection: {e_conn}")
        finally:
            kg_logger.debug("KuzuDB connection context manager exiting.")
            pass # KuzuDB Connection 对象没有显式的 close() 方法，依赖其 __del__

    def close(self):
        kg_logger.info(f"Closing KuzuDB for retriever using path: {self.db_path}")
        if self._db:
            del self._db # 依赖 KuzuDB Database 的 __del__ 方法进行清理和锁释放
            self._db = None
            kg_logger.info("KuzuDB Database object dereferenced (closed).")

    def _execute_cypher_query_sync(self, query: str, parameters: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        if not self._db:
            kg_logger.error("KuzuDB not initialized in _execute_cypher_query_sync. Cannot execute query.")
            return []
        kg_logger.info(f"--- Executing KuzuDB Cypher --- Query: {query}, Params: {parameters if parameters else 'No parameters'}")
        results_list: List[Dict[str, Any]] = []
        try:
            with self._get_connection() as conn:
                actual_params = parameters if parameters else {}
                query_result = conn.execute(query, parameters=actual_params)
                if hasattr(query_result, 'get_as_df'):
                    df = pd.DataFrame(query_result.get_as_df())
                    results_list = df.to_dict(orient='records')
                elif isinstance(query_result, list):
                    kg_logger.info("KuzuDB query did not return a DataFrame-convertible result, result is a list.")
                    # results_list = query_result # 如果列表内已经是dict
                else:
                    kg_logger.info(f"KuzuDB query did not return a DataFrame-convertible result. Type: {type(query_result)}")

                kg_logger.info(f"KuzuDB Cypher executed. Records count (from DataFrame): {len(results_list)}")
                if results_list: 
                    kg_logger.debug(f"First KuzuDB record (from DataFrame): {str(results_list[0])[:200]}")
                elif not hasattr(query_result, 'get_as_df'):
                     kg_logger.debug(f"KuzuDB query result (raw, not DataFrame): {str(query_result)[:200]}")
        except RuntimeError as kuzu_runtime_error:
             kg_logger.error(f"KuzuDB RuntimeError during Cypher execution: '{query}' with params: {parameters}. Error: {kuzu_runtime_error}", exc_info=True)
        except ConnectionError as conn_err:
             kg_logger.error(f"KuzuDB ConnectionError during Cypher execution: {conn_err}", exc_info=True)
        except TypeError as e_type:
            kg_logger.error(f"TypeError during KuzuDB Cypher execution: '{query}' with params: {parameters}. Error: {e_type}", exc_info=True)
        except Exception as e:
            kg_logger.error(f"Unexpected error executing KuzuDB Cypher query: '{query}' with params: {parameters}. Error: {e}", exc_info=True)
        return results_list

    def _format_kuzu_records_for_retrieval(self, records: List[Dict[str, Any]], query_context: str = "") -> List[Dict[str, Any]]:
        formatted_docs = []
        if not records:
            return formatted_docs

        for record_data in records:
            parts = []
            # 优先展示的、信息量大的key
            primary_info_keys = ['text', 'label', 'id_prop', 'name', 'related_text', 'related_label', 'relationship_type', 'source_node_text', '_score']
            
            for key in primary_info_keys:
                if key in record_data and record_data[key] is not None:
                    if key == '_score' and isinstance(record_data[key], float):
                        parts.append(f"Similarity Score: {record_data[key]:.4f}")
                    else:
                        parts.append(f"{key.replace('_', ' ').capitalize()}: {record_data[key]}")
            
            # 其他非核心但有用的信息
            additional_info_parts = []
            for key, value in record_data.items():
                if key not in primary_info_keys and value is not None:
                    if isinstance(value, dict): # 例如 KuzuDB 返回的复杂节点/关系对象
                        value_str = json.dumps(value, ensure_ascii=False, default=str)
                        if len(value_str) > 70: value_str = value_str[:70] + "..."
                        additional_info_parts.append(f"{key}: {value_str}")
                    else:
                        additional_info_parts.append(f"{key}: {str(value)}")
            
            if additional_info_parts:
                parts.append("Other Info: " + " | ".join(additional_info_parts))
            
            content_str = " | ".join(parts) if parts else "Retrieved graph data node/relation."
            
            # 确保元数据中也包含关键信息，方便调试和后续处理
            doc_metadata = {
                "original_user_query_for_kg": query_context,
                "kuzu_retrieved_id_prop": record_data.get("id_prop"),
                "kuzu_retrieved_text": record_data.get("text") or record_data.get("related_text"),
                "kuzu_retrieved_label": record_data.get("label") or record_data.get("related_label"),
                "kuzu_retrieved_relationship": record_data.get("relationship_type"),
                "kuzu_retrieved_score": record_data.get("_score")
            }
            # 清理元数据中的None值
            doc_metadata = {k: v for k, v in doc_metadata.items() if v is not None}

            doc_data = {
                "source_type": "knowledge_graph_kuzu",
                "content": content_str,
                "score": record_data.get('_score', 0.0), # 使用记录中的_score，如果没有则为0
                "metadata": doc_metadata
            }
            formatted_docs.append(doc_data)
        return formatted_docs

    async def retrieve(self, user_query: str, top_k: int = 3) -> List[Dict[str, Any]]:
        kg_logger.info(f"Starting KG retrieval for query: '{user_query}', top_k: {top_k}")

        extracted_info: Optional[ExtractedEntitiesAndRelationIntent] = await extract_entities_for_kg_query(user_query)

        search_text_for_vector: str
        if extracted_info and extracted_info.entities and extracted_info.entities[0].text:
            primary_entity_text = extracted_info.entities[0].text
            search_text_for_vector = primary_entity_text
            kg_logger.info(f"LLM extracted: Entities: {[e.model_dump() for e in extracted_info.entities]}, Relation Hint: {extracted_info.relation_hint}")
            kg_logger.info(f"Using primary extracted entity text for vector search: '{search_text_for_vector}'")
        else:
            search_text_for_vector = user_query
            kg_logger.info(f"No primary entities extracted or entity text is empty. Using original user query for vector search: '{user_query}'")

        all_kuzu_records: List[Dict[str, Any]] = []
        
        # --- Strategy 1: KuzuDB Vector Index Search (Preferred) ---
        if self._embedder:
            try:
                kg_logger.info(f"Generating embedding for vector search text: '{search_text_for_vector}'")
                query_vector_np = self._embedder.encode(search_text_for_vector, normalize_embeddings=True)
                query_vector_list = query_vector_np.tolist()
                
                # --- 修改：根据研究报告更新 KuzuDB 向量查询 ---
                # 假设索引名为 'entity_embedding_idx'，建立在 ExtractedEntity 表的 'embedding' (FLOAT[]) 列上
                vector_search_query = """
                    CALL QUERY_VECTOR_INDEX('ExtractedEntity', 'entity_embedding_idx', $query_vector, $top_k_param)
                    YIELD node, distance
                    RETURN node.id_prop AS id_prop, node.text AS text, node.label AS label, distance AS _score
                """
                vector_params = {"query_vector": query_vector_list, "top_k_param": top_k} 
                # --- 修改结束 ---
                
                kg_logger.info(f"Executing KuzuDB vector search. Table: 'ExtractedEntity', Index: 'entity_embedding_idx', Top K: {top_k}")
                # kg_logger.debug(f"Vector search query: {vector_search_query.strip()}")
                # kg_logger.debug(f"Vector search params (vector preview): {{'query_vector': {str(query_vector_list)[:100]}..., 'top_k_param': {top_k}}}")

                vector_results = self._execute_cypher_query_sync(vector_search_query, vector_params)
                
                if vector_results:
                    # 注意：返回的 _score 是 distance，越小越相似
                    all_kuzu_records.extend(vector_results)
                    kg_logger.info(f"Retrieved {len(vector_results)} records via KuzuDB vector search for text: '{search_text_for_vector}'")
                else:
                    kg_logger.info(f"No results from KuzuDB vector search for text: '{search_text_for_vector}'")

            except Exception as e_vector_search:
                kg_logger.error(f"Error during KuzuDB vector search: {e_vector_search}", exc_info=True)
        else:
            kg_logger.warning("Embedder not available in KGRetriever, skipping KuzuDB vector search.")

        # --- Strategy 2: Template-based Cypher (逻辑保持不变) ---
        # (这部分代码与您上一轮修改后的一致，此处省略以便聚焦向量检索部分) ...
        # 确保这里的 _score 也是越小越好，或者与向量搜索的 score 含义一致，或者在融合时能区分
        # 为了简化，我们之前给模板查询的 _score 是固定的正值，表示一个“置信度”而非距离
        # 这意味着在排序时需要特别处理，或者统一 score 的含义。
        # 暂时保持模板查询的 _score 为固定值，代表其“非向量搜索”的来源。
        if extracted_info and extracted_info.entities:
            for entity_info in extracted_info.entities:
                if not entity_info.text:
                    continue
                if entity_info.label: 
                    attr_query = """
                        MATCH (n:ExtractedEntity {text: $text, label: $label}) 
                        RETURN n.id_prop AS id_prop, n.text AS text, n.label AS label, 0.85 AS _score_template 
                        LIMIT 1 
                    """ # 将模板查询的score命名为_score_template以区分
                    attr_params = {"text": entity_info.text, "label": entity_info.label.upper()}
                    # ... (后续模板查询逻辑不变) ...
                    kg_logger.info(f"Executing template attribute query for: '{entity_info.text}' (Label: {entity_info.label.upper()})")
                    attr_results = self._execute_cypher_query_sync(attr_query, attr_params)
                    if attr_results:
                         # 将 _score_template 转换为 _score，并赋予一个表示“非距离”的值，例如负值或很大的正值
                         # 或者，我们可以在排序时单独处理。简单起见，我们先让它有一个不同的键名。
                         # 更新：为了统一排序，我们将模板结果的“分数”也视为“距离”，但赋予一个较大的固定值
                         # 以便向量搜索结果（距离小）能排在前面。
                        for res in attr_results: res['_score'] = res.pop('_score_template', 100.0) # 假设100是很大的距离
                        all_kuzu_records.extend(attr_results)

                    if extracted_info.relation_hint:
                        mapped_rel_type = None
                        if "工作" in extracted_info.relation_hint and entity_info.label.upper() == "PERSON":
                            mapped_rel_type = "WorksAt"
                        elif "分配" in extracted_info.relation_hint and entity_info.label.upper() == "TASK":
                            mapped_rel_type = "AssignedTo"
                        
                        if mapped_rel_type:
                            neighbor_query = f"""
                                MATCH (src:ExtractedEntity {{text: $text, label: $label}})
                                      -[r:{mapped_rel_type}]->
                                      (tgt:ExtractedEntity)
                                RETURN tgt.id_prop AS id_prop, tgt.text AS related_text, tgt.label AS related_label, 
                                       label(r) AS relationship_type, src.text AS source_node_text, 101.0 AS _score 
                                LIMIT {top_k} 
                            """ # 邻居查询的距离更大
                            neighbor_params = {"text": entity_info.text, "label": entity_info.label.upper()}
                            kg_logger.info(f"Executing template neighbor query for: '{entity_info.text}' via relation '{mapped_rel_type}'")
                            neighbor_results = self._execute_cypher_query_sync(neighbor_query, neighbor_params)
                            if neighbor_results:
                                all_kuzu_records.extend(neighbor_results)
                else:
                    kg_logger.info(f"Skipping template-based Cypher for entity '{entity_info.text}' as LLM did not provide a label for it.")

        if not all_kuzu_records:
            kg_logger.info(f"No records retrieved from KuzuDB for query: '{user_query}' after all strategies.")
            return []

        # --- Deduplication and Formatting ---
        # 确保所有记录都有 _score，且含义一致（越小越好）
        for rec in all_kuzu_records:
            rec.setdefault("_score", 200.0) # 给没有分数的记录一个非常大的距离值

        # 排序：按 _score 升序 (距离越小越好)
        sorted_kuzu_records = sorted(all_kuzu_records, key=lambda x: x.get('_score', 200.0), reverse=False)
        
        formatted_docs = self._format_kuzu_records_for_retrieval(sorted_kuzu_records, user_query)

        final_unique_docs_map: Dict[str, Dict[str, Any]] = {}
        for doc_dict in formatted_docs:
            content_key = doc_dict.get("content", "")
            current_score = doc_dict.get("score", 200.0) # score 仍然是距离

            if content_key not in final_unique_docs_map:
                final_unique_docs_map[content_key] = doc_dict
            else:
                # 如果内容已存在，保留距离更小的（分数更优的）
                existing_score = final_unique_docs_map[content_key].get("score", 200.0)
                if current_score < existing_score:
                    final_unique_docs_map[content_key] = doc_dict
        
        final_results_list = list(final_unique_docs_map.values())
        # 再次根据score（距离）排序，因为字典转列表后顺序可能打乱
        final_results_sorted_by_score = sorted(final_results_list, key=lambda x: x.get('score', 200.0), reverse=False)

        kg_logger.info(f"Total {len(all_kuzu_records)} records initially retrieved, "
                       f"{len(sorted_kuzu_records)} after initial sort by distance, "
                       f"{len(formatted_docs)} after formatting, "
                       f"{len(final_results_sorted_by_score)} after final content deduplication & re-sort. "
                       f"Returning top {min(top_k, len(final_results_sorted_by_score))} docs.")
        
        return final_results_sorted_by_score[:top_k]
    --- END OF FILE CONTENT ---


Directory: zhz_rag/core_rag/retrievers/
  File: __init__.py
    --- START OF FILE CONTENT ---
# zhz_agent/core_rag/retrievers/__init__.py
from .chromadb_retriever import ChromaDBRetriever
from .file_bm25_retriever import FileBM25Retriever
    --- END OF FILE CONTENT ---

  File: chromadb_retriever.py
    --- START OF FILE CONTENT ---
from typing import List, Dict, Any, Optional
from sentence_transformers import SentenceTransformer
import chromadb
import logging


# 配置ChromaDBRetriever的日志记录器
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

class ChromaDBRetriever:
    def __init__(
        self,
        collection_name: str = "rag_documents",
        persist_directory: str = "/home/zhz/dagster_home/chroma_data",
        embedding_model_name_or_path: str = "/home/zhz/models/bge-small-zh-v1.5",
    ):
        """
        初始化ChromaDBRetriever。

        Args:
            collection_name (str): 要查询的ChromaDB集合名称。
            persist_directory (str): ChromaDB数据持久化的目录。
            embedding_model_name_or_path (str): 用于查询向量化的SentenceTransformer模型名称或路径。
            device (str): 模型运行的设备 (例如 "cpu", "cuda")。
        """
        self.collection_name = collection_name
        self.persist_directory = persist_directory
        self.embedding_model_name_or_path = embedding_model_name_or_path
        self._client: Optional[chromadb.Client] = None
        self._collection: Optional[chromadb.Collection] = None
        self._embedding_model: Optional[SentenceTransformer] = None

        self._initialize_retriever()

    def _initialize_retriever(self):
        """
        初始化ChromaDB客户端、集合和嵌入模型。
        """
        try:
            logger.info(f"Initializing ChromaDB client from path: {self.persist_directory}")
            self._client = chromadb.PersistentClient(path=self.persist_directory)
            logger.info(f"Getting ChromaDB collection: {self.collection_name}")
            self._collection = self._client.get_collection(name=self.collection_name)
            if self._collection.count() == 0:
                logger.warning(f"ChromaDB collection '{self.collection_name}' is empty!")
            else:
                logger.info(f"ChromaDB collection '{self.collection_name}' loaded. Item count: {self._collection.count()}")
        except Exception as e:
            logger.error(f"Failed to initialize ChromaDB client or collection: {e}")
            raise

        try:
            logger.info(f"Loading SentenceTransformer model: {self.embedding_model_name_or_path}")
            self._embedding_model = SentenceTransformer(self.embedding_model_name_or_path)
            logger.info("SentenceTransformer model loaded successfully.")
        except Exception as e:
            logger.error(f"Failed to load SentenceTransformer model: {e}")
            raise
            
    def retrieve(self, query_text: str, n_results: int = 5, include_fields: Optional[List[str]] = None) -> List[Dict[str, Any]]:
        """
        根据查询文本从ChromaDB检索相似的文档块。

        Args:
            query_text (str): 用户查询的文本。
            n_results (int): 希望返回的最大结果数量。
            include_fields (Optional[List[str]]): 希望从ChromaDB返回的字段列表，
                                               例如 ["metadatas", "documents", "distances"]。
                                               如果为None，则ChromaDB通常会返回默认字段。
                                               我们存储时，chunk_text在metadatas里。

        Returns:
            List[Dict[str, Any]]: 检索到的文档块列表。每个字典通常包含：
                                   'id' (chunk_id), 
                                   'text' (chunk_text from metadata), 
                                   'metadata' (原始元数据),
                                   'distance' (相似度距离，越小越相似)
                                   具体结构取决于ChromaDB的返回和我们的处理。
        """
        if self._collection is None or self._embedding_model is None:
            logger.error("Retriever is not properly initialized.")
            # 尝试重新初始化，或者直接返回错误/空列表
            try:
                self._initialize_retriever()
                if self._collection is None or self._embedding_model is None: # 再次检查
                    return []
            except Exception as e:
                logger.error(f"Failed to re-initialize retriever during retrieve call: {e}")
                return []

        logger.info(f"Retrieving documents for query: '{query_text[:100]}...' with n_results={n_results}")
        
        try:
            # 1. 将查询文本向量化并归一化
            query_embedding_np = self._embedding_model.encode(
                query_text, 
                convert_to_tensor=False, 
                normalize_embeddings=True # <--- 关键：归一化查询嵌入
            )
            query_embedding = query_embedding_np.tolist()
            
            # 2. 在ChromaDB中查询 (include_fields_query 的逻辑不变)
            if include_fields is None:
                include_fields_query = ["metadatas", "distances"] 
            else:
                include_fields_query = include_fields
            results = self._collection.query(
                query_embeddings=[query_embedding], 
                n_results=n_results,
                include=include_fields_query 
            )
            # 3. 处理并格式化结果
            retrieved_docs = []
            if results and results.get("ids") and results.get("ids")[0]:
                ids_list = results["ids"][0]
                metadatas_list = results.get("metadatas", [[]])[0] 
                distances_list = results.get("distances", [[]])[0] 

                for i in range(len(ids_list)):
                    # ... (提取 chunk_id, metadata, distance 的代码不变) ...
                    chunk_id = ids_list[i]
                    metadata = metadatas_list[i] if metadatas_list and i < len(metadatas_list) else {}
                    distance = distances_list[i] if distances_list and i < len(distances_list) else float('inf')
                    chunk_text_content = metadata.get("chunk_text", "[Chunk text not found in metadata]")

                    # 计算相似度分数
                    # 如果ChromaDB使用cosine距离 (范围0-2, 0表示最相似)
                    # 相似度 = 1 - (distance / 2)  => 范围 0-1, 1表示最相似
                    # 或者直接用 cosine_similarity = 1 - distance (如果distance是 1-cos_sim)
                    # ChromaDB的cosine距离是 sqrt(2-2*cos_sim) 的平方，即 2-2*cos_sim (如果向量已归一化)
                    # 所以 cos_sim = 1 - distance / 2
                    # 我们希望分数越大越好
                    score = (1 - distance / 2.0) if distance != float('inf') and distance <=2.0 else 0.0 
                    # 确保分数在合理范围，如果distance意外地大于2，则score为0

                    retrieved_docs.append({
                        "id": chunk_id,
                        "text": chunk_text_content,
                        "score": score, # <--- 更新了分数计算
                        "distance": distance, 
                        "metadata": metadata 
                    })
                
                logger.info(f"Retrieved {len(retrieved_docs)} documents from ChromaDB.")
            else:
                logger.info("No documents retrieved from ChromaDB for the query.")

            return retrieved_docs

        except Exception as e:
            logger.error(f"Error during ChromaDB retrieval: {e}")
            return []

if __name__ == '__main__':
    # 简单的测试代码
    logger.info("--- ChromaDBRetriever Test ---")
    
    # 确保您的ChromaDB数据库中已经有通过Dagster流水线存入的数据
    # 并且模型路径正确
    try:
        retriever = ChromaDBRetriever()
        
        # 测试查询
        test_query = "人工智能的应用有哪些？" 
        # 根据您doc1.txt "这是第一个测试文档，关于人工智能和机器学习。" 应该能召回一些
        
        retrieved_results = retriever.retrieve(test_query, n_results=3)
        
        if retrieved_results:
            print(f"\n--- Retrieved Results for query: '{test_query}' ---")
            for i, doc in enumerate(retrieved_results):
                print(f"Result {i+1}:")
                print(f"  ID: {doc['id']}")
                print(f"  Text (first 100 chars): {doc['text'][:100]}...")
                print(f"  Score: {doc['score']:.4f} (Distance: {doc['distance']:.4f})")
                print(f"  Metadata: {doc['metadata']}")
                print("-" * 20)
        else:
            print(f"\nNo results retrieved for query: '{test_query}'")
            
        # 测试一个可能没有结果的查询
        # test_query_no_results = "恐龙为什么会灭绝？"
        # retrieved_no_results = retriever.retrieve(test_query_no_results, n_results=3)
        # if not retrieved_no_results:
        #     print(f"\nCorrectly retrieved no results for query: '{test_query_no_results}'")

    except Exception as e:
        print(f"An error occurred during the test: {e}")

    --- END OF FILE CONTENT ---

  File: file_bm25_retriever.py
    --- START OF FILE CONTENT ---
from typing import List, Dict, Any, Optional, Tuple
import jieba
import bm25s # 我们确认使用 bm25s
import pickle
import os
import logging
import numpy as np

# 配置日志记录器
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

class FileBM25Retriever:
    def __init__(
        self,
        index_directory_path: str = "/home/zhz/dagster_home/bm25_index_data/", # 与Dagster中配置一致
        # user_dict_path: Optional[str] = None 
    ):
        self.index_directory_path = index_directory_path
        # self.user_dict_path = user_dict_path

        self._bm25_model: Optional[bm25s.BM25] = None
        self._doc_ids: Optional[List[str]] = None 
        # self._tokenizer: Optional[bm25s.Tokenizer] = None # bm25s.tokenize 是一个函数，或者可以用Tokenizer类

        self._initialize_retriever()

    def _initialize_retriever(self):
        # if self.user_dict_path and os.path.exists(self.user_dict_path):
        #     # ... (加载jieba用户词典的逻辑不变) ...
        
        if not os.path.isdir(self.index_directory_path): # <--- 修改：检查目录是否存在
            logger.error(f"BM25 index directory not found at: {self.index_directory_path}")
            raise FileNotFoundError(f"BM25 index directory not found: {self.index_directory_path}")

        try:
            logger.info(f"Loading BM25 model from directory: {self.index_directory_path}")
            # 使用bm25s的load类方法
            # load_corpus=False 因为我们不期望在模型文件中包含原始语料库文本
            # mmap=False (默认) 先不使用内存映射，除非索引非常大
            self._bm25_model = bm25s.BM25.load(
                self.index_directory_path,
                load_corpus=False, # 通常我们不需要在这里加载原始语料库
                # mmap=False 
            )
            
            if self._bm25_model is None: # load 失败通常会抛异常，但以防万一
                logger.error("Failed to load BM25 model (returned None).")
                raise ValueError("Failed to load BM25 model.")
            logger.info("BM25 model loaded successfully.")

            # 单独加载 document_ids 列表
            doc_ids_path = os.path.join(self.index_directory_path, "doc_ids.pkl")
            if not os.path.exists(doc_ids_path):
                logger.error(f"doc_ids.pkl not found in {self.index_directory_path}")
                raise FileNotFoundError(f"doc_ids.pkl not found in {self.index_directory_path}")
            
            with open(doc_ids_path, 'rb') as f_in:
                self._doc_ids = pickle.load(f_in)
            
            if self._doc_ids is None: # pickle 加载空文件可能返回None
                 logger.warning(f"doc_ids.pkl loaded, but it was empty or invalid.")
                 self._doc_ids = [] # 设为空列表以避免后续错误
            logger.info(f"Document IDs loaded successfully. Number of indexed documents: {len(self._doc_ids)}")

        except Exception as e:
            logger.error(f"Failed to load BM25 index or document IDs: {e}")
            raise
            
    def retrieve(self, query_text: str, n_results: int = 5) -> List[Dict[str, Any]]:
        # ... (初始化检查和空索引检查不变) ...
        if self._bm25_model is None or self._doc_ids is None:
            logger.error("BM25 Retriever is not properly initialized.")
            # ... (尝试重新初始化或返回空的逻辑不变) ...
            return []
        
        if not self._doc_ids: 
            logger.info("BM25 index is empty, no results to retrieve.")
            return []

        logger.info(f"Retrieving documents with BM25 for query: '{query_text[:100]}...' with n_results={n_results}")

        try:
            # 1. 将查询文本分词
            # bm25s 有自己的 tokenize 函数，它会处理停用词、词干提取（如果配置了）并返回模型期望的格式
            # 我们需要确保查询时的分词方式与索引时一致。
            # 如果索引时用了jieba，查询时也应该用jieba。
            # bm25s.BM25 对象在加载后，其内部应该已经有了词汇表 (vocab_dict)，
            # 它的 get_scores 方法期望的是与词汇表ID对应的输入，或者它能自己处理分词后的文本列表。
            
            query_tokenized_jieba = list(jieba.cut_for_search(query_text))
            logger.debug(f"Tokenized query (jieba): {query_tokenized_jieba}")

            # 2. 使用BM25模型进行查询
            # bm25s 的 retrieve 方法可以直接返回文档索引和分数
            # 它接受分词后的查询 (list of str)
            # results 是文档索引 (numpy array), scores 是对应的分数 (numpy array)
            # 它们都是 (n_queries, k) 的形状，我们只有一个查询，所以是 (1, k)
            
            # 确保 k 不超过实际文档数
            actual_k = min(n_results, len(self._doc_ids))
            if actual_k == 0: # 如果索引中没有文档
                return []

            results_indices, results_scores = self._bm25_model.get_top_n(
                query_tokenized_jieba, 
                corpus=None, # 我们不需要在这里提供原始语料库，它返回的是索引
                n=actual_k
            )
            # get_top_n 返回的是一个列表（每个查询一个结果列表），我们只有一个查询
            # 每个结果列表中的元素是 (doc_index, score) 吗？还是直接是doc_index?
            # 查阅 bm25s 文档：get_top_n(query_tokens, documents, n=5)
            #   - query_tokens: list of tokens for the query.
            #   - documents: list of documents (list of tokens).
            #   - n: number of top documents to retrieve.
            # Returns: list of top n documents.
            # 这看起来是返回文档本身，不是我们想要的。

            # 让我们回到使用 get_scores 然后自己排序的方式，这更可控
            all_scores = self._bm25_model.get_scores(query_tokenized_jieba)
            
            top_n_indices = np.argsort(all_scores)[-actual_k:][::-1] # 降序取前N

            retrieved_docs = []
            for doc_corpus_index in top_n_indices:
                doc_id = self._doc_ids[doc_corpus_index] # 从0-based索引映射到我们的chunk_id
                score = float(all_scores[doc_corpus_index]) 
                retrieved_docs.append({
                    "id": doc_id,
                    "score": score
                })
            
            logger.info(f"Retrieved {len(retrieved_docs)} documents using BM25.")
            return retrieved_docs

        except Exception as e:
            logger.error(f"Error during BM25 retrieval: {e}")
            return []
            
    def retrieve(self, query_text: str, n_results: int = 5) -> List[Dict[str, Any]]:
        """
        根据查询文本使用BM25检索相关的文档块ID和分数。

        Args:
            query_text (str): 用户查询的文本。
            n_results (int): 希望返回的最大结果数量。

        Returns:
            List[Dict[str, Any]]: 检索到的文档块列表。每个字典包含：
                                   'id' (chunk_id), 
                                   'score' (BM25分数)
        """
        if self._bm25_model is None or self._doc_ids is None:
            logger.error("BM25 Retriever is not properly initialized.")
            try:
                self._initialize_retriever()
                if self._bm25_model is None or self._doc_ids is None:
                    return []
            except Exception as e:
                logger.error(f"Failed to re-initialize BM25 retriever during retrieve call: {e}")
                return []
        
        if not self._doc_ids: # 如果索引为空
            logger.info("BM25 index is empty, no results to retrieve.")
            return []

        logger.info(f"Retrieving documents with BM25 for query: '{query_text[:100]}...' with n_results={n_results}")

        try:
            # 1. 将查询文本分词
            query_tokenized = list(jieba.cut_for_search(query_text))
            logger.debug(f"Tokenized query: {query_tokenized}")

            # 2. 使用BM25模型进行查询
            # bm25s 的 get_scores 方法返回所有文档的分数
            # bm25s 的 get_batch_results (或类似名称) 可能更适合获取top-N
            # 我们需要查阅 bm25s 的API来获取top-N的文档索引和分数
            # 假设它有一个类似 get_top_n 的方法，或者我们需要自己处理 get_scores 的结果

            # 查阅 bm25s 文档，它通常使用 `bm25_model.get_scores(query_tokenized)` 得到所有分数
            # 然后我们需要自己排序并取top N
            # 或者，bm25s.BM25 可能有更直接的方法，例如 `search` 或 `query`
            # 经过快速查阅，bm25s 似乎没有直接的 top_n 方法，但其设计是为了快速计算所有分数。
            # `bm25_model.get_scores(query_tokenized)` 返回一个numpy数组，包含每个文档的分数。

            all_scores = self._bm25_model.get_scores(query_tokenized)
            
            # 获取分数最高的n_results个文档的索引
            # 注意：如果实际文档数少于n_results，则取实际数量
            actual_n_results = min(n_results, len(self._doc_ids))
            
            # 使用numpy的argsort来获取排序后的索引，然后取最后N个（因为argsort默认升序）
            # 或者取负数再取前N个
            top_n_indices = np.argsort(all_scores)[-actual_n_results:][::-1] # 降序取前N

            retrieved_docs = []
            for index in top_n_indices:
                doc_id = self._doc_ids[index]
                score = float(all_scores[index]) # 转换为Python float
                # 我们只返回ID和分数，文本内容由上层逻辑获取
                retrieved_docs.append({
                    "id": doc_id,
                    "score": score
                })
            
            logger.info(f"Retrieved {len(retrieved_docs)} documents using BM25.")
            return retrieved_docs

        except Exception as e:
            logger.error(f"Error during BM25 retrieval: {e}")
            return []

if __name__ == '__main__':
    logger.info("--- FileBM25Retriever Test ---")
    
    # 确保您的BM25索引文件已通过Dagster流水线创建
    # (例如 /home/zhz/dagster_home/bm25_index/rag_bm25_index.pkl)
    try:
        retriever = FileBM25Retriever()
        
        # 测试查询 (与ChromaDBRetriever使用相同的查询，以便后续比较和融合)
        test_query = "人工智能的应用有哪些？" 
        
        retrieved_results = retriever.retrieve(test_query, n_results=3)
        
        if retrieved_results:
            print(f"\n--- BM25 Retrieved Results for query: '{test_query}' ---")
            for i, doc in enumerate(retrieved_results):
                print(f"Result {i+1}:")
                print(f"  ID: {doc['id']}")
                print(f"  Score: {doc['score']:.4f}")
                # 我们这里不获取文本，由rag_service负责
                print("-" * 20)
        else:
            print(f"\nNo results retrieved with BM25 for query: '{test_query}'")

    except Exception as e:
        print(f"An error occurred during the BM25 test: {e}")
    --- END OF FILE CONTENT ---


Directory: zhz_rag/llm/
  File: __init__.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/zhz_rag/llm/__init__.py

from .llm_interface import call_llm_via_openai_api_local_only # 导入新函数

call_sglang_llm = call_llm_via_openai_api_local_only # 别名指向新函数

from .llm_interface import (
    generate_answer_from_context,
    generate_cypher_query,
    generate_expanded_queries,
    generate_intent_classification, # 这个现在直接用 litellm 调用 Gemini
    generate_clarification_question,
    generate_clarification_options,
    NO_ANSWER_PHRASE_ANSWER_CLEAN,
    NO_ANSWER_PHRASE_KG_CLEAN
)
    --- END OF FILE CONTENT ---

  File: custom_crewai_llms.py
    --- START OF FILE CONTENT ---
#/home/zhz/zhz_agent/custom_llm.py
import os
import json
import httpx
import asyncio
import traceback
from typing import List, Dict, Any, Optional, Union, Sequence, Type 
# --- CrewAI & LiteLLM Imports ---
from crewai.tools import BaseTool
from crewai.llms.base_llm import BaseLLM as CrewAIBaseLLM
import litellm

# --- [修改] Local Imports -> 改为绝对导入 ---
from zhz_rag.llm.llm_interface import call_sglang_llm # For SGLang LLM
from dotenv import load_dotenv

load_dotenv()

# --- SGLang Config ---
SGLANG_API_URL_FOR_LLM = os.getenv("SGLANG_API_URL", "http://localhost:30000/generate")

# --- CustomGeminiLLM (from ceshi/run_agent.py with fixes) ---
class CustomGeminiLLM(CrewAIBaseLLM):
    model_name: str
    api_key: str
    max_tokens: Optional[int] = 2048
    tool_config: Optional[Dict[str, Any]] = None
    stop: Optional[List[str]] = None
    _gemini_tools_cache: Optional[List[Dict[str, Any]]] = None

    def __init__(self, model: str, api_key: str, temperature: float = 0.1, max_tokens: Optional[int] = 2048, tool_config: Optional[Dict[str, Any]] = None, stop: Optional[List[str]] = None, agent_tools: Optional[List[BaseTool]] = None, **kwargs):
        super().__init__(model=model, temperature=temperature)
        self.model_name = model
        self.api_key = api_key
        self.max_tokens = max_tokens
        self.tool_config = tool_config or {"function_calling_config": {"mode": "AUTO"}}
        self.stop = stop
        if agent_tools:
            self._gemini_tools_cache = self._convert_crewai_tools_to_gemini_format(agent_tools)
            print(f"CustomGeminiLLM __init__: Cached {len(self._gemini_tools_cache)} tools.")
        else:
            print("CustomGeminiLLM __init__: No agent_tools provided for caching.")

    def _remove_unwanted_fields(self, schema: Dict[str, Any]) -> Dict[str, Any]:
        if not isinstance(schema, dict):
            return schema

        schema.pop('title', None)

        if "properties" in schema:
            if "type" not in schema:
                schema["type"] = "object"
            for prop_name, prop_def in list(schema["properties"].items()):
                if isinstance(prop_def, dict):
                    prop_def.pop('default', None)
                    prop_def.pop('title', None)
                    self._remove_unwanted_fields(prop_def)
        elif schema.get("type") == "object" and "properties" not in schema:
            schema["properties"] = {}

        keys_to_delete = [k for k, v in schema.items() if k == 'default']
        for k in keys_to_delete:
            del schema[k]

        for k, v in schema.items():
            if isinstance(v, dict):
                self._remove_unwanted_fields(v)
            elif isinstance(v, list):
                for i, item in enumerate(v):
                    if isinstance(item, dict):
                        v[i] = self._remove_unwanted_fields(item)
        return schema

    def _convert_crewai_tools_to_gemini_format(self, tools: Optional[List[BaseTool]]) -> Optional[List[Dict[str, Any]]]:
        if not tools:
            return None
        gemini_tool_declarations = []
        for tool_instance in tools:
            tool_name = tool_instance.name
            tool_description = tool_instance.description
            if not hasattr(tool_instance, 'args_schema') or not tool_instance.args_schema:
                parameters_schema = {"type": "object", "properties": {}}
            else:
                try:
                    if hasattr(tool_instance.args_schema, 'model_json_schema'):
                        pydantic_schema = tool_instance.args_schema.model_json_schema()
                    else:
                        pydantic_schema = tool_instance.args_schema.schema()
                    cleaned_schema = self._remove_unwanted_fields(pydantic_schema.copy())
                    parameters_schema = cleaned_schema
                except Exception as e:
                    print(f"Error processing schema for tool {tool_name}: {e}")
                    parameters_schema = {"type": "object", "properties": {}}
            gemini_tool_declarations.append({
                "name": tool_name,
                "description": tool_description,
                "parameters": parameters_schema
            })
        final_tools_for_litellm = []
        for declaration in gemini_tool_declarations:
            final_tools_for_litellm.append({
                "type": "function",
                "function": declaration
            })
        return final_tools_for_litellm

    def call(self, messages: Union[str, List[Dict[str, str]]], tools: Optional[List[dict]] = None, callbacks: Optional[List[Any]] = None, **kwargs: Any) -> Union[str, Any]:
        print(f"CustomGeminiLLM CALL method invoked.")
        print(f"  CALL - Tools received by CustomLLM.call: {'Yes' if tools else 'No'}")
        print(f"  CALL - Callbacks received by CustomLLM.call: {'Yes' if callbacks else 'No'}")

        if isinstance(messages, str):
            processed_messages = [{"role": "user", "content": messages}]
        else:
            processed_messages = messages

        litellm_params = {
            "model": self.model_name,
            "messages": processed_messages,
            "api_key": self.api_key,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "stop": self.stop
        }

        # --- Proxy Addition ---
        proxy_url = os.getenv("LITELLM_PROXY_URL")
        if proxy_url:
            litellm_params["proxy"] = {
                "http": proxy_url,
                "https": proxy_url,
            }
            print(f"CustomGeminiLLM.call - Using proxy: {proxy_url}")
        else:
            print("CustomGeminiLLM.call - No proxy configured (LITELLM_PROXY_URL not set).")

        # --- Tool Handling (tools: null fix) ---
        final_tools_for_litellm = None
        received_tools_to_process = tools
        if not received_tools_to_process and self._gemini_tools_cache:
            print("  CALL - INFO: Tools argument was None, using cached tools.")
            received_tools_to_process = self._gemini_tools_cache

        if received_tools_to_process:
            cleaned_tools_for_litellm = []
            for tool_dict in received_tools_to_process:
                current_tool_def = tool_dict.copy()
                if current_tool_def.get("type") == "function" and "function" in current_tool_def:
                    func_def = current_tool_def["function"].copy()
                    if "parameters" in func_def:
                        func_def["parameters"] = self._remove_unwanted_fields(func_def["parameters"].copy())
                    current_tool_def["function"] = func_def
                    cleaned_tools_for_litellm.append(current_tool_def)
                else:
                    cleaned_tools_for_litellm.append(tool_dict)
            final_tools_for_litellm = cleaned_tools_for_litellm

        if final_tools_for_litellm:
            litellm_params["tools"] = final_tools_for_litellm
            fc_config = self.tool_config.get("function_calling_config", {})
            mode = fc_config.get("mode", "AUTO").upper()
            allowed_names = fc_config.get("allowed_function_names")

            if mode == "ANY" and allowed_names:
                litellm_params["tool_choice"] = {
                    "type": "function",
                    "function": {"name": allowed_names[0]}
                }
            elif mode in ["AUTO", "ANY", "NONE"]:
                litellm_params["tool_choice"] = mode.lower()
            else:
                litellm_params["tool_choice"] = "auto"
            print(f"CustomGeminiLLM DEBUG: Setting tool_choice to: {litellm_params['tool_choice']}")

        if callbacks:
            litellm_params["callbacks"] = callbacks

        try:
            print(f"CustomGeminiLLM.call - LiteLLM PARAMS (Preview): model={litellm_params['model']}, msgs_count={len(litellm_params['messages'])}, tools={'Yes' if 'tools' in litellm_params else 'No'}, tool_choice={litellm_params.get('tool_choice')}, proxy={'Yes' if 'proxy' in litellm_params else 'No'}")
            response = litellm.completion(**litellm_params)
        except Exception as e:
            print(f"CRITICAL ERROR: LiteLLM completion call failed: {e}")
            if callbacks:
                for handler in callbacks:
                    if hasattr(handler, 'on_llm_error'):
                        try:
                            handler.on_llm_error(error=e, llm=self, **kwargs)
                        except Exception as cb_err:
                            print(f"Error in callback on_llm_error: {cb_err}")
            raise

        llm_message_response = response.choices[0].message
        if hasattr(llm_message_response, 'tool_calls') and llm_message_response.tool_calls:
            print(f"CustomGeminiLLM.call - Detected tool_calls: {llm_message_response.tool_calls}")
            # --- ReAct Format Workaround (AttributeError fix) ---
            tool_call = llm_message_response.tool_calls[0]
            action = tool_call.function.name
            action_input = tool_call.function.arguments
            react_string = f"Action: {action}\nAction Input: {action_input}"
            print(f"CustomGeminiLLM.call - Returning ReAct string: {react_string}")
            return react_string
        else:
            print(f"CustomGeminiLLM.call - Returning text content.")
            return llm_message_response.content or ""

    def get_token_counter_instance(self):
        class GeminiTokenCounter:
            def __init__(self, model_name):
                self.model_name = model_name

            def count_tokens(self, text: Union[str, List[Dict[str,str]]]) -> int:
                try:
                    if isinstance(text, list):
                        return litellm.token_counter(model=self.model_name, messages=text)
                    return litellm.token_counter(model=self.model_name, text=str(text))
                except Exception as e:
                    print(f"Warning: Token counting failed ({e}), falling back to rough estimate.")
                    if isinstance(text, list):
                        return sum(len(str(m.get("content","")).split()) for m in text)
                    return len(str(text).split())
        return GeminiTokenCounter(model_name=self.model_name)


# --- CustomSGLangLLM (from hybrid_rag/custom_llm.py) ---
class CustomSGLangLLM(CrewAIBaseLLM):
    endpoint_url: str = SGLANG_API_URL_FOR_LLM
    model_name: str = "qwen2-3b-instruct"
    temperature: float = 0.1
    max_new_tokens_val: int = 1024

    def __init__(self, endpoint: Optional[str] = None, model: Optional[str] = None, temperature: Optional[float] = None, max_new_tokens: Optional[int] = None, **kwargs: Any):
        super().__init__(**kwargs)
        if endpoint: self.endpoint_url = endpoint
        if model: self.model_name = model
        if temperature is not None: self.temperature = temperature
        if max_new_tokens is not None: self.max_new_tokens_val = max_new_tokens
        print(f"CustomSGLangLLM initialized. Endpoint: {self.endpoint_url}, Model: {self.model_name}, Temp: {self.temperature}, MaxTokens: {self.max_new_tokens_val}")

    def _prepare_sglang_prompt(self, messages: Sequence[Dict[str, str]]) -> str:
        prompt_str = ""
        for message in messages:
            role = message.get("role")
            content = message.get("content")
            if role and content:
                prompt_str += f"<|im_start|>{role}\n{content}<|im_end|>\n"
        prompt_str += "<|im_start|>assistant\n"
        return prompt_str

    def call(self, messages: Sequence[Dict[str, str]], **kwargs: Any) -> str:
        print(f"CustomSGLangLLM.call received messages: {messages}")
        sglang_prompt = self._prepare_sglang_prompt(messages)
        print(f"CustomSGLangLLM.call prepared sglang_prompt (first 200 chars): {sglang_prompt[:200]}...")
        stop_sequences_for_sglang = kwargs.get("stop", ["<|im_end|>", "<|endoftext|>"])

        try:
            try:
                loop = asyncio.get_running_loop()
            except RuntimeError:
                loop = None

            async def async_runner():
                return await call_sglang_llm(
                    prompt=sglang_prompt,
                    temperature=self.temperature,
                    max_new_tokens=self.max_new_tokens_val,
                    stop_sequences=stop_sequences_for_sglang
                )

            if loop and loop.is_running():
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                    future = executor.submit(asyncio.run, async_runner())
                    response_text = future.result(timeout=120)
            else:
                response_text = asyncio.run(async_runner())

        except Exception as e:
            print(f"CustomSGLangLLM.call: Error during SGLang call: {type(e).__name__} - {e}")
            traceback.print_exc()
            return f"LLM_CALL_ERROR: 调用SGLang服务失败 - {str(e)}"

        if response_text is None:
            print("CustomSGLangLLM.call: SGLang returned None.")
            return "LLM_CALL_ERROR: SGLang服务未返回任何文本。"

        print(f"CustomSGLangLLM.call: SGLang returned text (first 200 chars): {response_text[:200]}...")
        return response_text

    def get_token_ids(self, text: str) -> List[int]:
        print("CustomSGLangLLM.get_token_ids: Not implemented, returning empty list.")
        return []

    @property
    def support_function_calling(self) -> bool:
        return False

    @property
    def support_stop_words(self) -> bool:
        return True

    @property
    def available_models(self) -> List[str]:
        return [self.model_name]

    @property
    def context_window(self) -> int:
        return 32768

    @property
    def identifying_params(self) -> Dict[str, Any]:
        return {
            "model": self.model_name,
            "endpoint_url": self.endpoint_url,
            "temperature": self.temperature,
            "max_new_tokens": self.max_new_tokens_val,
        }
    --- END OF FILE CONTENT ---

  File: llm_interface.py
    --- START OF FILE CONTENT ---
# zhz_agent/llm.py (renamed to llm_interface.py as per typical module naming)
# or more accurately, this is the content for llm_interface.py based on the inputs

import os
import httpx  # 用于异步HTTP请求
import json  # 用于处理JSON数据
import asyncio  # 用于 asyncio.to_thread
from typing import List, Dict, Any, Optional, Union # Added Union
from dotenv import load_dotenv
import traceback  # Ensure traceback is imported

from zhz_rag.utils.common_utils import log_interaction_data # 导入通用日志函数
from zhz_rag.config.constants import NEW_KG_SCHEMA_DESCRIPTION # <--- 确保导入这个常量
from zhz_rag.utils.common_utils import log_interaction_data # 导入通用日志函数
from zhz_rag.config.pydantic_models import ExtractedEntitiesAndRelationIntent
# 提示词导入
from zhz_rag.llm.rag_prompts import (
    get_answer_generation_messages, 
    get_clarification_question_messages,
    get_entity_relation_extraction_messages, # <--- 添加导入
    # get_cypher_generation_messages_with_templates # 这个可以暂时保留或注释掉
)


import logging
import re
import uuid  # 用于生成 interaction_id
from datetime import datetime, timezone  # 用于生成时间戳
import litellm # <--- 确保这个导入存在

load_dotenv()  # 确保加载.env文件

_LLM_DIR = os.path.dirname(os.path.abspath(__file__))
RAG_INTERACTION_LOGS_DIR = os.path.join(_LLM_DIR, '..', '..', 'stored_data', 'rag_interaction_logs')


if not os.path.exists(RAG_INTERACTION_LOGS_DIR):
    try:
        os.makedirs(RAG_INTERACTION_LOGS_DIR)
    except Exception:
        pass

def get_llm_log_filepath() -> str:
    """获取当前LLM交互日志文件的完整路径，按天分割。"""
    today_str = datetime.now(timezone.utc).strftime("%Y%m%d")
    return os.path.join(RAG_INTERACTION_LOGS_DIR, f"rag_interactions_{today_str}.jsonl")

async def log_llm_interaction_to_jsonl(interaction_data: Dict[str, Any]):
    """
    将单条LLM交互数据异步追加到JSONL文件中。
    (This function might be part of what log_interaction_data uses, or an alternative logger. Keeping for completeness from original llm.py)
    """
    filepath = get_llm_log_filepath()
    try:
        def _write_sync():
            with open(filepath, 'a', encoding='utf-8') as f:
                f.write(json.dumps(interaction_data, ensure_ascii=False) + "\n")
        await asyncio.to_thread(_write_sync)
        llm_py_logger.debug(f"Successfully logged LLM interaction to {filepath}")
    except Exception as e:
        llm_py_logger.error(f"Failed to log LLM interaction to {filepath}: {e}", exc_info=True)

llm_py_logger = logging.getLogger("LLMUtilsLogger")
llm_py_logger.setLevel(os.getenv("LLM_LOG_LEVEL", "INFO").upper())

if not llm_py_logger.hasHandlers():
    _llm_console_handler = logging.StreamHandler()
    _llm_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
    _llm_console_handler.setFormatter(_llm_formatter)
    llm_py_logger.addHandler(_llm_console_handler)
    llm_py_logger.propagate = False

llm_py_logger.info("--- LLMUtilsLogger configured ---")

NO_ANSWER_PHRASE_ANSWER_CLEAN = "根据目前提供的资料，我无法找到关于您问题的明确信息。"
NO_ANSWER_PHRASE_KG_CLEAN = "从知识图谱中未找到直接相关信息。"
UNIQUE_STOP_TOKEN = "<|im_endofunable|>"
NO_ANSWER_PHRASE_ANSWER_WITH_STOP_TOKEN = f"{NO_ANSWER_PHRASE_ANSWER_CLEAN}{UNIQUE_STOP_TOKEN}"
NO_ANSWER_PHRASE_KG_WITH_STOP_TOKEN = f"{NO_ANSWER_PHRASE_KG_CLEAN}{UNIQUE_STOP_TOKEN}"

# Placeholder for the schema description. Replace with actual schema.
NEW_KG_SCHEMA_DESCRIPTION = """
{
  "node_labels": ["Person", "Project", "Task", "Document", "Region", "SalesAmount", "Product"],
  "relationship_types": ["WORKS_ON", "ASSIGNED_TO", "HAS_DOCUMENT", "HAS_SALES_AMOUNT", "RELATED_TO"],
  "node_properties": {
    "Person": [{"property": "name", "type": "STRING"}, {"property": "role", "type": "STRING"}],
    "Project": [{"property": "name", "type": "STRING"}, {"property": "status", "type": "STRING"}],
    "Task": [{"property": "name", "type": "STRING"}, {"property": "status", "type": "STRING"}, {"property": "priority", "type": "STRING"}],
    "Document": [{"property": "id", "type": "STRING"}, {"property": "title", "type": "STRING"}, {"property": "type", "type": "STRING"}],
    "Region": [{"property": "name", "type": "STRING"}],
    "SalesAmount": [{"property": "period", "type": "STRING"}, {"property": "numeric_amount", "type": "FLOAT"}, {"property": "unit", "type": "STRING"}],
    "Product": [{"property": "name", "type": "STRING"}, {"property": "category", "type": "STRING"}]
  },
  "relationship_properties": {},
  "output_format_guidance": {
    "description": "Your response MUST be a JSON object with two fields: 'status' and 'query'.",
    "status_field": {
      "description": "The 'status' field can be one of two values: 'success' or 'unable_to_generate'.",
      "success": "If you can generate a Cypher query, status should be 'success'.",
      "unable_to_generate": "If you cannot generate a Cypher query based on the question and schema, status should be 'unable_to_generate'."
    },
    "query_field": {
      "description": "The 'query' field contains the Cypher query as a string if status is 'success'.",
      "success_example": "MATCH (n) RETURN n LIMIT 1",
      "unable_to_generate_example": "无法生成Cypher查询."
    }
  },
  "examples": [
    {
      "User Question": "Who is task 'FixBug123' assigned to?",
      "Your EXACT Response": {
        "status": "success",
        "query": "MATCH (t:Task {name: 'FixBug123'})<-[:ASSIGNED_TO]-(p:Person) RETURN p.name AS assignedTo"
      }
    },
    {
      "User Question": "What is the color of the sky?",
      "Your EXACT Response": {
        "status": "unable_to_generate",
        "query": "无法生成Cypher查询."
      }
    }
  ]
}
"""

LLM_API_URL = os.getenv("SGLANG_API_URL", "http://localhost:8088/v1/chat/completions")

async def call_llm_via_openai_api_local_only( # 改个名字以示区分
    prompt: Union[str, List[Dict[str, str]]], # prompt 可以是字符串或消息列表
    temperature: float = 0.2,
    max_new_tokens: Optional[int] = 1024,
    stop_sequences: Optional[List[str]] = None,
    task_type: str = "unknown_local_llm_call",
    user_query_for_log: Optional[str] = None,
    model_name_for_log: str = "local_qwen_via_openai_api_compat",
    application_version_for_log: str = "0.1.0_local_compat"
) -> Optional[str]:
    llm_py_logger.info(f"Calling LOCAL LLM ({model_name_for_log}) for task: {task_type}, Target API: {LLM_API_URL}")

    current_messages: List[Dict[str, str]]
    original_prompt_for_log: str

    if isinstance(prompt, str): # 假设旧的SGLang风格的prompt字符串
        original_prompt_for_log = prompt
        # 尝试从SGLang格式转换为OpenAI messages格式
        # 这个转换逻辑需要根据您SGLang prompt的具体格式来定
        # 一个简化的例子，可能需要调整：
        current_messages = []
        # 简单的假设：如果prompt以<|im_start|>system开头，则提取system和user部分
        if prompt.startswith("<|im_start|>system"):
            parts = prompt.split("<|im_start|>")
            for part in parts:
                if not part.strip(): continue
                role_content = part.split("<|im_end|>")[0].strip()
                if "\n" in role_content:
                    role, content = role_content.split("\n", 1)
                    current_messages.append({"role": role.strip().lower(), "content": content.strip()})
        if not current_messages: # 如果转换失败或不是SGLang格式，则认为是单个user消息
            current_messages = [{"role": "user", "content": prompt}]
    elif isinstance(prompt, list):
        current_messages = prompt
        original_prompt_for_log = "Messages list provided directly."
    else:
        llm_py_logger.error(f"Invalid 'prompt' argument type: {type(prompt)}")
        return None

    payload = {
        "model": model_name_for_log, # 这个model名会被本地服务忽略，但符合OpenAI格式
        "messages": current_messages,
        "temperature": temperature,
        "max_tokens": max_new_tokens,
    }
    if stop_sequences:
        payload["stop"] = stop_sequences

    headers = {"Content-Type": "application/json"}
    llm_parameters_for_log = {k:v for k,v in payload.items() if k not in ['messages', 'model']} # model已在顶层记录
    raw_llm_output_text = None
    error_info = None

    try:
        async with httpx.AsyncClient(timeout=120.0) as client:
            response = await client.post(LLM_API_URL, json=payload, headers=headers) # LLM_API_URL 指向本地服务
            response.raise_for_status()
            response_json = response.json()
            if response_json.get("choices") and response_json["choices"][0].get("message"):
                raw_llm_output_text = response_json["choices"][0]["message"].get("content", "")
            else:
                raw_llm_output_text = "[[LLM_RESPONSE_MALFORMED_CHOICES_OR_MESSAGE_LOCAL]]"
            llm_py_logger.info(f"FULL Local LLM Raw Output for task '{task_type}': >>>{raw_llm_output_text}<<<")

    except Exception as e:
        llm_py_logger.error(f"Error calling local LLM service: {e}", exc_info=True)
        error_info = str(e)
        # 确保记录错误
        log_error_data = {
            "interaction_id": str(uuid.uuid4()), "timestamp_utc": datetime.now(timezone.utc).isoformat(),
            "task_type": task_type + "_local_error", "user_query_for_task": user_query_for_log,
            "llm_input_messages": current_messages,
            "llm_input_original_prompt_if_string": original_prompt_for_log if isinstance(prompt, str) else None,
            "llm_parameters": llm_parameters_for_log,
            "raw_llm_output": f"Error: {error_info}. Partial raw output: {str(raw_llm_output_text)[:200] if raw_llm_output_text else 'N/A'}",
            "error_details": traceback.format_exc(), "application_version": application_version_for_log
        }
        await log_interaction_data(log_error_data)
        return None # 出错时返回None

    # 记录成功的调用
    log_success_data = {
        "interaction_id": str(uuid.uuid4()), "timestamp_utc": datetime.now(timezone.utc).isoformat(),
        "task_type": task_type, "user_query_for_task": user_query_for_log,
        "llm_input_messages": current_messages,
        "llm_input_original_prompt_if_string": original_prompt_for_log if isinstance(prompt, str) else None,
        "llm_parameters": llm_parameters_for_log,
        "raw_llm_output": raw_llm_output_text, "application_version": application_version_for_log
    }
    await log_interaction_data(log_success_data)
    return raw_llm_output_text

async def generate_cypher_query(user_question: str) -> Optional[str]: # kg_schema_description 参数可以移除了，因为它已包含在新的prompt函数中
    llm_py_logger.info(f"Attempting to generate Cypher query (template-based) for: '{user_question}' via local service.")

    messages_for_llm = get_cypher_generation_messages_with_templates(user_question)

    cypher_stop_sequences = ['<|im_end|>', '```'] # 如果输出包含markdown的json块

    llm_response_json_str = await call_llm_via_openai_api_local_only( 
        prompt=messages_for_llm,
        temperature=0.0, # 对于精确的JSON和Cypher生成，温度设为0
        max_new_tokens=1024, # 允许足够的空间输出JSON和Cypher
        stop_sequences=cypher_stop_sequences,
        task_type="cypher_generation_template_based_local_service",
        user_query_for_log=user_question,
        model_name_for_log="qwen3_gguf_cypher_template_local"
    )

    if not llm_response_json_str:
        llm_py_logger.warning(f"LLM call for Cypher (template-based) returned None or empty. User question: '{user_question}'")
        return json.dumps({"status": "unable_to_generate", "query": "无法生成Cypher查询."}) # 始终返回JSON字符串

    cleaned_json_str = llm_response_json_str.strip()
    if cleaned_json_str.startswith("```json"):
        cleaned_json_str = cleaned_json_str[len("```json"):].strip()
    if cleaned_json_str.endswith("```"):
        cleaned_json_str = cleaned_json_str[:-len("```")].strip()

    try:

        parsed_for_validation = json.loads(cleaned_json_str)
        if isinstance(parsed_for_validation, dict) and \
           "status" in parsed_for_validation and \
           "query" in parsed_for_validation:
            llm_py_logger.info(f"LLM returned valid JSON for Cypher (template-based): {cleaned_json_str}")
            return cleaned_json_str
        else:
            llm_py_logger.warning(f"LLM output for Cypher (template-based) was JSON but not expected structure: {cleaned_json_str}")
            return json.dumps({"status": "unable_to_generate", "query": "LLM输出JSON结构错误."})
    except json.JSONDecodeError:
        llm_py_logger.error(f"Failed to parse JSON response for Cypher (template-based): '{cleaned_json_str}'", exc_info=True)
        # 如果不是有效的JSON，但包含"MATCH"，可能LLM直接输出了Cypher，尝试包装它
        if "MATCH" in cleaned_json_str.upper() or "RETURN" in cleaned_json_str.upper():
             llm_py_logger.warning("LLM output for Cypher (template-based) was not JSON but looks like Cypher, wrapping it.")
             return json.dumps({"status": "success", "query": cleaned_json_str})
        return json.dumps({"status": "unable_to_generate", "query": "LLM输出非JSON格式."})

async def generate_answer_from_context(user_query: str, context_str: str) -> Optional[str]: # context 参数名改为 context_str
    llm_py_logger.info(f"Generating answer for query: '{user_query[:100]}...' using provided context.")
    messages_for_llm = get_answer_generation_messages(user_query, context_str)
    raw_answer = await call_llm_via_openai_api_local_only(
        prompt=messages_for_llm, 
        temperature=0.05,
        max_new_tokens=1024, 
        stop_sequences=['<|im_end|>', UNIQUE_STOP_TOKEN],
        task_type="answer_generation_from_context",
        user_query_for_log=user_query,
        model_name_for_log="qwen3_gguf_answer_gen"
    )
    
    if raw_answer and raw_answer.strip() and \
       raw_answer.strip() != "[[LLM_RESPONSE_MALFORMED_CHOICES_OR_MESSAGE]]" and \
       raw_answer.strip() != "[[CONTENT_NOT_FOUND]]":
        
        final_answer = raw_answer.strip()
        if final_answer == NO_ANSWER_PHRASE_ANSWER_CLEAN: # NO_ANSWER_PHRASE_ANSWER_CLEAN 可以从 rag_prompts.py 导入或在 llm_interface.py 中也定义
            llm_py_logger.info("LLM indicated unable to answer from context.")
        return final_answer
    else:
        llm_py_logger.warning(f"Answer generation returned None, empty, or placeholder. Query: {user_query}")
        return NO_ANSWER_PHRASE_ANSWER_CLEAN

async def generate_simulated_kg_query_response(user_query: str, kg_schema_description: str, kg_data_summary_for_prompt: str) -> Optional[str]:
    prompt_str = f"""<|im_start|>system
你是一个知识图谱查询助手。你的任务是根据用户提出的问题、知识图谱Schema描述和图谱中的数据摘要，直接抽取出与问题最相关的1-2个事实片段作为答案。
只输出事实片段，不要解释，不要生成Cypher语句，不要包含任何额外对话或标记。
如果找不到直接相关的事实，请**直接且完整地**回答：“{NO_ANSWER_PHRASE_KG_WITH_STOP_TOKEN}”<|im_end|>
<|im_start|>user
知识图谱Schema描述:
{kg_schema_description}

知识图谱数据摘要: 
{kg_data_summary_for_prompt}

用户问题: {user_query}<|im_end|>
<|im_start|>assistant
"""
    stop_sequences = ["<|im_end|>", UNIQUE_STOP_TOKEN]
    return await call_llm_via_openai_api_local_only(
        prompt=prompt_str,
        temperature=0.5,
        max_new_tokens=256,
        stop_sequences=stop_sequences,
        task_type="simulated_kg_query_response",
        user_query_for_log=user_query
    )

async def generate_expanded_queries(original_query: str) -> List[str]:
    prompt_str = f"""<|im_start|>system
你是一个专家查询分析师。根据用户提供的查询，生成3个不同但相关的子问题，以探索原始查询的不同方面。这些子问题将用于检索更全面的信息。
你的回答必须是一个JSON数组（列表），其中每个元素是一个字符串（子问题）。
只输出JSON数组，不要包含任何其他解释、对话标记或代码块。

示例:
用户查询: "公司年度财务报告和未来一年的预算规划"
助手:
[
  "公司最近的年度财务报告总结是什么？",
  "未来一年的详细预算规划有哪些主要构成？",
  "对比往年，公司财务状况有何显著变化？"
]<|im_end|>
<|im_start|>user
原始查询: {original_query}<|im_end|>
<|im_start|>assistant
"""
    stop_sequences = ["<|im_end|>"]
    
    llm_py_logger.info(f"调用LLM API进行查询扩展 (Prompt长度: {len(prompt_str)} 字符)...")
    llm_output = await call_llm_via_openai_api_local_only(
        prompt=prompt_str,
        temperature=0.7,
        max_new_tokens=512,
        stop_sequences=stop_sequences,
        task_type="query_expansion",
        user_query_for_log=original_query
    )
    expanded_queries = []
    if llm_output:
        try:
            json_str = llm_output.strip()
            if json_str.startswith("```json"):
                json_str = json_str[len("```json"):].strip()
            if json_str.endswith("```"):
                json_str = json_str[:-len("```")].strip()
            
            parsed_queries = json.loads(json_str)
            if isinstance(parsed_queries, list) and all(isinstance(q, str) for q in parsed_queries):
                expanded_queries = parsed_queries
                llm_py_logger.info(f"LLM成功生成 {len(expanded_queries)} 个扩展查询。")
            else:
                llm_py_logger.warning(f"LLM生成的扩展查询JSON格式不符合预期 (不是字符串列表): {llm_output[:200]}...")
        except json.JSONDecodeError as e:
            llm_py_logger.error(f"解析LLM扩展查询JSON失败: {e}. 原始输出: {llm_output[:200]}...", exc_info=True)
        except Exception as e:
            llm_py_logger.error(f"处理LLM扩展查询时发生未知错误: {e}. 原始输出: {llm_output[:200]}...", exc_info=True)
    else:
        llm_py_logger.warning("LLM未能生成扩展查询。")

    # Always include the original query
    if original_query not in expanded_queries:
        expanded_queries.append(original_query)
    return expanded_queries


async def generate_clarification_question(original_query: str, uncertainty_reason: str) -> Optional[str]:
    llm_py_logger.info(f"调用LLM API生成澄清问题。原始查询: '{original_query}', 原因: '{uncertainty_reason}'")
    messages_for_llm = get_clarification_question_messages(original_query, uncertainty_reason)

    clarification_question_raw = await call_llm_via_openai_api_local_only(
        prompt=messages_for_llm,
        temperature=0.5,
        max_new_tokens=128,
        stop_sequences=['<|im_end|>'], # 对于Qwen系列，<|im_end|> 是一个常见的结束标记
        task_type="clarification_question_generation",
        user_query_for_log=original_query
    )
    
    if not clarification_question_raw or not clarification_question_raw.strip():
        llm_py_logger.warning("LLM未能生成澄清问题，返回默认提示。")
        return "抱歉，我不太理解您的意思，请您再具体说明一下。"  
    cleaned_question_from_llm = clarification_question_raw.strip()
    llm_py_logger.debug(f"LLM原始澄清输出 (清理后): '{cleaned_question_from_llm}'")
    potential_lines = cleaned_question_from_llm.splitlines()
    
    final_extracted_question = None

    for line in reversed(potential_lines):
        line_stripped = line.strip()
        if not line_stripped: # 跳过空行
            continue
        if line_stripped.endswith("？") or line_stripped.endswith("?"):
            if not (line_stripped.startswith("好的，") or \
                    line_stripped.startswith("首先，") or \
                    line_stripped.startswith("因此，") or \
                    line_stripped.startswith("所以，") or \
                    line_stripped.startswith("根据这个原因，") or \
                    "我需要生成一个" in line_stripped or \
                    "可能的澄清问题是" in line_stripped or \
                    "澄清问题应该是" in line_stripped or \
                    "接下来，" in line_stripped):
                final_extracted_question = line_stripped
                llm_py_logger.info(f"通过行分割和问号结尾提取到澄清问题: '{final_extracted_question}'")
                break 
        elif any(line_stripped.startswith(prefix) for prefix in ["请问您", "您对", "您具体指的是"]):
            final_extracted_question = line_stripped
            llm_py_logger.info(f"通过行分割和特定前缀提取到澄清问题: '{final_extracted_question}'")
            break

    if final_extracted_question:
        llm_py_logger.info(f"LLM成功生成并提取到最终澄清问题: {final_extracted_question}")
        return final_extracted_question
    else:
        potential_sentences = re.split(r'(?<=[。？！?])\s*', cleaned_question_from_llm)
        for sentence in reversed(potential_sentences):
            sentence_stripped = sentence.strip()
            if not sentence_stripped:
                continue
            if sentence_stripped.endswith("？") or sentence_stripped.endswith("?") or \
               any(sentence_stripped.startswith(prefix) for prefix in ["请问您", "您是想", "您具体指的是", "关于您提到的"]):
                if not (sentence_stripped.startswith("好的，") or \
                        sentence_stripped.startswith("首先，") or \
                        "我需要生成一个" in sentence_stripped or \
                        "可能的澄清问题是" in sentence_stripped): # 避免选择思考过程
                    final_extracted_question = sentence_stripped
                    llm_py_logger.info(f"通过句子分割和启发式规则提取到澄清问题: '{final_extracted_question}'")
                    break
        
        if final_extracted_question:
            llm_py_logger.info(f"LLM成功生成并提取到最终澄清问题 (后备逻辑): {final_extracted_question}")
            return final_extracted_question
        else:
            llm_py_logger.warning(f"未能通过所有启发式规则从LLM输出中提取明确的澄清问句。原始输出为: '{cleaned_question_from_llm}'。将返回默认澄清。")

            if len(cleaned_question_from_llm) < 70 and (cleaned_question_from_llm.endswith("？") or cleaned_question_from_llm.endswith("?")): # 70是个经验值
                 llm_py_logger.info(f"原始输出较短且以问号结尾，将其作为澄清问题返回: '{cleaned_question_from_llm}'")
                 return cleaned_question_from_llm
            return "抱歉，我不太理解您的意思，请您再具体说明一下。"


async def generate_clarification_options(original_query: str, uncertainty_reason: str) -> List[str]:
    prompt_str = f"""<|im_start|>system
你是一个智能助手，擅长根据用户查询的模糊性提供具体的澄清选项。
你的任务是根据用户原始查询和系统检测到的不确定性原因，生成3-5个具体的、可供用户选择的澄清选项。
每个选项都应该是一个简洁的短语或问题，帮助用户明确其意图。
你的回答必须是一个JSON数组（列表），其中每个元素是一个字符串（澄清选项）。
只输出JSON数组，不要包含任何其他解释、对话标记或代码块。

示例:
用户查询: "帮我预定机票。"
不确定性原因: "缺少出发城市、目的地、日期等信息。"
助手:
[
  "请问您想从哪个城市出发？",
  "请问您的目的地是哪里？",
  "请问您希望在哪一天出行？",
  "您有偏好的航空公司或舱位等级吗？"
]<|im_end|>
<|im_start|>user
用户原始查询: {original_query}
不确定性原因: {uncertainty_reason}

请生成澄清选项:<|im_end|>
<|im_start|>assistant
"""
    stop_sequences = ["<|im_end|>"]
    llm_py_logger.info(f"调用LLM API生成澄清选项 (Prompt长度: {len(prompt_str)} 字符)...")
    llm_output = await call_llm_via_openai_api_local_only(
        prompt=prompt_str,
        temperature=0.7,
        max_new_tokens=256,
        stop_sequences=stop_sequences,
        task_type="clarification_options_generation",
        user_query_for_log=original_query
    )

    options = []
    if llm_output:
        try:
            json_str = llm_output.strip()
            if json_str.startswith("```json"):
                json_str = json_str[len("```json"):].strip()
            if json_str.endswith("```"):
                json_str = json_str[:-len("```")].strip()
            
            parsed_options = json.loads(json_str)
            if isinstance(parsed_options, list) and all(isinstance(o, str) for o in parsed_options):
                options = parsed_options
                llm_py_logger.info(f"LLM成功生成 {len(options)} 个澄清选项。")
            else:
                llm_py_logger.warning(f"LLM生成的澄清选项JSON格式不符合预期 (不是字符串列表): {llm_output[:200]}...")
        except json.JSONDecodeError as e:
            llm_py_logger.error(f"解析LLM澄清选项JSON失败: {e}. 原始输出: {llm_output[:200]}...", exc_info=True)
        except Exception as e:
            llm_py_logger.error(f"处理LLM澄清选项时发生未知错误: {e}. 原始输出: {llm_output[:200]}...", exc_info=True)
    else:
        llm_py_logger.warning("LLM未能生成澄清选项。")
    
    if not options:
        options.append("请提供更多详细信息。")
    
    return options


INTENT_CLASSIFICATION_JSON_SCHEMA = {
    "type": "object",
    "properties": {
        "clarification_needed": {"type": "boolean"},
        "reason": {"type": "string"}
    },
    "required": ["clarification_needed", "reason"]
}

async def generate_intent_classification(user_query: str) -> Dict[str, Any]:
    llm_py_logger.info(f"Generating intent classification for query: '{user_query[:100]}...' using Gemini.")
    
    # 针对Gemini优化的Prompt，强调直接输出JSON
    system_prompt_for_intent = f"""你是一个智能意图分类器。你的任务是分析用户查询，判断该查询是否清晰明确，或者是否存在歧义、信息不足导致需要进一步澄清。
如果查询包含具体的命名实体（如人名“张三”、项目名“项目X”、产品名“新产品A”等），并且问题是关于这些实体的特定信息（例如“张三的职位是什么？”、“项目X的截止日期是哪天？”、“新产品A的功能有哪些？”），则通常认为查询是清晰的，不需要澄清。
只有当查询缺少定位关键信息所必需的核心实体，或者询问的范围过于宽泛无法直接操作时，才需要澄清。

如果查询需要澄清，请说明原因。
你的【唯一输出】必须是一个严格符合以下结构的JSON对象，不要包含任何其他文本、解释或markdown标记:
{{
  "clarification_needed": true/false,
  "reason": "如果需要澄清，请简要说明原因；如果不需要，则为空字符串。"
}}

示例1 (需要澄清 - 信息不足):
用户查询: "帮我预定明天去上海的机票。"
助手 JSON 输出:
{{
  "clarification_needed": true,
  "reason": "缺少出发城市、具体时间（上午/下午/晚上）、舱位等级等信息。"
}}

示例2 (不需要澄清 - 清晰):
用户查询: "公司最新的销售额报告在哪里可以找到？"
助手 JSON 输出:
{{
  "clarification_needed": false,
  "reason": ""
}}
"""
    
    messages_for_gemini = [
        {"role": "system", "content": system_prompt_for_intent},
        {"role": "user", "content": f"用户查询: {user_query}"}
    ]

    # 从环境变量获取Gemini配置
    gemini_model_name = os.getenv("CLOUD_LLM_MODEL_NAME_FOR_LITELLM", "gemini/gemini-1.5-flash-latest")
    gemini_api_key = os.getenv("GEMINI_API_KEY") # 或者 GOOGLE_API_KEY
    proxy_url = os.getenv("LITELLM_PROXY_URL")

    litellm_params: Dict[str, Any] = {
        "model": gemini_model_name,
        "messages": messages_for_gemini,
        "api_key": gemini_api_key,
        "temperature": 0.1, 
        "max_tokens": 256,  # 意图分类的JSON输出通常较短
        # "response_format": {"type": "json_object"} # LiteLLM的Gemini集成可能尚不支持此参数，暂时注释
    }
    if proxy_url:
        # LiteLLM 的 proxy 参数期望一个字典，或者直接是一个字符串URL (取决于LiteLLM版本和具体实现)
        # 为保险起见，我们按文档常见的字典格式提供
        litellm_params["proxy"] = {
            "http": proxy_url,
            "https": proxy_url,
        }
        # 或者，如果您的LiteLLM版本支持直接传递字符串URL作为代理：
        # litellm_params["api_base"] = proxy_url # 这会将代理用于所有请求，如果Gemini也通过此代理
        # litellm_params["base_url"] = proxy_url # 有些版本用 base_url
        # 更通用的方式是设置环境变量 HTTP_PROXY 和 HTTPS_PROXY，LiteLLM通常会读取它们
        # 但为了显式，我们这里尝试通过参数传递给litellm.acompletion

    llm_py_logger.info(f"Calling Gemini (via LiteLLM) for intent classification. Model: {gemini_model_name}")
    debug_params = {k:v for k,v in litellm_params.items() if k not in ['messages', 'api_key']}
    llm_py_logger.debug(f"LiteLLM params for intent (excluding messages & api_key): {debug_params}")
    
    raw_gemini_output_text = None
    error_info_intent = None
    parsed_result_dict: Optional[Dict[str, Any]] = None # 用于存储最终解析结果

    try:
        response = await litellm.acompletion(**litellm_params)
        if response and response.choices and response.choices[0].message and response.choices[0].message.content:
            raw_gemini_output_text = response.choices[0].message.content.strip()
            llm_py_logger.info(f"Gemini intent classification raw output: {raw_gemini_output_text[:300]}...")
            
            # 尝试解析JSON (与之前的提取逻辑类似)
            json_str_candidate = raw_gemini_output_text
            # 1. 尝试从 markdown block 中提取
            markdown_match = re.search(r"```json\s*(\{[\s\S]*?\})\s*```", json_str_candidate, re.DOTALL)
            if markdown_match:
                json_str_candidate = markdown_match.group(1)
                llm_py_logger.debug(f"Extracted JSON candidate from markdown: {json_str_candidate[:200]}...")
            
            # 2. 如果没有markdown，或者提取后仍然不是纯JSON，尝试直接解析或查找第一个 '{' 和最后一个 '}'
            try:
                parsed_result_dict = json.loads(json_str_candidate)
            except json.JSONDecodeError: # 如果直接解析失败
                first_brace = json_str_candidate.find('{')
                last_brace = json_str_candidate.rfind('}')
                if first_brace != -1 and last_brace != -1 and last_brace > first_brace:
                    json_str_candidate = json_str_candidate[first_brace : last_brace+1]
                    llm_py_logger.debug(f"Extracted JSON candidate by braces: {json_str_candidate[:200]}...")
                    try:
                        parsed_result_dict = json.loads(json_str_candidate)
                    except json.JSONDecodeError as e_json_brace:
                        error_info_intent = f"Failed to decode JSON from Gemini intent (braces): {e_json_brace}"
                        llm_py_logger.error(error_info_intent, exc_info=True)
                else: # 没有找到有效的花括号对
                    error_info_intent = "No valid JSON object found in Gemini intent output."
                    llm_py_logger.error(error_info_intent + f" Raw: {raw_gemini_output_text[:200]}")
            
            # 验证解析后的JSON结构
            if parsed_result_dict and isinstance(parsed_result_dict, dict) and \
               "clarification_needed" in parsed_result_dict and \
               "reason" in parsed_result_dict:
                llm_py_logger.info(f"Gemini successfully classified intent: {parsed_result_dict}")
                # 记录成功的调用
                log_data_intent = {
                    "interaction_id": str(uuid.uuid4()), "timestamp_utc": datetime.now(timezone.utc).isoformat(),
                    "task_type": "intent_classification_gemini", "user_query_for_task": user_query,
                    "llm_input_messages": messages_for_gemini, "llm_parameters": {k:v for k,v in litellm_params.items() if k not in ['messages', 'api_key', 'proxy']},
                    "raw_llm_output": raw_gemini_output_text, "application_version": "0.1.0_intent_gemini"
                }
                await log_interaction_data(log_data_intent)
                return parsed_result_dict
            else: # 解析成功但结构不对
                if parsed_result_dict: # 避免对None调用get
                    error_info_intent = f"Gemini intent output JSON structure mismatch. Parsed: {parsed_result_dict}"
                else: # parsed_result_dict 为 None (例如，花括号提取失败后)
                    error_info_intent = "Gemini intent output JSON structure mismatch (parsed_result_dict is None)."
                llm_py_logger.warning(error_info_intent)
        else: # response.choices[0].message.content 为空或不存在
            error_info_intent = "Gemini intent call returned empty or malformed response content."
            llm_py_logger.error(f"{error_info_intent} Full response object: {response}")

    except Exception as e_gemini_call:
        error_info_intent = f"Error calling Gemini for intent: {e_gemini_call}"
        llm_py_logger.error(error_info_intent, exc_info=True)

    # 如果执行到这里，说明出错了或者没有得到期望的JSON
    log_error_data_intent = {
        "interaction_id": str(uuid.uuid4()), "timestamp_utc": datetime.now(timezone.utc).isoformat(),
        "task_type": "intent_classification_gemini_error", "user_query_for_task": user_query,
        "llm_input_messages": messages_for_gemini, "llm_parameters": {k:v for k,v in litellm_params.items() if k not in ['messages', 'api_key', 'proxy']},
        "raw_llm_output": raw_gemini_output_text or "N/A", "error_details": error_info_intent,
        "application_version": "0.1.0_intent_gemini"
    }
    await log_interaction_data(log_error_data_intent)
    
    llm_py_logger.warning(f"Gemini failed to generate valid intent classification, defaulting to no clarification needed. Error: {error_info_intent or 'Unknown reason'}")
    return {"clarification_needed": False, "reason": f"Intent classification by Gemini failed: {error_info_intent or 'Unknown reason'}"}

# --- 新增：用于提取实体和关系意图的函数 ---
async def extract_entities_for_kg_query(user_question: str) -> Optional[ExtractedEntitiesAndRelationIntent]:
    """
    调用LLM从用户查询中提取核心实体和关系意图，并返回一个结构化的Pydantic对象。
    """
    llm_py_logger.info(f"Attempting to extract entities and relation intent for KG query from: '{user_question}'")

    messages_for_llm = get_entity_relation_extraction_messages(user_question)

    llm_response_json_str = await call_llm_via_openai_api_local_only(
        prompt=messages_for_llm,
        temperature=0.1, # 较低的温度
        max_new_tokens=512, # 应该足够输出期望的JSON
        stop_sequences=['<|im_end|>', '```'], # 尝试在代码块结束时停止
        task_type="kg_entity_relation_extraction",
        user_query_for_log=user_question,
        model_name_for_log="qwen3_gguf_kg_entity_extraction"
    )

    if not llm_response_json_str:
        llm_py_logger.warning(f"LLM call for KG entity/relation extraction returned None or empty. User question: '{user_question}'")
        return None

    cleaned_json_str = llm_response_json_str.strip()

    # --- 修改/添加这一行 ---
    llm_py_logger.info(f"FULL Cleaned JSON string from LLM for entity extraction: >>>{cleaned_json_str}<<<")
    # --- 修改/添加结束 ---

    try:
        parsed_data = json.loads(cleaned_json_str)
        # 使用Pydantic模型进行验证和转换
        extracted_info = ExtractedEntitiesAndRelationIntent(**parsed_data)
        llm_py_logger.info(f"Successfully parsed Pydantic model from LLM output: {extracted_info.model_dump_json(indent=2)}")
        return extracted_info
    except json.JSONDecodeError as e_json:
        llm_py_logger.error(f"Failed to decode JSON from LLM for entity extraction: '{cleaned_json_str}'. Error: {e_json}", exc_info=True)
        return None
    except Exception as e_pydantic: # Catch Pydantic validation errors or other issues
        llm_py_logger.error(f"Failed to validate or parse Pydantic model from LLM JSON for entity extraction: '{cleaned_json_str}'. Error: {e_pydantic}", exc_info=True)
        return None
    --- END OF FILE CONTENT ---

  File: rag_prompts.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_rag/llm/rag_prompts.py
from typing import List, Dict, Any
from zhz_rag.config.constants import NEW_KG_SCHEMA_DESCRIPTION


# 可以将 NO_ANSWER_PHRASE_ANSWER_CLEAN 也移到这里，或者从 constants.py 导入
NO_ANSWER_PHRASE_ANSWER_CLEAN = "根据目前提供的资料，我无法找到关于您问题的明确信息。" # 保持与 llm_interface.py 一致

def get_answer_generation_messages(user_query: str, context_str: str) -> List[Dict[str, str]]:
    """
    构建用于从上下文中生成答案的LLM输入messages。
    """
    system_prompt_for_answer = f"""
你是一个非常严谨、客观且专业的AI问答助手。你的核心任务是根据【上下文信息】回答【用户问题】。

**核心指令与行为准则：**

1.  **【绝对忠实于上下文】**: 你的回答【必须且只能】使用【上下文信息】中明确提供的文字和事实。严禁进行任何形式的推断、联想、猜测或引入外部知识。如果上下文信息不足或不相关，请明确指出。
2.  **【逐点核实与直接证据】**: 对于用户问题中的每一个具体信息点或子问题，你都必须在【上下文信息】中找到清晰、直接的证据来支持你的回答。如果没有直接证据，则视为无法回答该点。
3.  **【引用来源 (如果可能且适用)】**: 如果你的答案直接引用或高度依赖【上下文信息】中的特定片段，请尽可能简要地指出信息来源。例如，如果上下文片段被标记了来源（如“来源文档A第3段”），你可以说“根据文档A第3段，...”。如果上下文没有明确的来源标记，则无需强行编造。**此项为次要优先级，准确回答问题是首要的。**
4.  **【处理无法回答的情况】**:
 *   **完全无法回答**: 如果【上下文信息】完全不包含与【用户问题】相关的任何信息，或者无法找到任何直接证据来回答问题的任何部分，请【只回答】：“{NO_ANSWER_PHRASE_ANSWER_CLEAN}”
 *   **部分无法回答**: 如果【用户问题】包含多个子问题或方面，而【上下文信息】只能回答其中的一部分：
     *   请只回答你能找到直接证据支持的部分。
     *   对于【上下文信息】中没有直接证据支持的其他子问题或方面，请明确指出，例如：“关于您提到的[某子问题/方面]，提供的上下文中未包含相关信息。”
     *   **禁止**对未提供信息的部分进行任何形式的猜测或尝试回答。
5.  **【答案风格：专业、简洁、直接】**:
 *   回答应直接针对用户问题，避免不必要的寒暄或冗余信息。
 *   语言表达应专业、客观、清晰易懂。
 *   如果答案包含多个要点，可以使用简洁的列表格式。
6.  **【避免重复与冗余】**: 如果多个上下文片段提供了相同的信息，请综合它们并给出不重复的答案。
/no_think

请严格遵守以上指令，以最高的准确性和忠实度来完成回答。
"""
    messages = [
        {"role": "system", "content": system_prompt_for_answer},
        {"role": "user", "content": f"用户问题: {user_query}\n\n上下文信息:\n{context_str}"}
    ]
    return messages

def get_clarification_question_messages(original_query: str, uncertainty_reason: str) -> List[Dict[str, str]]:
    """
    构建用于生成澄清问题的LLM输入messages。
    """
    system_prompt_for_clarification = f"""你的【唯一任务】是根据用户提供的【用户原始查询】和【不确定性原因】，生成一个【简洁、明确、友好且直接的澄清问句】。
    

**【严格的输出要求】**
*   你的【最终且唯一】的输出【必须】是这个澄清问句本身。
*   【绝对禁止】输出任何思考过程、解释、前缀、后缀或任何与澄清问句无关的文字。
*   澄清问句本身不应包含用户的原始查询或不确定性原因的复述。
/no_think


**示例：**

<example>
  <user_original_query>帮我查查天气</user_original_query>
  <uncertainty_reason>缺少地点信息</uncertainty_reason>
  <assistant_clarification_question>请问您想查询哪个城市的天气呢？</assistant_clarification_question>
</example>

<example>
  <user_original_query>分析一下销售数据</user_original_query>
  <uncertainty_reason>用户没有说明具体想对销售数据做什么操作，例如是汇总、筛选还是查找特定记录。</uncertainty_reason>
  <assistant_clarification_question>请问您希望对销售数据进行哪种具体操作，例如汇总统计、筛选特定条件，还是查找某些记录？</assistant_clarification_question>
</example>

<example>
  <user_original_query>给我推荐一些关于人工智能的书籍</user_original_query>
  <uncertainty_reason>用户没有说明偏好的人工智能子领域或书籍类型（入门/进阶/技术/哲学等）。</uncertainty_reason>
  <assistant_clarification_question>您对人工智能的哪个子领域或什么类型的书籍（如入门、技术实践、哲学探讨等）更感兴趣？</assistant_clarification_question>
</example>

<example>
  <user_original_query>我们公司的年假政策是怎么样的？</user_original_query>
  <uncertainty_reason>缺少公司名称，无法定位到具体的年假政策文档。</uncertainty_reason>
  <assistant_clarification_question>请问您的公司全称是什么？</assistant_clarification_question>
</example>

<example>
  <user_original_query>处理一下这个文件。</user_original_query>
  <uncertainty_reason>用户没有说明要对文件进行何种处理，也没有指明是哪个文件。</uncertainty_reason>
  <assistant_clarification_question>请问您希望对哪个文件进行什么具体操作呢？</assistant_clarification_question>
</example>
"""
    user_content = f"""用户原始查询: {original_query}
不确定性原因: {uncertainty_reason}

你应该输出的澄清问句:""" # 改为“澄清问句”

    messages = [
        {"role": "system", "content": system_prompt_for_clarification},
        {"role": "user", "content": user_content}
    ]
    return messages

# --- 精简的Cypher模板定义 (只保留一个核心模板) ---
SIMPLIFIED_CYPHER_TEMPLATES = [
    {
        "id": "template_find_entity_attributes_by_text_label",
        "description": "根据提供的实体文本和实体标签，查找该实体的所有基本属性。",
        "template": "MATCH (n:ExtractedEntity {{text: $entity_text, label: $entity_label}}) RETURN n.text AS text, n.label AS label, n.id_prop AS id_prop LIMIT 1",
        "params_needed": ["entity_text", "entity_label"]
    }
]

# --- 精简的Schema描述 (只保留与上述模板最相关的部分) ---
# 注意：NEW_KG_SCHEMA_DESCRIPTION 本身已经很详细了，这里我们为了测试，
# 可以考虑创建一个更精简的字符串，或者在prompt中只引用NEW_KG_SCHEMA_DESCRIPTION中与ExtractedEntity相关的部分。
# 为了简单起见，我们先仍然使用完整的 NEW_KG_SCHEMA_DESCRIPTION，但会在prompt中强调只关注ExtractedEntity。
# 如果依然太长导致问题，下一步可以尝试动态构建一个更小的schema片段传给LLM。

def get_cypher_generation_messages_with_templates(user_question: str) -> List[Dict[str, str]]: # 函数名保持一致
    """
    构建用于（基于【单个指定模板】）生成Cypher查询的LLM输入messages。
    这个版本用于测试模型对单个模板的参数提取能力。
    """
    
    # 在这个测试版本中，我们假设总是使用第一个（也是唯一一个）模板
    selected_template = SIMPLIFIED_CYPHER_TEMPLATES[0]
    
    template_description_for_prompt = f"""你将使用以下Cypher查询模板：
Template ID: {selected_template['id']}
Description: {selected_template['description']}
Cypher Structure: {selected_template['template']}
Parameters Needed: {', '.join(selected_template['params_needed'])}
"""

    system_prompt_for_cypher = f"""你是一个精确的参数提取助手。你的任务是根据用户问题，为下面提供的【唯一Cypher查询模板】提取参数，并构建一个Cypher查询。

**【图谱Schema核心部分参考】**
(你主要关注 `:ExtractedEntity` 节点及其属性: `text`, `label`, `id_prop`。其中 `label` 的常见值是 "PERSON", "ORGANIZATION", "TASK"。)
{NEW_KG_SCHEMA_DESCRIPTION} 
# ^^^ Schema描述已包含输出JSON格式 {{"status": "success/unable_to_generate", "query": "..."}} 的指导，请严格遵循该JSON输出格式。

**【当前需要填充的Cypher查询模板】**
{template_description_for_prompt}

**【你的任务与输出要求】**
1.  仔细分析【用户问题】，理解其核心查询意图。
2.  判断该意图是否与提供的【当前需要填充的Cypher查询模板】描述相符。
3.  如果相符：
    a.  从【用户问题】中提取填充该模板所需的所有【Parameters Needed】。确保参数值与Schema中的实体文本和标签格式相符（例如，标签应为大写 "PERSON", "ORGANIZATION", "TASK"）。
    b.  将提取的参数值替换到模板的Cypher语句中（例如，`$entity_text` 替换为提取到的实体名）。
    c.  最终输出一个JSON对象，格式为：`{{"status": "success", "query": "填充好参数的Cypher语句"}}`。
4.  如果不相符（例如，用户问题意图与模板描述不符，或无法从问题中提取到模板所需的所有关键参数）：
    a.  最终输出一个JSON对象，格式为：`{{"status": "unable_to_generate", "query": "无法生成Cypher查询."}}`。
5.  【绝对禁止】输出任何除了上述指定JSON对象之外的文本、解释或思考过程。


**【处理示例】**
<example>
  <user_question>我想知道张三的详细信息。</user_question>
  <assistant_output_json>{{
    "status": "success",
    "query": "MATCH (n:ExtractedEntity {{text: '张三', label: 'PERSON'}}) RETURN n.text AS text, n.label AS label, n.id_prop AS id_prop LIMIT 1"
  }}</assistant_output_json>
</example>
<example>
  <user_question>项目Alpha的文档编写任务是什么？</user_question>
  <assistant_output_json>{{
    "status": "success",
    "query": "MATCH (n:ExtractedEntity {{text: '项目alpha的文档编写任务', label: 'TASK'}}) RETURN n.text AS text, n.label AS label, n.id_prop AS id_prop LIMIT 1"
  }}</assistant_output_json>
</example>
<example>
  <user_question>法国的首都是哪里？</user_question>
  <assistant_output_json>{{
    "status": "unable_to_generate",
    "query": "无法生成Cypher查询."
  }}</assistant_output_json>
</example>
"""
    user_content = f"用户问题: {user_question}"

    messages = [
        {"role": "system", "content": system_prompt_for_cypher},
        {"role": "user", "content": user_content}
    ]
    return messages

# --- 新增：实体与关系意图提取的提示词生成函数 ---
def get_entity_relation_extraction_messages(user_question: str) -> List[Dict[str, str]]:
    """
    构建用于从用户查询中提取核心实体和关系意图的LLM输入messages。
    目标是输出一个符合 ExtractedEntitiesAndRelationIntent Pydantic 模型结构的JSON。
    """
    # 从 NEW_KG_SCHEMA_DESCRIPTION 中提取允许的实体标签，以便在提示中告知LLM
    # 这是一个简化的提取，实际应用中可能需要更精确地从Schema中获取
    # 假设 NEW_KG_SCHEMA_DESCRIPTION 中有类似 "label: STRING (实体类型。允许的值: "PERSON", "ORGANIZATION", "TASK")" 的描述
    import re
    match = re.search(r'label\s*:\s*STRING\s*\(实体类型。\s*允许的值\s*:\s*("([^"]+)"(?:,\s*"([^"]+)")*)\)', NEW_KG_SCHEMA_DESCRIPTION)
    allowed_entity_labels_str = "PERSON, ORGANIZATION, TASK" # 默认值
    if match:
        # 提取所有带引号的标签
        labels_group = match.group(1)
        extracted_labels = re.findall(r'"([^"]+)"', labels_group)
        if extracted_labels:
            allowed_entity_labels_str = ", ".join(extracted_labels)
    
    system_prompt_for_entity_extraction = f"""你的任务是仔细分析用户提供的【用户问题】，并识别出其中与知识图谱查询相关的核心信息。

**你需要识别以下内容：**
1.  **核心实体**：问题中提到的1到2个最关键的实体（人名、组织名、任务名等）。
2.  **实体类型**：为每个识别出的实体，从以下参考类型中推断其最可能的类型：{allowed_entity_labels_str}。如果无法确定，可以不指定类型。
3.  **关系意图**：如果用户问题暗示了实体间的特定关系，请用简洁的文本描述这个关系意图（例如 “查询工作地点”, “查找负责人”, “获取销售额”）。如果只是查询单个实体的属性，则关系意图不明确。

请在你的回答中清晰地包含这些分析结果。最终的结构化输出将由系统根据你的分析自动完成。
你只需要专注于准确地理解和提取这些信息。
"""
    # 移除了所有关于JSON输出格式的指令和示例，因为将由response_format处理
    # 也不再需要 /no_think，因为我们期望约束生成能处理好输出

    user_content = f"用户问题: {user_question}\n\n请分析此问题并提取相关实体和关系意图："

    messages = [
        {"role": "system", "content": system_prompt_for_entity_extraction},
        {"role": "user", "content": user_content}
    ]
    return messages
    --- END OF FILE CONTENT ---


Directory: zhz_rag/llm/rag_eval_data/

Directory: zhz_rag/task_management/
  File: __init__.py
    --- File is empty ---

  File: db_models.py
    --- START OF FILE CONTENT ---
# zhz_agent/database_models.py
from sqlalchemy import Column, String, DateTime, Integer, Text, Enum as SQLAlchemyEnum, ForeignKey, Boolean, JSON
from sqlalchemy.sql import func
import uuid

# --- [修改] 从 pydantic_models 导入枚举 -> 改为绝对导入 ---
from zhz_rag.config.pydantic_models import TaskStatus, ReminderMethod

# --- [修改] 从新的 database.py 导入 Base -> 改为绝对导入 ---
from zhz_rag.utils.db_utils import Base # <--- 确保只从这里导入 Base #

class TaskDB(Base): # 命名为 TaskDB 以区分 Pydantic 的 TaskModel
    __tablename__ = "tasks"

    id = Column(String, primary_key=True, index=True, default=lambda: str(uuid.uuid4()))
    title = Column(String, index=True, nullable=False)
    description = Column(Text, nullable=True) #
    status = Column(SQLAlchemyEnum(TaskStatus), default=TaskStatus.PENDING, nullable=False) #
    created_at = Column(DateTime(timezone=True), server_default=func.now(), nullable=False) #
    updated_at = Column(DateTime(timezone=True), server_default=func.now(), onupdate=func.now(), nullable=False) #
    due_date = Column(DateTime(timezone=True), nullable=True) #
    reminder_time = Column(DateTime(timezone=True), nullable=True) #
    reminder_offset_minutes = Column(Integer, nullable=True) #
    reminder_methods = Column(JSON, default=[ReminderMethod.NOTIFICATION.value], nullable=False) #
    priority = Column(Integer, default=0, nullable=False) #
    tags = Column(JSON, default=[], nullable=False) #
    action_type = Column(String, nullable=True) #
    action_payload = Column(JSON, default={}, nullable=True) #
    execution_result = Column(Text, nullable=True) #
    last_executed_at = Column(DateTime(timezone=True), nullable=True) #

    def __repr__(self):
        return f"<TaskDB(id={self.id}, title='{self.title}', status='{self.status.value}')>"
    --- END OF FILE CONTENT ---

  File: jobs.py
    --- START OF FILE CONTENT ---
# zhz_agent/task_jobs.py
from datetime import datetime
from typing import Dict, Any
import os
import traceback
import httpx # <--- 确保 httpx 已导入
import json # <--- 确保 json 已导入

# 从 .database 导入 database 对象以便查询任务详情
# 从 .pydantic_models 导入 TaskModel 以便类型转换
# 从 .main 导入 scheduler 以便在需要时重新调度（虽然通常作业函数不直接操作调度器）
# 更好的做法是通过参数传递必要的信息，而不是依赖全局导入
WINDOWS_HOST_IP = os.getenv("WINDOWS_HOST_IP_FOR_WSL", "192.168.3.11") # <--- 请务必替换为您真实的Windows IP
LOCAL_AGENT_PORT = os.getenv("LOCAL_AGENT_PORT", "8003") # 与 local_agent_app.py 中的端口一致

# 如果 WINDOWS_HOST_IP 仍然是占位符，给出提示
if WINDOWS_HOST_IP == "在此处填写您上一步找到的Windows主机IP":
    print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
    print("REMINDER_JOB WARNING: WINDOWS_HOST_IP 未正确设置在 task_jobs.py 中!")
    print("请编辑 task_jobs.py 文件，将 '在此处填写您上一步找到的Windows主机IP' 替换为实际IP地址。")
    print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")

LOCAL_AGENT_NOTIFY_URL = f"http://{WINDOWS_HOST_IP}:{LOCAL_AGENT_PORT}/notify"

async def send_task_reminder(task_id: str, task_title: str, reminder_methods: list):
    """
    实际发送任务提醒的函数。
    """
    print(f"REMINDER_JOB: [{datetime.utcnow()}] 正在为任务 '{task_id}' - '{task_title}' 发送提醒。")
    for method in reminder_methods:
        if method == "notification": # 假设 ReminderMethod.NOTIFICATION.value 是 "notification"
            print(f"  REMINDER_JOB: 尝试通过 Local Agent 发送桌面通知: '{task_title}'")
            try:
                async with httpx.AsyncClient(timeout=10.0) as client:
                    response = await client.post(
                        LOCAL_AGENT_NOTIFY_URL,
                        json={"title": f"任务提醒: {task_title}", "message": f"任务 '{task_title}' 即将到期或需要关注。"}
                    )
                    response.raise_for_status() # Raise an exception for bad status codes
                    print(f"  REMINDER_JOB: 本地代理通知请求发送成功. 状态: {response.status_code}")
            except httpx.RequestError as e:
                print(f"  REMINDER_JOB: 通过本地代理发送通知失败 (RequestError): {e}")
                traceback.print_exc()
            except Exception as e:
                print(f"  REMINDER_JOB: 通过本地代理发送通知失败 (General Error): {e}")
                traceback.print_exc()
        # elif method == "email": #
        #     print(f"  REMINDER_JOB: 模拟发送邮件提醒...")

async def execute_task_action(task_id: str, action_type: str, action_payload: Dict[str, Any]):
    """
    实际执行任务动作的函数。
    """
    print(f"EXECUTION_JOB: [{datetime.utcnow()}] 正在为任务 '{task_id}' 执行动作 '{action_type}'。")
    print(f"  EXECUTION_JOB: 动作参数: {action_payload}")

    final_result = f"动作 '{action_type}' 已模拟执行。"
    success = True

    if action_type == "navigate":
        destination = action_payload.get("destination")
        if destination:
            print(f"  EXECUTION_JOB: 模拟导航到 '{destination}'...")
            final_result = f"已模拟为导航到 '{destination}' 准备好路线。"
        else:
            final_result = "导航动作失败：缺少目的地。"
            success = False
    elif action_type == "log_event":
        event_details = action_payload.get("event_details", "无详情")
        print(f"  EXECUTION_JOB: 记录事件: '{event_details}'")
        final_result = f"事件 '{event_details}' 已记录。"
    else:
        final_result = f"未知的动作类型: {action_type}"
        success = False

    # 更新数据库中的任务状态和结果 (需要访问数据库)
    # 这部分逻辑最好通过API调用或服务层来完成，以避免循环导入和分散DB操作
    # 这里我们只打印信息，实际应用中需要实现DB更新
    print(f"  EXECUTION_JOB: 任务 '{task_id}' 执行完毕。结果: {final_result}, 状态: {'COMPLETED' if success else 'FAILED'}")

    --- END OF FILE CONTENT ---


Directory: zhz_rag/finetuning/
  File: __init__.py
    --- File is empty ---

  File: refine_answer_data.py
    --- START OF FILE CONTENT ---
# zhz_agent/refine_answer_finetune_data.py
import json
import os
import pandas as pd
from typing import List, Dict, Any, Optional
import glob
from datetime import datetime

# 假设 utils.py 和 constants.py 在同一个 zhz_agent 包内
try:
    from zhz_rag.utils.common_utils import (
    RAG_INTERACTION_LOGS_DIR,
    EVALUATION_RESULTS_LOGS_DIR,
    FINETUNING_GENERATED_DATA_DIR,
    find_latest_rag_interaction_log # 确保这个也被导入
)
    from zhz_rag.config.pydantic_models import RetrievedDocument
    # NO_ANSWER_PHRASE_ANSWER_CLEAN 将从 llm.py 导入，或者在constants.py中定义
    # 我们需要与 llm.py -> generate_answer_from_context 一致的 "无法回答" 短语
    from zhz_rag.llm.llm_interface import NO_ANSWER_PHRASE_ANSWER_CLEAN 
except ImportError as e:
    print(f"ERROR: Could not import necessary modules for refine_answer_finetune_data: {e}")
    exit(1)

import logging

# 配置此脚本的logger
refine_answer_logger = logging.getLogger("RefineAnswerFinetuneDataLogger")
refine_answer_logger.setLevel(logging.INFO)
if not refine_answer_logger.hasHandlers():
    _console_handler = logging.StreamHandler()
    _formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
    _console_handler.setFormatter(_formatter)
    refine_answer_logger.addHandler(_console_handler)
    refine_answer_logger.info("--- RefineAnswerFinetuneDataLogger configured ---")

# --- 配置 ---
RAG_LOG_DIR = "zhz_rag/stored_data/evaluation_results_logs/"
EVAL_LOG_DIR = "zhz_rag/stored_data/evaluation_results_logs/"
FINETUNE_DATA_DIR = "zhz_rag/finetuning/generated_data/"
os.makedirs(FINETUNE_DATA_DIR, exist_ok=True)

# --- 与 run_batch_answer_evaluation.py 中类似的上下文格式化函数 ---
def format_contexts_for_prompt(context_docs_raw: List[Dict[str, Any]]) -> str:
    """
    将从日志中解析出的上下文文档列表格式化为单一字符串，用于构建LLM的输入Prompt。
    这个格式应该与 llm.py -> generate_answer_from_context 中构建上下文的方式一致。
    """
    context_strings_for_llm = []
    if not context_docs_raw:
        return "No context provided."
        
    for i, doc_data in enumerate(context_docs_raw):
        try:
            # 尝试使用RetrievedDocument模型解析，如果原始日志中已经是这个结构
            # 但通常日志中可能是字典列表
            doc_content = doc_data.get("content", "[Content not available]")
            doc_source = doc_data.get("source_type", "unknown_source")
            doc_score = doc_data.get("score")
            
            # 与 rag_service.py 中准备上下文给LLM的格式保持一致
            # 在 rag_service.py 中是:
            # f"Source Type: {doc.source_type}, Score: {doc.score:.4f}\nContent: {doc.content}"
            # 我们这里也尽量模拟，但日志中的score可能不存在或格式不同
            header = f"Source Type: {doc_source}"
            if doc_score is not None:
                try:
                    header += f", Score: {float(doc_score):.4f}"
                except ValueError:
                    header += f", Score: {doc_score}" # 如果分数不是数字，直接用原始值
            
            context_strings_for_llm.append(f"{header}\nContent: {doc_content}")

        except Exception as e:
            refine_answer_logger.warning(f"Could not parse a context document fully for prompt: {doc_data}. Error: {e}")
            content = doc_data.get("content", "[Content not available]")
            context_strings_for_llm.append(f"Content: {content}") # 简化版

    return "\n\n---\n\n".join(context_strings_for_llm) if context_strings_for_llm else "No context provided."


def construct_qwen_answer_input_prompt(user_question: str, formatted_context: str) -> str:
    """
    根据用户问题和格式化的上下文构建Qwen生成答案时的完整输入Prompt。
    这个函数必须与 llm.py 中 generate_answer_from_context 内部构建Prompt的逻辑完全一致。
    """
    # --- 从 llm.py 的 generate_answer_from_context 函数复制并粘贴完整的 prompt 模板 ---
    # 注意：这里需要确保模板与 llm.py 中的完全一致
    prompt = f"""
<|im_start|>system
你是一个AI问答助手。你的任务是根据【上下文信息】回答【用户问题】。

**核心指令：**

1.  **尝试直接回答：** 请首先仔细阅读【上下文信息】，如果其中包含能直接回答【用户问题】的内容，请用上下文中的信息直接、简洁地回答。
2.  **忠实原文：** 你的回答必须严格基于【上下文信息】，禁止加入任何外部知识或个人观点。
3.  **如果无法回答：** 如果你分析了【上下文信息】后，确认其中确实没有能回答【用户问题】的明确信息，那么请只回答以下这句话：
    "根据目前提供的资料，我无法找到关于您问题的明确信息。"
    **不要添加任何其他解释、建议或反问。**

**请直接给出答案，或者只给出上述那句固定的“无法找到信息”的回复。**
<|im_start|>user
用户问题: {user_question}

上下文信息:
{formatted_context}
<|im_end|>
<|im_start|>assistant
"""
    return prompt

def load_logs_to_dict(filepath: str, key_field: str = "interaction_id") -> Dict[str, Dict[str, Any]]:
    """将JSONL文件加载到一个以指定字段为键的字典中。"""
    data_dict = {}
    if not os.path.exists(filepath):
        refine_answer_logger.error(f"Log file not found: {filepath}")
        return data_dict
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                log_entry = json.loads(line.strip())
                if key_field in log_entry and log_entry[key_field]: # 确保key_field的值不是None或空
                    data_dict[log_entry[key_field]] = log_entry
                elif key_field == "original_interaction_id_ref" and log_entry.get("original_interaction_id_ref"):
                    data_dict[log_entry["original_interaction_id_ref"]] = log_entry
            except json.JSONDecodeError:
                refine_answer_logger.warning(f"Skipping malformed JSON line in {filepath}")
    return data_dict

def generate_finetune_samples_for_answer(
    rag_interaction_logs: Dict[str, Dict[str, Any]],
    answer_evaluation_logs: Dict[str, Dict[str, Any]]
) -> List[Dict[str, str]]:
    finetune_samples = []
    processed_ids = set()

    refine_answer_logger.info(f"Processing {len(rag_interaction_logs)} RAG interaction logs and {len(answer_evaluation_logs)} Answer evaluation logs.")

    for interaction_id, rag_log in rag_interaction_logs.items():
        if rag_log.get("task_type") != "rag_query_processing_success":
            continue

        if interaction_id in processed_ids:
            continue
        processed_ids.add(interaction_id)

        user_question = rag_log.get("user_query")
        qwen_generated_answer_raw = rag_log.get("processed_llm_output") # Qwen的原始答案
        # retrieved_context_docs 在 rag_log 中可能是 "retrieved_context_docs" 或 "retrieved_documents_summary"
        # 我们需要原始的、完整的上下文文档
        retrieved_context_docs_raw = rag_log.get("retrieved_context_docs") 
        
        if not retrieved_context_docs_raw and rag_log.get("debug_info"): # 尝试从debug_info获取
             retrieved_context_docs_raw = rag_log.get("debug_info",{}).get("retrieved_context_docs")


        if qwen_generated_answer_raw is None or not qwen_generated_answer_raw.strip():
            qwen_generated_answer = NO_ANSWER_PHRASE_ANSWER_CLEAN # 空答案视为无法回答
        else:
            qwen_generated_answer = qwen_generated_answer_raw.strip()

        if not user_question or not retrieved_context_docs_raw:
            refine_answer_logger.warning(f"Skipping RAG log {interaction_id} due to missing user_question or retrieved_context_docs.")
            continue
        
        # 构建Prompt
        formatted_contexts_for_prompt = format_contexts_for_prompt(retrieved_context_docs_raw)
        qwen_answer_input_prompt = construct_qwen_answer_input_prompt(user_question, formatted_contexts_for_prompt)

        ideal_answer_output = None
        source_of_ideal = "unknown"
        gemini_scores_for_log = {}

        eval_log = answer_evaluation_logs.get(interaction_id)

        if eval_log and eval_log.get("eval_llm_processed_output_json"):
            eval_json = eval_log["eval_llm_processed_output_json"]
            summary_eval = eval_json.get("evaluation_summary", {})
            dimensions_eval = eval_json.get("dimensions", {})
            
            overall_score_str = summary_eval.get("overall_answer_quality_score")
            faithfulness_score_str = dimensions_eval.get("faithfulness", {}).get("score")
            relevance_score_str = dimensions_eval.get("relevance", {}).get("score")
            completeness_score_str = dimensions_eval.get("completeness", {}).get("score")
            context_sufficiency = dimensions_eval.get("completeness", {}).get("context_sufficiency_assessment", "Unknown")
            gemini_suggestion_answer = eval_json.get("suggestion_for_answer_improvement", "").strip()

            try:
                overall_score = int(overall_score_str) if overall_score_str is not None else 0
                faithfulness_score = int(faithfulness_score_str) if faithfulness_score_str is not None else 0
                relevance_score = int(relevance_score_str) if relevance_score_str is not None else 0
                completeness_score = int(completeness_score_str) if completeness_score_str is not None else 0
                gemini_scores_for_log = {
                    "overall": overall_score,
                    "faithfulness": faithfulness_score,
                    "relevance": relevance_score,
                    "completeness": completeness_score,
                    "context_sufficiency": context_sufficiency
                }
            except (ValueError, TypeError) as e:
                refine_answer_logger.warning(f"Could not parse one or more scores for {interaction_id}: {e}")
                overall_score = faithfulness_score = relevance_score = completeness_score = 0
                gemini_scores_for_log = { # 记录解析失败
                    "overall": "parse_error", "faithfulness": "parse_error", 
                    "relevance": "parse_error", "completeness": "parse_error",
                    "context_sufficiency": context_sufficiency
                }
                
            # --- Completion选择逻辑 (改进版) ---
            ideal_answer_output = None # 重新初始化
            source_of_ideal = "unknown" # 重新初始化

            # 规则 0: Qwen的原始答案就是标准的“无法回答”短语
            is_qwen_standard_no_answer = (qwen_generated_answer == NO_ANSWER_PHRASE_ANSWER_CLEAN)

            # 规则 1: 上下文不足，且Qwen正确地给出了标准的“无法回答”
            if is_qwen_standard_no_answer and \
               context_sufficiency == "Insufficient" and \
               overall_score >= 4: # Gemini认为Qwen的这个“无法回答”是高质量的
                ideal_answer_output = NO_ANSWER_PHRASE_ANSWER_CLEAN
                source_of_ideal = "qwen_standard_no_answer_confirmed_by_gemini_context_insufficient"
            
            # 规则 2: 上下文不足，Qwen可能没有给出标准“无法回答”，但Gemini建议应指出信息不足
            elif not is_qwen_standard_no_answer and \
                 context_sufficiency == "Insufficient" and \
                 completeness_score <=2 and \
                 ("information is not available" in gemini_suggestion_answer.lower() or \
                  "context does not contain" in gemini_suggestion_answer.lower() or \
                  "cannot be answered from the context" in gemini_suggestion_answer.lower() or \
                  "should state that the information is not found" in gemini_suggestion_answer.lower()):
                ideal_answer_output = NO_ANSWER_PHRASE_ANSWER_CLEAN
                source_of_ideal = "gemini_suggests_no_answer_due_to_insufficient_context"

            # 规则 3: Gemini 整体评分很高 (例如 overall, faithfulness, relevance 都 >= 4)
            # 并且 Qwen 的答案不是标准的“无法回答”（如果已经是，则由规则1处理）
            elif not is_qwen_standard_no_answer and \
                 overall_score >= 4 and faithfulness_score >= 4 and relevance_score >= 4:
                ideal_answer_output = qwen_generated_answer
                source_of_ideal = "qwen_high_score_by_gemini"
                # 如果此时 Gemini 仍有改进建议，可以额外标记
                if gemini_suggestion_answer and \
                   gemini_suggestion_answer != "No improvement needed." and \
                   "suggestion" not in source_of_ideal: # 避免重复标记
                    source_of_ideal += "_with_minor_gemini_suggestion"


            # 规则 4: Qwen的答案评分不高，但Gemini给出了具体的改进建议
            # 我们将这类样本标记出来，completion暂时使用Qwen的答案，供人工审核和优化
            elif overall_score < 4 and \
                 gemini_suggestion_answer and \
                 gemini_suggestion_answer != "No improvement needed." and \
                 len(gemini_suggestion_answer) > 10: # 假设太短的建议可能不具体
                ideal_answer_output = qwen_generated_answer # 保留Qwen答案作为基础
                source_of_ideal = "qwen_low_score_with_gemini_suggestion_for_review"
                refine_answer_logger.info(f"Answer log {interaction_id} (Qwen: '{qwen_generated_answer[:100]}...') marked for review due to low score but has Gemini suggestion: '{gemini_suggestion_answer[:100]}...'")
            
            # 规则 5: 如果Qwen的答案是标准“无法回答”，但上下文其实是充分的，或者Gemini认为可以回答
            # 这通常意味着Qwen可能错误地判断无法回答，或者Gemini的评估与Qwen的判断不一致
            elif is_qwen_standard_no_answer and \
                 (context_sufficiency == "Sufficient" or (context_sufficiency == "Partially Sufficient" and completeness_score >=3)) and \
                 overall_score < 4 : # Gemini不认可这个“无法回答”
                ideal_answer_output = qwen_generated_answer # 保留Qwen的“无法回答”
                source_of_ideal = "qwen_no_answer_but_gemini_disagrees_or_context_sufficient_for_review"
                refine_answer_logger.info(f"Answer log {interaction_id}: Qwen said 'no answer', but Gemini scores/context sufficiency suggest it might be answerable. Marked for review. Gemini scores: {gemini_scores_for_log}, Suggestion: '{gemini_suggestion_answer[:100]}...'")

            # 规则 6: 其他所有情况，暂时跳过，等待更明确的规则或人工审核
            else:
                refine_answer_logger.info(f"Answer log {interaction_id} (Qwen: '{qwen_generated_answer[:100]}...') did not meet current finetune criteria. Needs manual review or rule adjustment. Gemini scores: {gemini_scores_for_log}, Suggestion: '{gemini_suggestion_answer[:100]}...'")
                continue

        else: # 没有有效的Gemini评估日志
            refine_answer_logger.warning(f"No valid Gemini evaluation found for Answer log {interaction_id}. Qwen's output: '{qwen_generated_answer[:100]}...'. Skipping for finetune data.")
            continue
            
        if ideal_answer_output is not None:
            finetune_samples.append({
                "prompt": qwen_answer_input_prompt,
                "completion": ideal_answer_output.strip(),
                "original_qwen_answer": qwen_generated_answer_raw.strip() if qwen_generated_answer_raw else NO_ANSWER_PHRASE_ANSWER_CLEAN, # 记录Qwen最原始的输出
                "gemini_scores": gemini_scores_for_log,
                "gemini_suggestion": gemini_suggestion_answer if eval_log and eval_log.get("eval_llm_processed_output_json") else None,
                "source_of_ideal": source_of_ideal,
                "interaction_id": interaction_id
            })

    refine_answer_logger.info(f"Generated {len(finetune_samples)} Answer finetuning samples.")
    return finetune_samples


if __name__ == "__main__":
    rag_log_file = find_latest_rag_interaction_log(RAG_INTERACTION_LOGS_DIR)
    
    eval_log_file = None
    if rag_log_file:
        rag_log_basename = os.path.basename(rag_log_file)
        date_str_match = "".join(filter(str.isdigit, rag_log_basename))
        if len(date_str_match) >= 8:
            date_str = date_str_match[:8]
            evaluation_name = "answer_gemini_flash" # 与 evaluation.py 中一致
            eval_file_name = f"eval_results_{evaluation_name}_{date_str}.jsonl"
            eval_log_file = os.path.join(EVAL_LOG_DIR, eval_file_name)
            refine_answer_logger.info(f"Attempting to load Answer evaluation results from: {eval_log_file}")
        else:
            refine_answer_logger.error(f"Could not reliably extract date from RAG log filename: {rag_log_basename}")

    if not rag_log_file or not eval_log_file or not os.path.exists(eval_log_file):
        refine_answer_logger.error("Required log files for answer finetune data generation not found. Exiting.")
    else:
        rag_interactions = load_logs_to_dict(rag_log_file, key_field="interaction_id")
        answer_evaluations = load_logs_to_dict(eval_log_file, key_field="original_interaction_id_ref")

        if rag_interactions and answer_evaluations:
            finetune_data = generate_finetune_samples_for_answer(rag_interactions, answer_evaluations)
            
            if finetune_data:
                today_for_filename = datetime.now().strftime("%Y%m%d")
                output_filepath = os.path.join(FINETUNING_GENERATED_DATA_DIR, f"answer_finetune_samples_{today_for_filename}.jsonl")
                
                with open(output_filepath, 'w', encoding='utf-8') as f_out:
                    for sample in finetune_data:
                        f_out.write(json.dumps(sample, ensure_ascii=False) + "\n")
                refine_answer_logger.info(f"Successfully saved {len(finetune_data)} Answer finetuning samples to: {output_filepath}")
                
                try:
                    df = pd.DataFrame(finetune_data)
                    csv_output_filepath = os.path.join(FINETUNING_GENERATED_DATA_DIR, f"answer_finetune_samples_review_{today_for_filename}.csv")
                    df.to_csv(csv_output_filepath, index=False, encoding='utf-8-sig')
                    refine_answer_logger.info(f"Reviewable CSV for answers saved to: {csv_output_filepath}")
                except Exception as e_csv:
                    refine_answer_logger.error(f"Failed to save answer review CSV: {e_csv}")
            else:
                refine_answer_logger.info("No answer finetuning samples were generated.")
        else:
            refine_answer_logger.error("Failed to load data from log files for answer finetuning.")
    --- END OF FILE CONTENT ---

  File: refine_cypher_data.py
    --- START OF FILE CONTENT ---
# zhz_agent/refine_cypher_finetune_data.py
import json
import os
import pandas as pd
from typing import List, Dict, Any, Optional, Tuple
import glob
from datetime import datetime

# 假设 utils.py 和 constants.py 在同一个 zhz_agent 包内
try:
    from zhz_rag.utils.common_utils import get_interaction_log_filepath, get_evaluation_result_log_filepath, find_latest_rag_interaction_log # <--- 修改这里
    from zhz_rag.config.constants import NEW_KG_SCHEMA_DESCRIPTION
except ImportError as e:
    print(f"ERROR: Could not import necessary modules: {e}")
    # ... (错误处理)
    exit(1)

import logging

# 配置此脚本的logger
refine_logger = logging.getLogger("RefineFinetuneDataLogger")
refine_logger.setLevel(logging.INFO)
if not refine_logger.hasHandlers():
    _console_handler = logging.StreamHandler()
    _formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
    _console_handler.setFormatter(_formatter)
    refine_logger.addHandler(_console_handler)
    refine_logger.info("--- RefineFinetuneDataLogger configured ---")

# --- 配置 ---
# 原始RAG交互日志的目录 (包含cypher_generation类型)
RAG_LOG_DIR = "zhz_rag/stored_data/rag_interaction_logs/"
# Gemini评估结果日志的目录
EVAL_LOG_DIR = "zhz_rag/stored_data/evaluation_results_logs/"
# 输出微调数据文件的目录
FINETUNE_DATA_DIR = "zhz_rag/finetuning/generated_data/"
os.makedirs(FINETUNE_DATA_DIR, exist_ok=True)


def load_logs_to_dict(filepath: str, key_field: str = "interaction_id") -> Dict[str, Dict[str, Any]]:
    """将JSONL文件加载到一个以指定字段为键的字典中。"""
    data_dict = {}
    if not os.path.exists(filepath):
        refine_logger.error(f"Log file not found: {filepath}")
        return data_dict
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                log_entry = json.loads(line.strip())
                if key_field in log_entry:
                    data_dict[log_entry[key_field]] = log_entry
                # 对于评估日志，我们可能需要用 original_interaction_id_ref 作为键
                elif key_field == "original_interaction_id_ref" and log_entry.get("original_interaction_id_ref"):
                    data_dict[log_entry["original_interaction_id_ref"]] = log_entry
            except json.JSONDecodeError:
                refine_logger.warning(f"Skipping malformed JSON line in {filepath}")
    return data_dict

def construct_qwen_input_prompt(user_question: str, schema_description: str) -> str:
    """
    根据用户问题和Schema描述构建Qwen生成Cypher时的完整输入Prompt。
    这个函数应该与 llm.py 中 generate_cypher_query 内部构建Prompt的逻辑一致。
    """
    # 这是我们在 llm.py -> generate_cypher_query 中使用的Prompt模板
    # 我们需要确保这里的模板与Qwen实际接收到的一致
    # 注意：这里使用了最新的V7版本（或您当前使用的版本）的Schema描述作为基础
    # 如果您的 generate_cypher_query 中的模板不同，请相应调整
    prompt = f"""<|im_start|>system
你是顶级Neo4j Cypher查询生成专家。你的任务是根据用户问题和严格提供的【知识图谱Schema】，生成一个【语法正确】、【逻辑合理】且【高效】的Cypher查询。

**【核心指令与约束 - 必须严格遵守！】**

1.  **【Schema绝对绑定 - 最高优先级】**:
    *   你生成的Cypher查询中所有用到的【节点标签】、【关系类型】、【属性名称】及其对应的【数据类型】，都**必须严格存在于**下面提供的 "知识图谱Schema描述" 中。
    *   在构建查询的每一步，都要反复与Schema核对。**严禁臆断、猜测或使用任何Schema中未明确定义的元素。**
    *   **属性名称的大小写和确切拼写必须与Schema完全一致。**
    *   **关系类型的名称和方向必须与Schema完全一致。** 例如，如果Schema定义为 `(Person)-[:WORKS_ON]->(Project)`，则查询中不能是 `(Project)-[:WORKS_ON]->(Person)`，除非Schema中也定义了反向关系。

2.  **【纯净输出格式 - 严格要求】**:
    *   如果能生成有效查询，你的回答**必须只包含纯粹的Cypher查询语句本身**。
    *   如果根据问题和Schema无法生成有效的Cypher查询（例如，问题超出了Schema表达能力，或问题本身逻辑不通），则**必须只输出固定的短语：“无法生成Cypher查询。”**
    *   **绝对禁止**在有效的Cypher语句前后添加任何前缀（如“Cypher查询: ”）、后缀、解释、注释、markdown标记（如 ```cypher ```）或任何其他多余的文本。

3.  **【属性与值的使用】**:
    *   当在`WHERE`子句中对属性进行匹配时，确保值的类型与Schema中定义的属性类型一致。例如，如果`name`是字符串，则匹配 `name: '张三'`；如果`year`是数字，则匹配 `year: 2023`。
    *   对于数值计算（如`SUM`, `AVG`），**必须**使用Schema中明确指定的数字类型属性（例如，`SalesAmount`节点的 `numeric_amount`）。

4.  **【查询构建逻辑指引】**:
    *   **实体识别**: 准确识别用户问题中的核心实体及其在Schema中对应的节点标签和属性。
    *   **关系路径**: 基于问题和Schema构建清晰的`MATCH`路径。
    *   **条件过滤**: 使用`WHERE`子句添加必要的过滤条件。
    *   **结果返回**: 使用`RETURN`子句指定需要返回的信息，并用`AS`为返回的列指定清晰、合法的别名（字母或下划线开头）。
    *   **多步查询**: 对于需要关联多个信息点的问题，合理使用`WITH`传递中间结果。
    *   **聚合**: 如需统计或汇总，正确使用`COUNT()`, `SUM()`, `COLLECT()`等聚合函数。

**【知识图谱Schema描述】**:
{schema_description}

**【查询示例 - 严格基于上述Schema】**:

*   用户问题: "张三参与了哪个项目？"
    Cypher查询: MATCH (p:Person {{name: '张三'}})-[:WORKS_ON]->(proj:Project) RETURN proj.name AS projectName

*   用户问题: "华东区域2024年第一季度的销售额是多少？"
    Cypher查询: MATCH (r:Region {{name: '华东'}})-[:HAS_SALES_AMOUNT]->(sa:SalesAmount {{period: '2024年第一季度'}}) RETURN sa.numeric_amount AS salesAmount, sa.unit AS salesUnit

*   用户问题: "查询所有产品的名称。"
    Cypher查询: MATCH (prod:Product) RETURN prod.name AS productName

*   用户问题: "项目X有哪些人参与？"
    Cypher查询: MATCH (p:Person)-[:WORKS_ON]->(proj:Project {{name: '项目X'}}) RETURN p.name AS participantName

*   用户问题: "2024年第一季度所有区域的总销售额是多少？"
    Cypher查询: MATCH (r:Region)-[:HAS_SALES_AMOUNT]->(sa:SalesAmount {{period: '2024年第一季度'}}) RETURN sum(sa.numeric_amount) AS totalSales, sa.unit AS commonUnit LIMIT 1 
    (此查询假设所有相关销售额的单位是相同的，并取第一个出现的单位作为代表)

*   用户问题: "与新产品A相关的文档ID是什么？"
    Cypher查询: MATCH (p:Product {{name: '新产品A'}})-[:RELATED_TO]->(d:Document) RETURN d.id AS documentId

*   用户问题: "公司CEO是谁？" (假设Schema中没有CEO信息)
    Cypher查询: 无法生成Cypher查询。

现在，请根据以下用户问题和上述Schema及规则生成Cypher查询。
<|im_end|>
<|im_start|>user
用户问题: {user_question}
<|im_end|>
<|im_start|>assistant
"""
    return prompt

def generate_finetune_samples_for_cypher(
    rag_interaction_logs: Dict[str, Dict[str, Any]],
    cypher_evaluation_logs: Dict[str, Dict[str, Any]]
) -> List[Dict[str, str]]:
    """
    根据原始交互日志和Gemini评估日志，生成用于Cypher微调的样本。
    返回一个列表，每个元素是 {"prompt": "...", "completion": "..."}
    """
    finetune_samples = []
    processed_ids = set()

    refine_logger.info(f"Processing {len(rag_interaction_logs)} RAG interaction logs and {len(cypher_evaluation_logs)} Cypher evaluation logs.")

    for interaction_id, rag_log in rag_interaction_logs.items():
        if rag_log.get("task_type") != "cypher_generation":
            continue

        if interaction_id in processed_ids:
            continue
        processed_ids.add(interaction_id)

        user_question = rag_log.get("user_query")
        qwen_generated_cypher_raw = rag_log.get("processed_llm_output") # 这是Qwen原始输出

        # --- 改进点: 处理Qwen输出为空或仅包含空白的情况 ---
        if qwen_generated_cypher_raw is None or not qwen_generated_cypher_raw.strip():
            qwen_generated_cypher = "无法生成Cypher查询." # 将空输出也视为无法生成
            refine_logger.info(f"Interaction {interaction_id}: Qwen output was empty/None, treating as '无法生成Cypher查询.'.")
        else:
            qwen_generated_cypher = qwen_generated_cypher_raw.strip()


        qwen_input_prompt = rag_log.get("llm_input_prompt")
        if not qwen_input_prompt:
            if user_question:
                qwen_input_prompt = construct_qwen_input_prompt(user_question, NEW_KG_SCHEMA_DESCRIPTION)
            else:
                refine_logger.warning(f"Skipping Cypher log {interaction_id} due to missing user_question for prompt reconstruction.")
                continue
        
        if not user_question: # qwen_generated_cypher 已确保非None
            refine_logger.warning(f"Skipping Cypher log {interaction_id} due to missing user_question.")
            continue

        ideal_cypher_output = None
        source_of_ideal = "unknown"
        gemini_score_for_log = None # 用于记录

        eval_log = cypher_evaluation_logs.get(interaction_id)

        if eval_log and eval_log.get("eval_llm_processed_output_json"):
            eval_json = eval_log["eval_llm_processed_output_json"]
            overall_score_str = eval_json.get("evaluation_summary", {}).get("overall_quality_score_cypher")
            gemini_suggestion_raw = eval_json.get("suggestion_for_improvement_cypher", "").strip()
            
            try:
                overall_score = int(overall_score_str)
                gemini_score_for_log = overall_score
            except (ValueError, TypeError):
                refine_logger.warning(f"Could not parse overall_quality_score_cypher for {interaction_id}: {overall_score_str}")
                overall_score = 0 # 默认给个低分
                gemini_score_for_log = 0

            # --- 规则1: Qwen自己就说无法生成 ---
            if qwen_generated_cypher == "无法生成Cypher查询.":
                # 如果Gemini也认为无法生成或评分低，那么采纳
                if "无法生成Cypher查询" in gemini_suggestion_raw or overall_score <= 2:
                    ideal_cypher_output = "无法生成Cypher查询."
                    source_of_ideal = "qwen_and_gemini_cannot_generate"
                # 如果Qwen说无法生成，但Gemini给出了高分建议，这很奇怪，需要人工看
                elif overall_score >=4 and "MATCH" in gemini_suggestion_raw.upper():
                     refine_logger.info(f"Cypher log {interaction_id}: Qwen said '无法生成', but Gemini suggested a high-score query '{gemini_suggestion_raw}'. Needs manual review.")
                     continue
                else: # Qwen说无法生成，Gemini建议不明确或中低分，也采纳Qwen的
                    ideal_cypher_output = "无法生成Cypher查询."
                    source_of_ideal = "qwen_cannot_generate_gemini_unclear"


            # --- 规则2: Qwen生成了查询，看Gemini评估 ---
            elif overall_score >= 4: # Gemini认为Qwen的输出质量高
                ideal_cypher_output = qwen_generated_cypher
                source_of_ideal = "qwen_high_score_by_gemini"

            
            # --- 规则3: Qwen的查询质量不高，但Gemini给出了具体的、看起来像Cypher的建议 ---
            elif gemini_suggestion_raw and \
                "无法生成Cypher查询" not in gemini_suggestion_raw and \
                "cannot be improved" not in gemini_suggestion_raw.lower() and \
                "needs to be extended" not in gemini_suggestion_raw.lower() and \
                ("MATCH " in gemini_suggestion_raw.upper() or \
                    "RETURN " in gemini_suggestion_raw.upper() or \
                    "CREATE " in gemini_suggestion_raw.upper() or \
                    "MERGE " in gemini_suggestion_raw.upper() or \
                    "WITH " in gemini_suggestion_raw.upper() or \
                    "OPTIONAL MATCH " in gemini_suggestion_raw.upper()
                ):

                # 简化处理：直接将 Gemini 的原始建议作为 completion 的候选
                # 清洗工作主要交给人工审核阶段
                # 我们仍然可以做非常基础的清理，比如首尾空格和常见的 markdown
                
                temp_completion = gemini_suggestion_raw.strip()
                if temp_completion.startswith("```") and temp_completion.endswith("```"):
                    temp_completion = temp_completion[3:-3].strip()
                    if temp_completion.lower().startswith("cypher"):
                        temp_completion = temp_completion[len("cypher"):].strip()
                elif temp_completion.startswith("`") and temp_completion.endswith("`"):
                    temp_completion = temp_completion[1:-1].strip()

                # 只要建议中包含核心Cypher关键字，我们就认为它有价值被审核
                core_cypher_keywords_check = ["MATCH", "RETURN", "CREATE", "MERGE", "WITH", "OPTIONAL MATCH"]
                suggestion_contains_cypher_keyword = False
                if temp_completion:
                    for core_keyword in core_cypher_keywords_check:
                        if core_keyword in temp_completion.upper():
                            suggestion_contains_cypher_keyword = True
                            break
                
                if suggestion_contains_cypher_keyword:
                    ideal_cypher_output = temp_completion # 使用初步清理后的建议
                    source_of_ideal = "gemini_suggestion_for_review" # 明确标记为需要审核
                    refine_logger.info(f"Interaction {interaction_id}: Gemini suggestion adopted for review. Raw: '{gemini_suggestion_raw[:150]}...', Processed for completion: '{ideal_cypher_output[:150]}...'")
                else:
                    refine_logger.warning(f"Interaction {interaction_id}: Gemini suggestion '{gemini_suggestion_raw[:150]}...' did not appear to contain core Cypher keywords after basic cleaning. Skipping.")
                    continue

            
            # --- 规则4: Gemini明确建议“无法生成” 或 Qwen的查询质量低且有严重问题 ---
            elif "无法生成Cypher查询" in gemini_suggestion_raw or \
                 (overall_score <= 2 and ("hallucinated" in eval_log.get("eval_llm_raw_output", "").lower() or \
                                         "schema violation" in eval_log.get("eval_llm_raw_output", "").lower() or \
                                         "syntax error" in eval_log.get("eval_llm_raw_output", "").lower())):
                ideal_cypher_output = "无法生成Cypher查询."
                source_of_ideal = "gemini_explicitly_cannot_generate_or_qwen_low_quality"
            
            # --- 规则5: 其他情况，需要人工审核 ---
            else:
                refine_logger.info(f"Cypher log {interaction_id} (Qwen: '{qwen_generated_cypher[:100]}...') needs manual review. Gemini score: {overall_score}, Suggestion: '{gemini_suggestion_raw[:100]}...'")
                continue 
        
        # --- 如果没有Gemini评估日志 ---
        else:
            refine_logger.warning(f"No valid Gemini evaluation found for Cypher log {interaction_id}. Qwen's output: '{qwen_generated_cypher[:100]}...'. Skipping for finetune data.")
            continue
            
        if ideal_cypher_output is not None:
            finetune_samples.append({
                "prompt": qwen_input_prompt,
                "completion": ideal_cypher_output.strip(), # 确保completion也strip
                "original_qwen_cypher": qwen_generated_cypher,
                "gemini_score": gemini_score_for_log,
                "source_of_ideal": source_of_ideal,
                "interaction_id": interaction_id
            })

    refine_logger.info(f"Generated {len(finetune_samples)} Cypher finetuning samples.")
    return finetune_samples


if __name__ == "__main__":
    # 1. 确定要处理的原始RAG交互日志文件 (包含cypher_generation)
    #    和对应的Gemini评估结果日志文件 (包含cypher_evaluation_result)
    
    # 自动查找最新的原始RAG交互日志
    rag_log_file = find_latest_rag_interaction_log(RAG_LOG_DIR) # utils.py中的函数
    
    # 构造对应的Gemini Cypher评估结果文件名
    # 假设评估文件名与原始日志文件名日期部分相同，且评估类型固定
    eval_log_file = None
    if rag_log_file:
        rag_log_basename = os.path.basename(rag_log_file)
        date_str_match = "".join(filter(str.isdigit, rag_log_basename)) # 提取文件名中的日期部分
        if len(date_str_match) >= 8: # 确保提取到至少YYYYMMDD
            date_str = date_str_match[:8]

            # 根据 evaluation.py 中 log_interaction_data 的 evaluation_name_for_file 参数构造
            evaluation_name = "cypher_gemini_flash" 
            eval_file_name = f"eval_results_{evaluation_name}_{date_str}.jsonl"
            
            eval_log_file = os.path.join(EVAL_LOG_DIR, eval_file_name)
            refine_logger.info(f"Attempting to load Cypher evaluation results from: {eval_log_file}")
        else:
            refine_logger.error(f"Could not reliably extract date from RAG log filename: {rag_log_basename}")
    
    if not rag_log_file or not eval_log_file or not os.path.exists(eval_log_file):
        refine_logger.error("Required log files not found. Exiting.")
        if not rag_log_file: refine_logger.error(f"RAG interaction log missing (expected pattern rag_interactions_*.jsonl in {RAG_LOG_DIR})")
        if rag_log_file and (not eval_log_file or not os.path.exists(eval_log_file)): refine_logger.error(f"Cypher evaluation result log missing (expected: {eval_log_file})")
    else:
        rag_interactions = load_logs_to_dict(rag_log_file, key_field="interaction_id")
        cypher_evaluations = load_logs_to_dict(eval_log_file, key_field="original_interaction_id_ref")

        if rag_interactions and cypher_evaluations:
            finetune_data = generate_finetune_samples_for_cypher(rag_interactions, cypher_evaluations)
            
            if finetune_data:
                # 获取当前日期用于文件名
                today_for_filename = datetime.now().strftime("%Y%m%d")
                output_filepath = os.path.join(FINETUNE_DATA_DIR, f"cypher_finetune_samples_{today_for_filename}.jsonl")
                
                with open(output_filepath, 'w', encoding='utf-8') as f_out:
                    for sample in finetune_data:
                        f_out.write(json.dumps(sample, ensure_ascii=False) + "\n")
                refine_logger.info(f"Successfully saved {len(finetune_data)} Cypher finetuning samples to: {output_filepath}")
                
                # 也可以同时保存一个CSV版本供人工审查
                try:
                    df = pd.DataFrame(finetune_data)
                    csv_output_filepath = os.path.join(FINETUNE_DATA_DIR, f"cypher_finetune_samples_review_{today_for_filename}.csv")
                    df.to_csv(csv_output_filepath, index=False, encoding='utf-8-sig')
                    refine_logger.info(f"Reviewable CSV saved to: {csv_output_filepath}")
                except Exception as e_csv:
                    refine_logger.error(f"Failed to save review CSV: {e_csv}")
            else:
                refine_logger.info("No finetuning samples were generated.")
        else:
            refine_logger.error("Failed to load data from log files.")
    --- END OF FILE CONTENT ---


Directory: zhz_rag/finetuning/generated_data/
  File (skipped, extension not included): answer_finetune_samples_20250531.jsonl
  File: answer_finetune_samples_review_20250531.csv
    --- START OF FILE CONTENT ---
﻿prompt,completion,original_qwen_answer,gemini_scores,gemini_suggestion,source_of_ideal,interaction_id
"
<|im_start|>system
你是一个AI问答助手。你的任务是根据【上下文信息】回答【用户问题】。

**核心指令：**

1.  **尝试直接回答：** 请首先仔细阅读【上下文信息】，如果其中包含能直接回答【用户问题】的内容，请用上下文中的信息直接、简洁地回答。
2.  **忠实原文：** 你的回答必须严格基于【上下文信息】，禁止加入任何外部知识或个人观点。
3.  **如果无法回答：** 如果你分析了【上下文信息】后，确认其中确实没有能回答【用户问题】的明确信息，那么请只回答以下这句话：
    ""根据目前提供的资料，我无法找到关于您问题的明确信息。""
    **不要添加任何其他解释、建议或反问。**

**请直接给出答案，或者只给出上述那句固定的“无法找到信息”的回复。**
<|im_start|>user
用户问题: 总结一下关于AI在医疗领域应用的主要内容。

上下文信息:
Source Type: vector_chroma, Score: 0.9200
Content: AI技术在医疗影像分析方面取得了显著进展，能够帮助医生更早发现癌症等疾病。例如，深度学习模型可以分析X光片、CT扫描结果，其准确率有时甚至超过人类专家。

---

Source Type: vector_chroma, Score: 0.8800
Content: 个性化医疗是未来的趋势，AI通过分析大量的患者数据，包括基因序列、生活方式、病史等，为每位患者量身定制治疗方案和用药指导，从而提高疗效并减少副作用。

---

Source Type: keyword_bm25s, Score: 15.5000
Content: 药物研发周期长、成本高是长期存在的问题。AI可以通过模拟筛选化合物、预测药物相互作用，极大地缩短新药从研发到上市的时间。
<|im_end|>
<|im_start|>assistant
",AI在医疗领域的主要应用包括辅助诊断（如通过图像识别分析医疗影像，识别病变），个性化治疗方案的制定（基于患者基因和生活习惯数据），以及新药研发加速（通过预测分子结构和药物反应）。,AI在医疗领域的主要应用包括辅助诊断（如通过图像识别分析医疗影像，识别病变），个性化治疗方案的制定（基于患者基因和生活习惯数据），以及新药研发加速（通过预测分子结构和药物反应）。,"{'overall': 5, 'faithfulness': 5, 'relevance': 5, 'completeness': 5, 'context_sufficiency': 'Sufficient'}",No improvement needed.,qwen_high_score_by_gemini,a1-rag-001
"
<|im_start|>system
你是一个AI问答助手。你的任务是根据【上下文信息】回答【用户问题】。

**核心指令：**

1.  **尝试直接回答：** 请首先仔细阅读【上下文信息】，如果其中包含能直接回答【用户问题】的内容，请用上下文中的信息直接、简洁地回答。
2.  **忠实原文：** 你的回答必须严格基于【上下文信息】，禁止加入任何外部知识或个人观点。
3.  **如果无法回答：** 如果你分析了【上下文信息】后，确认其中确实没有能回答【用户问题】的明确信息，那么请只回答以下这句话：
    ""根据目前提供的资料，我无法找到关于您问题的明确信息。""
    **不要添加任何其他解释、建议或反问。**

**请直接给出答案，或者只给出上述那句固定的“无法找到信息”的回复。**
<|im_start|>user
用户问题: 我们公司上季度的营销活动预算是多少？

上下文信息:
Source Type: vector_chroma, Score: 0.9500
Content: 会议纪要：Q3营销计划讨论...市场部张经理提出，本季度营销活动预算初步定为50万元，主要用于线上推广和内容制作...

---

Source Type: knowledge_graph, Score: 1.0000
Content: 事实: 营销活动Q3 -> 预算: 50万元
<|im_end|>
<|im_start|>assistant
",根据提供的信息，我们公司上季度的营销活动预算是50万元。,根据提供的信息，我们公司上季度的营销活动预算是50万元。,"{'overall': 4, 'faithfulness': 5, 'relevance': 5, 'completeness': 4, 'context_sufficiency': 'Sufficient'}",可以考虑在答案中提及信息来源，例如：'根据会议纪要和知识图谱信息，公司上季度营销活动预算为50万元。',qwen_high_score_by_gemini_with_minor_gemini_suggestion,a2-rag-002
"
<|im_start|>system
你是一个AI问答助手。你的任务是根据【上下文信息】回答【用户问题】。

**核心指令：**

1.  **尝试直接回答：** 请首先仔细阅读【上下文信息】，如果其中包含能直接回答【用户问题】的内容，请用上下文中的信息直接、简洁地回答。
2.  **忠实原文：** 你的回答必须严格基于【上下文信息】，禁止加入任何外部知识或个人观点。
3.  **如果无法回答：** 如果你分析了【上下文信息】后，确认其中确实没有能回答【用户问题】的明确信息，那么请只回答以下这句话：
    ""根据目前提供的资料，我无法找到关于您问题的明确信息。""
    **不要添加任何其他解释、建议或反问。**

**请直接给出答案，或者只给出上述那句固定的“无法找到信息”的回复。**
<|im_start|>user
用户问题: 请告诉我关于月球的三个有趣事实。

上下文信息:
Source Type: vector_chroma, Score: 0.8500
Content: 月球是地球的唯一天然卫星，其直径约为地球的四分之一。

---

Source Type: vector_chroma, Score: 0.9000
Content: 由于月球的自转周期和公转周期相等（潮汐锁定），我们从地球上始终只能看到月球的同一面。
<|im_end|>
<|im_start|>assistant
",月球是地球唯一的天然卫星，它的自转周期与公转周期相同，因此总是以同一面朝向地球。此外，月球的引力导致了地球上的潮汐现象，并且月球上没有大气层。,月球是地球唯一的天然卫星，它的自转周期与公转周期相同，因此总是以同一面朝向地球。此外，月球的引力导致了地球上的潮汐现象，并且月球上没有大气层。,"{'overall': 2, 'faithfulness': 2, 'relevance': 4, 'completeness': 2, 'context_sufficiency': 'Partially Sufficient'}",答案应严格基于提供的上下文。如果上下文不足以提供三个事实，应诚实说明，例如：'根据提供的信息，月球是地球唯一的天然卫星，并且总是以同一面朝向地球。目前信息不足以提供第三个事实。',qwen_low_score_with_gemini_suggestion_for_review,a3-rag-003
"
<|im_start|>system
你是一个AI问答助手。你的任务是根据【上下文信息】回答【用户问题】。

**核心指令：**

1.  **尝试直接回答：** 请首先仔细阅读【上下文信息】，如果其中包含能直接回答【用户问题】的内容，请用上下文中的信息直接、简洁地回答。
2.  **忠实原文：** 你的回答必须严格基于【上下文信息】，禁止加入任何外部知识或个人观点。
3.  **如果无法回答：** 如果你分析了【上下文信息】后，确认其中确实没有能回答【用户问题】的明确信息，那么请只回答以下这句话：
    ""根据目前提供的资料，我无法找到关于您问题的明确信息。""
    **不要添加任何其他解释、建议或反问。**

**请直接给出答案，或者只给出上述那句固定的“无法找到信息”的回复。**
<|im_start|>user
用户问题: Alpha项目的负责人是谁？

上下文信息:
Source Type: vector_chroma, Score: 0.7000
Content: 项目Beta的负责人是李明，项目Gamma的负责人是王芳。

---

Source Type: keyword_bm25s, Score: 10.1000
Content: 所有项目列表：项目Beta, 项目Gamma, 项目Delta。
<|im_end|>
<|im_start|>assistant
",根据目前提供的资料，我无法找到关于您问题的明确信息。,根据目前提供的资料，我无法找到关于您问题的明确信息。,"{'overall': 5, 'faithfulness': 5, 'relevance': 5, 'completeness': 5, 'context_sufficiency': 'Insufficient'}",No improvement needed.,qwen_standard_no_answer_confirmed_by_gemini_context_insufficient,a4-rag-004
"
<|im_start|>system
你是一个AI问答助手。你的任务是根据【上下文信息】回答【用户问题】。

**核心指令：**

1.  **尝试直接回答：** 请首先仔细阅读【上下文信息】，如果其中包含能直接回答【用户问题】的内容，请用上下文中的信息直接、简洁地回答。
2.  **忠实原文：** 你的回答必须严格基于【上下文信息】，禁止加入任何外部知识或个人观点。
3.  **如果无法回答：** 如果你分析了【上下文信息】后，确认其中确实没有能回答【用户问题】的明确信息，那么请只回答以下这句话：
    ""根据目前提供的资料，我无法找到关于您问题的明确信息。""
    **不要添加任何其他解释、建议或反问。**

**请直接给出答案，或者只给出上述那句固定的“无法找到信息”的回复。**
<|im_start|>user
用户问题: 解释一下什么是量子纠缠？

上下文信息:
Source Type: vector_chroma, Score: 0.9300
Content: 根据量子力学，当两个或多个粒子共同组成一个量子态时，即使它们在空间上分离，对其中一个粒子的测量也会瞬间影响到其他粒子的状态，这种现象被称为量子纠缠。

---

Source Type: vector_chroma, Score: 0.7800
Content: 爱因斯坦称量子纠缠为“鬼魅般的超距作用”。
<|im_end|>
<|im_start|>assistant
",量子纠缠是指当几个粒子在彼此相互作用后，由于各个粒子所拥有的特性已综合成为整体性质，无法单独描述各个粒子的性质，只能描述整体系统的性质的现象。,量子纠缠是指当几个粒子在彼此相互作用后，由于各个粒子所拥有的特性已综合成为整体性质，无法单独描述各个粒子的性质，只能描述整体系统的性质的现象。,"{'overall': 3, 'faithfulness': 3, 'relevance': 4, 'completeness': 2, 'context_sufficiency': 'Sufficient'}",答案应该更好地利用上下文信息，特别是上下文片段1中关于'即使它们在空间上分离，对其中一个粒子的测量也会瞬间影响到其他粒子的状态'的描述，这比当前答案更易于理解。可以考虑直接改写或总结上下文片段1的核心内容。,qwen_low_score_with_gemini_suggestion_for_review,a5-rag-005

    --- END OF FILE CONTENT ---

  File (skipped, extension not included): cypher_finetune_samples_20250531.jsonl
  File: cypher_finetune_samples_review_20250531.csv
    --- START OF FILE CONTENT ---
﻿prompt,completion,original_qwen_cypher,gemini_score,source_of_ideal,interaction_id
"<|im_start|>system
你是顶级Neo4j Cypher查询生成专家... (省略完整Schema和指令) ...【知识图谱Schema描述】
:ExtractedEntity {text: STRING, label: STRING (PERSON, ORGANIZATION, TASK)}
(:ExtractedEntity {label:""PERSON""})-[:WORKS_AT]->(:ExtractedEntity {label:""ORGANIZATION""})
(:ExtractedEntity {label:""TASK""})-[:ASSIGNED_TO]->(:ExtractedEntity {label:""PERSON""})
... (省略示例) ...现在，请根据以下用户问题和上述Schema及规则生成Cypher查询。
<|im_end|>
<|im_start|>user
用户问题: 张三在哪里工作？
<|im_end|>
<|im_start|>assistant
","MATCH (p:ExtractedEntity {text: '张三', label: 'PERSON'})-[:WORKS_AT]->(org:ExtractedEntity {label: 'ORGANIZATION'}) RETURN org.text AS organizationName","MATCH (p:ExtractedEntity {text: '张三', label: 'PERSON'})-[:WORKS_AT]->(org:ExtractedEntity {label: 'ORGANIZATION'}) RETURN org.text AS organizationName",5,qwen_high_score_by_gemini,c1-rag-001
"<|im_start|>system
你是顶级Neo4j Cypher查询生成专家... (同上) ...
<|im_end|>
<|im_start|>user
用户问题: 李四负责了哪些任务，并且这些任务的负责人是谁？
<|im_end|>
<|im_start|>assistant
","为了更准确地回答，查询应该确保只返回李四作为负责人的任务。可以考虑修改为：MATCH (task:ExtractedEntity {label: 'TASK'})-[:ASSIGNED_TO]->(p:ExtractedEntity {text: '李四', label: 'PERSON'}) RETURN task.text AS taskName, p.text AS assigneeName","MATCH (task:ExtractedEntity {label: 'TASK'})-[:ASSIGNED_TO]->(p:ExtractedEntity {text: '李四', label: 'PERSON'}), (task)-[:ASSIGNED_TO]->(assignee:ExtractedEntity {label: 'PERSON'}) RETURN task.text AS taskName, assignee.text AS assigneeName",3,gemini_suggestion_for_review,c2-rag-002
"<|im_start|>system
你是顶级Neo4j Cypher查询生成专家... (同上) ...
<|im_end|>
<|im_start|>user
用户问题: 项目Alpha的状态是什么？
<|im_end|>
<|im_start|>assistant
",无法生成Cypher查询.,MATCH (t:Task {name: '项目Alpha'}) RETURN t.status,1,gemini_explicitly_cannot_generate_or_qwen_low_quality,c3-rag-003
"<|im_start|>system
你是顶级Neo4j Cypher查询生成专家... (同上) ...
<|im_end|>
<|im_start|>user
用户问题: 王五的直属上级是谁？
<|im_end|>
<|im_start|>assistant
",无法生成Cypher查询.,无法生成Cypher查询.,5,qwen_and_gemini_cannot_generate,c4-rag-004
"<|im_start|>system
你是顶级Neo4j Cypher查询生成专家... (同上) ...
<|im_end|>
<|im_start|>user
用户问题: 数据库中有多少人？
<|im_end|>
<|im_start|>assistant
",无法生成Cypher查询.,无法生成Cypher查询.,1,qwen_and_gemini_cannot_generate,c6-rag-006

    --- END OF FILE CONTENT ---


Directory: zhz_rag/stored_data/
  File: __init__.py
    --- File is empty ---


Directory: zhz_rag/stored_data/rag_interaction_logs/
  File (skipped, extension not included): rag_interactions_20250605.jsonl
  File (skipped, extension not included): rag_interactions_20250606.jsonl
  File (skipped, extension not included): rag_interactions_20250607.jsonl
  File (skipped, extension not included): rag_interactions_20250608.jsonl

Directory: zhz_rag/stored_data/evaluation_results_logs/

Directory: zhz_rag/stored_data/kuzu_default_db/
  File (skipped, extension not included): .lock
  File (skipped, extension not included): .shadow
  File (skipped, extension not included): .wal
  File (skipped, extension not included): catalog.kz
  File (skipped, extension not included): data.kz
  File (skipped, extension not included): metadata.kz
  File (skipped, extension not included): n-0.hindex
  File (skipped, extension not included): n-0.hindex.ovf

Directory: zhz_rag/stored_data/chromadb_index/
  File (skipped, extension not included): chroma.sqlite3

Directory: zhz_rag/stored_data/chromadb_index/fc39f817-3d12-4574-a52b-5bdfe2d8f24b/
  File (skipped, extension not included): data_level0.bin
  File (skipped, extension not included): header.bin
  File (skipped, extension not included): length.bin
  File (skipped, extension not included): link_lists.bin

Directory: zhz_rag/stored_data/kuzu_test_visibility_db/
  File (skipped, extension not included): .lock
  File (skipped, extension not included): .shadow
  File (skipped, extension not included): .wal
  File (skipped, extension not included): catalog.kz
  File (skipped, extension not included): data.kz
  File (skipped, extension not included): metadata.kz
  File (skipped, extension not included): n-0.hindex
  File (skipped, extension not included): n-0.hindex.ovf

Directory: zhz_rag/stored_data/bm25_index/
  File (skipped, extension not included): data.csc.index.npy
  File (skipped, extension not included): doc_ids.pkl
  File (skipped, extension not included): indices.csc.index.npy
  File (skipped, extension not included): indptr.csc.index.npy
  File: params.index.json
    --- START OF FILE CONTENT ---
{
    "k1": 1.5,
    "b": 0.75,
    "delta": 0.5,
    "method": "lucene",
    "idf_method": "lucene",
    "dtype": "float32",
    "int_dtype": "int32",
    "num_docs": 2,
    "version": "0.2.13",
    "backend": "numpy"
}
    --- END OF FILE CONTENT ---

  File: vocab.index.json
    --- START OF FILE CONTENT ---
{"三":0,"的":1,"分配":2,"文档":3,"工作":4,"第二个":5,"自然":6,"语言":7,"自然语言":8,"谷歌":9,"了":10,"技术":11,"任务":12,"。":13,"第二":14,"处理":15,"编写":16,"给":17,"张":18,"Alpha":19,"张三在":20,"项目":21,"二个":22,"讨论":23,"任务分配":24,"":25}
    --- END OF FILE CONTENT ---


Directory: zhz_rag/evaluation/
  File: __init__.py
    --- File is empty ---

  File: analyze_answer.py
    --- START OF FILE CONTENT ---
# zhz_rag/evaluation/analyze_answer.py
import json
import os
import pandas as pd
from typing import List, Dict, Any, Optional
from collections import Counter
from datetime import datetime
import glob 

# --- 从项目中导入必要的模块 ---
try:
    from zhz_rag.utils.common_utils import (
        load_jsonl_file, # <--- 使用新的通用函数
        EVALUATION_RESULTS_LOGS_DIR # 导入评估日志目录常量
    )
except ImportError as e:
    print(f"ERROR: Could not import necessary modules in analyze_answer.py: {e}")
    print("Make sure this script is run in an environment where 'zhz_rag' package is accessible.")
    exit(1)

import logging

# --- 配置此脚本的logger ---
analyze_answer_logger = logging.getLogger("AnalyzeAnswerLogger")
analyze_answer_logger.setLevel(logging.INFO)
if not analyze_answer_logger.hasHandlers():
    _console_handler = logging.StreamHandler()
    _formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
    _console_handler.setFormatter(_formatter)
    analyze_answer_logger.addHandler(_console_handler)
    analyze_answer_logger.propagate = False
analyze_answer_logger.info("--- AnalyzeAnswerLogger configured ---")

# --- 核心功能函数 ---

def extract_answer_evaluation_details(log_entry: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """
    从单条已解析的答案评估日志条目中提取关键信息。
    这个函数与您之前在 analyze_answer.py 中的版本基本一致，稍作调整。
    """
    details = {}
    eval_data = log_entry.get("eval_llm_processed_output_json")

    if not eval_data or not isinstance(eval_data, dict):
        analyze_answer_logger.warning(f"Skipping log entry due to missing or invalid 'eval_llm_processed_output_json'. Interaction ID ref: {log_entry.get('original_interaction_id_ref')}")
        return None

    details["interaction_id_ref"] = log_entry.get("original_interaction_id_ref")
    details["user_question"] = log_entry.get("user_question_for_eval")
    details["generated_answer"] = log_entry.get("generated_answer_for_eval")
    
    summary = eval_data.get("evaluation_summary", {})
    dimensions = eval_data.get("dimensions", {})
    
    details["overall_answer_quality_score"] = summary.get("overall_answer_quality_score")
    details["main_strengths_answer"] = summary.get("main_strengths_answer")
    details["main_weaknesses_answer"] = summary.get("main_weaknesses_answer")
    
    faithfulness = dimensions.get("faithfulness", {})
    details["faithfulness_score"] = faithfulness.get("score")
    details["faithfulness_reasoning"] = faithfulness.get("reasoning")
    # 确保 problematic_answer_segments_faithfulness 是列表，然后 join
    problematic_segments = faithfulness.get("problematic_answer_segments_faithfulness", [])
    if isinstance(problematic_segments, list):
        details["faithfulness_problematic_segments"] = "; ".join(problematic_segments)
    else:
        details["faithfulness_problematic_segments"] = str(problematic_segments) # 以防万一不是列表

    relevance = dimensions.get("relevance", {})
    details["relevance_score"] = relevance.get("score")
    details["relevance_reasoning"] = relevance.get("reasoning")
    
    completeness = dimensions.get("completeness", {})
    details["completeness_context_sufficiency"] = completeness.get("context_sufficiency_assessment")
    details["completeness_context_reasoning"] = completeness.get("context_sufficiency_reasoning")
    details["completeness_score"] = completeness.get("score")
    details["completeness_reasoning"] = completeness.get("reasoning")
    
    coherence = dimensions.get("coherence_fluency", {}) # 键名可能与prompt中的一致
    details["coherence_score"] = coherence.get("score")
    details["coherence_reasoning"] = coherence.get("reasoning")

    actionability = dimensions.get("actionability_usability", {}) # 键名可能与prompt中的一致
    details["actionability_score"] = actionability.get("score")
    details["actionability_reasoning"] = actionability.get("reasoning")
    
    details["gemini_suggestion_answer"] = eval_data.get("suggestion_for_answer_improvement")

    return details

def perform_answer_evaluation_analysis(
    evaluation_log_filepath: str,
    output_csv_filepath: str
) -> bool:
    """
    加载答案评估日志，进行分析，并保存结果到CSV。

    Args:
        evaluation_log_filepath (str): 答案评估结果日志文件的路径。
        output_csv_filepath (str): 分析结果CSV文件的保存路径。

    Returns:
        bool: 如果分析和保存成功则返回True，否则返回False。
    """
    analyze_answer_logger.info(f"Starting Answer evaluation analysis for log file: {evaluation_log_filepath}")
    analyze_answer_logger.info(f"Analysis results will be saved to: {output_csv_filepath}")

    evaluation_logs = load_jsonl_file(evaluation_log_filepath)

    if not evaluation_logs:
        analyze_answer_logger.warning(f"No evaluation logs found or loaded from {evaluation_log_filepath}. Analysis aborted.")
        return False

    extracted_details_list = []
    for log_entry in evaluation_logs:
        if log_entry.get("task_type") == "answer_evaluation_result": # 确保是答案评估日志
            details = extract_answer_evaluation_details(log_entry)
            if details:
                extracted_details_list.append(details)
        else:
            analyze_answer_logger.debug(f"Skipping log entry with task_type '{log_entry.get('task_type')}' as it's not 'answer_evaluation_result'.")


    if not extracted_details_list:
        analyze_answer_logger.info("No valid Answer evaluation details extracted from the logs. No CSV will be generated.")
        return False

    df = pd.DataFrame(extracted_details_list)
    
    score_columns = [
        "overall_answer_quality_score", "faithfulness_score", "relevance_score",
        "completeness_score", "coherence_score", "actionability_score"
    ]
    for col in score_columns:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')

    analyze_answer_logger.info(f"\n--- Preliminary Answer Evaluation Analysis (from {len(extracted_details_list)} entries) ---")
    analyze_answer_logger.info(f"Total evaluation entries processed: {len(df)}")

    for col_name in score_columns:
        if col_name in df.columns and not df[col_name].isnull().all():
            analyze_answer_logger.info(f"\nDimension: {col_name}")
            analyze_answer_logger.info(f"{df[col_name].describe()}")
            analyze_answer_logger.info("Score Distribution:")
            analyze_answer_logger.info(f"{df[col_name].value_counts(dropna=False).sort_index()}")
        else:
            analyze_answer_logger.info(f"\nDimension: {col_name} - No data or all NaN.")
            
    if "completeness_context_sufficiency" in df.columns and not df["completeness_context_sufficiency"].isnull().all():
        analyze_answer_logger.info("\nContext Sufficiency Assessment Distribution:")
        analyze_answer_logger.info(f"{df['completeness_context_sufficiency'].value_counts(dropna=False)}")
    else:
        analyze_answer_logger.info("\nContext Sufficiency Assessment Distribution: No data.")

    try:
        os.makedirs(os.path.dirname(output_csv_filepath), exist_ok=True)
        df.to_csv(output_csv_filepath, index=False, encoding='utf-8-sig')
        analyze_answer_logger.info(f"\nAnalysis results saved to: {output_csv_filepath}")
        return True
    except Exception as e:
        analyze_answer_logger.error(f"\nFailed to save CSV file: {e}", exc_info=True)
        return False

if __name__ == "__main__":
    EVALUATION_NAME_FOR_ANSWER = "answer_gemini_flash" 
    
    # --- 动态查找最新的评估结果日志文件 ---
    eval_logs_pattern = os.path.join(EVALUATION_RESULTS_LOGS_DIR, f"eval_results_{EVALUATION_NAME_FOR_ANSWER}_*.jsonl")
    all_eval_logs = sorted(glob.glob(eval_logs_pattern), key=os.path.getmtime, reverse=True)
    
    log_file_path_answer: Optional[str] = None
    output_csv_path_answer: Optional[str] = None

    if all_eval_logs:
        log_file_path_answer = all_eval_logs[0] # 获取最新的一个
        analyze_answer_logger.info(f"Found latest Answer evaluation log for analysis: {log_file_path_answer}")
        
        # 根据找到的日志文件名构造输出的 CSV 文件名
        base_log_name = os.path.basename(log_file_path_answer)
        # 从 "eval_results_answer_gemini_flash_YYYYMMDD.jsonl" 生成 "analysis_answer_gemini_flash_YYYYMMDD.csv"
        if base_log_name.startswith(f"eval_results_{EVALUATION_NAME_FOR_ANSWER}_") and base_log_name.endswith(".jsonl"):
            date_part_from_filename = base_log_name[len(f"eval_results_{EVALUATION_NAME_FOR_ANSWER}_"):-len(".jsonl")]
            output_csv_name_answer = f"analysis_{EVALUATION_NAME_FOR_ANSWER}_{date_part_from_filename}.csv"
            output_csv_path_answer = os.path.join(EVALUATION_RESULTS_LOGS_DIR, output_csv_name_answer)
        else: # Fallback naming for CSV
            today_str = datetime.now().strftime("%Y%m%d")
            output_csv_name_answer = f"analysis_{EVALUATION_NAME_FOR_ANSWER}_{today_str}_fallback.csv"
            output_csv_path_answer = os.path.join(EVALUATION_RESULTS_LOGS_DIR, output_csv_name_answer)
        analyze_answer_logger.info(f"Analysis CSV report will be saved to: {output_csv_path_answer}")
    else:
        analyze_answer_logger.error(f"No Answer evaluation log files found matching pattern: {eval_logs_pattern}")

    if log_file_path_answer and output_csv_path_answer and os.path.exists(log_file_path_answer):
        perform_answer_evaluation_analysis(
            evaluation_log_filepath=log_file_path_answer,
            output_csv_filepath=output_csv_path_answer
        )
    else:
        analyze_answer_logger.info("Answer evaluation analysis will not run as no suitable log file was identified or output path could not be determined.")
    --- END OF FILE CONTENT ---

  File: analyze_cypher.py
    --- START OF FILE CONTENT ---
# zhz_rag/evaluation/analyze_cypher.py
import json
import os
import pandas as pd
from typing import List, Dict, Any, Optional
from collections import Counter
from datetime import datetime
import glob 

# --- 从项目中导入必要的模块 ---
try:
    from zhz_rag.utils.common_utils import (
        load_jsonl_file, # <--- 使用新的通用函数
        EVALUATION_RESULTS_LOGS_DIR # 导入评估日志目录常量
    )
except ImportError as e:
    print(f"ERROR: Could not import necessary modules in analyze_cypher.py: {e}")
    print("Make sure this script is run in an environment where 'zhz_rag' package is accessible.")
    exit(1)

import logging

# --- 配置此脚本的logger ---
analyze_cypher_logger = logging.getLogger("AnalyzeCypherLogger")
analyze_cypher_logger.setLevel(logging.INFO)
if not analyze_cypher_logger.hasHandlers():
    _console_handler = logging.StreamHandler()
    _formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
    _console_handler.setFormatter(_formatter)
    analyze_cypher_logger.addHandler(_console_handler)
    analyze_cypher_logger.propagate = False
analyze_cypher_logger.info("--- AnalyzeCypherLogger configured ---")

# --- 核心功能函数 ---

def extract_cypher_evaluation_details(log_entry: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """
    从单条已解析的Cypher评估日志条目中提取关键信息。
    这个函数与您之前在 analyze_cypher.py 中的版本基本一致，稍作调整以适应通用加载。
    """
    details = {}
    # eval_llm_processed_output_json 字段包含了Gemini评估的JSON输出
    eval_data = log_entry.get("eval_llm_processed_output_json")

    if not eval_data or not isinstance(eval_data, dict):
        analyze_cypher_logger.warning(f"Skipping log entry due to missing or invalid 'eval_llm_processed_output_json'. Interaction ID ref: {log_entry.get('original_interaction_id_ref')}")
        return None # 如果核心评估数据缺失，则跳过此条目

    details["interaction_id_ref"] = log_entry.get("original_interaction_id_ref")
    details["user_question"] = log_entry.get("user_question_for_eval")
    details["generated_cypher"] = log_entry.get("generated_cypher_for_eval")
    
    summary = eval_data.get("evaluation_summary", {})
    dimensions = eval_data.get("dimensions", {})
    
    details["overall_quality_score"] = summary.get("overall_quality_score_cypher")
    details["main_strength"] = summary.get("main_strength_cypher")
    details["main_weakness"] = summary.get("main_weakness_cypher")
    
    syntax = dimensions.get("syntactic_correctness", {})
    details["syntax_score"] = syntax.get("score")
    details["syntax_reasoning"] = syntax.get("reasoning")
    
    schema = dimensions.get("schema_adherence", {})
    details["schema_overall_score"] = schema.get("overall_score")
    details["schema_node_label_correct"] = schema.get("node_label_correctness", {}).get("check_result")
    details["schema_entity_type_correct"] = schema.get("entity_type_property_correctness", {}).get("check_result")
    details["schema_rel_type_correct"] = schema.get("relationship_type_correctness", {}).get("check_result")
    details["schema_prop_name_correct"] = schema.get("property_name_correctness", {}).get("check_result")
    details["schema_hallucinated_present"] = schema.get("hallucinated_schema_elements", {}).get("check_result_hallucination_present")
    details["schema_hallucinated_elements"] = ", ".join(schema.get("hallucinated_schema_elements", {}).get("elements_found", []))
    details["schema_reasoning"] = schema.get("reasoning")
    
    intent = dimensions.get("intent_accuracy", {})
    details["intent_score"] = intent.get("score")
    details["intent_explanation_cypher"] = intent.get("explanation_of_cypher_retrieval")
    details["intent_alignment_notes"] = intent.get("semantic_alignment_with_question")
    details["intent_key_elements_notes"] = intent.get("key_element_coverage_notes")
    details["intent_reasoning"] = intent.get("reasoning")
    
    details["qwen_error_patterns"] = ", ".join(eval_data.get("qwen_error_patterns_identified", []))
    details["gemini_suggestion"] = eval_data.get("suggestion_for_improvement_cypher")

    return details

def perform_cypher_evaluation_analysis(
    evaluation_log_filepath: str,
    output_csv_filepath: str
) -> bool:
    """
    加载Cypher评估日志，进行分析，并保存结果到CSV。

    Args:
        evaluation_log_filepath (str): Cypher评估结果日志文件的路径。
        output_csv_filepath (str): 分析结果CSV文件的保存路径。

    Returns:
        bool: 如果分析和保存成功则返回True，否则返回False。
    """
    analyze_cypher_logger.info(f"Starting Cypher evaluation analysis for log file: {evaluation_log_filepath}")
    analyze_cypher_logger.info(f"Analysis results will be saved to: {output_csv_filepath}")

    # 使用通用函数加载评估日志
    # 注意：evaluate_cypher_with_gemini 保存的日志中 task_type 是 "cypher_evaluation_by_gemini"
    # load_jsonl_file 不关心 task_type，它会加载所有行
    evaluation_logs = load_jsonl_file(evaluation_log_filepath)

    if not evaluation_logs:
        analyze_cypher_logger.warning(f"No evaluation logs found or loaded from {evaluation_log_filepath}. Analysis aborted.")
        return False

    extracted_details_list = []
    for log_entry in evaluation_logs:
        # 确保只处理Cypher评估结果
        if log_entry.get("task_type") == "cypher_evaluation_by_gemini":
            details = extract_cypher_evaluation_details(log_entry)
            if details: # extract_cypher_evaluation_details 可能会返回 None
                extracted_details_list.append(details)
        else:
            analyze_cypher_logger.debug(f"Skipping log entry with task_type '{log_entry.get('task_type')}' as it's not 'cypher_evaluation_by_gemini'.")


    if not extracted_details_list:
        analyze_cypher_logger.info("No valid Cypher evaluation details extracted from the logs. No CSV will be generated.")
        return False

    df = pd.DataFrame(extracted_details_list)
    
    score_columns = ["overall_quality_score", "syntax_score", "schema_overall_score", "intent_score"]
    for col in score_columns:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')

    analyze_cypher_logger.info(f"\n--- Preliminary Cypher Evaluation Analysis (from {len(extracted_details_list)} entries) ---")
    analyze_cypher_logger.info(f"Total evaluation entries processed: {len(df)}")

    if "overall_quality_score" in df.columns and not df["overall_quality_score"].isnull().all():
        analyze_cypher_logger.info("\n1. Overall Quality Score:")
        analyze_cypher_logger.info(f"{df['overall_quality_score'].describe()}")
        analyze_cypher_logger.info("\nScore Distribution:")
        analyze_cypher_logger.info(f"{df['overall_quality_score'].value_counts(dropna=False).sort_index()}")
    else:
        analyze_cypher_logger.info("\n1. Overall Quality Score: No data or all NaN.")


    if "schema_overall_score" in df.columns and not df["schema_overall_score"].isnull().all():
        analyze_cypher_logger.info("\n2. Schema Adherence Overall Score:")
        analyze_cypher_logger.info(f"{df['schema_overall_score'].describe()}")
        analyze_cypher_logger.info("\nScore Distribution:")
        analyze_cypher_logger.info(f"{df['schema_overall_score'].value_counts(dropna=False).sort_index()}")
        
        schema_sub_checks = [
            "schema_node_label_correct", "schema_entity_type_correct", 
            "schema_rel_type_correct", "schema_prop_name_correct", 
            "schema_hallucinated_present"
        ]
        analyze_cypher_logger.info("\nSchema Adherence Sub-item Issues (False means issue, Hallucinated True means issue):")
        for check in schema_sub_checks:
            if check in df.columns:
                if check == "schema_hallucinated_present":
                    issue_count = df[df[check] == True].shape[0]
                    analyze_cypher_logger.info(f"  - {check} (Hallucination Present): {issue_count} entries")
                else:
                    issue_count = df[df[check] == False].shape[0]
                    analyze_cypher_logger.info(f"  - {check} (Incorrect): {issue_count} entries")
    else:
        analyze_cypher_logger.info("\n2. Schema Adherence Overall Score: No data or all NaN.")


    if "intent_score" in df.columns and not df["intent_score"].isnull().all():
        analyze_cypher_logger.info("\n3. Intent Accuracy Score:")
        analyze_cypher_logger.info(f"{df['intent_score'].describe()}")
        analyze_cypher_logger.info("\nScore Distribution:")
        analyze_cypher_logger.info(f"{df['intent_score'].value_counts(dropna=False).sort_index()}")
    else:
        analyze_cypher_logger.info("\n3. Intent Accuracy Score: No data or all NaN.")


    if "qwen_error_patterns" in df.columns and not df["qwen_error_patterns"].isnull().all():
        analyze_cypher_logger.info("\n4. Identified Qwen Error Patterns (Top 5):")
        all_patterns = []
        for pattern_list_str in df["qwen_error_patterns"].dropna():
            if pattern_list_str and isinstance(pattern_list_str, str):
                all_patterns.extend([p.strip() for p in pattern_list_str.split(",") if p.strip()])
        pattern_counts = Counter(all_patterns)
        analyze_cypher_logger.info(f"{pattern_counts.most_common(5)}")
    else:
        analyze_cypher_logger.info("\n4. Identified Qwen Error Patterns: No data.")
        
    try:
        # Ensure output directory exists
        os.makedirs(os.path.dirname(output_csv_filepath), exist_ok=True)
        df.to_csv(output_csv_filepath, index=False, encoding='utf-8-sig')
        analyze_cypher_logger.info(f"\nAnalysis results saved to: {output_csv_filepath}")
        return True
    except Exception as e:
        analyze_cypher_logger.error(f"\nFailed to save CSV file: {e}", exc_info=True)
        return False

if __name__ == "__main__":
    EVALUATION_NAME_FOR_CYPHER = "cypher_gemini_flash" 

    eval_logs_pattern = os.path.join(EVALUATION_RESULTS_LOGS_DIR, f"eval_results_{EVALUATION_NAME_FOR_CYPHER}_*.jsonl")
    all_eval_logs = sorted(glob.glob(eval_logs_pattern), key=os.path.getmtime, reverse=True)
    
    log_file_path_cypher: Optional[str] = None
    output_csv_path_cypher: Optional[str] = None

    if all_eval_logs:
        log_file_path_cypher = all_eval_logs[0]
        analyze_cypher_logger.info(f"Found latest Cypher evaluation log for analysis: {log_file_path_cypher}")
        
        base_log_name = os.path.basename(log_file_path_cypher)
        if base_log_name.startswith(f"eval_results_{EVALUATION_NAME_FOR_CYPHER}_") and base_log_name.endswith(".jsonl"):
            date_part_from_filename = base_log_name[len(f"eval_results_{EVALUATION_NAME_FOR_CYPHER}_"):-len(".jsonl")]
            output_csv_name_cypher = f"analysis_{EVALUATION_NAME_FOR_CYPHER}_{date_part_from_filename}.csv"
            output_csv_path_cypher = os.path.join(EVALUATION_RESULTS_LOGS_DIR, output_csv_name_cypher)
        else:
            today_str = datetime.now().strftime("%Y%m%d")
            output_csv_name_cypher = f"analysis_{EVALUATION_NAME_FOR_CYPHER}_{today_str}_fallback.csv"
            output_csv_path_cypher = os.path.join(EVALUATION_RESULTS_LOGS_DIR, output_csv_name_cypher)
        analyze_cypher_logger.info(f"Analysis CSV report will be saved to: {output_csv_path_cypher}")
    else:
        analyze_cypher_logger.error(f"No Cypher evaluation log files found matching pattern: {eval_logs_pattern}")

    if log_file_path_cypher and output_csv_path_cypher and os.path.exists(log_file_path_cypher):
        perform_cypher_evaluation_analysis(
            evaluation_log_filepath=log_file_path_cypher,
            output_csv_filepath=output_csv_path_cypher
        )
    else:
        analyze_cypher_logger.info("Cypher evaluation analysis will not run as no suitable log file was identified or output path could not be determined.")
    --- END OF FILE CONTENT ---

  File: batch_eval_answer.py
    --- START OF FILE CONTENT ---
# zhz_rag/evaluation/batch_eval_answer.py
import asyncio
import json
import os
from typing import List, Dict, Any, Optional, TYPE_CHECKING, Union
import glob
from datetime import datetime

try:
    from zhz_rag.evaluation.evaluator import evaluate_answer_with_gemini
    from zhz_rag.utils.common_utils import (
        find_latest_rag_interaction_log,
        load_jsonl_file,
        RAG_INTERACTION_LOGS_DIR
    )
    from zhz_rag.config.pydantic_models import RetrievedDocument
except ImportError as e:
    print(f"ERROR: Could not import necessary modules in batch_eval_answer.py: {e}")
    print("Make sure this script is run in an environment where 'zhz_rag' package is accessible.")
    exit(1)

if TYPE_CHECKING:
    from zhz_rag_pipeline_dagster.zhz_rag_pipeline.resources import GeminiAPIResource

import logging

batch_answer_eval_logger = logging.getLogger("BatchAnswerEvaluationLogger")
# 保留 DEBUG 级别，以便在需要时仍可查看详细日志，但常规 INFO 日志会更简洁
batch_answer_eval_logger.setLevel(logging.DEBUG) 
if not batch_answer_eval_logger.hasHandlers():
    _console_handler = logging.StreamHandler()
    _formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
    _console_handler.setFormatter(_formatter)
    batch_answer_eval_logger.addHandler(_console_handler)
    batch_answer_eval_logger.propagate = False
batch_answer_eval_logger.info("--- BatchAnswerEvaluationLogger configured (Level: DEBUG) ---")


def format_contexts_for_evaluation(context_docs_raw: List[Dict[str, Any]]) -> str:
    formatted_contexts = []
    if not context_docs_raw or not isinstance(context_docs_raw, list):
        batch_answer_eval_logger.warning("format_contexts_for_evaluation received no context or invalid format.")
        batch_answer_eval_logger.debug("DEBUG_FORMAT_CTX: context_docs_raw is empty or not a list.") 
        return "No context provided or context in unexpected format."

    batch_answer_eval_logger.debug(f"DEBUG_FORMAT_CTX: --- format_contexts_for_evaluation ---")
    batch_answer_eval_logger.debug(f"DEBUG_FORMAT_CTX: Received context_docs_raw length: {len(context_docs_raw)}")
    if context_docs_raw:
        batch_answer_eval_logger.debug(f"DEBUG_FORMAT_CTX: First item of context_docs_raw (type: {type(context_docs_raw[0])}): {str(context_docs_raw[0])[:500]}...")

    for i, doc_data in enumerate(context_docs_raw): 
        batch_answer_eval_logger.debug(f"DEBUG_FORMAT_CTX:   Processing doc_data item {i} (type: {type(doc_data)}): {str(doc_data)[:300]}...")
        try:
            content = doc_data.get("content", "[Content not available]") 
            source_type = doc_data.get("source_type", "unknown_source") 
            score = doc_data.get("score") 
            metadata = doc_data.get("metadata", {})
            chunk_id = metadata.get("chunk_id") or metadata.get("id") 

            context_str = f"--- Context Snippet {i+1} ---\n"
            context_str += f"Source Type: {source_type}\n"
            if score is not None:
                try:
                    context_str += f"Original Score: {float(score):.4f}\n"
                except (ValueError, TypeError):
                    context_str += f"Original Score: {score}\n" 
            if chunk_id:
                 context_str += f"Chunk ID: {chunk_id}\n"
            context_str += f"Content: {content}\n"
            formatted_contexts.append(context_str)
            batch_answer_eval_logger.debug(f"DEBUG_FORMAT_CTX:     Formatted context snippet {i+1} (content part first 100 chars): {str(content)[:100]}...")

        except Exception as e:
            batch_answer_eval_logger.warning(f"Could not parse a context document fully in format_contexts_for_evaluation: {doc_data}. Error: {e}")
            content = doc_data.get("content", "[Content not available]") 
            source_type = doc_data.get("source_type", "unknown_source") 
            formatted_contexts.append(f"--- Context Snippet {i+1} (Parsing Warning) ---\nSource Type: {source_type}\nContent: {content}\n")
    
    final_formatted_str = "\n\n".join(formatted_contexts) if formatted_contexts else "No context provided."
    batch_answer_eval_logger.debug(f"DEBUG_FORMAT_CTX: --- format_contexts_for_evaluation: Final formatted string (first 500 chars): {final_formatted_str[:500]}...")
    return final_formatted_str


async def run_answer_batch_evaluation(
    gemini_resource_for_evaluator: 'GeminiAPIResource',
    rag_interaction_log_filepath: str,
    app_version: str = "0.1.0",
    use_simulated_api: bool = False,
    api_call_delay: float = 4.1,
    target_task_types: Union[str, List[str]] = "rag_query_processing_full_log",
    field_mapping: Optional[Dict[str, Union[str, List[str]]]] = None
) -> Dict[str, int]:
    batch_answer_eval_logger.info(f"Starting batch Answer evaluation for log file: {rag_interaction_log_filepath}")
    batch_answer_eval_logger.info(f"Parameters: app_version='{app_version}', use_simulated_api={use_simulated_api}, api_call_delay={api_call_delay}s")
    batch_answer_eval_logger.info(f"Target task types: {target_task_types}")
    batch_answer_eval_logger.info(f"Field mapping: {field_mapping}")

    processed_count = 0
    evaluated_count = 0
    skipped_missing_data_count = 0
    skipped_task_type_mismatch = 0

    if use_simulated_api:
        batch_answer_eval_logger.warning("Batch Answer evaluation is USING SIMULATED Gemini responses.")
    else:
        batch_answer_eval_logger.info("Batch Answer evaluation is using REAL Gemini API calls.")

    interaction_logs = load_jsonl_file(rag_interaction_log_filepath)

    if not interaction_logs:
        batch_answer_eval_logger.error(f"No data loaded from RAG interaction log file: {rag_interaction_log_filepath}. Exiting.")
        return {"processed": 0, "evaluated": 0, "skipped_missing_data": 0, "skipped_task_type_mismatch": 0, "file_not_found_or_empty": 1}

    # --- 移除了临时筛选特定 interaction_id 的代码 ---

    if isinstance(target_task_types, str):
        target_task_types_list = [target_task_types]
    else:
        target_task_types_list = target_task_types

    default_field_map = {
        "user_query": ["user_query", "original_user_query", "query"],
        "generated_answer": ["generated_answer", "processed_llm_output", "final_answer_from_llm", "final_answer"],
        "interaction_id": ["interaction_id", "original_interaction_id"],
        "context_docs": ["final_context_docs_full", "final_context_docs_summary"]
    }
    current_field_map = default_field_map.copy()
    if field_mapping:
        for key, value in field_mapping.items():
            if isinstance(value, str):
                current_field_map[key] = [value] 
            else:
                current_field_map[key] = value

    def get_field_value(log_entry: Dict[str, Any], field_key: str) -> Any:
        for actual_field_name in current_field_map.get(field_key, []):
            if actual_field_name in log_entry:
                return log_entry[actual_field_name]
        return None

    for line_number, interaction_log in enumerate(interaction_logs, 1):
        processed_count += 1
        current_task_type = interaction_log.get("task_type")

        if current_task_type not in target_task_types_list:
            skipped_task_type_mismatch +=1
            continue
        
        # --- 添加日志打印 interaction_log 的键，用于调试 ---
        batch_answer_eval_logger.debug(f"DEBUG_INTERACTION_LOG: Keys in interaction_log for entry {line_number}: {list(interaction_log.keys())}")
        if "final_context_docs_full" in interaction_log:
            batch_answer_eval_logger.debug(f"DEBUG_INTERACTION_LOG: interaction_log['final_context_docs_full'] (first item preview): {str(interaction_log['final_context_docs_full'][0])[:200] if interaction_log['final_context_docs_full'] else 'Empty or None'}")
        else:
            batch_answer_eval_logger.debug(f"DEBUG_INTERACTION_LOG: 'final_context_docs_full' NOT in interaction_log for entry {line_number}.")
        if "final_context_docs_summary" in interaction_log:
            batch_answer_eval_logger.debug(f"DEBUG_INTERACTION_LOG: interaction_log['final_context_docs_summary'] (first item preview): {str(interaction_log['final_context_docs_summary'][0])[:200] if interaction_log['final_context_docs_summary'] else 'Empty or None'}")
        else:
            batch_answer_eval_logger.debug(f"DEBUG_INTERACTION_LOG: 'final_context_docs_summary' NOT in interaction_log for entry {line_number}.")
        # --- 结束日志打印 ---


        user_question = get_field_value(interaction_log, "user_query")
        generated_answer = get_field_value(interaction_log, "generated_answer")
        original_id = get_field_value(interaction_log, "interaction_id")
        context_docs_raw = get_field_value(interaction_log, "context_docs")
        
        batch_answer_eval_logger.debug(
            f"Log entry {line_number}: task_type='{current_task_type}', id='{original_id}', "
            f"q_present={bool(user_question)}, ans_present={bool(generated_answer)}, ctx_present_is_list={isinstance(context_docs_raw, list) if context_docs_raw else False}"
        )

        if user_question and generated_answer and original_id and context_docs_raw and isinstance(context_docs_raw, list):
            batch_answer_eval_logger.info(f"DEBUG_EVAL: For ID {original_id}, UserQ: '{str(user_question)[:50]}...', GenAns: '{str(generated_answer)[:50]}...', Contexts count: {len(context_docs_raw)}")
            retrieved_contexts_str_for_eval = format_contexts_for_evaluation(context_docs_raw)
            
            batch_answer_eval_logger.info(f"Evaluating Answer for interaction_id: {original_id} - User Question: '{str(user_question)[:50]}...' - Generated Answer: '{str(generated_answer)[:50]}...'")
            
            evaluation_result = await evaluate_answer_with_gemini(
                gemini_resource=gemini_resource_for_evaluator,
                user_question=str(user_question),
                retrieved_contexts=retrieved_contexts_str_for_eval,
                generated_answer=str(generated_answer),
                original_interaction_id=str(original_id),
                app_version=app_version
            )

            if evaluation_result:
                evaluated_count += 1
                summary = evaluation_result.get("evaluation_summary", {})
                overall_score = summary.get("overall_answer_quality_score", "N/A")
                batch_answer_eval_logger.info(f"Successfully evaluated Answer for interaction_id: {original_id}. Overall Score: {overall_score}")
            else:
                batch_answer_eval_logger.warning(f"Answer evaluation returned None or failed for interaction_id: {original_id}")
            
            if not use_simulated_api and evaluated_count > 0 : 
                batch_answer_eval_logger.info(f"Waiting for {api_call_delay} seconds before next API call...")
                await asyncio.sleep(api_call_delay)
        else:
            skipped_missing_data_count += 1
            log_preview = {
                "interaction_id": original_id, 
                "task_type": current_task_type, 
                "user_question_found": bool(user_question),
                "generated_answer_found": bool(generated_answer),
                "context_docs_found_and_list": isinstance(context_docs_raw, list) if context_docs_raw else False
            }
            batch_answer_eval_logger.warning(f"Skipping RAG log entry {line_number} due to missing critical data. Details: {log_preview}")
        
        if processed_count > 0 and processed_count % 10 == 0:
            batch_answer_eval_logger.info(f"Progress: Processed {processed_count} log entries. Evaluated {evaluated_count} answers so far. Skipped (type mismatch): {skipped_task_type_mismatch}. Skipped (missing data): {skipped_missing_data_count}.")

    summary = {
        "total_log_entries_read": processed_count,
        "target_task_type_entries_found": processed_count - skipped_task_type_mismatch,
        "answers_evaluated_successfully": evaluated_count,
        "skipped_due_to_missing_data_in_target_entries": skipped_missing_data_count,
    }
    batch_answer_eval_logger.info(f"Batch Answer evaluation finished. Summary: {summary}")
    return summary


if __name__ == "__main__":
    try:
        from zhz_rag_pipeline_dagster.zhz_rag_pipeline.resources import GeminiAPIResource, GeminiAPIResourceConfig
        
        gemini_model_name_env = os.getenv("GEMINI_MODEL_FOR_EVAL", "gemini/gemini-1.5-flash-latest")
        gemini_proxy_url_env = os.getenv("LITELLM_PROXY_URL") 

        gemini_resource_config = GeminiAPIResourceConfig(
            model_name=gemini_model_name_env,
            proxy_url=gemini_proxy_url_env
        )
        gemini_eval_resource = GeminiAPIResource(
            model_name=gemini_resource_config.model_name,
            proxy_url=gemini_resource_config.proxy_url,
            default_temperature=gemini_resource_config.default_temperature,
            default_max_tokens=gemini_resource_config.default_max_tokens
        )
        class MockContext: 
            def __init__(self):
                self.log = batch_answer_eval_logger
        
        if hasattr(gemini_eval_resource, 'setup_for_execution'):
             gemini_eval_resource.setup_for_execution(MockContext())
        batch_answer_eval_logger.info(f"GeminiAPIResource for evaluation initialized successfully using Dagster's resource class.")

    except ImportError:
        batch_answer_eval_logger.critical("CRITICAL: Could not import GeminiAPIResource. Ensure Dagster modules are in PYTHONPATH or installed.")
        gemini_eval_resource = None 
    except Exception as e_res_init:
        batch_answer_eval_logger.critical(f"CRITICAL: Error initializing GeminiAPIResource: {e_res_init}", exc_info=True)
        gemini_eval_resource = None


    log_file_to_evaluate = find_latest_rag_interaction_log(RAG_INTERACTION_LOGS_DIR)
    use_simulated_env = os.getenv("USE_SIMULATED_GEMINI_ANSWER_EVAL", "false").lower() == "true"
    api_delay_env = float(os.getenv("GEMINI_API_CALL_DELAY_SECONDS", "4.1"))
    app_version_tag_env = os.getenv("APP_VERSION_TAG", "0.1.4_batch_answer_flexible")
    if use_simulated_env:
        app_version_tag_env += "_simulated"

    rag_service_task_types = ["rag_query_processing_full_log"]
    rag_service_field_map = {
        "user_query": "original_user_query",
        "generated_answer": "final_answer_from_llm",
        "interaction_id": "interaction_id",
        "context_docs": ["final_context_docs_full", "final_context_docs_summary"] 
    }

    if not gemini_eval_resource:
        batch_answer_eval_logger.error("Cannot proceed with evaluation as GeminiAPIResource is not available.")
    elif log_file_to_evaluate:
        batch_answer_eval_logger.info(f"Found RAG interaction log to process for answer evaluation: {log_file_to_evaluate}")
        asyncio.run(run_answer_batch_evaluation(
            gemini_resource_for_evaluator=gemini_eval_resource,
            rag_interaction_log_filepath=log_file_to_evaluate,
            app_version=app_version_tag_env,
            use_simulated_api=use_simulated_env,
            api_call_delay=api_delay_env,
            target_task_types=rag_service_task_types, 
            field_mapping=rag_service_field_map      
        ))
    elif use_simulated_env:
        batch_answer_eval_logger.warning(f"RAG interaction log file not found, but USE_SIMULATED_GEMINI_ANSWER_EVAL is true. Running with a dummy path (will process 0 entries).")
        if gemini_eval_resource: # Check again if resource is available for simulated run
            asyncio.run(run_answer_batch_evaluation(
                gemini_resource_for_evaluator=gemini_eval_resource,
                rag_interaction_log_filepath="dummy_non_existent_file.jsonl", 
                app_version=app_version_tag_env + "_no_file",
                use_simulated_api=use_simulated_env,
                api_call_delay=api_delay_env,
                target_task_types=rag_service_task_types,
                field_mapping=rag_service_field_map
            ))
        else:
            batch_answer_eval_logger.error("GeminiAPIResource for evaluation could not be initialized (even for simulated run). Aborting simulated run.")
    else:
        batch_answer_eval_logger.warning(f"No suitable RAG interaction log file found in '{RAG_INTERACTION_LOGS_DIR}' and not using simulated responses. Batch Answer evaluation will not run.")
    --- END OF FILE CONTENT ---

  File: batch_eval_cypher.py
    --- START OF FILE CONTENT ---
# zhz_rag/evaluation/batch_eval_cypher.py
import asyncio
import json
import os
from typing import List, Dict, Any, Optional, TYPE_CHECKING, Union
import glob
from datetime import datetime

try:
    from zhz_rag.evaluation.evaluator import evaluate_cypher_with_gemini
    from zhz_rag.utils.common_utils import (
        find_latest_rag_interaction_log,
        load_jsonl_file,
        RAG_INTERACTION_LOGS_DIR
    )
except ImportError as e:
    print(f"ERROR: Could not import necessary modules in batch_eval_cypher.py: {e}")
    print("Make sure this script is run in an environment where 'zhz_rag' package is accessible.")
    exit(1)

if TYPE_CHECKING:
    from zhz_rag_pipeline_dagster.zhz_rag_pipeline.resources import GeminiAPIResource

import logging

batch_cypher_eval_logger = logging.getLogger("BatchCypherEvaluationLogger")
batch_cypher_eval_logger.setLevel(logging.DEBUG) # 设置为 DEBUG 以便查看详细日志
if not batch_cypher_eval_logger.hasHandlers():
    _console_handler = logging.StreamHandler()
    _formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
    _console_handler.setFormatter(_formatter)
    batch_cypher_eval_logger.addHandler(_console_handler)
    batch_cypher_eval_logger.propagate = False
batch_cypher_eval_logger.info("--- BatchCypherEvaluationLogger configured (Level: DEBUG) ---")


async def run_cypher_batch_evaluation(
    gemini_resource_for_evaluator: 'GeminiAPIResource',
    rag_interaction_log_filepath: str,
    app_version: str = "0.1.0",
    use_simulated_api: bool = False,
    api_call_delay: float = 4.1,
    target_task_types: Union[str, List[str]] = "cypher_generation_final_attempt_local_service",
    field_mapping: Optional[Dict[str, Union[str, List[str]]]] = None
) -> Dict[str, int]:
    batch_cypher_eval_logger.info(f"Starting batch Cypher evaluation for log file: {rag_interaction_log_filepath}")
    batch_cypher_eval_logger.info(f"Parameters: app_version='{app_version}', use_simulated_api={use_simulated_api}, api_call_delay={api_call_delay}s")
    batch_cypher_eval_logger.info(f"Target task types: {target_task_types}")
    batch_cypher_eval_logger.info(f"Field mapping: {field_mapping}")

    processed_count = 0
    evaluated_count = 0
    skipped_no_cypher_count = 0
    failed_to_extract_count = 0
    skipped_task_type_mismatch = 0

    if use_simulated_api:
        batch_cypher_eval_logger.warning("Batch Cypher evaluation is USING SIMULATED Gemini responses.")
    else:
        batch_cypher_eval_logger.info("Batch Cypher evaluation is using REAL Gemini API calls.")

    interaction_logs = load_jsonl_file(rag_interaction_log_filepath)

    if not interaction_logs:
        batch_cypher_eval_logger.error(f"No data loaded from RAG interaction log file: {rag_interaction_log_filepath}. Exiting.")
        return {"processed": 0, "evaluated": 0, "skipped_no_cypher":0, "failed_extract": 0, "skipped_task_type_mismatch":0, "file_not_found_or_empty": 1}

    if isinstance(target_task_types, str):
        target_task_types_list = [target_task_types]
    else:
        target_task_types_list = target_task_types

    default_field_map = {
        "user_query": ["user_query_for_task", "user_query", "original_user_query"],
        "generated_cypher": ["raw_llm_output", "processed_llm_output"], # raw_llm_output for cypher_generation_final_attempt_local_service
        "interaction_id": ["interaction_id", "original_interaction_id"]
    }
    current_field_map = default_field_map.copy()
    if field_mapping:
        for key, value in field_mapping.items():
            if isinstance(value, str):
                current_field_map[key] = [value]
            else:
                current_field_map[key] = value

    def get_field_value(log_entry: Dict[str, Any], field_key: str) -> Any:
        for actual_field_name in current_field_map.get(field_key, []):
            if actual_field_name in log_entry:
                return log_entry[actual_field_name]
        return None

    for line_number, interaction_log in enumerate(interaction_logs, 1):
        processed_count += 1
        current_task_type = interaction_log.get("task_type")

        if current_task_type not in target_task_types_list:
            skipped_task_type_mismatch +=1
            continue
            
        batch_cypher_eval_logger.debug(f"DEBUG_CYPHER_EVAL: Processing log entry {line_number} with task_type '{current_task_type}'")

        user_question = get_field_value(interaction_log, "user_query")
        generated_cypher_raw = get_field_value(interaction_log, "generated_cypher")
        original_id = get_field_value(interaction_log, "interaction_id")

        if user_question and original_id:
            generated_cypher_to_eval = None
            if isinstance(generated_cypher_raw, str):
                try:
                    # Cypher gen logs store the JSON string {"status": ..., "query": ...} in raw_llm_output
                    cypher_data = json.loads(generated_cypher_raw)
                    if isinstance(cypher_data, dict) and cypher_data.get("status") == "success":
                        generated_cypher_to_eval = cypher_data.get("query")
                    elif isinstance(cypher_data, dict) and cypher_data.get("status") == "unable_to_generate":
                        generated_cypher_to_eval = cypher_data.get("query") # Should be "无法生成Cypher查询."
                    else: # Not the expected JSON structure
                         batch_cypher_eval_logger.warning(f"Cypher log {original_id} has raw_llm_output but not in expected JSON format: {generated_cypher_raw[:100]}")
                except json.JSONDecodeError:
                    # If raw_llm_output is not JSON, it might be the Cypher directly (older log format?) or "无法生成..."
                    generated_cypher_to_eval = generated_cypher_raw 
            
            if not generated_cypher_to_eval or not str(generated_cypher_to_eval).strip():
                batch_cypher_eval_logger.info(f"Skipping evaluation for interaction_id '{original_id}' as extracted Cypher is empty.")
                skipped_no_cypher_count += 1
                continue
            
            # We will evaluate "无法生成Cypher查询." as well, Gemini should score it appropriately.
            batch_cypher_eval_logger.info(f"Evaluating Cypher for interaction_id: {original_id} - User Question: {str(user_question)[:50]}... - Cypher: {str(generated_cypher_to_eval)[:100]}...")
            
            evaluation_result = await evaluate_cypher_with_gemini(
                gemini_resource=gemini_resource_for_evaluator,
                user_question=str(user_question),
                generated_cypher=str(generated_cypher_to_eval),
                original_interaction_id=str(original_id),
                app_version=app_version
            )

            if evaluation_result:
                evaluated_count += 1
                summary = evaluation_result.get("evaluation_summary", {})
                overall_score = summary.get("overall_quality_score_cypher", "N/A")
                batch_cypher_eval_logger.info(f"Successfully evaluated Cypher for interaction_id: {original_id}. Overall Score: {overall_score}")
            else:
                batch_cypher_eval_logger.warning(f"Cypher evaluation returned None or failed for interaction_id: {original_id}")
            
            if not use_simulated_api and evaluated_count > 0:
                batch_cypher_eval_logger.info(f"Waiting for {api_call_delay} seconds before next API call...")
                await asyncio.sleep(api_call_delay)
        else:
            failed_to_extract_count += 1
            batch_cypher_eval_logger.warning(f"Skipping cypher_generation log entry {line_number} due to missing user_query or interaction_id. Log content: {str(interaction_log)[:200]}...")
        
        if processed_count > 0 and processed_count % 10 == 0:
            batch_cypher_eval_logger.info(f"Progress: Processed {processed_count} log entries. Evaluated {evaluated_count} Cypher queries. Skipped (no cypher): {skipped_no_cypher_count}. Failed extract: {failed_to_extract_count}. Type mismatch: {skipped_task_type_mismatch}")

    summary = {
        "total_log_entries_read": processed_count,
        "target_task_type_entries_found": processed_count - skipped_task_type_mismatch,
        "cypher_queries_evaluated_successfully": evaluated_count,
        "skipped_empty_or_no_cypher": skipped_no_cypher_count,
        "failed_to_extract_fields_for_eval": failed_to_extract_count
    }
    batch_cypher_eval_logger.info(f"Batch Cypher evaluation finished. Summary: {summary}")
    return summary


if __name__ == "__main__":
    try:
        from zhz_rag_pipeline_dagster.zhz_rag_pipeline.resources import GeminiAPIResource, GeminiAPIResourceConfig
        
        gemini_model_name_env = os.getenv("GEMINI_MODEL_FOR_EVAL", "gemini/gemini-1.5-flash-latest")
        gemini_proxy_url_env = os.getenv("LITELLM_PROXY_URL") 

        gemini_resource_config = GeminiAPIResourceConfig(
            model_name=gemini_model_name_env,
            proxy_url=gemini_proxy_url_env
        )
        gemini_eval_resource = GeminiAPIResource(
            model_name=gemini_resource_config.model_name,
            proxy_url=gemini_resource_config.proxy_url,
            default_temperature=gemini_resource_config.default_temperature,
            default_max_tokens=gemini_resource_config.default_max_tokens
        )
        class MockContext: 
            def __init__(self):
                self.log = batch_cypher_eval_logger
        
        if hasattr(gemini_eval_resource, 'setup_for_execution'):
             gemini_eval_resource.setup_for_execution(MockContext())
        batch_cypher_eval_logger.info(f"GeminiAPIResource for Cypher evaluation initialized successfully.")

    except ImportError:
        batch_cypher_eval_logger.critical("CRITICAL: Could not import GeminiAPIResource. Ensure Dagster modules are in PYTHONPATH or installed.")
        gemini_eval_resource = None
    except Exception as e_res_init:
        batch_cypher_eval_logger.critical(f"CRITICAL: Error initializing GeminiAPIResource: {e_res_init}", exc_info=True)
        gemini_eval_resource = None

    log_file_to_evaluate = find_latest_rag_interaction_log(RAG_INTERACTION_LOGS_DIR)
    use_simulated_env = os.getenv("USE_SIMULATED_GEMINI_CYPHER_EVAL", "false").lower() == "true"
    api_delay_env = float(os.getenv("GEMINI_API_CALL_DELAY_SECONDS", "4.1"))
    app_version_tag_env = os.getenv("APP_VERSION_TAG", "0.1.4_batch_cypher_flexible")
    if use_simulated_env:
        app_version_tag_env += "_simulated"

    # --- 配置目标 task_type 和字段映射 ---
    # llm_interface.py 中的 generate_cypher_query 记录的 task_type 是 "cypher_generation_final_attempt_local_service"
    # 其 "raw_llm_output" 字段包含的是 local_llm_service 返回的 JSON 字符串: {"status": ..., "query": ...}
    cypher_gen_task_types = ["cypher_generation_final_attempt_local_service", "cypher_generation"] # 包含旧的以防万一
    cypher_gen_field_map = {
        "user_query": "user_query_for_task", # 在 "cypher_generation_final_attempt_local_service" 中是这个
        "generated_cypher": "raw_llm_output", # 在 "cypher_generation_final_attempt_local_service" 中是这个
        "interaction_id": "interaction_id"
    }

    if not gemini_eval_resource:
        batch_cypher_eval_logger.error("Cannot proceed with Cypher evaluation as GeminiAPIResource is not available.")
    elif log_file_to_evaluate:
        batch_cypher_eval_logger.info(f"Found RAG interaction log to process for Cypher evaluation: {log_file_to_evaluate}")
        asyncio.run(run_cypher_batch_evaluation(
            gemini_resource_for_evaluator=gemini_eval_resource,
            rag_interaction_log_filepath=log_file_to_evaluate,
            app_version=app_version_tag_env,
            use_simulated_api=use_simulated_env,
            api_call_delay=api_delay_env,
            target_task_types=cypher_gen_task_types,
            field_mapping=cypher_gen_field_map
        ))
    elif use_simulated_env:
        batch_cypher_eval_logger.warning(f"RAG interaction log file not found, but USE_SIMULATED_GEMINI_CYPHER_EVAL is true. Running with a dummy path.")
        if gemini_eval_resource:
            asyncio.run(run_cypher_batch_evaluation(
                gemini_resource_for_evaluator=gemini_eval_resource,
                rag_interaction_log_filepath="dummy_non_existent_file.jsonl", 
                app_version=app_version_tag_env + "_no_file",
                use_simulated_api=use_simulated_env,
                api_call_delay=api_delay_env,
                target_task_types=cypher_gen_task_types,
                field_mapping=cypher_gen_field_map
            ))
        else:
            batch_cypher_eval_logger.error("GeminiAPIResource for Cypher evaluation could not be initialized (even for simulated run). Aborting.")
    else:
        batch_cypher_eval_logger.warning(f"No suitable RAG interaction log file found in '{RAG_INTERACTION_LOGS_DIR}' and not using simulated responses. Batch Cypher evaluation will not run.")
    --- END OF FILE CONTENT ---

  File: evaluator.py
    --- START OF FILE CONTENT ---
# zhz_rag/evaluation/evaluator.py
import os
import json
import traceback
from typing import Dict, Any, Optional, TYPE_CHECKING

# 导入共享的Schema描述 和 通用日志函数
from zhz_rag.config.constants import NEW_KG_SCHEMA_DESCRIPTION as KG_SCHEMA_FOR_EVALUATION
from zhz_rag.utils.common_utils import log_interaction_data

# --- 类型检查时导入资源类，避免循环导入 ---
if TYPE_CHECKING:
    from zhz_rag_pipeline_dagster.zhz_rag_pipeline.resources import GeminiAPIResource

import logging

# --- 配置此模块的logger ---
eval_logger = logging.getLogger("EvaluationLogger")
eval_logger.setLevel(logging.INFO)
eval_logger.propagate = False
if not eval_logger.hasHandlers():
    _eval_console_handler = logging.StreamHandler()
    _eval_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
    _eval_console_handler.setFormatter(_eval_formatter)
    eval_logger.addHandler(_eval_console_handler)
    eval_logger.info("--- EvaluationLogger configured ---")

CYPHER_EVALUATION_PROMPT_V1 = """You are an expert Neo4j Cypher query evaluator and debugging assistant. Your primary task is to meticulously analyze a Cypher query that was generated by another AI model (Qwen2.5-3B, a 3 billion parameter model) in response to a user's natural language question. Your evaluation must be based on the provided knowledge graph schema and the specific evaluation criteria outlined below.

**IMPORTANT SCHEMA CONTEXT (KG_SCHEMA_DESCRIPTION):**
Use code with caution.
Python
{{KG_SCHEMA_DESCRIPTION}}
**USER'S NATURAL LANGUAGE QUESTION:**
Use code with caution.
{{USER_QUESTION}}
**GENERATED CYPHER QUERY TO EVALUATE:**
Use code with caution.
{{GENERATED_CYPHER}}
**EVALUATION TASK:**

Please evaluate the "GENERATED CYPHER QUERY" based on the following criteria. Provide your evaluation in a **valid JSON format** strictly adhering to the structure specified at the end.

**EVALUATION CRITERIA:**

1.  **Syntactic Correctness:**
    *   Is the Cypher query syntactically valid and parsable by Neo4j?
    *   Score (1-5): 1 = Major syntax errors, unparsable; 3 = Minor issues, likely parsable with warnings; 5 = Syntactically perfect.
    *   Reasoning: Explain your score. If errors exist, briefly describe them.

2.  **Schema Adherence (Strictly based on the provided KG_SCHEMA_DESCRIPTION):**
    *   **Node Label Correctness:**
        *   Check: Does the query exclusively use ':ExtractedEntity' for all node patterns? (True/False)
        *   Detail: Briefly explain.
    *   **Entity Type via `label` Property Correctness:**
        *   Check: Are entity types (e.g., 'PERSON', 'ORGANIZATION', 'TASK') correctly queried using the `label` property of ':ExtractedEntity' nodes (e.g., `WHERE n.label = 'PERSON'`)? (True/False)
        *   Detail: Briefly explain.
    *   **Relationship Type and Direction Correctness:**
        *   Check: Does the query use only defined relationship types (e.g., `:WORKS_AT`, `:ASSIGNED_TO`) and their correct directions as specified in the schema? (True/False)
        *   Detail: Briefly explain.
    *   **Property Name Correctness:**
        *   Check: Does the query use only valid property names for nodes and relationships (e.g., `text`, `label` for nodes)? (True/False)
        *   Detail: Briefly explain.
    *   **Hallucinated Schema Elements:**
        *   Check: Does the query reference any labels, relationship types, or properties NOT defined in the schema? (True/False - True if hallucination is present)
        *   Detail: List any hallucinated elements.
    *   **Overall Schema Adherence Score (1-5):** Based on the sub-checks above, provide an overall schema adherence score. Explain your reasoning, considering the severity and number of any deviations. Perfect adherence means all sub-checks are True and no hallucinations are present. 1 = Gross violations; 3 = Some minor deviations; 5 = Perfectly adheres to schema.
    *   Reasoning for Overall Schema Adherence Score: Provide the explanation here.

3.  **Intent Accuracy (Semantic Correctness):**
    *   Score (1-5): 1 = Completely misses user intent; 3 = Partially addresses intent but has significant gaps or inaccuracies; 5 = Accurately and fully captures user intent.
    *   Explanation of Cypher Retrieval: In simple natural language that an average office worker could understand, what information would this Cypher query retrieve from a graph that matches the schema?
    *   Alignment with User Question: How well does this retrieved information align with what the user was asking in their "USER'S NATURAL LANGUAGE QUESTION"?
    *   Key Element Coverage: Does the Cypher query attempt to address all key entities, relationships, and conditions mentioned in the user's question? If not, what specific parts of the question seem to be missing or misinterpreted in the Cypher query?
    *   Reasoning: Explain your intent accuracy score, considering the explanation, alignment, and key element coverage.

4.  **Identification of Potential Qwen2.5-3B Error Patterns (Optional but helpful):**
    *   Based on your knowledge of smaller LLMs, does this query exhibit any common error patterns such as:
        *   Over-simplification of complex conditions?
        *   Incorrect handling of the ':ExtractedEntity' and 'label' property convention?
        *   Ignoring multiple constraints from the user question?
        *   Other (please specify)?
    *   Observed Patterns: List any observed patterns from the predefined list, or provide an empty list [] if none of the predefined patterns are clearly observed.

**OUTPUT JSON STRUCTURE (Strictly follow this format):**
```json
{
  "evaluation_summary": {
    "overall_quality_score_cypher": "<Integer score 1-5, your overall judgment of the Cypher query's quality. **Crucially, assign higher weight to 'Schema Adherence' and 'Intent Accuracy'.** A query with perfect syntax but critical flaws in schema adherence or intent accuracy should NOT receive a high overall score.>",
    "main_strength_cypher": "<Briefly describe the main strength of this Cypher query, if any>",
    "main_weakness_cypher": "<Briefly describe the main weakness or most critical issue>"
  },
  "dimensions": {
    "syntactic_correctness": {
      "score": "<Integer score 1-5>",
      "parsable_prediction": "<Boolean: true/false>",
      "reasoning": "<Text explanation>"
    },
    "schema_adherence": {
      "overall_score": "<Integer score 1-5>",
      "node_label_correctness": { "check_result": "<Boolean>", "detail": "<Text>" },
      "entity_type_property_correctness": { "check_result": "<Boolean>", "detail": "<Text>" },
      "relationship_type_correctness": { "check_result": "<Boolean>", "detail": "<Text>" },
      "property_name_correctness": { "check_result": "<Boolean>", "detail": "<Text>" },
      "hallucinated_schema_elements": { "check_result_hallucination_present": "<Boolean>", "elements_found": ["<List of strings or empty list>"] },
      "reasoning": "<Text explanation for overall schema adherence score>"
    },
    "intent_accuracy": {
      "score": "<Integer score 1-5>",
      "explanation_of_cypher_retrieval": "<Text>",
      "semantic_alignment_with_question": "<Text>",
      "key_element_coverage_notes": "<Text describing coverage of key elements, and what's missing/misinterpreted, if any>",
      "reasoning": "<Text explanation for intent accuracy score>"
    }
  },
  "qwen_error_patterns_identified": ["<List of strings describing observed patterns, or empty list>"],
  "suggestion_for_improvement_cypher": "<Actionable suggestions to improve this specific Cypher query, if applicable>"
}
```"""

async def evaluate_cypher_with_gemini(
    gemini_resource: 'GeminiAPIResource', # <--- 修改：接收 GeminiAPIResource 实例
    user_question: str,
    generated_cypher: str,
    original_interaction_id: Optional[str] = None,
    app_version: str = "0.1.0"
) -> Optional[Dict[str, Any]]:
    eval_logger.info(f"Starting Cypher evaluation. User question: '{user_question[:50]}...', Cypher: '{generated_cypher[:100]}...'")

    prompt_to_gemini = CYPHER_EVALUATION_PROMPT_V1.replace(
        "{{KG_SCHEMA_DESCRIPTION}}", KG_SCHEMA_FOR_EVALUATION
    ).replace(
        "{{USER_QUESTION}}", user_question
    ).replace(
        "{{GENERATED_CYPHER}}", generated_cypher
    )

    messages_for_gemini = [{"role": "user", "content": prompt_to_gemini}]
    evaluation_result_json: Optional[Dict[str, Any]] = None
    raw_gemini_output: Optional[str] = None
    error_info: Optional[str] = None

    # 模拟API调用的逻辑保持不变，但实际API调用将通过gemini_resource
    USE_SIMULATED_GEMINI_RESPONSE = os.getenv("USE_SIMULATED_GEMINI_CYPHER_EVAL", "false").lower() == "true"

    if USE_SIMULATED_GEMINI_RESPONSE:
        eval_logger.warning("USING SIMULATED GEMINI RESPONSE FOR CYPHER EVALUATION")
        simulated_json_output = {
            "evaluation_summary": {
                "overall_quality_score_cypher": 4,
                "main_strength_cypher": "Good attempt at capturing intent.",
                "main_weakness_cypher": "Minor schema deviation in node label."
            },
            "dimensions": {
                "syntactic_correctness": {"score": 5, "parsable_prediction": True, "reasoning": "Query appears syntactically valid."},
                "schema_adherence": {
                    "overall_score": 3,
                    "node_label_correctness": { "check_result": False, "detail": "Used :Person instead of :ExtractedEntity for one node." },
                    "entity_type_property_correctness": { "check_result": True, "detail": "Correctly used label property." },
                    "relationship_type_correctness": { "check_result": True, "detail": "Used defined relationships correctly." },
                    "property_name_correctness": { "check_result": True, "detail": "Used valid properties." },
                    "hallucinated_schema_elements": { "check_result_hallucination_present": False, "elements_found": [] },
                    "reasoning": "One instance of incorrect node label, otherwise good."
                },
                "intent_accuracy": {
                    "score": 4,
                    "explanation_of_cypher_retrieval": "The query attempts to find the organization where '张三' works.",
                    "semantic_alignment_with_question": "Well-aligned with the user's question about '张三's' workplace.",
                    "key_element_coverage_notes": "All key elements seem to be covered.",
                    "reasoning": "Good intent capture, minor improvement could be ensuring organization type is also filtered if ambiguous."
                }
            },
            "qwen_error_patterns_identified": ["Incorrect handling of the ':ExtractedEntity' and 'label' property convention"],
            "suggestion_for_improvement_cypher": "Change MATCH (p:Person...) to MATCH (p:ExtractedEntity {label: 'PERSON'})..."
        }
        raw_gemini_output = json.dumps(simulated_json_output)
        try:
            evaluation_result_json = json.loads(raw_gemini_output)
        except json.JSONDecodeError as e:
            eval_logger.error(f"Error decoding simulated JSON: {e}")
            error_info = f"Simulated JSON decode error: {str(e)}"
    else:
        try:
            eval_logger.info(f"Calling Gemini for Cypher evaluation via GeminiAPIResource. Model: {gemini_resource.model_name}. Prompt length: {len(prompt_to_gemini)}")
            # --- 修改：通过 gemini_resource 调用 ---
            raw_gemini_output = await gemini_resource.call_completion(
                messages=messages_for_gemini
                # temperature 和 max_tokens 将使用 gemini_resource 的默认值或配置值
            )
            if raw_gemini_output:
                eval_logger.info(f"Raw Gemini output for Cypher eval (first 300 chars): {raw_gemini_output[:300]}...")
                cleaned_output = raw_gemini_output.strip()
                if cleaned_output.startswith("```json"):
                    cleaned_output = cleaned_output[len("```json"):].strip()
                if cleaned_output.endswith("```"):
                    cleaned_output = cleaned_output[:-len("```")].strip()
                evaluation_result_json = json.loads(cleaned_output)
                eval_logger.info("Successfully parsed Gemini evaluation result for Cypher.")
            else:
                eval_logger.error("Gemini call via resource returned None or empty for Cypher evaluation.")
                error_info = "Gemini call via resource returned None/empty"
        except json.JSONDecodeError as e_json:
            eval_logger.error(f"Failed to decode JSON from Gemini Cypher evaluation. Raw output: {raw_gemini_output[:500] if raw_gemini_output else 'N/A'}", exc_info=True)
            error_info = f"JSONDecodeError: {str(e_json)}"
        except Exception as e_gen: # 更通用的异常捕获，因为 call_completion 可能自己处理了 LiteLLM 异常
            eval_logger.error(f"Unexpected error during Cypher evaluation with Gemini resource: {e_gen}", exc_info=True)
            error_info = f"Unexpected error with Gemini resource: {str(e_gen)}"
            if raw_gemini_output is None: # 如果 call_completion 返回 None 且没有设置 raw_gemini_output
                raw_gemini_output = error_info

    eval_log_data = {
        "task_type": "cypher_evaluation_by_gemini",
        "original_interaction_id_ref": original_interaction_id,
        "user_question_for_eval": user_question,
        "generated_cypher_for_eval": generated_cypher,
        "eval_llm_input_prompt_char_count": len(prompt_to_gemini),
        "eval_llm_model": gemini_resource.model_name, # <--- 修改：从资源获取模型名称
        "eval_llm_raw_output": raw_gemini_output,
        "eval_llm_processed_output_json": evaluation_result_json,
        "eval_error_info": error_info,
        "application_version": app_version
    }
    await log_interaction_data(
        eval_log_data,
        is_evaluation_result=True,
        evaluation_name_for_file="cypher_gemini_flash"
    )
    if evaluation_result_json:
        eval_logger.info(f"Cypher evaluation completed. Overall score: {evaluation_result_json.get('evaluation_summary', {}).get('overall_quality_score_cypher')}")
    else:
        eval_logger.warning("Cypher evaluation did not produce a valid JSON result.")
        
    return evaluation_result_json


# V1 Answer Evaluation Prompt
ANSWER_EVALUATION_PROMPT_V1 = """
You are an expert AI Answer Evaluator, specializing in assessing the quality of responses from a Retrieval Augmented Generation (RAG) system designed as an "Office Worker Assistant". Your evaluation must be objective, strictly based on the provided user question, the context the RAG system used, and the generated answer.

**USER'S NATURAL LANGUAGE QUESTION:**
Use code with caution.
Python
{{USER_QUESTION}}
**CONTEXT PROVIDED TO THE RAG SYSTEM'S GENERATION MODEL (this is the information the AI had to base its answer on):**
Use code with caution.
{{RETRIEVED_CONTEXTS}}
**AI-GENERATED ANSWER TO EVALUATE:**
Use code with caution.
{{GENERATED_ANSWER}}
**EVALUATION TASK:**

Please evaluate the "AI-GENERATED ANSWER" based on the following criteria. For each dimension, provide a score from 1 to 5 (where 5 is best) and a brief reasoning for your score. Also, if applicable, identify specific phrases or sentences in the answer that exemplify an issue.

**EVALUATION DIMENSIONS & SCORING GUIDELINES:**

1.  **Faithfulness/Traceability (Score 1-5):**
    *   Is all factual information in the answer directly supported by the "CONTEXT PROVIDED"?
    *   Does the answer avoid making up information (hallucinations) or contradicting the context?
    *   **5 (Completely Faithful):** All key factual claims in the answer are directly and accurately supported by the context. No external information introduced.
    *   **4 (Mostly Faithful):** The vast majority of factual claims are supported. May contain very minor, reasonable inferences closely tied to the context, but no direct contradictions.
    *   **3 (Partially Faithful):** Some key claims are supported, but there are noticeable unsupported claims, slight misinterpretations of the context, or minor, non-critical hallucinations.
    *   **2 (Minimally Faithful):** Significant portions of the answer are not supported by the context, or there are clear contradictions or misleading hallucinations.
    *   **1 (Not Faithful):** The answer is largely based on information 외부 from the context, contains severe hallucinations, or directly contradicts the context.

2.  **Relevance to User Query (Score 1-5):**
    *   Does the answer directly and precisely address the "USER'S NATURAL LANGUAGE QUESTION"?
    *   **5 (Perfectly Relevant):** Directly and fully answers the user's core question(s).
    *   **4 (Highly Relevant):** Accurately answers the main aspects of the question; minor aspects might be less directly addressed.
    *   **3 (Moderately Relevant):** Addresses parts of the question but may miss key aspects or include some less relevant information.
    *   **2 (Slightly Relevant):** Touches upon the topic of the question but largely misses the core intent.
    *   **1 (Not Relevant):** Does not answer the user's question at all.

3.  **Completeness (Score 1-5):**
    *   **First, assess Context Sufficiency:** Based *only* on the "CONTEXT PROVIDED", does it seem to contain enough information to fully answer the "USER'S NATURAL LANGUAGE QUESTION"? (Answer: "Sufficient", "Partially Sufficient", or "Insufficient"). Provide a brief reason for your assessment of context sufficiency.
    *   **Then, score Completeness based on the answer's performance given the context:**
        *   Does the answer address all aspects of the user's query, making good use of the relevant information available in the context?
        *   If the context was insufficient, does the answer appropriately acknowledge this or focus on what can be answered?
        *   **5 (Very Complete):** (If context sufficient) Fully addresses all aspects of the query using all relevant context. (If context insufficient) Makes the best use of available context and clearly indicates limitations.
        *   **4 (Mostly Complete):** (If context sufficient) Addresses main aspects, minor details from context might be missed. (If context insufficient) Good use of available context, fair indication of limitations.
        *   **3 (Partially Complete):** (If context sufficient) Misses some important aspects or underutilizes relevant context. (If context insufficient) Poor use of available context or unclear about limitations.
        *   **2 (Slightly Complete):** (If context sufficient) Addresses only a small part, much relevant context ignored. (If context insufficient) Very poor use of limited context.
        *   **1 (Not Complete):** Fails to address the query meaningfully, even if relevant context was available.

4.  **Coherence/Fluency (Score 1-5):**
    *   Is the answer well-written, grammatically correct, logically structured, and easy to understand?
    *   **5 (Very Fluent):** Perfectly written, clear, natural, and easy to understand. No grammatical errors.
    *   **4 (Fluent):** Well-written, mostly clear, minor or no grammatical errors.
    *   **3 (Moderately Fluent):** Understandable, but may have some awkward phrasing or minor grammatical errors that don't impede core understanding.
    *   **2 (Slightly Fluent):** Difficult to understand due to grammatical errors, awkward phrasing, or poor logical flow.
    *   **1 (Not Fluent):** Largely incomprehensible.

5.  **Actionability & Usability (for an Office Worker Assistant) (Score 1-5):**
    *   Does the answer provide clear, practical, and easy-to-understand steps, information, or suggestions that would directly help an office worker achieve their task or make a decision?
    *   **5 (Highly Actionable & Usable):** Provides clear, specific, and immediately applicable steps/information. Language is professional and easy for an office worker to understand. Format facilitates quick information retrieval.
    *   **4 (Mostly Actionable & Usable):** Provides generally clear guidance or useful information. Might require minor clarification for full actionability, or presentation could be slightly improved, but core content is helpful.
    *   **3 (Partially Actionable & Usable):** Offers some relevant information or suggestions, but lacks specific steps, is too vague for direct action, or requires significant effort to understand/apply.
    *   **2 (Minimally Actionable & Usable):** Contains some related information but no clear action plan, is impractical, or very difficult to understand/use. Offers little practical help.
    *   **1 (Not Actionable & Unusable):** Provides no actionable information, is irrelevant to practical office tasks, or is misleading/confusing.

**OUTPUT JSON STRUCTURE (Strictly follow this format):**
```json
{
  "evaluation_summary": {
    "overall_answer_quality_score": "<Integer score 1-5, your overall judgment of the answer's quality, considering all dimensions. Faithfulness and Relevance are most critical.>",
    "main_strengths_answer": "<Briefly describe the main strength(s) of this answer, if any. Be specific.>",
    "main_weaknesses_answer": "<Briefly describe the main weakness(es) or most critical issue(s) with this answer. Be specific.>"
  },
  "dimensions": {
    "faithfulness": {
      "score": "<Integer score 1-5>",
      "reasoning": "<Text explanation for faithfulness score. If not fully faithful, specify which parts are unsupported or hallucinated, referencing the answer text.>",
      "problematic_answer_segments_faithfulness": ["<List of specific phrases/sentences from the answer that are not faithful, or empty list if none>"]
    },
    "relevance": {
      "score": "<Integer score 1-5>",
      "reasoning": "<Text explanation for relevance score. Explain how well it addresses the user's core question.>"
    },
    "completeness": {
      "context_sufficiency_assessment": "<String: 'Sufficient', 'Partially Sufficient', or 'Insufficient'>",
      "context_sufficiency_reasoning": "<Brief reason for the context sufficiency assessment. If not sufficient, what key information is missing from the context?>",
      "score": "<Integer score 1-5, based on how well the answer uses the available context to address the query>",
      "reasoning": "<Text explanation for completeness score. If not fully complete, what aspects of the query were missed, or what relevant context was underutilized? If context was insufficient, how well did the answer handle this?>"
    },
    "coherence_fluency": {
      "score": "<Integer score 1-5>",
      "reasoning": "<Text explanation for coherence/fluency score. Note any grammatical issues or awkward phrasing.>"
    },
    "actionability_usability": {
      "score": "<Integer score 1-5>",
      "reasoning": "<Text explanation for actionability/usability score, considering an office worker context.>"
    }
  },
  "suggestion_for_answer_improvement": "<Provide specific, actionable suggestions on how this AI-generated answer could be improved, if applicable. If excellent, state 'No improvement needed'.>"
}
Use code with caution.
FEW-SHOT EXAMPLES:
Example 1: Good Answer
USER'S NATURAL LANGUAGE QUESTION:
如何申请本年度的带薪年假，并确保符合公司规定？
Use code with caution.
CONTEXT PROVIDED TO THE RAG SYSTEM'S GENERATION MODEL:
上下文片段1 (来源: 公司政策文档):
员工每年享有15天带薪年假。申请年假需通过公司内部HRM系统提交，并至少提前两周提出。所有年假申请必须得到直属经理的批准。未经批准的休假将按事假处理。

上下文片段2 (来源: HRM系统操作指南):
HRM系统年假申请流程：1. 登录HRM门户 (hrm.example-company.com)。 2. 导航至“我的假期”->“假期申请”。 3. 选择“年假”类型，填写休假起止日期和休假天数。 4. 在备注中简要说明休假事由（可选）。 5. 提交申请，等待经理审批。审批状态可在系统中实时查看。

上下文片段3 (来源: 知识图谱精确信息):
事实: 年假政策规定 -> 提前申请周期: 2周。
事实: 年假政策规定 -> 审批人: 直属经理。
事实: 年假政策规定 -> 年假天数上限: 15天/年。
Use code with caution.
AI-GENERATED ANSWER TO EVALUATE:
您可以按照以下步骤申请本年度的带薪年假并确保符合公司规定：

1.  **提前规划与提交：** 根据公司政策，您需要至少提前两周提交年假申请。
2.  **通过HRM系统申请：**
    *   登录公司HRM门户网站：hrm.example-company.com
    *   进入“我的假期”模块，然后选择“假期申请”。
    *   选择“年假”类型，并准确填写您希望休假的起止日期和天数。
3.  **获取经理批准：** 所有年假申请都必须得到您直属经理的批准。提交申请后，请留意系统中的审批状态。
4.  **年假额度：** 请注意，公司规定员工每年享有15天带薪年假。

请确保您的申请符合以上规定，以顺利安排您的休假。
Use code with caution.
EXPECTED EVALUATION JSON OUTPUT:
{
  "evaluation_summary": {
    "overall_answer_quality_score": 5,
    "main_strengths_answer": "答案准确、完整、步骤清晰，非常实用。",
    "main_weaknesses_answer": "无明显缺点。"
  },
  "dimensions": {
    "faithfulness": {
      "score": 5,
      "reasoning": "答案中的所有信息点（提前两周、HRM系统操作、经理批准、15天年假）均能在提供的上下文中找到明确依据，无任何捏造。",
      "problematic_answer_segments_faithfulness": []
    },
    "relevance": {
      "score": 5,
      "reasoning": "答案完美地回应了用户关于年假申请流程和合规性的问题。"
    },
    "completeness": {
      "context_sufficiency_assessment": "Sufficient",
      "context_sufficiency_reasoning": "上下文提供了申请年假所需的所有核心信息（流程、提前时间、审批人、额度）。",
      "score": 5,
      "reasoning": "答案全面覆盖了用户问题的各个方面，并充分利用了上下文中的所有关键信息，包括操作步骤和政策规定。"
    },
    "coherence_fluency": {
      "score": 5,
      "reasoning": "答案语言表达清晰、专业，逻辑步骤明确，易于理解。"
    },
    "actionability_usability": {
      "score": 5,
      "reasoning": "答案提供了非常清晰、可直接执行的申请步骤，并点明了关键的合规性要求，对办公室员工非常实用且易于操作。"
    }
  },
  "suggestion_for_answer_improvement": "No improvement needed."
}
Use code with caution.
Json
Example 2: Answer with Hallucination
USER'S NATURAL LANGUAGE QUESTION:
我们公司最新的季度IT安全演练是什么时候举行的，主要发现了哪些问题？
Use code with caution.
CONTEXT PROVIDED TO THE RAG SYSTEM'S GENERATION MODEL:
上下文片段1 (来源: 内部邮件):
主题：关于第三季度IT安全意识培训的通知
内容：各位同事，为提升全员信息安全意识，公司计划于8月15日举行第三季度IT安全意识培训，主要内容包括密码安全、钓鱼邮件识别等。请大家准时参加。

上下文片段2 (来源: IT部门公告):
IT服务台提醒：近期发现多起针对财务部门的仿冒邮件，请大家提高警惕，不要点击不明链接。如有疑问，请及时联系IT支持。演练活动将在下月进行。
Use code with caution.
AI-GENERATED ANSWER TO EVALUATE:
公司最新的季度IT安全演练已于上个月（7月）成功举行。演练主要发现了两个问题：一是部分员工的办公电脑未及时更新防病毒软件，二是演练中模拟的钓鱼邮件成功欺骗了约15%的员工。后续将加强相关培训。
Use code with caution.
EXPECTED EVALUATION JSON OUTPUT:
{
  "evaluation_summary": {
    "overall_answer_quality_score": 1,
    "main_strengths_answer": "答案结构尚可。",
    "main_weaknesses_answer": "答案内容完全是编造的（幻觉），与提供的上下文信息完全不符，具有严重误导性。"
  },
  "dimensions": {
    "faithfulness": {
      "score": 1,
      "reasoning": "答案中关于演练已于7月举行、发现的两个具体问题（未更新防病毒软件、15%员工被钓鱼邮件欺骗）在上下文中完全找不到任何依据，是严重的幻觉。",
      "problematic_answer_segments_faithfulness": ["演练已于上个月（7月）成功举行。", "演练主要发现了两个问题：一是部分员工的办公电脑未及时更新防病毒软件，二是演练中模拟的钓鱼邮件成功欺骗了约15%的员工。"]
    },
    "relevance": {
      "score": 2,
      "reasoning": "答案表面上回应了问题（演练时间和问题），但由于内容是虚假的，其实际相关性很低。"
    },
    "completeness": {
      "context_sufficiency_assessment": "Partially Sufficient",
      "context_sufficiency_reasoning": "上下文提到了计划中的培训和演练（下月进行），以及一些安全问题（仿冒邮件），但没有给出已完成演练的具体时间和发现的问题。",
      "score": 1,
      "reasoning": "答案完全没有利用上下文中的有效信息（如计划中的培训和演练），而是编造了内容。"
    },
    "coherence_fluency": {
      "score": 4,
      "reasoning": "答案的语言表达本身是通顺的，语法基本正确。"
    },
    "actionability_usability": {
      "score": 1,
      "reasoning": "虚假的信息完全不可用，且具有误导性，对办公室工作有害无益。"
    }
  },
  "suggestion_for_answer_improvement": "AI模型必须严格基于提供的上下文生成答案，严禁编造任何上下文中未提及的事实。如果上下文信息不足，应明确指出。"
}
Use code with caution.
Json
Example 3: Incomplete Answer
USER'S NATURAL LANGUAGE QUESTION:
请总结一下我们和ABC公司最近一次会议的主要议题和达成的三项关键共识。
Use code with caution.
CONTEXT PROVIDED TO THE RAG SYSTEM'S GENERATION MODEL:
上下文片段1 (来源: 会议纪要 - ABC公司会议_20250515.docx):
会议日期：2025年5月15日
与会方：我方（李明、王芳），ABC公司（张总、赵经理）
主要议题：
1.  回顾Q1合作项目进展。
2.  讨论Q2新产品联合推广计划。
3.  探讨长期战略合作框架。
关键共识：
1.  双方同意Q1项目按计划完成，成果符合预期。
2.  Q2新产品联合推广预算初定为50万，具体方案下周讨论。
3.  双方均表达了加强长期战略合作的意愿，将成立联合工作组进一步商议。
4.  下次会议暂定于6月初。
Use code with caution.
AI-GENERATED ANSWER TO EVALUATE:
我们和ABC公司最近一次会议（2025年5月15日）的主要议题包括回顾Q1项目进展和讨论Q2新产品联合推广计划。会议达成的一项关键共识是双方同意Q1项目按计划完成。
Use code with caution.
EXPECTED EVALUATION JSON OUTPUT:
{
  "evaluation_summary": {
    "overall_answer_quality_score": 3,
    "main_strengths_answer": "答案忠实于上下文，相关性较好，语言通顺。",
    "main_weaknesses_answer": "答案在完整性方面有明显不足，遗漏了多个重要议题和关键共识。"
  },
  "dimensions": {
    "faithfulness": {
      "score": 5,
      "reasoning": "答案中提到的信息点（会议日期、部分议题、一项共识）均能在上下文中找到准确依据。",
      "problematic_answer_segments_faithfulness": []
    },
    "relevance": {
      "score": 4,
      "reasoning": "答案回应了用户关于会议议题和共识的问题，但不够全面。"
    },
    "completeness": {
      "context_sufficiency_assessment": "Sufficient",
      "context_sufficiency_reasoning": "上下文详细列出了3个主要议题和4项关键共识，足以完整回答用户问题。",
      "score": 2,
      "reasoning": "答案严重不完整。议题方面遗漏了“探讨长期战略合作框架”。关键共识方面，用户要求三项，但答案只给出了一项，遗漏了“Q2推广预算初定”、“加强长期战略合作意愿将成立工作组”这两项重要共识（甚至还有第四项共识也未提及）。"
    },
    "coherence_fluency": {
      "score": 5,
      "reasoning": "答案语言表达清晰、语法正确。"
    },
    "actionability_usability": {
      "score": 3,
      "reasoning": "答案提供了一些信息，但由于信息不完整，其实用性打了折扣。用户可能需要再次查找才能获得全部关键信息。"
    }
  },
  "suggestion_for_answer_improvement": "答案应更全面地从上下文中提取信息。应完整列出所有主要议题，并至少满足用户要求的三个关键共识。例如，可以补充：'其他主要议题还包括探讨长期战略合作框架。达成的其他关键共识有：Q2新产品联合推广预算初定为50万；双方将成立联合工作组进一步商议加强长期战略合作的意愿。'"
}
Use code with caution.
Json
NOW, EVALUATE THE FOLLOWING:
USER'S NATURAL LANGUAGE QUESTION:
{{USER_QUESTION}}
Use code with caution.
CONTEXT PROVIDED TO THE RAG SYSTEM'S GENERATION MODEL (this is the information the AI had to base its answer on):
{{RETRIEVED_CONTEXTS}}
Use code with caution.
AI-GENERATED ANSWER TO EVALUATE:
{{GENERATED_ANSWER}}
Use code with caution.
YOUR EVALUATION (Strictly in the JSON format defined above):
// Your JSON output here
Use code with caution.
Json
"""

async def evaluate_answer_with_gemini(
    gemini_resource: 'GeminiAPIResource', # <--- 修改：接收 GeminiAPIResource 实例
    user_question: str,
    retrieved_contexts: str,
    generated_answer: str,
    original_interaction_id: Optional[str] = None,
    app_version: str = "0.1.0"
) -> Optional[Dict[str, Any]]:
    eval_logger.info(f"Starting Answer evaluation. User question: '{user_question[:50]}...', Answer: '{generated_answer[:50]}...'")

    prompt_to_gemini = ANSWER_EVALUATION_PROMPT_V1.replace(
        "{{USER_QUESTION}}", user_question
    ).replace(
        "{{RETRIEVED_CONTEXTS}}", retrieved_contexts
    ).replace(
        "{{GENERATED_ANSWER}}", generated_answer
    )

    messages_for_gemini = [{"role": "user", "content": prompt_to_gemini}]
    evaluation_result_json: Optional[Dict[str, Any]] = None
    raw_gemini_output: Optional[str] = None
    error_info: Optional[str] = None

    USE_SIMULATED_GEMINI_RESPONSE = os.getenv("USE_SIMULATED_GEMINI_ANSWER_EVAL", "false").lower() == "true"

    if USE_SIMULATED_GEMINI_RESPONSE:
        eval_logger.warning("USING SIMULATED GEMINI RESPONSE FOR ANSWER EVALUATION")
        simulated_json_output = {
            "evaluation_summary": {
                "overall_answer_quality_score": 4,
                "main_strengths_answer": "Answer is mostly faithful and relevant.",
                "main_weaknesses_answer": "Could be more complete by utilizing more context."
            },
            "dimensions": {
                "faithfulness": {
                    "score": 4,
                    "reasoning": "Most claims are supported by context.",
                    "problematic_answer_segments_faithfulness": []
                },
                "relevance": {
                    "score": 5,
                    "reasoning": "Directly addresses the user's question."
                },
                "completeness": {
                    "context_sufficiency_assessment": "Partially Sufficient",
                    "context_sufficiency_reasoning": "Context provides some info but lacks specific detail X.",
                    "score": 3,
                    "reasoning": "Answer uses available context but misses detail Y which was present."
                },
                "coherence_fluency": {
                    "score": 5,
                    "reasoning": "Well-written and easy to understand."
                },
                "actionability_usability": {
                    "score": 4,
                    "reasoning": "Provides useful information, could have more direct steps."
                }
            },
            "suggestion_for_answer_improvement": "Consider adding detail Y from context if relevant."
        }
        raw_gemini_output = json.dumps(simulated_json_output)
        try:
            evaluation_result_json = json.loads(raw_gemini_output)
        except json.JSONDecodeError as e:
            eval_logger.error(f"Error decoding simulated JSON for answer eval: {e}")
            error_info = f"Simulated JSON decode error for answer: {str(e)}"
    else:
        try:
            eval_logger.info(f"Calling Gemini for Answer evaluation via GeminiAPIResource. Model: {gemini_resource.model_name}. Prompt length: {len(prompt_to_gemini)}")
            # --- 修改：通过 gemini_resource 调用 ---
            raw_gemini_output = await gemini_resource.call_completion(
                messages=messages_for_gemini
            )
            if raw_gemini_output:
                eval_logger.info(f"Raw Gemini output for Answer eval (first 300 chars): {raw_gemini_output[:300]}...")
                cleaned_output = raw_gemini_output.strip()
                if cleaned_output.startswith("```json"):
                    cleaned_output = cleaned_output[len("```json"):].strip()
                if cleaned_output.endswith("```"):
                    cleaned_output = cleaned_output[:-len("```")].strip()
                evaluation_result_json = json.loads(cleaned_output)
                eval_logger.info("Successfully parsed Gemini evaluation result for Answer.")
            else:
                eval_logger.error("Gemini call via resource returned None or empty for Answer evaluation.")
                error_info = "Gemini call via resource returned None/empty for answer"
        except json.JSONDecodeError as e_json:
            eval_logger.error(f"Failed to decode JSON from Gemini Answer evaluation. Raw output: {raw_gemini_output[:500] if raw_gemini_output else 'N/A'}", exc_info=True)
            error_info = f"JSONDecodeError for answer: {str(e_json)}"
        except Exception as e_gen:
            eval_logger.error(f"Unexpected error during Answer evaluation with Gemini resource: {e_gen}", exc_info=True)
            error_info = f"Unexpected error for answer with Gemini resource: {str(e_gen)}"
            if raw_gemini_output is None:
                raw_gemini_output = error_info

    eval_log_data = {
        "task_type": "answer_evaluation_result",
        "original_interaction_id_ref": original_interaction_id,
        "user_question_for_eval": user_question,
        "retrieved_contexts_for_eval_char_count": len(retrieved_contexts),
        "generated_answer_for_eval": generated_answer,
        "eval_llm_input_prompt_char_count": len(prompt_to_gemini),
        "eval_llm_model": gemini_resource.model_name, # <--- 修改：从资源获取模型名称
        "eval_llm_raw_output": raw_gemini_output,
        "eval_llm_processed_output_json": evaluation_result_json,
        "eval_error_info": error_info,
        "application_version": app_version
    }
    await log_interaction_data(
        eval_log_data,
        is_evaluation_result=True,
        evaluation_name_for_file="answer_gemini_flash"
    )

    if evaluation_result_json:
        eval_logger.info(f"Answer evaluation completed. Overall score: {evaluation_result_json.get('evaluation_summary', {}).get('overall_answer_quality_score')}")
    else:
        eval_logger.warning("Answer evaluation did not produce a valid JSON result.")
        
    return evaluation_result_json

if __name__ == '__main__':
    import asyncio

    # --- 模拟一个 GeminiAPIResource 实例用于测试 ---
    # 在实际的 Dagster 环境中，这个资源会被正确注入
    # 为了让这里的测试能跑通，我们需要一个模拟的资源或配置
    
    class MockGeminiAPIResource:
        def __init__(self, api_key, model_name, proxy_url=None, temp=0.1, max_tokens=1024):
            self.api_key = api_key
            self.model_name = model_name
            self.proxy_url = proxy_url
            self.default_temperature = temp
            self.default_max_tokens = max_tokens
            self._logger = eval_logger # 使用eval_logger进行模拟
            self._logger.info("MockGeminiAPIResource initialized for testing.")
            if not self.api_key:
                 self._logger.warning("MockGeminiAPIResource: API key is missing!")


        async def call_completion(self, messages: List[Dict[str, str]], **kwargs) -> Optional[str]:
            self._logger.info(f"MockGeminiAPIResource.call_completion called with {len(messages)} messages.")
            # 这是一个非常简化的模拟，实际测试时可能需要更复杂的模拟响应
            # 或者直接依赖环境变量 USE_SIMULATED_GEMINI_..._EVAL 来触发 evaluator 内部的模拟
            if "cypher" in messages[0]["content"].lower():
                return json.dumps({
                    "evaluation_summary": {"overall_quality_score_cypher": 5, "main_strength_cypher": "Mocked Cypher OK", "main_weakness_cypher": "None"},
                    "dimensions": {}, "qwen_error_patterns_identified": [], "suggestion_for_improvement_cypher": "Mocked suggestion"
                })
            else:
                return json.dumps({
                     "evaluation_summary": {"overall_answer_quality_score": 5, "main_strengths_answer": "Mocked Answer OK", "main_weaknesses_answer": "None"},
                     "dimensions": {}, "suggestion_for_answer_improvement": "Mocked suggestion for answer"
                })

    async def test_evaluators_with_mock_resource():
        eval_logger.info("--- Running tests for evaluators with MockGeminiAPIResource ---")
        
        mock_api_key = os.getenv("GEMINI_API_KEY") or "MOCK_API_KEY_IF_NOT_SET"
        mock_resource = MockGeminiAPIResource(
            api_key=mock_api_key,
            model_name="gemini-1.5-flash-latest" # 与资源默认值一致
        )

        # --- Test Cypher Evaluation ---
        eval_logger.info("\n--- Testing Cypher Evaluation with Mock Resource ---")
        cypher_result = await evaluate_cypher_with_gemini(
            gemini_resource=mock_resource,
            user_question="Test Cypher Question?",
            generated_cypher="MATCH (n) RETURN n",
            original_interaction_id="test_cypher_eval_001"
        )
        if cypher_result:
            eval_logger.info(f"Mocked Cypher Eval Result: {json.dumps(cypher_result, indent=2, ensure_ascii=False)}")
        else:
            eval_logger.warning("Mocked Cypher Eval returned None.")

        # --- Test Answer Evaluation ---
        eval_logger.info("\n--- Testing Answer Evaluation with Mock Resource ---")
        answer_result = await evaluate_answer_with_gemini(
            gemini_resource=mock_resource,
            user_question="Test Answer Question?",
            retrieved_contexts="Some sample context.",
            generated_answer="A sample answer based on context.",
            original_interaction_id="test_answer_eval_001"
        )
        if answer_result:
            eval_logger.info(f"Mocked Answer Eval Result: {json.dumps(answer_result, indent=2, ensure_ascii=False)}")
        else:
            eval_logger.warning("Mocked Answer Eval returned None.")

    # 如果希望在直接运行 evaluator.py 时执行测试：
    # asyncio.run(test_evaluators_with_mock_resource())
    
    # 保留您之前的测试代码（如果需要独立运行并实际调用API，但要注意资源注入）
    # os.environ["USE_SIMULATED_GEMINI_ANSWER_EVAL"] = "true"
    # asyncio.run(test_answer_evaluation()) # 您之前的测试函数名
    pass # 通常 evaluator.py 不会直接运行，而是被其他脚本调用
    --- END OF FILE CONTENT ---


Directory: zhz_rag/api/
  File: __init__.py
    --- File is empty ---

  File: main_api.py
    --- START OF FILE CONTENT ---
# zhz_agent/main.py
from fastapi import FastAPI
from contextlib import asynccontextmanager
import uvicorn
import logging
from typing import Type, List, Dict, Any, Optional, ClassVar

# --- [修改] 导入 -> 改为绝对导入 ---
from zhz_rag.utils.db_utils import database, sqlalchemy_engine, Base, get_scheduler
from zhz_rag.api.task_manager_api import router as tasks_router
from zhz_rag.task_management import db_models # 确保 SQLAlchemy 模型被导入

@asynccontextmanager
async def lifespan(app_instance: FastAPI):
    print("--- Main FastAPI 应用启动 (已集成任务管理API 和 APScheduler) ---") # [修改] 更新了描述

    # --- 数据库初始化 ---
    await database.connect()
    print("数据库已连接。")
    try:
        Base.metadata.create_all(bind=sqlalchemy_engine)
        print("数据库表已执行 create_all。")
        from sqlalchemy import inspect
        inspector = inspect(sqlalchemy_engine)
        if inspector.has_table("tasks"):
            print("'tasks' 表已成功创建/存在于数据库中。")
        else:
            print("警告: 'tasks' 表在 create_all 之后仍未找到！这通常意味着模型没有在 create_all 之前被正确导入。")
            print(f"   已知的表: {inspector.get_table_names()}") # 打印所有实际创建的表
            print(f"   Base.metadata.tables: {Base.metadata.tables.keys()}") # 打印 SQLAlchemy 元数据中已注册的表
    except Exception as e:
        print(f"创建或检查数据库表时出错: {e}")
        import traceback
        traceback.print_exc() # 打印详细的异常堆栈


    # --- [修改] APScheduler 初始化 (使用 get_scheduler) ---
    current_scheduler = get_scheduler() # <--- 获取调度器实例
    try:
        logging.getLogger('apscheduler').setLevel(logging.DEBUG) # 设置为 DEBUG 级别

        if not current_scheduler.running: # <--- 只有在未运行时才启动
            current_scheduler.start()
            print("APScheduler 已启动并使用数据库作业存储。")
        else:
            print("APScheduler 已在运行。")
    except Exception as e:
        print(f"APScheduler 启动失败: {e}")

    print("RAG 组件的初始化和管理在 zhz_agent_mcp_server.py。")
    print("任务管理API已在 /tasks 路径下可用。")

    yield # FastAPI 应用在此运行

    print("--- Main FastAPI 应用关闭 ---")
    current_scheduler_on_shutdown = get_scheduler() # <--- 再次获取以确保是同一个实例
    if current_scheduler_on_shutdown and current_scheduler_on_shutdown.running:
        current_scheduler_on_shutdown.shutdown()
        print("APScheduler 已关闭。")
    await database.disconnect()
    print("数据库已断开连接。")
    print("RAG 组件的清理在 zhz_agent_mcp_server.py。")

# --- App 定义 (保持不变) ---
app = FastAPI(
    title="Hybrid RAG Backend with Task Management",
    description="主 FastAPI 应用，负责接收请求、编排 Agent，并提供任务管理API。",
    version="0.2.1",
    lifespan=lifespan
)

app.include_router(tasks_router)

@app.get("/")
async def read_root():
    return {
        "message": "Welcome to the Hybrid RAG Backend Main App.",
        "available_services": {
            "task_management": "/tasks/docs",
            "rag_via_mcpo": "mcpo proxy at port 8006 (see mcpo_servers.json)"
        }
    }

if __name__ == "__main__":
    print("--- 启动 Main FastAPI 服务器 (包含任务管理API) ---")
    uvicorn.run("zhz_agent.main:app", host="0.0.0.0", port=8000, reload=True) # Ensure correct run command
    --- END OF FILE CONTENT ---

  File: rag_mcp_service.py
    --- START OF FILE CONTENT ---
import os
import json
import asyncio
import traceback
from contextlib import asynccontextmanager
from typing import List, Dict, Any, Optional, AsyncIterator
from dataclasses import dataclass, field # 确保导入 field
import time
import logging
import sys
import hashlib # <--- 添加
from datetime import datetime, timezone # <--- 添加
import uuid # <--- 添加


# MCP 框架导入
from mcp.server.fastmcp import FastMCP, Context

# --- 配置 rag_service 的专用日志 ---
_rag_service_py_dir = os.path.dirname(os.path.abspath(__file__))
_rag_service_log_file = os.path.join(_rag_service_py_dir, 'rag_service_debug.log')

rag_logger = logging.getLogger("RagServiceLogger")
rag_logger.setLevel(logging.DEBUG)
rag_logger.propagate = False

if rag_logger.hasHandlers():
    rag_logger.handlers.clear()

try:
    _file_handler = logging.FileHandler(_rag_service_log_file, mode='w')
    _file_handler.setLevel(logging.DEBUG)
    _formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
    _file_handler.setFormatter(_formatter)
    rag_logger.addHandler(_file_handler)
    rag_logger.info("--- RagServiceLogger configured to write to rag_service_debug.log ---")
except Exception as e:
    print(f"CRITICAL: Failed to configure RagServiceLogger: {e}")


# --- 从项目内部导入所有 RAG 模块 ---
from zhz_rag.config.pydantic_models import QueryRequest, HybridRAGResponse, RetrievedDocument
from zhz_rag.llm.llm_interface import (
    generate_answer_from_context,
    generate_expanded_queries,
    generate_cypher_query,
    generate_clarification_question,
    generate_intent_classification,
    NO_ANSWER_PHRASE_ANSWER_CLEAN
)
from zhz_rag.core_rag.retrievers.chromadb_retriever import ChromaDBRetriever
from zhz_rag.core_rag.retrievers.file_bm25_retriever import FileBM25Retriever
from zhz_rag.core_rag.kg_retriever import KGRetriever
from zhz_rag.core_rag.fusion_engine import FusionEngine
from zhz_rag.utils.common_utils import log_interaction_data

from dotenv import load_dotenv

# 加载 .env 文件
# __file__ 是当前 rag_service.py 的路径: /home/zhz/zhz_agent/rag_service.py
# os.path.dirname(os.path.abspath(__file__)) 是 /home/zhz/zhz_agent 目录
# .env 文件与 rag_service.py 在同一个目录下 (zhz_agent 目录)
current_dir = os.path.dirname(os.path.abspath(__file__))
dotenv_path = os.path.join(current_dir, '.env')

if os.path.exists(dotenv_path):
    load_dotenv(dotenv_path=dotenv_path)
    rag_logger.info(f"Loaded .env file from: {dotenv_path}")
else:
    rag_logger.warning(f".env file not found at {dotenv_path}, will rely on environment variables or defaults.")
    # 仍然尝试加载，因为python-dotenv的默认行为是查找当前工作目录和上级目录的.env
    load_dotenv()

# --- 应用上下文 Dataclass ---
@dataclass
class AppContext:
    # vector_retriever: VectorRetriever # 旧的
    chroma_retriever: Optional[ChromaDBRetriever] = None # 新的
    kg_retriever: Optional[KGRetriever] = None
    # bm25_retriever: BM25Retriever # 旧的
    file_bm25_retriever: Optional[FileBM25Retriever] = None # 新的
    fusion_engine: Optional[FusionEngine] = None
    # llm_generator: Optional[Any] = None # LLMGenerator在您的代码中没有被实例化并放入AppContext

# --- MCP 服务器生命周期管理 ---
@asynccontextmanager
async def app_lifespan_for_rag_service(server: FastMCP) -> AsyncIterator[AppContext]:
    rag_logger.info("--- RAG Service (FastMCP): 正在初始化 RAG 组件 (新版) ---")
    
    chroma_retriever_instance: Optional[ChromaDBRetriever] = None
    kg_retriever_instance: Optional[KGRetriever] = None
    file_bm25_retriever_instance: Optional[FileBM25Retriever] = None
    fusion_engine_instance: Optional[FusionEngine] = None

    # 初始化 ChromaDB Retriever
    try:
        # 这些路径和名称应该与Dagster流水线中配置的一致
        # 优先从环境变量读取，如果不存在则使用默认值（如果适用）
        chroma_persist_dir = os.getenv("CHROMA_PERSIST_DIRECTORY", "/home/zhz/dagster_home/chroma_data")
        chroma_collection_name = os.getenv("CHROMA_COLLECTION_NAME", "rag_documents")
        embedding_model_path = os.getenv("EMBEDDING_MODEL_PATH", "/home/zhz/models/bge-small-zh-v1.5")

        if not os.path.isdir(chroma_persist_dir):
                rag_logger.warning(f"ChromaDB persist directory '{chroma_persist_dir}' not found. Retrieval may fail or use an in-memory DB if ChromaDBRetriever handles this.")
        if not os.path.exists(embedding_model_path): # embedding_model_path 应该是目录
            rag_logger.warning(f"Embedding model path '{embedding_model_path}' not found. ChromaDBRetriever initialization might fail.")

        chroma_retriever_instance = ChromaDBRetriever(
            collection_name=chroma_collection_name,
            persist_directory=chroma_persist_dir,
            embedding_model_name_or_path=embedding_model_path
        )
        rag_logger.info("RAG Service: ChromaDBRetriever 初始化成功。")
    except Exception as e:
        rag_logger.error(f"RAG Service: ChromaDBRetriever 初始化失败: {e}", exc_info=True)
        # 不在此处抛出异常，允许服务在部分组件失败时仍能启动（如果设计如此）
    
    # 初始化 File BM25 Retriever
    try:
        bm25_index_dir = os.getenv("BM25_INDEX_DIRECTORY", "/home/zhz/dagster_home/bm25_index_data/")
        if not os.path.isdir(bm25_index_dir):
            rag_logger.warning(f"BM25 index directory '{bm25_index_dir}' not found. FileBM25Retriever initialization might fail.")
            
        file_bm25_retriever_instance = FileBM25Retriever(
            index_directory_path=bm25_index_dir
        )
        rag_logger.info("RAG Service: FileBM25Retriever 初始化成功。")
    except Exception as e:
        rag_logger.error(f"RAG Service: FileBM25Retriever 初始化失败: {e}", exc_info=True)

    # 初始化 KG Retriever
    try:
        # KGRetriever 内部会从环境变量读取NEO4J配置
        # generate_cypher_query 是从 zhz_agent.llm 导入的
        kg_retriever_instance = KGRetriever(llm_cypher_generator_func=generate_cypher_query)
        rag_logger.info("RAG Service: KGRetriever 初始化成功。")
    except Exception as e:
        rag_logger.error(f"RAG Service: KGRetriever 初始化失败: {e}", exc_info=True)
        if kg_retriever_instance and hasattr(kg_retriever_instance, 'close'): # 确保在失败前尝试关闭
            kg_retriever_instance.close()
            
    # 初始化 Fusion Engine
    try:
        fusion_engine_instance = FusionEngine(logger=rag_logger)
        rag_logger.info("RAG Service: FusionEngine 初始化成功。")
    except Exception as e:
        rag_logger.error(f"RAG Service: FusionEngine 初始化失败: {e}", exc_info=True)

    rag_logger.info("--- RAG Service (FastMCP): RAG 组件初始化尝试完成。---")

    ctx = AppContext(
        chroma_retriever=chroma_retriever_instance,
        kg_retriever=kg_retriever_instance,
        file_bm25_retriever=file_bm25_retriever_instance,
        fusion_engine=fusion_engine_instance
    )
    try:
        yield ctx
    finally:
        rag_logger.info("--- RAG Service (FastMCP): 正在清理资源 ---")
        if kg_retriever_instance: # 确保只在成功初始化后才调用close
            kg_retriever_instance.close() 
        rag_logger.info("--- RAG Service (FastMCP): 清理完成 ---")

# --- 初始化 FastMCP 应用 ---
rag_mcp_application = FastMCP(
    name="zhz_agent_rag_service", # 修改了服务名称以区分
    description="Upgraded Hybrid RAG 服务，使用持久化知识库。",
    lifespan=app_lifespan_for_rag_service,
)

@rag_mcp_application.tool()
async def query_rag_v2( # 重命名工具函数以避免与旧的混淆 (如果需要)
    ctx: Context,
    query: str, # 直接使用 query 作为输入，而不是 QueryRequest 对象
    top_k_vector: int = 3,
    top_k_kg: int = 2,
    top_k_bm25: int = 3,
    top_k_final: int = 3 # 最终融合后返回的文档数
) -> str: 
    rag_logger.info(f"\n--- RAG Service (query_rag_v2): 接收到查询: '{query}' ---")
    rag_logger.info(f"      Params: top_k_vector={top_k_vector}, top_k_kg={top_k_kg}, top_k_bm25={top_k_bm25}, top_k_final={top_k_final}")
    start_time_total = time.time()

    app_ctx: AppContext = ctx.request_context.lifespan_context
    response_payload = {} 
    original_query_for_response = query 
    final_json_output = ""
    # --- [新增日志变量] ---
    log_all_raw_retrievals_summary: List[Dict[str, Any]] = []
    log_final_context_docs_summary: List[Dict[str, Any]] = [] # This will store the full doc model dumps
    log_fused_context_text_for_llm_snippet: str = "N/A"
    # --- [确保所有日志变量都有初始值] ---
    log_final_answer_from_llm: str = "N/A"
    log_intent_classification_result: Optional[Dict[str, Any]] = None
    log_expanded_queries: Optional[List[str]] = None
    # --- [结束确保所有日志变量都有初始值] ---

    try:
        # --- 1. LLM 驱动的意图分类和澄清触发 ---
        rag_logger.info(f"--- [TIME] 开始意图分类 at {time.time() - start_time_total:.2f}s ---")
        start_time_intent = time.time()
        intent_classification_result = await generate_intent_classification(query)
        log_intent_classification_result = intent_classification_result # <--- 记录日志
        rag_logger.info(f"--- [TIME] 结束意图分类, 耗时: {time.time() - start_time_intent:.2f}s. Result: {intent_classification_result}")

        if intent_classification_result.get("clarification_needed"):
            uncertainty_reason = intent_classification_result.get("reason", "查询可能存在歧义或信息不足。")
            clarification_question_text = await generate_clarification_question(query, uncertainty_reason)
            response_payload = {
                "status": "clarification_needed",
                "clarification_question": clarification_question_text,
                "original_query": original_query_for_response,
                "debug_info": {"uncertainty_reason": uncertainty_reason, "source": "intent_classification"}
            }
            rag_logger.info(f"--- 需要澄清，返回: {response_payload}")
            # final_json_output will be set before finally block

        else: 
             # --- 启用查询扩展 ---
            rag_logger.info(f"--- 查询清晰，无需澄清。将对原始查询 '{query}' 进行查询扩展 ---")
            start_time_expansion = time.time()
            expanded_queries = await generate_expanded_queries(query) # <--- 取消注释
            log_expanded_queries = expanded_queries # <--- 记录实际的扩展查询
            
            if not expanded_queries or query not in expanded_queries: # 确保原始查询一定在里面
                # 如果 generate_expanded_queries 返回空或不包含原始查询，至少处理原始查询
                if query not in (expanded_queries or []): # 处理 expanded_queries 可能为 None 的情况
                    expanded_queries = [query] + (expanded_queries or [])
                elif not expanded_queries:
                    expanded_queries = [query]

            rag_logger.info(f"--- 扩展后的查询列表 (共 {len(expanded_queries)} 个): {expanded_queries}. 耗时: {time.time() - start_time_expansion:.2f}s ---")
            
            all_raw_retrievals: List[RetrievedDocument] = []
            
            queries_to_process = expanded_queries # <--- 修改：现在处理所有扩展后的查询
            rag_logger.info(f"--- [TIME] 开始并行召回 for {len(queries_to_process)} queries at {time.time() - start_time_total:.2f}s ---")
            start_time_retrieval = time.time()

            for current_query_text in queries_to_process:
                rag_logger.info(f"Processing retrievals for query: '{current_query_text}'")
                
                # 向量检索 (ChromaDB)
                if app_ctx.chroma_retriever:
                    try:
                        chroma_docs_raw = app_ctx.chroma_retriever.retrieve(query_text=current_query_text, n_results=top_k_vector)
                        rag_logger.debug(f"   ChromaDB for '{current_query_text}' raw output: {chroma_docs_raw}") 
                        for doc_raw in chroma_docs_raw:
                            retrieved_doc = RetrievedDocument(
                                source_type="vector_chroma",
                                content=doc_raw.get("text", ""),
                                score=doc_raw.get("score", 0.0),
                                metadata={**doc_raw.get("metadata", {}), "original_query_part": current_query_text}
                            )
                            all_raw_retrievals.append(retrieved_doc)
                            log_all_raw_retrievals_summary.append(retrieved_doc.model_dump()) 
                        rag_logger.info(f"   ChromaDB for '{current_query_text}': found {len(chroma_docs_raw)} docs.")
                    except Exception as e_chroma:
                        rag_logger.error(f"   Error during ChromaDB retrieval for '{current_query_text}': {e_chroma}", exc_info=True)
                
                # 关键词检索 (BM25)
                if app_ctx.file_bm25_retriever:
                    try:
                        bm25_docs_raw = app_ctx.file_bm25_retriever.retrieve(query_text=current_query_text, n_results=top_k_bm25)
                        rag_logger.debug(f"   BM25 for '{current_query_text}' raw output (IDs and scores): {bm25_docs_raw}") 
                        for doc_raw_bm25 in bm25_docs_raw:
                            bm25_chunk_id = doc_raw_bm25.get("id")
                            text_content_for_bm25 = f"[BM25: Text for ID {bm25_chunk_id} pending]"
                            found_in_chroma = False
                            for existing_doc in all_raw_retrievals: 
                                if (existing_doc.metadata and (existing_doc.metadata.get("chunk_id") == bm25_chunk_id or existing_doc.metadata.get("id") == bm25_chunk_id)):
                                    text_content_for_bm25 = existing_doc.content
                                    found_in_chroma = True
                                    break
                            if not found_in_chroma and app_ctx.chroma_retriever and bm25_chunk_id: 
                                try:
                                    specific_chroma_doc = app_ctx.chroma_retriever._collection.get(ids=[bm25_chunk_id], include=["metadatas", "documents"]) # Also fetch documents for content
                                    if specific_chroma_doc:
                                        if specific_chroma_doc.get("documents") and specific_chroma_doc.get("documents")[0]:
                                            text_content_for_bm25 = specific_chroma_doc["documents"][0]
                                        elif specific_chroma_doc.get("metadatas") and specific_chroma_doc.get("metadatas")[0]: # Fallback to chunk_text in metadata
                                            text_content_for_bm25 = specific_chroma_doc["metadatas"][0].get("chunk_text", text_content_for_bm25)

                                except Exception as e_chroma_get:
                                    rag_logger.warning(f"   Failed to get text for BM25 ID {bm25_chunk_id} from Chroma: {e_chroma_get}")
                            
                            retrieved_doc = RetrievedDocument(
                                source_type="keyword_bm25s",
                                content=text_content_for_bm25,
                                score=doc_raw_bm25.get("score", 0.0),
                                metadata={"chunk_id": bm25_chunk_id, "original_query_part": current_query_text}
                            )
                            all_raw_retrievals.append(retrieved_doc)
                            log_all_raw_retrievals_summary.append(retrieved_doc.model_dump()) 
                        rag_logger.info(f"   BM25s for '{current_query_text}': found {len(bm25_docs_raw)} potential docs.")
                    except Exception as e_bm25:
                        rag_logger.error(f"   Error during BM25 retrieval for '{current_query_text}': {e_bm25}", exc_info=True)

                # 知识图谱检索
                if app_ctx.kg_retriever:
                    try:
                        rag_logger.info(f"   Performing KG retrieval for query: '{current_query_text}'")
                        kg_docs = await app_ctx.kg_retriever.retrieve_with_llm_cypher(query=current_query_text, top_k=top_k_kg)
                        rag_logger.debug(f"   KG for '{current_query_text}' raw output: {kg_docs}") 
                        for kg_doc_data in kg_docs: # kg_docs is List[Dict], needs conversion
                            retrieved_doc = RetrievedDocument(**kg_doc_data) # Convert dict to Pydantic model
                            if retrieved_doc.metadata:
                                retrieved_doc.metadata["original_query_part"] = current_query_text
                            else:
                                retrieved_doc.metadata = {"original_query_part": current_query_text}
                            all_raw_retrievals.append(retrieved_doc)
                            log_all_raw_retrievals_summary.append(retrieved_doc.model_dump()) 
                        rag_logger.info(f"   KG Retrieval for '{current_query_text}': found {len(kg_docs)} results.")
                    except Exception as e_kg:
                        rag_logger.error(f"   Error during KG retrieval for '{current_query_text}': {e_kg}", exc_info=True)
            
            rag_logger.info(f"--- [TIME] 结束所有召回, 耗时: {time.time() - start_time_retrieval:.2f}s ---")
            rag_logger.info(f"--- 总计从各路召回（所有查询处理后）的结果数: {len(all_raw_retrievals)} ---")
            if all_raw_retrievals:
                for i_doc, doc_retrieved in enumerate(all_raw_retrievals[:3]): # 日志只打印前3条摘要
                        rag_logger.debug(f"   Raw Doc {i_doc} (Summary): type={doc_retrieved.source_type}, score={doc_retrieved.score}, content='{str(doc_retrieved.content)[:50]}...'")

            if not all_raw_retrievals: 
                response_payload = {
                    "status": "success", 
                    "final_answer": "抱歉，根据您提供的查询，未能从知识库中找到相关信息。",
                    "original_query": original_query_for_response,
                    "retrieved_context_docs": [], 
                    "debug_info": {"message": "No documents retrieved from any source."}
                }
            else:
                rag_logger.info(f"--- [TIME] 开始结果融合与重排序 at {time.time() - start_time_total:.2f}s ---")
                start_time_fusion = time.time()
                final_context_docs: List[RetrievedDocument]
                if not app_ctx.fusion_engine:
                    rag_logger.error("FusionEngine not available! Skipping fusion and reranking.")
                    final_context_docs = sorted(all_raw_retrievals, key=lambda d: d.score if d.score is not None else -float('inf'), reverse=True)[:top_k_final]
                else:
                    final_context_docs = await app_ctx.fusion_engine.fuse_results(
                        all_raw_retrievals, 
                        original_query_for_response,
                        top_n_final=top_k_final
                    ) 
                log_final_context_docs_summary = [doc.model_dump() for doc in final_context_docs] 

                # --- 新增日志，检查 model_dump 的输出 ---
                if log_final_context_docs_summary:
                    rag_logger.info(f"DEBUG_MODEL_DUMP: First item of log_final_context_docs_summary (from model_dump()): {json.dumps(log_final_context_docs_summary[0], ensure_ascii=False, default=str)}")
                # --- 结束新增日志 ---

                rag_logger.info(f"--- [TIME] 结束结果融合与重排序, 耗时: {time.time() - start_time_fusion:.2f}s. Final context docs: {len(final_context_docs)} ---")
                if final_context_docs:
                    for i_fdoc, fdoc_retrieved in enumerate(final_context_docs[:3]): # 日志只打印前3条摘要
                        rag_logger.debug(f"   Fused Doc {i_fdoc} (Summary): type={fdoc_retrieved.source_type}, score={fdoc_retrieved.score}, content='{str(fdoc_retrieved.content)[:50]}...'")
                
                if not final_context_docs: 
                    fused_context_text_for_llm = "未在知识库中找到相关信息。"
                    final_answer_from_llm = "根据现有知识，未能找到您查询的相关信息。"
                    response_payload = {
                        "status": "success",
                        "final_answer": final_answer_from_llm,
                        "original_query": original_query_for_response,
                        "retrieved_context_docs": [], 
                        "debug_info": {"message": "No relevant context found after fusion."}
                    }
                else:
                    context_strings_for_llm = []
                    for doc in final_context_docs:
                        score_str = f"{doc.score:.4f}" if isinstance(doc.score, float) else str(doc.score if doc.score is not None else 'N/A')
                        context_strings_for_llm.append(
                            f"Source Type: {doc.source_type}, Score: {score_str}\nContent: {doc.content}"
                        )
                    fused_context_text_for_llm = "\n\n---\n\n".join(context_strings_for_llm)
                    log_fused_context_text_for_llm_snippet = fused_context_text_for_llm[:500] 

                    rag_logger.info(f"\n--- FUSED CONTEXT for LLM (length: {len(fused_context_text_for_llm)} chars) ---")
                    rag_logger.info(f"{fused_context_text_for_llm[:1000]}...") 
                    rag_logger.info(f"--- END OF FUSED CONTEXT ---\n")

                    rag_logger.info(f"--- [TIME] 开始最终答案生成 at {time.time() - start_time_total:.2f}s ---")
                    start_time_answer_gen = time.time()
                    final_answer_from_llm = await generate_answer_from_context(query, fused_context_text_for_llm)
                    log_final_answer_from_llm = final_answer_from_llm or "N/A" 
                    rag_logger.info(f"--- [TIME] 结束最终答案生成, 耗时: {time.time() - start_time_answer_gen:.2f}s ---")

                    if not final_answer_from_llm or final_answer_from_llm.strip() == NO_ANSWER_PHRASE_ANSWER_CLEAN:
                        final_answer_from_llm = "根据您提供的信息，我暂时无法给出明确的回答。"
                    
                    response_payload = {
                        "status": "success",
                        "final_answer": final_answer_from_llm,
                        "original_query": original_query_for_response,
                        "retrieved_context_docs": [doc.model_dump() for doc in final_context_docs], 
                        "debug_info": {"total_raw_retrievals_count": len(all_raw_retrievals)}
                    }

        final_json_output = json.dumps(response_payload, ensure_ascii=False)
        rag_logger.info(f"--- 'query_rag_v2' 逻辑执行完毕, 总耗时: {time.time() - start_time_total:.2f}s. ---")
        
    except Exception as e_main:
        rag_logger.error(f"RAG Service CRITICAL ERROR in 'query_rag_v2' (main try-except): {type(e_main).__name__} - {str(e_main)}", exc_info=True)
        user_query_for_err_log = original_query_for_response if 'original_query_for_response' in locals() and original_query_for_response else query
        response_payload = {
            "status": "error",
            "error_code": "RAG_SERVICE_INTERNAL_ERROR",
            "error_message": f"RAG服务内部发生未预期错误: {str(e_main)}",
            "original_query": user_query_for_err_log,
            "debug_info": {"exception_type": type(e_main).__name__}
        }
        final_json_output = json.dumps(response_payload, ensure_ascii=False)
    finally: 
        interaction_id_for_log = str(uuid.uuid4())
        current_app_version = "zhz_rag_mcp_service_0.2.1" 

        processed_final_context_docs_for_log = []
        temp_log_final_context_docs = locals().get('log_final_context_docs_summary') # 安全获取

        if temp_log_final_context_docs: # 如果 RAG 流程成功并且 final_context_docs 被处理了
            for doc_dict in temp_log_final_context_docs: # temp_log_final_context_docs 是 model_dump() 后的列表
                cleaned_doc = {}
                for key, value in doc_dict.items():
                    if isinstance(value, float) and (value != value or value == float('inf') or value == float('-inf')): 
                        cleaned_doc[key] = None 
                    else:
                        cleaned_doc[key] = value
                processed_final_context_docs_for_log.append(cleaned_doc)
        # 如果 temp_log_final_context_docs 为 None (例如澄清路径)，则 processed_final_context_docs_for_log 保持为 []

        full_log_entry = {
            "timestamp_utc": datetime.now(timezone.utc).isoformat(),
            "interaction_id": interaction_id_for_log,
            "original_query_interaction_id_ref": locals().get('original_query_interaction_id'), 
            "task_type": "rag_query_processing_full_log",
            "app_version": current_app_version,
            "original_user_query": locals().get('original_query_for_response', query), # query 总是有定义的
            "query_params": {
                "top_k_vector": top_k_vector, "top_k_kg": top_k_kg, 
                "top_k_bm25": top_k_bm25, "top_k_final": top_k_final
            },
            "intent_classification_result": locals().get('log_intent_classification_result'),
            "expanded_queries": locals().get('log_expanded_queries', []), # 默认为空列表
            "all_raw_retrievals_count": len(locals().get('log_all_raw_retrievals_summary', [])),
            "final_context_docs_count": len(processed_final_context_docs_for_log), # 使用清理后列表的长度
            "final_context_docs_summary": [ 
                {
                    "source_type": doc.get("source_type"), 
                    "score": doc.get("score"), 
                    "id": (doc.get("metadata",{}).get("chunk_id") or doc.get("metadata",{}).get("id")) if doc.get("metadata") else None, 
                    "content_preview": str(doc.get("content","N/A"))[:50]+"..."
                } 
                for doc in processed_final_context_docs_for_log[:5] # 使用清理后列表的摘要
            ], 
            "final_context_docs_full": processed_final_context_docs_for_log, # <--- 使用清理后的完整列表
            "fused_context_text_for_llm_snippet": locals().get('log_fused_context_text_for_llm_snippet', "N/A"),
            "final_answer_from_llm": locals().get('log_final_answer_from_llm', "N/A"),
            "final_response_payload_status": locals().get('response_payload', {}).get("status", "Unknown"),
            "total_processing_time_seconds": round(time.time() - start_time_total, 2) if 'start_time_total' in locals() else -1,
        }

        try:
            await log_interaction_data(full_log_entry) 
            rag_logger.info(f"Full RAG interaction log (ID: {interaction_id_for_log}) has been written.")
        except Exception as e_log_final:
            rag_logger.error(f"CRITICAL: Failed to write full RAG interaction log: {e_log_final}", exc_info=True)
        
        sys.stdout.flush(); sys.stderr.flush() 
    
    return final_json_output

if __name__ == "__main__":
    rag_logger.info("--- Starting RAG Service (FastMCP for mcpo via direct run) ---")
    rag_mcp_application.run()
    --- END OF FILE CONTENT ---

  File (skipped, extension not included): rag_service_debug.log
  File: task_manager_api.py
    --- START OF FILE CONTENT ---
# zhz_agent/task_manager_service.py
from fastapi import APIRouter, HTTPException, Depends, Body, Query, Path, status
from typing import List, Optional, Any, cast
from datetime import datetime, timedelta
import uuid
import traceback # 导入 traceback
import pytz

# --- [修改] 从 pydantic_models 导入我们定义的模型 -> 改为绝对导入 ---
from zhz_rag.config.pydantic_models import TaskModel, CreateTaskRequest, UpdateTaskRequest, TaskStatus, ReminderMethod

# --- [修改] 从 database_models 导入 SQLAlchemy 表模型 -> 改为绝对导入 ---
from zhz_rag.task_management.db_models import TaskDB

# --- [修改] 从新的 database.py 导入 database 对象 和 get_scheduler -> 改为绝对导入 ---
from zhz_rag.utils.db_utils import database, get_scheduler # 将 db_utils 修改为 database

# --- [修改] 从 .task_jobs 导入作业函数 -> 改为绝对导入 ---
from zhz_rag.task_management.jobs import send_task_reminder, execute_task_action
from apscheduler.triggers.date import DateTrigger # 用于指定精确的运行时间
from apscheduler.jobstores.base import JobLookupError # <--- [修改] 导入 JobLookupError 的正确路径

# APIRouter 实例
router = APIRouter(
    prefix="/tasks",
    tags=["Task Management"],
    responses={404: {"description": "Not found"}},
)

def _ensure_utc(dt: datetime) -> datetime:
    """确保 datetime 对象是 UTC 时区感知的。"""
    if dt.tzinfo is None:
        return pytz.utc.localize(dt) # 如果是朴素时间，假定它是UTC并设为UTC
    return dt.astimezone(pytz.utc) # 如果是其他时区，转换为UTC

def _schedule_task_jobs(task: TaskModel):
    current_scheduler = get_scheduler() # 获取 scheduler 实例
    print(f"DEBUG SCHEDULER: _schedule_task_jobs called. Scheduler instance: {current_scheduler}, Is running: {current_scheduler.running if current_scheduler else 'N/A'}")
    if not current_scheduler or not current_scheduler.running:
        print("SCHEDULER_ERROR: APScheduler 未运行，无法调度作业。")
        return

    # 提醒作业
    if task.reminder_time and task.status == TaskStatus.PENDING:
        reminder_job_id = f"reminder_{task.id}"
        try:
            reminder_methods_list = task.reminder_methods
            reminder_utc = _ensure_utc(task.reminder_time)
            print(f"SCHEDULER DEBUG: Passing reminder_methods to job: {reminder_methods_list}") # 添加日志

            current_scheduler.add_job(
                send_task_reminder,
                trigger=DateTrigger(run_date=reminder_utc),
                args=[task.id, task.title, reminder_methods_list], # <--- [修复] 直接传递列表
                id=reminder_job_id,
                name=f"Reminder for task {task.title[:20]}",
                replace_existing=True
            )
            print(f"SCHEDULER: 已为任务 '{task.id}' 添加/更新提醒作业，运行于 {task.reminder_time}")
        except Exception as e:
            print(f"SCHEDULER_ERROR: 添加提醒作业失败 for task '{task.id}': {e}")
            traceback.print_exc() # 打印详细错误堆栈

    # 执行作业
    if task.due_date and task.status == TaskStatus.PENDING:
        execution_job_id = f"execution_{task.id}"
        try:
            due_utc = _ensure_utc(task.due_date) # <--- [新增] 确保时间是 UTC 感知的
            print(f"SCHEDULER DEBUG: Adding execution job at {due_utc} ({due_utc.tzinfo})") # <--- [新增] 添加时区日志
            current_scheduler.add_job(
                execute_task_action,
                trigger=DateTrigger(run_date=due_utc),
                args=[task.id, task.action_type, task.action_payload],
                id=execution_job_id,
                name=f"Execution for task {task.title[:20]}",
                replace_existing=True
            )
            print(f"SCHEDULER: 已为任务 '{task.id}' 添加/更新执行作业，运行于 {task.due_date}")
        except Exception as e:
            print(f"SCHEDULER_ERROR: 添加执行作业失败 for task '{task.id}': {e}")

def _cancel_task_jobs(task_id: str):
    """从 APScheduler 取消作业"""
    current_scheduler = get_scheduler()
    if not current_scheduler or not current_scheduler.running:
        print("SCHEDULER_ERROR: APScheduler 未运行，无法取消作业。")
        return

    reminder_job_id = f"reminder_{task_id}"
    execution_job_id = f"execution_{task_id}"

    try:
        current_scheduler.remove_job(reminder_job_id)
        print(f"SCHEDULER: 已移除提醒作业 for task '{task_id}'")
    except JobLookupError:
        print(f"SCHEDULER_INFO: 提醒作业 '{reminder_job_id}' 未找到，无需移除。")
    except Exception as e:
        print(f"SCHEDULER_ERROR: 移除提醒作业失败 for task '{task_id}': {e}")

    try:
        current_scheduler.remove_job(execution_job_id)
        print(f"SCHEDULER: 已移除执行作业 for task '{task_id}'")
    except JobLookupError:
        print(f"SCHEDULER_INFO: 执行作业 '{execution_job_id}' 未找到，无需移除。")
    except Exception as e:
        print(f"SCHEDULER_ERROR: 移除执行作业失败 for task '{task_id}': {e}")

@router.post("/", response_model=TaskModel, status_code=status.HTTP_201_CREATED)
async def create_task(task_request: CreateTaskRequest = Body(...)):
    """
    创建一个新任务。
    """
    now = datetime.utcnow()
    task_id = str(uuid.uuid4())

    reminder_time_val = None
    if task_request.due_date and task_request.reminder_offset_minutes is not None:
        reminder_time_val = task_request.due_date - timedelta(minutes=task_request.reminder_offset_minutes)

    reminder_methods_values = [
        method.value if hasattr(method, 'value') else str(method)
        for method in (task_request.reminder_methods or [ReminderMethod.NOTIFICATION])
    ]

    insert_query = TaskDB.__table__.insert().values(
        id=task_id,
        title=task_request.title,
        description=task_request.description,
        status=TaskStatus.PENDING,
        created_at=now,
        updated_at=now,
        due_date=task_request.due_date,
        reminder_time=reminder_time_val,
        reminder_offset_minutes=task_request.reminder_offset_minutes,
        reminder_methods=reminder_methods_values, # <--- 确保存入的是字符串列表
        priority=task_request.priority or 0,
        tags=task_request.tags or [],
        action_type=task_request.action_type,
        action_payload=task_request.action_payload or {}
    )

    try:
        await database.execute(insert_query)
    except Exception as e:
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Failed to create task in database: {e}")

    created_task_db = await database.fetch_one(TaskDB.__table__.select().where(TaskDB.id == task_id))
    if not created_task_db:
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to retrieve task after creation")

    response_task = TaskModel.model_validate(dict(created_task_db))

    response_task.reminder_methods = [
        m.value if hasattr(m, 'value') else str(m)
        for m in response_task.reminder_methods
    ]

    _schedule_task_jobs(response_task)
    print(f"TASK_MANAGER: Created task '{response_task.id}' with title '{response_task.title}' in DB")
    return response_task

@router.get("/", response_model=List[TaskModel])
async def list_tasks(
    status_filter: Optional[TaskStatus] = Query(None, alias="status"),
    priority_filter: Optional[int] = Query(None, alias="priority"),
    skip: int = Query(0, ge=0),
    limit: int = Query(10, ge=1, le=100)
):
    """
    获取任务列表，支持过滤和分页。
    """
    query = TaskDB.__table__.select()
    if status_filter:
        query = query.where(TaskDB.status == status_filter)
    if priority_filter is not None:
        query = query.where(TaskDB.priority == priority_filter)

    query = query.order_by(TaskDB.created_at.desc()).offset(skip).limit(limit)

    db_tasks = await database.fetch_all(query)
    return [TaskModel.model_validate(dict(task)) for task in db_tasks]

@router.get("/{task_id}", response_model=TaskModel)
async def get_task(task_id: str = Path(..., description="要获取的任务ID")):
    """
    根据ID获取单个任务的详细信息。
    """
    query = TaskDB.__table__.select().where(TaskDB.id == task_id)
    db_task = await database.fetch_one(query)
    if not db_task:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Task not found")
    return TaskModel.model_validate(dict(db_task))

@router.put("/{task_id}", response_model=TaskModel)
async def update_task(
    task_id: str = Path(..., description="要更新的任务ID"),
    task_update: UpdateTaskRequest = Body(...)
):
    """
    更新现有任务。
    """
    existing_task_query = TaskDB.__table__.select().where(TaskDB.id == task_id)
    db_task = await database.fetch_one(existing_task_query)
    if not db_task:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Task not found")

    update_data = task_update.model_dump(exclude_unset=True)
    update_data["updated_at"] = datetime.utcnow()

    if "reminder_methods" in update_data and update_data["reminder_methods"] is not None:
        update_data["reminder_methods"] = [
            method.value if hasattr(method, 'value') else str(method)
            for method in update_data["reminder_methods"]
        ]

    current_due_date = update_data.get("due_date", cast(Optional[datetime], db_task.due_date))
    current_offset = update_data.get("reminder_offset_minutes", cast(Optional[int], db_task.reminder_offset_minutes))

    if current_due_date and current_offset is not None:
        update_data["reminder_time"] = current_due_date - timedelta(minutes=current_offset)
    elif "due_date" in update_data and current_offset is None:
         update_data["reminder_time"] = None


    update_query = TaskDB.__table__.update().where(TaskDB.id == task_id).values(**update_data)
    await database.execute(update_query)

    updated_db_task = await database.fetch_one(existing_task_query)
    if not updated_db_task:
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to retrieve task after update")

    response_task = TaskModel.model_validate(dict(updated_db_task))

    response_task.reminder_methods = [
        m.value if hasattr(m, 'value') else str(m)
        for m in response_task.reminder_methods
    ]

    _cancel_task_jobs(task_id)
    _schedule_task_jobs(response_task)
    print(f"TASK_MANAGER: Updated task '{response_task.id}' in DB")
    return response_task

@router.delete("/{task_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_task(task_id: str = Path(..., description="要删除的任务ID")):
    """
    删除一个任务。
    """
    existing_task_query = TaskDB.__table__.select().where(TaskDB.id == task_id)
    db_task = await database.fetch_one(existing_task_query)
    if not db_task:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Task not found")

    delete_query = TaskDB.__table__.delete().where(TaskDB.id == task_id)
    await database.execute(delete_query)

    _cancel_task_jobs(task_id)
    print(f"TASK_MANAGER: Deleted task '{task_id}' from DB")
    return None

@router.post("/{task_id}/complete", response_model=TaskModel)
async def mark_task_as_complete(task_id: str = Path(..., description="要标记为完成的任务ID")):
    """
    将任务标记为已完成。
    """
    existing_task_query = TaskDB.__table__.select().where(TaskDB.id == task_id)
    db_task_row = await database.fetch_one(existing_task_query)
    if not db_task_row:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Task not found")

    db_task = TaskModel.model_validate(dict(db_task_row))
    if db_task.status == TaskStatus.COMPLETED:
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="Task is already completed")

    update_data = {
        "status": TaskStatus.COMPLETED,
        "updated_at": datetime.utcnow()
    }
    update_query = TaskDB.__table__.update().where(TaskDB.id == task_id).values(**update_data)
    await database.execute(update_query)

    completed_db_task = await database.fetch_one(existing_task_query)
    if not completed_db_task:
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to retrieve task after marking complete")

    response_task = TaskModel.model_validate(dict(completed_db_task))
    _cancel_task_jobs(task_id)
    print(f"TASK_MANAGER: Marked task '{response_task.id}' as completed in DB")
    return response_task
    --- END OF FILE CONTENT ---


Directory: data/
  File: sample_documents.json
    --- START OF FILE CONTENT ---
[
  {
    "id": "doc_sales_q1",
    "content": "2024年第一季度销售报告显示，华东区域销售额增长最快，达到46.5万元，主要得益于新产品A的推出。华北区域总销售额为36.5万元，但利润率有所下降。",
    "metadata": {"source": "Q1_Sales_Report.xlsx", "date": "2024-04-15", "type": "sales_report"}
  },
  {
    "id": "doc_hr_policy",
    "content": "公司最新人力资源政策规定，员工每年享有15天带薪年假，病假需提供医生证明。弹性工作制适用于部分岗位，需部门经理批准。",
    "metadata": {"source": "HR_Policy_2024.pdf", "date": "2024-01-01", "type": "HR_policy"}
  },
  {
    "id": "doc_it_security",
    "content": "IT安全指南强调，所有员工必须定期更换密码，并启用双因素认证。任何可疑邮件应立即报告给IT部门。",
    "metadata": {"source": "IT_Security_Guide.docx", "date": "2023-11-20", "type": "IT_guideline"}
  },
  {
    "id": "doc_meeting_notes",
    "content": "周例会纪要：讨论了项目X的进度，张三负责市场调研，李四负责技术开发。王五提出了关于预算优化的建议。下周将进行客户演示。",
    "metadata": {"source": "Weekly_Meeting_2024-05-10.txt", "date": "2024-05-10", "type": "meeting_notes"}
  },
  {
    "id": "doc_product_launch",
    "content": "新产品A的发布取得了巨大成功，市场反馈积极。预计将在下半年推出产品B，进一步扩大市场份额。",
    "metadata": {"source": "Product_Launch_Review.pptx", "date": "2024-05-01", "type": "product_review"}
  }
]
    --- END OF FILE CONTENT ---

  File: sample_kg.json
    --- START OF FILE CONTENT ---
{
  "schema_description": "这是一个关于公司内部实体和关系的简化知识图谱。\n节点类型包括：\n- Person (人): 属性例如 name (姓名, String类型, 唯一)。\n- Product (产品): 属性例如 name (产品名称, String类型, 唯一)。\n- Region (区域): 属性例如 name (区域名称, String类型, 唯一, 如 '华东', '华北')。\n- Project (项目): 属性例如 name (项目名称, String类型, 唯一)。\n- Document (文档): 属性例如 id (文档ID, String类型, 唯一), content (内容摘要, String类型)。\n- SalesAmount (销售额记录): 属性例如 numeric_amount (数字金额, Float类型), unit (单位, String类型, 如 '万元'), period (时期, String类型, 如 '2024年第一季度')。\n\n关系类型包括：\n- WORKS_ON (人参与项目): (Person)-[:WORKS_ON]->(Project)。\n- AUTHORED_BY (文档由人撰写): (Document)-[:AUTHORED_BY]->(Person)。\n- LOCATED_IN (实体位于区域): 例如 (Person)-[:LOCATED_IN]->(Region)。\n- HAS_SALES_PRODUCT (区域销售了产品): (Region)-[:HAS_SALES_PRODUCT]->(Product)。\n- HAS_SALES_AMOUNT (区域有销售额记录): (Region)-[:HAS_SALES_AMOUNT]->(SalesAmount)。\n- RELATED_TO (实体间相关): 例如 (Product)-[:RELATED_TO]->(Document)。",
  "data_summary": "知识图谱中包含以下关键事实：\n- 人物：张三、李四、王五。\n- 产品：新产品A、产品B。\n- 区域：华东、华北。\n- 项目：项目X。\n- 关系：张三参与项目X，李四参与项目X。华东区域有新产品A的销售额。新产品A的发布文档是doc_product_launch。华东区域2024年第一季度销售额为46.5万元，华北区域2024年第一季度销售额为36.5万元。",
  "facts": [
    {"subject": "张三", "relation": "WORKS_ON", "object": "项目X", "type": "person_project"},
    {"subject": "李四", "relation": "WORKS_ON", "object": "项目X", "type": "person_project"},
    {"subject": "王五", "relation": "PROPOSED_IDEA", "object": "预算优化", "type": "person_idea"}, 
    {"subject": "华东", "relation": "HAS_SALES_PRODUCT", "object": "新产品A", "type": "region_product"},
    {"subject": "新产品A", "relation": "RELATED_TO", "object": "doc_product_launch", "type": "product_document"},
    {"subject": "华东", "relation": "HAS_SALES_AMOUNT", "object": "46.5万元", "type": "region_sales_amount", "period": "2024年第一季度"},
    {"subject": "华北", "relation": "HAS_SALES_AMOUNT", "object": "36.5万元", "type": "region_sales_amount", "period": "2024年第一季度"}
  ]
}
    --- END OF FILE CONTENT ---

  File (skipped, extension not included): test.xlsx

Directory: data/raw_documents/
  File: doc1.txt
    --- START OF FILE CONTENT ---
项目Alpha的文档编写任务分配给了张三。张三在谷歌工作。
    --- END OF FILE CONTENT ---

  File: doc2.txt
    --- START OF FILE CONTENT ---
第二个文档讨论了自然语言处理技术。

    --- END OF FILE CONTENT ---


Directory: scripts/
  File: __init__.py
    --- File is empty ---

  File: test_kg_retriever_directly.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/scripts/manual_tests/test_kg_retriever_directly.py
import asyncio
import os
import sys
import logging

# --- 配置项目根目录到 sys.path ---
# 这使得我们可以像在项目根目录运行一样导入模块
# 例如 from zhz_rag.core_rag.kg_retriever import KGRetriever
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)
# --- 结束 sys.path 配置 ---

try:
    from zhz_rag.core_rag.kg_retriever import KGRetriever
    from zhz_rag.llm.llm_interface import generate_cypher_query # KGRetriever 依赖它
    from zhz_rag.config.constants import NEW_KG_SCHEMA_DESCRIPTION # generate_cypher_query 依赖它
except ImportError as e:
    print(f"Error importing necessary modules: {e}")
    print("Please ensure you are running this script from a context where 'zhz_rag' package is discoverable,")
    print(f"or that PYTHONPATH is set correctly. Current sys.path: {sys.path}")
    sys.exit(1)

# --- 日志配置 (与 KGRetriever 内部的日志级别和格式一致或更详细) ---
# KGRetriever 和 sglang_wrapper 内部都有自己的 logger
# 为了看到它们的输出，我们可以配置根 logger，或者确保它们的 logger 能输出到控制台
logging.basicConfig(
    level=logging.DEBUG, # 设置为 DEBUG 可以看到更多信息
    format='%(asctime)s - %(name)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)] # 输出到标准输出
)
logger = logging.getLogger(__name__)

async def main():
    logger.info("--- Starting KGRetriever Direct Test ---")

    # 确保 local_llm_service.py 正在运行 (端口 8088)
    # 确保 KuzuDB 数据库路径正确且包含数据

    # 从环境变量获取 KuzuDB 路径，如果 KGRetriever 内部也这样做的话
    # KGRetriever 默认使用 KUZU_DB_PATH_ENV，其默认值是 "/home/zhz/zhz_agent/zhz_rag/stored_data/kuzu_default_db"
    # 如果您的 .env 文件或环境变量中设置了不同的 KUZU_DB_PATH，请确保这里也一致，或者让 KGRetriever 使用其默认值。
    kuzu_db_path_for_test = os.getenv("KUZU_DB_PATH", "/home/zhz/zhz_agent/zhz_rag/stored_data/kuzu_default_db")
    logger.info(f"Using KuzuDB path for test: {kuzu_db_path_for_test}")


    # 实例化 KGRetriever
    # 它会使用 sglang_wrapper.generate_cypher_query 作为默认的 Cypher 生成函数
    # generate_cypher_query 会调用 local_llm_service.py
    try:
        kg_retriever = KGRetriever(db_path=kuzu_db_path_for_test)
        logger.info("KGRetriever instance created.")
    except Exception as e_init:
        logger.error(f"Failed to initialize KGRetriever: {e_init}", exc_info=True)
        return

    test_queries = [
        {"query": "张三在哪里工作？", "top_k": 2},
        {"query": "项目Alpha的文档编写任务分配给了谁？", "top_k": 2},
        {"query": "法国的首都是哪里？", "top_k": 2} # 测试无法生成 Cypher 的情况
    ]

    for test_case in test_queries:
        user_query = test_case["query"]
        top_k_results = test_case["top_k"]
        logger.info(f"\n--- Testing query: '{user_query}' with top_k={top_k_results} ---")
        
        try:
            retrieved_kg_docs = await kg_retriever.retrieve_with_llm_cypher(
                query=user_query,
                top_k=top_k_results
            )

            if retrieved_kg_docs:
                logger.info(f"Retrieved {len(retrieved_kg_docs)} documents from KG for query '{user_query}':")
                for i, doc in enumerate(retrieved_kg_docs):
                    logger.info(f"  Doc {i+1}:")
                    logger.info(f"    Source Type: {doc.get('source_type')}")
                    logger.info(f"    Content: {doc.get('content')}")
                    logger.info(f"    Score: {doc.get('score')}")
                    logger.info(f"    Metadata: {doc.get('metadata')}")
            else:
                logger.info(f"No documents retrieved from KG for query '{user_query}'. This might be expected if Cypher was '无法生成'.")

        except Exception as e_retrieve:
            logger.error(f"Error during KG retrieval for query '{user_query}': {e_retrieve}", exc_info=True)
        
        logger.info("--- End of test case ---")

    # 关闭 KGRetriever (如果它有 close 方法且需要显式关闭)
    # KGRetriever 的 close 方法会删除 _db 对象，依赖其 __del__
    if hasattr(kg_retriever, 'close'):
        kg_retriever.close()
        logger.info("KGRetriever closed.")

if __name__ == "__main__":
    # 确保 local_llm_service.py 在端口 8088 上运行
    # 确保 KuzuDB 数据库路径正确且包含数据
    if not os.getenv("SGLANG_API_URL"): # sglang_wrapper.py 会使用这个环境变量
        os.environ["SGLANG_API_URL"] = "http://localhost:8088/v1/chat/completions" # 指向我们的本地服务
        logger.info(f"SGLANG_API_URL not set, defaulting to: {os.environ['SGLANG_API_URL']}")

    asyncio.run(main())
    logger.info("--- KGRetriever Direct Test Finished ---")
    --- END OF FILE CONTENT ---


Directory: scripts/manual_tests/
  File: __init__.py
    --- File is empty ---

  File: minimal_kuzu_param_test.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/scripts/manual_tests/minimal_kuzu_param_test.py
import kuzu
import os
import shutil

DB_PATH = "./test_kuzu_param_db" # 使用一个全新的临时数据库

def main():
    print(f"KuzuDB Python version: {kuzu.__version__}")

    if os.path.exists(DB_PATH):
        shutil.rmtree(DB_PATH)
        print(f"Removed existing test DB at {DB_PATH}")

    db = None
    conn = None
    try:
        print(f"Creating new test DB at {DB_PATH}...")
        db = kuzu.Database(DB_PATH)
        conn = kuzu.Connection(db)
        print("Test DB and connection created.")

        print("\n--- Creating Node Table ---")
        conn.execute("CREATE NODE TABLE User(name STRING, age INT64, PRIMARY KEY (name))")
        print("Node table 'User' created.")

        print("\n--- Testing Parameterized Insert via PreparedStatement ---")
        insert_query_cypher = "CREATE (u:User {name: $input_name, age: $input_age})"
        params_to_insert = {"input_name": "Alice", "input_age": 30}
        
        try:
            print(f"Preparing statement: {insert_query_cypher}")
            prepared_insert = conn.prepare(insert_query_cypher)
            print(f"Executing prepared statement with params: {params_to_insert}")
            # 直接传递参数字典
            conn.execute(prepared_insert, params_to_insert) 
            print("Parameterized insert successful.")
        except Exception as e_insert:
            print(f"ERROR during parameterized insert: {e_insert}")
            import traceback
            traceback.print_exc()
            return # 如果插入失败，后续查询无意义

        print("\n--- Verifying Inserted Data ---")
        select_query_cypher = "MATCH (u:User {name: $find_name}) RETURN u.name, u.age"
        params_to_select = {"find_name": "Alice"}
        try:
            print(f"Preparing statement: {select_query_cypher}")
            prepared_select = conn.prepare(select_query_cypher)
            print(f"Executing prepared statement with params: {params_to_select}")
            query_result = conn.execute(prepared_select, params_to_select)
            
            if query_result.has_next():
                row = query_result.get_next()
                print(f"Query Result: Name={row}, Age={row}") # Kuzu返回的是元组
                if row == "Alice" and row == 30:
                    print("Data verification successful!")
                else:
                    print(f"Data verification FAILED. Expected ('Alice', 30), got {row}")
            else:
                print("Data verification FAILED: No data returned.")
            query_result.close()
        except Exception as e_select:
            print(f"ERROR during data verification: {e_select}")
            import traceback
            traceback.print_exc()

    except Exception as e:
        print(f"An overall error occurred: {e}")
        import traceback
        traceback.print_exc()
    finally:
        print("\n--- Test Finished ---")
        if os.path.exists(DB_PATH):
            print(f"Cleaning up test DB at {DB_PATH}")
            # 在finally块中确保db和conn对象存在才尝试del
            if conn is not None:
                del conn # KuzuDB的Connection没有close()
            if db is not None:
                del db   # 依赖析构函数关闭数据库
            # shutil.rmtree(DB_PATH) # 暂时注释掉，方便查看数据库文件

if __name__ == "__main__":
    main()

    --- END OF FILE CONTENT ---

  File: test_kg_retriever_new_strategy.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/scripts/manual_tests/test_kg_retriever_new_strategy.py
import asyncio
import os
import sys
import logging
import json # 确保导入
from typing import List, Dict, Any, Optional, Callable, Iterator
from sentence_transformers import SentenceTransformer

# --- 配置项目根目录到 sys.path ---
PROJECT_ROOT = os.path.abspath(os.path.join(os.path.dirname(__file__), "..", ".."))
if PROJECT_ROOT not in sys.path:
    sys.path.insert(0, PROJECT_ROOT)
# --- 结束 sys.path 配置 ---

try:
    from zhz_rag.core_rag.kg_retriever import KGRetriever
    from dotenv import load_dotenv
except ImportError as e:
    print(f"Error importing necessary modules: {e}")
    print("Please ensure 'zhz_rag' package is discoverable and all dependencies are installed.")
    sys.exit(1)

# 加载环境变量
load_dotenv(os.path.join(PROJECT_ROOT, '.env'))

# --- 日志配置 ---
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s - %(name)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)
logger = logging.getLogger(__name__)

# --- 测试用例定义 ---
test_cases = [
    {
        "name": "TC1: 查询实体属性 (张三)",
        "user_query": "我想知道张三的详细信息。",
        "top_k": 2,
        "expected_keywords_in_content": ["张三", "PERSON"]
    },
    {
        "name": "TC2: 查询实体属性 (项目Alpha的文档编写任务)",
        "user_query": "项目Alpha的文档编写任务的具体内容是什么？",
        "top_k": 2,
        "expected_keywords_in_content": ["项目alpha的文档编写任务", "TASK"] # 假设LLM会小写化
    },
    {
        "name": "TC3: 查询关系 (张三在哪里工作)",
        "user_query": "张三在哪个公司工作？",
        "top_k": 3,
        "expected_keywords_in_content": ["张三", "谷歌", "WORKS_AT"] # 假设图谱中有张三在谷歌工作的数据
    },
    {
        "name": "TC4: 查询关系 (项目Alpha分配给谁)",
        "user_query": "项目Alpha的文档编写任务是分配给谁的？",
        "top_k": 3,
        "expected_keywords_in_content": ["项目alpha的文档编写任务", "张三", "ASSIGNED_TO"] # 假设分配给了张三
    },
    {
        "name": "TC5: 模糊查询，LLM可能提取不出精确实体用于模板",
        "user_query": "关于AI在项目管理中的应用有哪些讨论？",
        "top_k": 3,
        "expected_keywords_in_content": [] # 期望返回空，因为LLM可能无法提取精确实体给模板
                                          # 如果实现了向量检索，这里可能会有结果
    },
    {
        "name": "TC6: 与图谱无关的问题",
        "user_query": "马德里的天气怎么样？",
        "top_k": 3,
        "expected_keywords_in_content": [] # 期望返回空结果
    },
    {
        "name": "TC7: 查询实体属性 (谷歌公司)",
        "user_query": "谷歌公司的信息？",
        "top_k": 2,
        "expected_keywords_in_content": ["谷歌", "ORGANIZATION"] # 期望LLM能识别"谷歌公司"为"谷歌"且类型为ORGANIZATION
    }
]

async def run_tests():
    logger.info("--- Starting New KGRetriever Strategy Tests ---")
    if not os.getenv("SGLANG_API_URL"):
        os.environ["SGLANG_API_URL"] = "http://localhost:8088/v1/chat/completions"
        logger.info(f"SGLANG_API_URL not set, defaulting to: {os.environ['SGLANG_API_URL']}")

    kg_retriever: Optional[KGRetriever] = None
    embedder_for_test: Optional[SentenceTransformer] = None # 新增
    successful_tests = 0
    failed_tests = 0
    
    try:
        # --- 新增：初始化嵌入器 ---
        embedding_model_path = os.getenv("EMBEDDING_MODEL_PATH", "/home/zhz/models/bge-small-zh-v1.5") # 从环境变量或默认值获取
        if os.path.exists(embedding_model_path):
            embedder_for_test = SentenceTransformer(embedding_model_path)
            logger.info(f"Test script: SentenceTransformer embedder initialized from {embedding_model_path}.")
        else:
            logger.error(f"Test script: Embedding model path '{embedding_model_path}' not found. KGRetriever vector search will fail.")
        # --- 结束新增 ---

        # --- 修改：传递嵌入器给 KGRetriever ---
        kg_retriever = KGRetriever(embedder=embedder_for_test) # <--- 修改这里
        logger.info("KGRetriever instance created with embedder.")

        for i, case in enumerate(test_cases):
            logger.info(f"\n--- Test Case {i+1}: {case['name']} ---")
            logger.info(f"User Query: {case['user_query']}")
            logger.info(f"Expected Keywords in Content: {case['expected_keywords_in_content']}")
            
            retrieved_docs = await kg_retriever.retrieve(
                user_query=case['user_query'],
                top_k=case['top_k']
            )
            
            logger.info(f"\nKGRetriever returned {len(retrieved_docs)} documents:")
            
            case_passed = False
            if not retrieved_docs and not case["expected_keywords_in_content"]:
                logger.info("  RESULT: PASSED (Correctly returned no documents)")
                case_passed = True
            elif retrieved_docs and not case["expected_keywords_in_content"]: # 期望空，但返回了内容
                logger.error("  RESULT: FAILED (Expected no documents, but got some)")
            elif not retrieved_docs and case["expected_keywords_in_content"]: # 期望有内容，但返回空
                logger.error("  RESULT: FAILED (Expected documents with keywords, but got no documents)")
            else: # 期望有内容，也返回了内容，检查关键词
                found_all_keywords_in_at_least_one_doc = False
                if retrieved_docs: # 确保列表不为空
                    # 检查是否至少有一个文档包含了所有期望的关键词
                    # （更宽松的检查，因为一个查询可能返回多个相关片段，不一定每个片段都包含所有关键词）
                    for doc in retrieved_docs:
                        doc_content_lower = str(doc.get('content', "")).lower()
                        current_doc_has_all = True
                        for keyword in case["expected_keywords_in_content"]:
                            if keyword.lower() not in doc_content_lower:
                                current_doc_has_all = False
                                break
                        if current_doc_has_all:
                            found_all_keywords_in_at_least_one_doc = True
                            break 
                    
                    if found_all_keywords_in_at_least_one_doc:
                        logger.info("  RESULT: PASSED (At least one retrieved document contains all expected keywords)")
                        case_passed = True
                    else:
                        logger.error("  RESULT: FAILED (No single retrieved document contains all expected keywords)")
                else: # retrieved_docs 为空，但期望有关键词，上面已经处理
                    pass

            if case_passed:
                successful_tests +=1
            else:
                failed_tests +=1

            # 打印检索到的文档详情（无论通过与否）
            if retrieved_docs:
                for doc_idx, doc in enumerate(retrieved_docs):
                    logger.info(f"  Document {doc_idx + 1}:")
                    logger.info(f"    Source Type: {doc.get('source_type')}")
                    logger.info(f"    Score: {doc.get('score')}")
                    logger.info(f"    Content: {doc.get('content')}")
                    logger.info(f"    Metadata: {json.dumps(doc.get('metadata', {}), ensure_ascii=False, indent=2)}")
            
            logger.info("--- End of Test Case ---")

    except Exception as e_main:
        logger.error(f"Error during KGRetriever test execution: {e_main}", exc_info=True)
    finally:
        if kg_retriever and hasattr(kg_retriever, 'close'):
            kg_retriever.close()
            logger.info("KGRetriever closed.")
        
        logger.info(f"\n--- KGRetriever Test Summary ---")
        logger.info(f"Total tests: {len(test_cases)}")
        logger.info(f"Passed: {successful_tests}")
        logger.info(f"Failed: {failed_tests}")
        logger.info("--- All New KGRetriever Strategy Tests Finished ---")

if __name__ == "__main__":
    asyncio.run(run_tests())
    --- END OF FILE CONTENT ---


Directory: scripts/manual_tests/test_kuzu_param_db/
  File (skipped, extension not included): .lock
  File (skipped, extension not included): .shadow
  File (skipped, extension not included): .wal
  File (skipped, extension not included): catalog.kz
  File (skipped, extension not included): data.kz
  File (skipped, extension not included): metadata.kz
  File (skipped, extension not included): n-0.hindex
  File (skipped, extension not included): n-0.hindex.ovf

Directory: utils/
  File: __init__.py
    --- File is empty ---

  File: common_utils.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/utils/common_utils.py

import httpx
import json
import traceback
import os
import glob
from dotenv import load_dotenv
from datetime import datetime, timezone
import uuid
import logging
import asyncio
from typing import List, Dict, Any, Optional
import re
import unicodedata

load_dotenv()

# --- Logger Configuration ---
# 使用一个统一的 logger 名称，方便管理
# 您可以根据需要在调用此模块的顶层配置这个 logger
# 例如，在 agent_orchestrator_service.py 的开头配置 "ZhzAgentUtils"
utils_logger = logging.getLogger("ZhzAgentUtils")
if not utils_logger.handlers: # 避免重复添加处理器
    utils_logger.setLevel(os.getenv("UTILS_LOG_LEVEL", "INFO").upper())
    _utils_console_handler = logging.StreamHandler()
    _utils_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
    _utils_console_handler.setFormatter(_utils_formatter)
    utils_logger.addHandler(_utils_console_handler)
    utils_logger.propagate = False # 通常在自定义logger中设置为False
utils_logger.info("--- ZhzAgentUtils Logger configured ---")


# --- MCP Configuration ---
MCPO_BASE_URL = os.getenv("MCPO_BASE_URL", "http://localhost:8006")

# --- Directory Paths (统一管理，基于此文件位置推断项目结构) ---
_CURRENT_FILE_DIR = os.path.dirname(os.path.abspath(__file__))
# 假设 common_utils.py 在 zhz_agent/utils/ 或 zhz_agent/zhz_rag/utils/
# 我们需要项目根目录 zhz_agent/
_PROJECT_ROOT_GUESS_1 = os.path.abspath(os.path.join(_CURRENT_FILE_DIR, "..")) # 如果在 utils/ 下
_PROJECT_ROOT_GUESS_2 = os.path.abspath(os.path.join(_CURRENT_FILE_DIR, "..", "..")) # 如果在 zhz_rag/utils/ 下

# 尝试确定正确的项目根目录
if os.path.basename(_PROJECT_ROOT_GUESS_1) == "zhz_agent":
    PROJECT_ROOT_DIR = _PROJECT_ROOT_GUESS_1
elif os.path.basename(_PROJECT_ROOT_GUESS_2) == "zhz_agent":
    PROJECT_ROOT_DIR = _PROJECT_ROOT_GUESS_2
else:
    # 如果都猜不到，就用一个相对路径或者发出警告
    utils_logger.warning(f"Could not reliably determine PROJECT_ROOT_DIR from common_utils.py location. Using current dir as fallback for relative paths: {_CURRENT_FILE_DIR}")
    PROJECT_ROOT_DIR = _CURRENT_FILE_DIR # 或者您的固定路径

STORED_DATA_ROOT_DIR = os.path.join(PROJECT_ROOT_DIR, 'zhz_rag', 'stored_data')
RAG_INTERACTION_LOGS_DIR = os.path.join(STORED_DATA_ROOT_DIR, 'rag_interaction_logs')
EVALUATION_RESULTS_LOGS_DIR = os.path.join(STORED_DATA_ROOT_DIR, 'evaluation_results_logs')
FINETUNING_GENERATED_DATA_DIR = os.path.join(PROJECT_ROOT_DIR, 'zhz_rag', 'finetuning', 'generated_data')

_DIRECTORIES_TO_CREATE = [
    STORED_DATA_ROOT_DIR, # 确保父目录也创建
    RAG_INTERACTION_LOGS_DIR,
    EVALUATION_RESULTS_LOGS_DIR,
    FINETUNING_GENERATED_DATA_DIR
]
for dir_path in _DIRECTORIES_TO_CREATE:
    if not os.path.exists(dir_path):
        try:
            os.makedirs(dir_path, exist_ok=True)
            utils_logger.info(f"Successfully created directory: {dir_path}")
        except Exception as e:
            utils_logger.error(f"Error creating directory {dir_path}: {e}. Consider creating it manually.")

# --- Log File Path Getters ---
def get_interaction_log_filepath() -> str:
    today_str = datetime.now(timezone.utc).strftime("%Y%m%d")
    return os.path.join(RAG_INTERACTION_LOGS_DIR, f"rag_interactions_{today_str}.jsonl")

def get_evaluation_result_log_filepath(evaluation_name: str) -> str:
    today_str = datetime.now(timezone.utc).strftime("%Y%m%d")
    return os.path.join(EVALUATION_RESULTS_LOGS_DIR, f"eval_results_{evaluation_name}_{today_str}.jsonl")

def find_latest_rag_interaction_log(log_dir: str = RAG_INTERACTION_LOGS_DIR) -> Optional[str]:
    utils_logger.debug(f"Searching for RAG interaction logs in: {log_dir}")
    rag_log_pattern = os.path.join(log_dir, "rag_interactions_*.jsonl")
    candidate_rag_logs = glob.glob(rag_log_pattern)
    if candidate_rag_logs:
        candidate_rag_logs.sort(key=os.path.getmtime, reverse=True)
        utils_logger.info(f"Automatically selected RAG interaction log: {candidate_rag_logs[0]}")
        return candidate_rag_logs[0]
    else:
        utils_logger.warning(f"No RAG interaction log files found matching pattern: {rag_log_pattern} in directory {log_dir}")
        return None

# --- Logging Function ---
async def log_interaction_data(
    interaction_data: Dict[str, Any],
    is_evaluation_result: bool = False,
    evaluation_name_for_file: Optional[str] = None
):
    if is_evaluation_result:
        if not evaluation_name_for_file:
            evaluation_name_for_file = interaction_data.get("task_type", "general_eval_result")
        filepath = get_evaluation_result_log_filepath(evaluation_name=evaluation_name_for_file)
    else:
        filepath = get_interaction_log_filepath()

    if "timestamp_utc" not in interaction_data:
        interaction_data["timestamp_utc"] = datetime.now(timezone.utc).isoformat()
    if "interaction_id" not in interaction_data and not is_evaluation_result:
        interaction_data["interaction_id"] = str(uuid.uuid4())
    elif "interaction_id" not in interaction_data and is_evaluation_result and "original_interaction_id_ref" in interaction_data:
         interaction_data["interaction_id"] = str(uuid.uuid4())

    try:
        def _write_sync():
            log_file_dir = os.path.dirname(filepath)
            if not os.path.exists(log_file_dir):
                try:
                    os.makedirs(log_file_dir, exist_ok=True)
                    utils_logger.info(f"Created directory for log file: {log_file_dir}")
                except Exception as e_mkdir:
                    utils_logger.error(f"Error creating directory {log_file_dir} for log file: {e_mkdir}")
                    return
            with open(filepath, 'a', encoding='utf-8') as f:
                json_string_to_write = json.dumps(interaction_data, ensure_ascii=False, default=str)
                f.write(json_string_to_write + "\n")
        await asyncio.to_thread(_write_sync)
    except Exception as e:
        utils_logger.error(f"Failed to log interaction data to {filepath}: {e}", exc_info=True)

# --- MCP Tool Calling Utility ---
async def call_mcpo_tool(tool_name_with_prefix: str, payload: Dict[str, Any]) -> Dict[str, Any]:
    api_url = f"{MCPO_BASE_URL}/{tool_name_with_prefix}"
    cleaned_payload = {k: v for k, v in (payload or {}).items() if v is not None}

    utils_logger.info(f"CALL_MCPO_TOOL: Attempting to call {api_url}")
    utils_logger.debug(f"CALL_MCPO_TOOL: Payload: {json.dumps(cleaned_payload, ensure_ascii=False)}")

    timeout_config = httpx.Timeout(120.0, connect=10.0, read=120.0, write=10.0)
    
    async with httpx.AsyncClient(timeout=timeout_config) as client:
        response: Optional[httpx.Response] = None
        try:
            headers = {
                "Content-Type": "application/json",
                "Accept": "application/json",
                "User-Agent": "ZhzAgent/1.0 (call_mcpo_tool)"
            }
            utils_logger.debug(f"CALL_MCPO_TOOL: Sending POST request to {api_url} with headers: {headers}")
            response = await client.post(api_url, json=cleaned_payload, headers=headers)
            
            utils_logger.info(f"CALL_MCPO_TOOL: Response from {api_url} - Status: {response.status_code}")
            utils_logger.debug(f"CALL_MCPO_TOOL: Response Headers: {response.headers}")
            
            try:
                response_text_snippet = response.text[:500]
                utils_logger.debug(f"CALL_MCPO_TOOL: Response Text Snippet (first 500 chars): {response_text_snippet}")
            except Exception as e_read_snippet:
                utils_logger.warning(f"CALL_MCPO_TOOL: Could not read response text snippet: {e_read_snippet}")

            if response.status_code == 200:
                try:
                    result_data = response.json()
                    utils_logger.info(f"CALL_MCPO_TOOL: Successfully received and parsed JSON response from {api_url}.")
                    
                    if isinstance(result_data, dict) and result_data.get("isError"):
                        error_content_list = result_data.get("content", [{"type": "text", "text": "Unknown error from MCP tool"}])
                        error_text_from_mcp_payload = "Unknown error from MCP tool"
                        for item in error_content_list:
                            if item.get("type") == "text":
                                error_text_from_mcp_payload = item.get("text", error_text_from_mcp_payload)
                                break
                        utils_logger.error(f"CALL_MCPO_TOOL: MCP Tool '{tool_name_with_prefix}' reported an application-level error (isError=true): {error_text_from_mcp_payload}")
                        return {
                            "success": False,
                            "error": f"MCP tool '{tool_name_with_prefix}' reported failure: {error_text_from_mcp_payload}",
                            "error_type": "MCP_APPLICATION_ERROR",
                            "status_code": response.status_code
                        }
                    return {
                            "success": True, 
                            "data": result_data 
                    }
                except json.JSONDecodeError:
                    utils_logger.warning(f"CALL_MCPO_TOOL: Response from {api_url} was 200 OK but not valid JSON. Assuming plain text response. Raw text: {response.text[:200]}...")
                    return {
                        "success": True,
                        "data": { 
                            "content": [{"type": "text", "text": response.text}]
                        } 
                    }
            else:
                error_message = f"HTTP Error {response.status_code} when calling {api_url}."
                utils_logger.error(f"CALL_MCPO_TOOL: {error_message} Response: {response.text[:500]}", exc_info=False)
                error_detail_from_response = response.text
                try:
                    parsed_error_json = response.json()
                    if isinstance(parsed_error_json, dict) and "detail" in parsed_error_json:
                        error_detail_from_response = parsed_error_json["detail"]
                    elif isinstance(parsed_error_json, dict) and "error" in parsed_error_json: 
                        error_detail_from_response = parsed_error_json["error"]
                except json.JSONDecodeError:
                    pass 
                return {
                    "success": False,
                    "error": f"HTTP error from MCP service: {error_detail_from_response}",
                    "error_type": "HTTP_STATUS_ERROR",
                    "status_code": response.status_code,
                    "raw_response_snippet": response.text[:500] if response else "N/A"
                }
        except httpx.TimeoutException as exc_timeout:
            utils_logger.error(f"CALL_MCPO_TOOL: Timeout when calling {api_url}. Error: {exc_timeout}", exc_info=True)
            return {
                "success": False,
                "error": f"Request to MCP service timed out after {timeout_config.read if timeout_config else 'default'}s.",
                "error_type": "TIMEOUT_ERROR",
                "status_code": None 
            }
        except httpx.ConnectError as exc_connect:
            utils_logger.error(f"CALL_MCPO_TOOL: Connection error when calling {api_url}. Is the MCP service running at {MCPO_BASE_URL}? Error: {exc_connect}", exc_info=True)
            return {
                "success": False,
                "error": f"Could not connect to MCP service at {MCPO_BASE_URL}.",
                "error_type": "CONNECTION_ERROR",
                "status_code": None
            }
        except httpx.RequestError as exc_request_other: 
            utils_logger.error(f"CALL_MCPO_TOOL: Network request error when calling {api_url}. Error: {exc_request_other}", exc_info=True)
            return {
                "success": False,
                "error": f"A network request error occurred: {str(exc_request_other)}",
                "error_type": type(exc_request_other).__name__,
                "status_code": None
            }
        except Exception as exc_unexpected:
            utils_logger.error(f"CALL_MCPO_TOOL: Unexpected error when calling {api_url}. Error: {exc_unexpected}", exc_info=True)
            return {
                "success": False,
                "error": f"An unexpected error occurred during MCP call: {str(exc_unexpected)}",
                "error_type": type(exc_unexpected).__name__,
                "status_code": response.status_code if response else None, 
                "traceback": traceback.format_exc() 
            }

# --- JSONL File Loading Utility ---
def load_jsonl_file(filepath: str, encoding: str = 'utf-8') -> List[Dict[str, Any]]:
    data_list: List[Dict[str, Any]] = []
    if not os.path.exists(filepath):
        utils_logger.error(f"File not found: {filepath}")
        return data_list
    try:
        with open(filepath, 'r', encoding=encoding) as f:
            for line_number, line in enumerate(f, 1):
                try:
                    if line.strip():
                        data_list.append(json.loads(line.strip()))
                except json.JSONDecodeError as e_json:
                    utils_logger.warning(f"Skipping malformed JSON line {line_number} in {filepath}: {e_json}")
                except Exception as e_line:
                    utils_logger.warning(f"Error processing line {line_number} in {filepath}: {e_line}")
    except FileNotFoundError:
        utils_logger.error(f"File not found during open: {filepath}")
    except Exception as e_file:
        utils_logger.error(f"Error reading or processing file {filepath}: {e_file}", exc_info=True)
        return []
    utils_logger.info(f"Successfully loaded {len(data_list)} entries from {filepath}")
    return data_list

# --- Text Normalization Utility ---
def normalize_text_for_id(text: str) -> str:
    if not isinstance(text, str):
        return str(text)
    try:
        normalized_text = unicodedata.normalize('NFKD', text)
        normalized_text = normalized_text.lower()
        normalized_text = normalized_text.strip()
        normalized_text = re.sub(r'\s+', ' ', normalized_text) # Collapse multiple whitespaces
        return normalized_text
    except Exception: # pylint: disable=broad-except
        # Fallback to original text if normalization fails for any reason
        return text

# --- DB Utils (Moved from zhz_rag.utils.db_utils to here for consolidation if needed) ---
# from databases import Database # This would require 'databases' and 'aiosqlite' in requirements
# from sqlalchemy import create_engine
# from sqlalchemy.orm import declarative_base
# import pytz
# from apscheduler.schedulers.asyncio import AsyncIOScheduler
# from apscheduler.jobstores.sqlalchemy import SQLAlchemyJobStore

# ZHZ_AGENT_DIR_FOR_DB = PROJECT_ROOT_DIR # Use the determined project root
# DB_NAME = "ZHZ_AGENT_tasks.db" # Or your desired DB name
# DATABASE_FILE_PATH = os.path.join(ZHZ_AGENT_DIR_FOR_DB, DB_NAME) # Store DB in project root
# DATABASE_URL = f"sqlite+aiosqlite:///{DATABASE_FILE_PATH}"

# database_instance: Optional[Database] = None
# sqlalchemy_engine_instance = None
# SQLAlchemyBase = declarative_base()
# scheduler_instance: Optional[AsyncIOScheduler] = None

# def get_database() -> Database:
#     global database_instance
#     if database_instance is None:
#         database_instance = Database(DATABASE_URL)
#     return database_instance

# def get_sqlalchemy_engine():
#     global sqlalchemy_engine_instance
#     if sqlalchemy_engine_instance is None:
#         sqlalchemy_engine_instance = create_engine(DATABASE_URL.replace("+aiosqlite", ""))
#     return sqlalchemy_engine_instance

# def get_scheduler() -> AsyncIOScheduler:
#     global scheduler_instance
#     if scheduler_instance is None:
#         jobstore_url = f"sqlite:///{DATABASE_FILE_PATH}" # Scheduler uses its own tables in the same DB
#         jobstores = {
#             'default': SQLAlchemyJobStore(url=jobstore_url, tablename='apscheduler_jobs_v2_common_utils')
#         }
#         scheduler_instance = AsyncIOScheduler(jobstores=jobstores, timezone=pytz.utc)
#         logging.getLogger('apscheduler').setLevel(os.getenv("SCHEDULER_LOG_LEVEL", "WARNING").upper())
#         utils_logger.info(f"APScheduler (common_utils) initialized with timezone: {pytz.utc}")
#     return scheduler_instance

# async def connect_database():
#     db_to_connect = get_database()
#     if not db_to_connect.is_connected:
#         await db_to_connect.connect()
#         utils_logger.info("Database connected (common_utils).")

# async def disconnect_database():
#     db_to_disconnect = get_database()
#     if db_to_disconnect.is_connected:
#         await db_to_disconnect.disconnect()
#         utils_logger.info("Database disconnected (common_utils).")

# def create_db_tables():
#     engine = get_sqlalchemy_engine()
#     SQLAlchemyBase.metadata.create_all(bind=engine)
#     utils_logger.info("SQLAlchemy tables created via common_utils (if not exist).")

# def start_scheduler():
#     sched = get_scheduler()
#     if not sched.running:
#         sched.start()
#         utils_logger.info("APScheduler started via common_utils.")

# def shutdown_scheduler():
#     sched = get_scheduler()
#     if sched.running:
#         sched.shutdown()
#         utils_logger.info("APScheduler shutdown via common_utils.")

# (如果决定将数据库和调度器逻辑也合并到这里，请取消注释上面的DB相关部分，
# 并确保相关的依赖（databases, sqlalchemy, apscheduler, pytz）已添加到项目的 requirements.txt)
    --- END OF FILE CONTENT ---


Directory: kuzu_temp/
  File (skipped, extension not included): libvector.kuzu_extension

Directory: stored_data/

Directory: stored_data/rag_interaction_logs/

Directory: local_agent/
  File: local_agent_app.py
    --- START OF FILE CONTENT ---
# local_agent_app.py
import sys
import os
import logging
import traceback
from typing import Dict, Any, List, Optional, Union
from xml.etree import ElementTree

# --- Path and DLL setup ---
print("--- sys.path at the very beginning of local_agent_app.py ---")
for p in sys.path:
    print(p)
print(f"--- os.getcwd() at the very beginning: {os.getcwd()} ---")
print("-----------------------------------------------------------")

venv_path = os.path.dirname(sys.executable)
scripts_path = os.path.join(venv_path)
dlls_path = os.path.join(venv_path, "..", "DLLs")

if os.path.exists(scripts_path) and hasattr(os, 'add_dll_directory'):
    try:
        os.add_dll_directory(scripts_path)
        print(f"INFO: Added to DLL search path: {scripts_path}")
    except Exception as e_dll_add:
        print(f"WARN: Could not add {scripts_path} to DLL search path: {e_dll_add}")

if os.path.exists(dlls_path) and hasattr(os, 'add_dll_directory'):
    try:
        os.add_dll_directory(dlls_path)
        print(f"INFO: Added to DLL search path: {dlls_path}")
    except Exception as e_dll_add:
        print(f"WARN: Could not add {dlls_path} to DLL search path: {e_dll_add}")

# --- Library Imports and Global Status ---
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
from winotify import Notification

# Global variables to store import status and library objects
_PANDAS_IMPORTED = False
_PANDASQL_IMPORTED = False
pd = None
PandaSQL = None

try:
    import pandas as pd_temp
    pd = pd_temp
    logging.info(f"Successfully imported pandas. pd is: {pd}")
    logging.info(f"pandas version: {pd.__version__}")
    _PANDAS_IMPORTED = True
except ImportError as e_pandas_global:
    logging.critical(f"CRITICAL_IMPORT_ERROR: Failed to import pandas at global scope: {e_pandas_global}", exc_info=True)
except Exception as e_pandas_other_global:
    logging.critical(f"CRITICAL_OTHER_ERROR: Error during pandas import or version check: {e_pandas_other_global}", exc_info=True)

try:
    from pandasql import PandaSQL as PandaSQL_temp
    PandaSQL = PandaSQL_temp
    logging.info(f"Successfully imported PandaSQL. PandaSQL is: {PandaSQL}")
    _PANDASQL_IMPORTED = True
except ImportError as e_pandasql_global:
    logging.warning(f"IMPORT_WARNING: Failed to import pandasql at global scope: {e_pandasql_global}")
except Exception as e_pandasql_other_global:
    logging.warning(f"OTHER_IMPORT_WARNING: Error during pandasql import: {e_pandasql_other_global}")

# --- Configuration ---
LOCAL_AGENT_PORT = 8003
LOG_LEVEL = logging.INFO
logging.basicConfig(level=LOG_LEVEL, format='%(asctime)s - %(levelname)s - LocalAgent - %(message)s')
logger = logging.getLogger(__name__)

app = FastAPI(
    title="ZhzAI Local Agent",
    description="本地代理程序，提供处理本地文件和执行特定任务的MCP服务。",
    version="0.1.1"
)

# --- Pydantic Model Definitions ---
class ExecuteSQORequest(BaseModel):
    sqo: Dict[str, Any] = Field(description="必需。一个结构化查询对象 (SQO) 的JSON字典...")

class SQOResponse(BaseModel):
    success: bool
    result: Any = None
    error: Optional[str] = None
    error_details: Optional[str] = None

class NotificationRequest(BaseModel):
    title: str = Field(default="任务提醒", description="通知的标题")
    message: str = Field(description="通知的主要内容")
    app_name: str = Field(default="终端大脑助手", description="显示在通知来源的应用名称")

class NotificationResponse(BaseModel):
    success: bool
    message: str

# --- Filter Helper Function ---
def apply_filters_to_dataframe(df: 'pd.DataFrame', filters: List[Dict[str, Any]]) -> 'pd.DataFrame':
    if not filters:
        return df
    df_filtered = df.copy()
    for f_obj in filters:
        column = f_obj.get("column")
        operator = f_obj.get("operator")
        value = f_obj.get("value")
        is_date = f_obj.get("is_date", False)

        if not all([column, operator, value is not None]):
            logger.warning(f"Skipping invalid filter object: {f_obj}")
            continue

        if column not in df_filtered.columns:
            logger.warning(f"Filter column '{column}' not found in DataFrame. Available: {df_filtered.columns.tolist()}. Skipping filter: {f_obj}")
            continue

        try:
            series_to_filter = df_filtered[column]
            if is_date:
                try:
                    series_to_filter = pd.to_datetime(series_to_filter, errors='coerce')
                    filter_value_dt = pd.to_datetime(value, errors='coerce')
                    if pd.isna(filter_value_dt) and not (isinstance(value, list) and operator in ["in", "is_in_list", "not in", "is_not_in_list"]):
                        logger.warning(f"Cannot convert filter value '{value}' to date for column '{column}'. Skipping filter.")
                        continue
                    value = filter_value_dt
                except Exception as e_date:
                    logger.warning(f"Error converting column '{column}' or value '{value}' to datetime: {e_date}. Skipping filter.")
                    continue

            condition = None
            if operator in ["equals", "=="]: condition = (series_to_filter == value)
            elif operator in ["not_equals", "!="]: condition = (series_to_filter != value)
            elif operator in ["greater_than", ">"]: condition = (series_to_filter > value)
            elif operator in ["greater_than_or_equals", ">="]: condition = (series_to_filter >= value)
            elif operator in ["less_than", "<"]: condition = (series_to_filter < value)
            elif operator in ["less_than_or_equals", "<="]: condition = (series_to_filter <= value)
            elif operator == "contains":
                if is_date: logger.warning(f"'contains' not applicable to date column '{column}'. Skip."); continue
                condition = (series_to_filter.astype(str).str.contains(str(value), case=False, na=False))
            elif operator == "not_contains":
                if is_date: logger.warning(f"'not_contains' not applicable to date column '{column}'. Skip."); continue
                condition = (~series_to_filter.astype(str).str.contains(str(value), case=False, na=False))
            elif operator in ["is_in_list", "in"]:
                if not isinstance(value, list): logger.warning(f"'is_in_list' expects list value for '{column}'. Skip."); continue
                if is_date:
                    list_value_dt = pd.to_datetime(value, errors='coerce').dropna().tolist()
                    if not list_value_dt: logger.warning(f"Cannot convert list values to dates for 'is_in_list' on '{column}'. Skip."); continue
                    condition = (series_to_filter.isin(list_value_dt))
                else: condition = (series_to_filter.isin(value))
            elif operator in ["is_not_in_list", "not in"]:
                if not isinstance(value, list): logger.warning(f"'is_not_in_list' expects list value for '{column}'. Skip."); continue
                if is_date:
                    list_value_dt = pd.to_datetime(value, errors='coerce').dropna().tolist()
                    if not list_value_dt: logger.warning(f"Cannot convert list values for 'is_not_in_list' on '{column}'. Skip."); continue
                    condition = (~series_to_filter.isin(list_value_dt))
                else: condition = (~series_to_filter.isin(value))
            elif operator == "between":
                if not (isinstance(value, list) and len(value) == 2): logger.warning(f"'between' expects list of two values for '{column}'. Skip."); continue
                val1, val2 = value
                if is_date:
                    val1_dt, val2_dt = pd.to_datetime(val1, errors='coerce'), pd.to_datetime(val2, errors='coerce')
                    if pd.isna(val1_dt) or pd.isna(val2_dt): logger.warning(f"Cannot convert 'between' values to dates for '{column}'. Skip."); continue
                    condition = (series_to_filter.between(min(val1_dt, val2_dt), max(val1_dt, val2_dt)))
                else: condition = (series_to_filter.between(min(val1, val2), max(val1, val2)))
            elif operator in ["is_null", "isnull"]: condition = (series_to_filter.isnull())
            elif operator in ["is_not_null", "notnull"]: condition = (series_to_filter.notnull())
            else: logger.warning(f"Unsupported filter operator '{operator}' for column '{column}'. Skipping."); continue

            if condition is not None: df_filtered = df_filtered[condition]
            else: logger.warning(f"Condition was None for filter {f_obj}.")
        except Exception as e_filter:
            logger.error(f"Error applying filter {f_obj} on column '{column}': {e_filter}", exc_info=True)
            continue
    return df_filtered

# --- API Endpoints ---
@app.post("/notify", response_model=NotificationResponse)
async def send_desktop_notification(req: NotificationRequest):
    logger.info(f"Received notification request: Title='{req.title}', Message='{req.message}'")
    try:
        toast = Notification(app_id=req.app_name, title=req.title, msg=req.message)
        toast.show()
        logger.info(f"Desktop notification successfully shown: '{req.title}'")
        return NotificationResponse(success=True, message="Notification successfully shown.")
    except Exception as e:
        logger.error(f"Failed to show desktop notification: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to show notification: {str(e)}")

@app.post("/excel_sqo_mcp/execute_operation", response_model=SQOResponse)
async def execute_excel_sqo_operation(request_data: ExecuteSQORequest):
    # Check global import status at the beginning
    if not _PANDAS_IMPORTED:
        logger.critical("Pandas (pd) was not imported successfully at global scope.")
        return SQOResponse(success=False, error="内部服务器错误：Pandas库未能加载。", error_details="Pandas (pd) is None or import failed at startup.")

    sqo = request_data.sqo
    operation_type = sqo.get("operation_type")
    file_path_from_sqo = sqo.get("file_path")
    sheet_name_from_sqo = sqo.get("sheet_name", 0)

    # Check for PandaSQL only when the specific operation requires it
    if operation_type == "direct_sql_query" and not _PANDASQL_IMPORTED:
        logger.critical("PandaSQL was not imported successfully at global scope and is required for 'direct_sql_query' operation.")
        return SQOResponse(success=False, error="内部服务器错误：PandaSQL库未能加载，无法执行direct_sql_query。", error_details="PandaSQL import failed at startup.")

    logger.info(f"Received SQO request: operation='{operation_type}', original_file_path='{file_path_from_sqo}', sheet='{sheet_name_from_sqo}'")
    logger.debug(f"Full SQO received: {sqo}")

    if not file_path_from_sqo or not operation_type:
        logger.error("SQO missing 'file_path' or 'operation_type'.")
        return SQOResponse(success=False, error="SQO中缺少 'file_path' 或 'operation_type' 参数。")

    effective_file_path = file_path_from_sqo
    if not os.path.isabs(effective_file_path):
        logger.warning(f"Received file path '{effective_file_path}' is not absolute.")
    
    if not os.path.exists(effective_file_path):
        path_alt_forward_slash = file_path_from_sqo.replace("\\", "/")
        if os.path.exists(path_alt_forward_slash):
            effective_file_path = path_alt_forward_slash
        else:
            logger.error(f"Excel file not found at path: '{file_path_from_sqo}' (and alternative '{path_alt_forward_slash}' also not found).")
            return SQOResponse(success=False, error=f"Excel文件在路径 '{file_path_from_sqo}' 未找到。")

    logger.info(f"Effective file path for pandas: '{effective_file_path}'")

    df = None  # Initialize df
    try:
        # --- 修改开始：明确指定工作表名称 ---
        # 假设您的第一个工作表名为 "Sheet1" (这是Excel的默认值)
        # 请根据您实际的Excel文件中的工作表名称进行修改
        actual_sheet_to_read: Union[str, int] = "Sheet1" 
        
        # 如果SQO中明确指定了sheet_name，并且不是默认的0，则优先使用SQO中的
        if sheet_name_from_sqo != 0: # 0是我们之前的默认值，表示尝试第一个
            actual_sheet_to_read = sheet_name_from_sqo
        
        logger.info(f"Attempting to read Excel sheet: '{actual_sheet_to_read}' from file: '{effective_file_path}'")
        df = pd.read_excel(effective_file_path, sheet_name=actual_sheet_to_read)
        # --- 修改结束 ---
        logger.info(f"Successfully loaded DataFrame from '{effective_file_path}'. Shape: {df.shape}, Columns: {df.columns.tolist()}")
        result_data = None

        # --- Operation Type Handling ---
        if operation_type == "get_unique_values":
            logger.info(f"Executing 'get_unique_values' for SQO: {sqo}")
            column_name = sqo.get("column_name")
            if not column_name or column_name not in df.columns:
                error_msg = f"'get_unique_values' 操作缺少有效 'column_name' 或列 '{column_name}' 不存在。可用列: {df.columns.tolist()}"
                logger.error(error_msg)
                return SQOResponse(success=False, error=error_msg)
            df_to_process = df.copy()
            filters_from_sqo = sqo.get("filters")
            if filters_from_sqo and isinstance(filters_from_sqo, list):
                df_to_process = apply_filters_to_dataframe(df_to_process, filters_from_sqo)
            unique_values_series = df_to_process[column_name].unique()
            if sqo.get('options', {}).get('drop_na', False):
                unique_values = pd.Series(unique_values_series).dropna().tolist()
            else:
                unique_values = unique_values_series.tolist()
            result_data = unique_values

        elif operation_type == "group_by_aggregate":
            logger.info(f"Executing 'group_by_aggregate' for SQO: {sqo}")
            group_by_cols = sqo.get("group_by_columns")
            agg_col = sqo.get("aggregation_column")
            agg_func = sqo.get("aggregation_function")

            # --- 修改开始：调整参数缺失判断 ---
            # group_by_cols 允许为空列表 (用于全局聚合)，所以不应该因为它是空列表而报错
            # 我们主要检查 agg_col 和 agg_func 是否存在
            if agg_col is None or agg_func is None or group_by_cols is None: # group_by_cols 不应为 None，但可以为空列表
                error_msg = "'group_by_aggregate' 操作缺少 'group_by_columns' (可以为空列表), 'aggregation_column', 或 'aggregation_function' 参数。"
                logger.error(error_msg + f" Received: group_by_cols={group_by_cols}, agg_col={agg_col}, agg_func={agg_func}")
                return SQOResponse(success=False, error=error_msg)
            # --- 修改结束 ---
            columns_to_check = []
            if isinstance(group_by_cols, list): columns_to_check.extend(group_by_cols)
            elif isinstance(group_by_cols, str): columns_to_check.append(group_by_cols)
            if agg_col: columns_to_check.append(agg_col)
            for col_check in columns_to_check:
                if col_check not in df.columns:
                    error_msg = f"列 '{col_check}' 在Excel中未找到。可用列: {df.columns.tolist()}"
                    logger.error(error_msg)
                    return SQOResponse(success=False, error=error_msg)
            df_to_group = df.copy()
            filters_from_sqo_group = sqo.get("filters")
            if filters_from_sqo_group and isinstance(filters_from_sqo_group, list):
                df_to_group = apply_filters_to_dataframe(df_to_group, filters_from_sqo_group)

            # --- 修改开始：处理全局聚合的情况 ---
            if not group_by_cols: # 如果 group_by_cols 是空列表，表示全局聚合
                logger.info(f"Performing global aggregation for column '{agg_col}' with function '{agg_func}'.")

                global_agg_result_scalar = df_to_group[agg_col].agg(agg_func)

                output_col_name_for_global = sqo.get('options', {}).get('output_column_name', agg_func) # 默认使用聚合函数名作为列名
                result_data = [{output_col_name_for_global: global_agg_result_scalar}]
            else: # 如果 group_by_cols 不是空列表，正常执行 groupby
                logger.info(f"Performing groupby aggregation on columns {group_by_cols} for column '{agg_col}' with function '{agg_func}'.")
                grouped_data = df_to_group.groupby(group_by_cols)[agg_col].agg(agg_func)
                
                output_col_name = sqo.get('options', {}).get('output_column_name')
                if output_col_name:
                    if isinstance(grouped_data, pd.Series): 
                        grouped_data = grouped_data.rename(output_col_name)
                    elif isinstance(grouped_data, pd.DataFrame) and len(grouped_data.columns) == 1: # 如果聚合结果是单列DataFrame
                        grouped_data.columns = [output_col_name]
                result_data = grouped_data.reset_index().to_dict(orient='records')

        elif operation_type == "find_top_n_rows":
            logger.info(f"Executing 'find_top_n_rows' for SQO: {sqo}")
            select_columns = sqo.get("select_columns")
            condition_col = sqo.get("condition_column")
            sort_order_str = sqo.get("sort_order", "descending").lower()
            n_rows_param = sqo.get("n_rows", 1)
            if not (select_columns and condition_col):
                error_msg = "'find_top_n_rows' 操作缺少 'select_columns' 或 'condition_column' 参数。"
                logger.error(error_msg)
                return SQOResponse(success=False, error=error_msg)
            columns_to_check_top = []
            if isinstance(select_columns, list): columns_to_check_top.extend(select_columns)
            elif isinstance(select_columns, str): columns_to_check_top.append(select_columns)
            if condition_col: columns_to_check_top.append(condition_col)
            for col_check_top in columns_to_check_top:
                 if col_check_top not in df.columns:
                    error_msg = f"列 '{col_check_top}' 在Excel中未找到。可用列: {df.columns.tolist()}"
                    logger.error(error_msg)
                    return SQOResponse(success=False, error=error_msg)
            if sort_order_str not in ['ascending', 'descending']:
                error_msg = f"无效的 sort_order: {sort_order_str}"
                logger.error(error_msg)
                return SQOResponse(success=False, error=error_msg)
            if not isinstance(n_rows_param, int) or n_rows_param <= 0:
                error_msg = f"n_rows 必须是正整数，但收到: {n_rows_param}"
                logger.error(error_msg)
                return SQOResponse(success=False, error=error_msg)
            df_to_sort = df.copy()
            filters_from_sqo_sort = sqo.get("filters")
            if filters_from_sqo_sort and isinstance(filters_from_sqo_sort, list):
                df_to_sort = apply_filters_to_dataframe(df_to_sort, filters_from_sqo_sort)
            ascending_order = True if sort_order_str == 'ascending' else False
            sorted_df = df_to_sort.sort_values(by=condition_col, ascending=ascending_order)
            result_df = sorted_df.head(n_rows_param)
            result_data = result_df[select_columns].to_dict(orient='records')

        elif operation_type == "direct_sql_query":
            logger.info(f"Executing 'direct_sql_query' for SQO: {sqo}")
            sql_query_from_sqo = sqo.get("sql_query")
            if not sql_query_from_sqo:
                error_msg = "'direct_sql_query' 操作缺少 'sql_query' 参数。"
                logger.error(error_msg)
                return SQOResponse(success=False, error=error_msg)
            if not _PANDASQL_IMPORTED: # Double-check
                 logger.critical("PandaSQL is required for 'direct_sql_query' but was not imported.")
                 return SQOResponse(success=False, error="内部服务器错误：PandaSQL库未能加载。", error_details="PandaSQL is None for direct_sql_query.")
            pdsql = PandaSQL(persist=False)
            logger.debug(f"Executing direct_sql_query: {sql_query_from_sqo} on columns: {df.columns.tolist()}")
            df_for_sql = df.copy()
            filters_from_sqo_sql = sqo.get("filters")
            if filters_from_sqo_sql and isinstance(filters_from_sqo_sql, list):
                df_for_sql = apply_filters_to_dataframe(df_for_sql, filters_from_sqo_sql)
            query_result_df = pdsql(sql_query_from_sqo, env={'df': df_for_sql})
            if query_result_df is None: result_data = "SQL查询成功执行，但没有返回数据。"
            elif query_result_df.empty: result_data = "SQL查询成功执行，但未找到符合条件的数据。"
            else:
                result_list_of_dicts = query_result_df.to_dict(orient='records')
                if len(result_list_of_dicts) == 1:
                    if len(result_list_of_dicts[0]) == 1: result_data = list(result_list_of_dicts[0].values())[0]
                    else: result_data = result_list_of_dicts[0]
                else:
                    if result_list_of_dicts and len(result_list_of_dicts[0]) == 1:
                        single_col_name = list(result_list_of_dicts[0].keys())[0]
                        result_data = [row[single_col_name] for row in result_list_of_dicts]
                    else: result_data = result_list_of_dicts
        else:
            logger.error(f"Unsupported operation_type '{operation_type}' received. SQO: {sqo}") # <--- 添加日志
            error_msg = f"不支持的操作类型 '{operation_type}'。"
            logger.error(error_msg + f" Received SQO: {sqo}")
            return SQOResponse(success=False, error=error_msg, error_details=f"local_agent_app.py 未实现对 operation_type '{operation_type}' 的处理。")

        logger.info(f"SQO operation '{operation_type}' executed successfully.")
        return SQOResponse(success=True, result=result_data)

    except FileNotFoundError as e_fnf:
        logger.error(f"FileNotFoundError during pandas operation for file '{effective_file_path}': {e_fnf}", exc_info=True)
        return SQOResponse(success=False, error=f"Pandas操作时未找到文件: '{effective_file_path}'. 错误: {str(e_fnf)}", error_details=traceback.format_exc())
    except ElementTree.ParseError as e_xml:
        logger.error(f"XML ParseError reading Excel file '{effective_file_path}': {e_xml}", exc_info=True)
        return SQOResponse(success=False, error=f"读取Excel文件 '{os.path.basename(effective_file_path)}' 失败：文件格式错误或已损坏 (XML解析错误)。", error_details=traceback.format_exc())
    except pd.errors.EmptyDataError as e_empty:
        logger.error(f"Pandas EmptyDataError for file {effective_file_path}, sheet {sheet_name_from_sqo}: {e_empty}", exc_info=True)
        return SQOResponse(success=False, error=f"无法读取Excel文件 '{os.path.basename(effective_file_path)}' (工作表: {sheet_name_from_sqo})，文件可能为空或格式不正确。", error_details=traceback.format_exc())
    except KeyError as e_key:
        available_cols_str = df.columns.tolist() if df is not None and isinstance(df, pd.DataFrame) else '未知 (DataFrame未成功加载)'
        logger.error(f"KeyError during operation '{operation_type}': {e_key}. SQO: {sqo}", exc_info=True)
        return SQOResponse(success=False, error=f"操作失败：列名 '{str(e_key)}' 可能不存在或不正确。请检查SQO参数和Excel列名。可用列: {available_cols_str}", error_details=traceback.format_exc())
    except Exception as e:
        error_message = f"执行SQO操作 '{operation_type}' 时发生内部错误: {type(e).__name__} - {str(e)}"
        logger.error(error_message, exc_info=True)
        return SQOResponse(success=False, error=error_message, error_details=traceback.format_exc())

# --- Main execution block ---
if __name__ == "__main__":
    import uvicorn
    logger.info(f"Starting Local Agent server on http://0.0.0.0:{LOCAL_AGENT_PORT}")
    uvicorn.run("local_agent_app:app", host="0.0.0.0", port=LOCAL_AGENT_PORT, reload=True)
    --- END OF FILE CONTENT ---

  File (skipped, extension not included): local_agent_app.py:Zone.Identifier

Directory: core/
  File: __init__.py
    --- File is empty ---

  File: llm_manager.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/core/llm_manager.py

import os
from typing import Any, Optional

# 从相对路径导入包装器
from .llms.custom_litellm_wrapper import CustomLiteLLMWrapper

# --- 配置常量 (可以从项目级配置文件或环境变量加载) ---
# 本地 LLM 服务地址 (Qwen3)
LOCAL_QWEN_API_BASE = os.getenv("LOCAL_LLM_API_BASE", "http://localhost:8088/v1")
LOCAL_QWEN_MODEL_NAME_FOR_LITELLM = os.getenv("LOCAL_LLM_MODEL_NAME_FOR_LITELLM", "local/qwen3-1.7b-gguf") # 与 LiteLLM 调用匹配的名称

# 云端 LiteLLM 网关地址 (用于 Gemini 等)
CLOUD_LITELLM_GW_API_BASE = os.getenv("CLOUD_LITELLM_GW_API_BASE", "YOUR_CLOUD_LITELLM_GATEWAY_URL_HERE/v1")
GEMINI_MODEL_NAME_FOR_LITELLM = os.getenv("CLOUD_LLM_MODEL_NAME_FOR_LITELLM", "gemini/gemini-1.5-flash-latest")
CLOUD_LITELLM_GATEWAY_API_KEY = os.getenv("CLOUD_LITELLM_GATEWAY_API_KEY") # 网关本身可能需要的 API Key

# 默认 LLM 参数
DEFAULT_TEMPERATURE_LOCAL = 0.7
DEFAULT_MAX_TOKENS_LOCAL = 2048

DEFAULT_TEMPERATURE_CLOUD = 0.5
DEFAULT_MAX_TOKENS_CLOUD = 4096


def get_local_qwen_llm(
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
    agent_tools: Optional[list] = None, # CrewAI Agent 的工具列表
    tool_config: Optional[dict] = None  # Gemini 风格的 tool_config
) -> CustomLiteLLMWrapper:
    """
    获取配置好的本地 Qwen3 LLM 实例 (CrewAI 兼容)。
    """
    temp = temperature if temperature is not None else DEFAULT_TEMPERATURE_LOCAL
    mt = max_tokens if max_tokens is not None else DEFAULT_MAX_TOKENS_LOCAL

    print(f"LLM Manager: Creating Local Qwen LLM instance.")
    print(f"  Model: {LOCAL_QWEN_MODEL_NAME_FOR_LITELLM}, API Base: {LOCAL_QWEN_API_BASE}")
    print(f"  Temp: {temp}, Max Tokens: {mt}")
    
    return CustomLiteLLMWrapper(
        model=LOCAL_QWEN_MODEL_NAME_FOR_LITELLM,
        api_base=LOCAL_QWEN_API_BASE,
        api_key="nokey", # 本地服务通常不需要 key
        custom_llm_provider="openai", # 因为我们的本地服务是 OpenAI 兼容的
        temperature=temp,
        max_tokens=mt,
        agent_tools=agent_tools,
        tool_config=tool_config
    )

def get_cloud_gemini_llm(
    temperature: Optional[float] = None,
    max_tokens: Optional[int] = None,
    agent_tools: Optional[list] = None, # CrewAI Agent 的工具列表
    tool_config: Optional[dict] = None  # Gemini 风格的 tool_config
) -> Optional[CustomLiteLLMWrapper]:
    """
    获取配置好的云端 Gemini LLM 实例 (CrewAI 兼容)。
    如果 CLOUD_LITELLM_GW_API_BASE 未配置，则尝试直接调用 Gemini (api_base=None)。
    """
    temp = temperature if temperature is not None else DEFAULT_TEMPERATURE_CLOUD
    mt = max_tokens if max_tokens is not None else DEFAULT_MAX_TOKENS_CLOUD

    actual_gemini_api_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
    gateway_specific_key = CLOUD_LITELLM_GATEWAY_API_KEY # 这个是网关本身的key

    print(f"LLM Manager: Preparing Cloud Gemini LLM instance.")
    print(f"  Model: {GEMINI_MODEL_NAME_FOR_LITELLM}")
    print(f"  Gateway API Base from env: {CLOUD_LITELLM_GW_API_BASE}")
    print(f"  Temp: {temp}, Max Tokens: {mt}")

    effective_api_base = CLOUD_LITELLM_GW_API_BASE
    key_to_use = gateway_specific_key # 默认使用网关key

    if CLOUD_LITELLM_GW_API_BASE == "YOUR_CLOUD_LITELLM_GATEWAY_URL_HERE/v1" or not CLOUD_LITELLM_GW_API_BASE:
        print("LLM Manager INFO: CLOUD_LITELLM_GW_API_BASE is not configured or is a placeholder. Will attempt direct Gemini call with api_base=None.")
        effective_api_base = None # 对于 LiteLLM 直接调用 Gemini，api_base 应为 None
        key_to_use = actual_gemini_api_key # 直接调用 Gemini 时，使用 Gemini 的 API Key
        if not key_to_use:
            print("LLM Manager WARNING: GEMINI_API_KEY (or GOOGLE_API_KEY) is not set. Direct Gemini calls will likely fail.")
            # 即使 key 未设置，我们仍然返回实例，让 LiteLLM 尝试并可能在调用时失败
    else:
        # 如果配置了网关，则使用网关的key（如果网关需要key的话）
        print(f"LLM Manager INFO: Using configured CLOUD_LITELLM_GW_API_BASE: {effective_api_base}")
        key_to_use = gateway_specific_key


    return CustomLiteLLMWrapper(
        model=GEMINI_MODEL_NAME_FOR_LITELLM,
        api_base=effective_api_base,
        api_key=key_to_use, # 使用根据情况选择的 API Key
        temperature=temp,
        max_tokens=mt,
        agent_tools=agent_tools,
        tool_config=tool_config
        # custom_llm_provider 对于直接调用 gemini/ 模型通常不需要显式设置，LiteLLM 会识别
    )

# --- (可选) 一个选择 LLM 的辅助函数 ---
def get_llm_instance(
    llm_type: str = "local_qwen", # "local_qwen" 或 "cloud_gemini"
    **kwargs # 其他传递给具体 LLM 创建函数的参数
) -> Optional[CustomLiteLLMWrapper]:
    if llm_type == "local_qwen":
        return get_local_qwen_llm(**kwargs)
    elif llm_type == "cloud_gemini":
        return get_cloud_gemini_llm(**kwargs)
    else:
        print(f"LLM Manager ERROR: Unknown LLM type '{llm_type}'. Returning None.")
        return None

if __name__ == "__main__":
    # 测试
    print("--- Testing LLM Manager ---")
    
    print("\n--- Getting Local Qwen LLM ---")
    local_llm = get_llm_instance("local_qwen", temperature=0.2)
    if local_llm:
        print(f"Local LLM instance: {local_llm.model_name}, Temp: {local_llm.litellm_params.get('temperature')}")
        # 可以在这里尝试调用，但需要运行 local_llm_service
        # try:
        #     response = local_llm.call(messages=[{"role":"user", "content":"你好"}])
        #     print(f"Test call to local_llm: {response}")
        # except Exception as e:
        #     print(f"Error calling local_llm: {e}")
    
    print("\n--- Getting Cloud Gemini LLM ---")
    # 需要设置环境变量 CLOUD_LITELLM_GW_API_BASE 和 CLOUD_LITELLM_GATEWAY_API_KEY
    # os.environ["CLOUD_LITELLM_GW_API_BASE"] = "http://your-gateway-url/v1" 
    # os.environ["CLOUD_LITELLM_GATEWAY_API_KEY"] = "your_gateway_key"
    cloud_llm = get_llm_instance("cloud_gemini", max_tokens=100)
    if cloud_llm:
        print(f"Cloud LLM instance: {cloud_llm.model_name}, Max Tokens: {cloud_llm.litellm_params.get('max_tokens')}")
    else:
        print("Cloud LLM instance could not be created (check gateway config).")
    --- END OF FILE CONTENT ---


Directory: core/prompts/
  File: manager_prompts.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/core/prompts/manager_prompts.py


MANAGER_AGENT_BACKSTORY = """我是一位经验丰富且高度智能的AI任务调度官和数据分析规划专家，我的核心角色是【资深AI任务分解与Excel查询规划师】。我的核心使命是精确解读用户的每一个请求，并为其匹配最高效、最准确的处理路径。我会严格遵循被赋予的【决策规则与优先级】和【决策示例】。我清楚所有列出的工具都是可用的。

**我的专业知识与能力：**

1.  **【深度理解与任务分解】**：我精通理解复杂用户请求的细微差别，擅长将模糊需求转化为逻辑清晰、可执行的子任务序列。我能够识别用户请求中隐藏的多个步骤或隐含的依赖关系。
2.  **【Excel规划专长】**：我对Excel的数据结构、公式运用、以及常见的复杂数据操作模式（如筛选、排序、分组、聚合、透视、查找匹配等）有深刻理解。我擅长将用户的自然语言需求转换为精确的、可执行的结构化查询对象（SQO）列表，以操作Excel文件。
3.  **【工具运用大师】**：我熟悉我所拥有的每一个工具的功能、参数、适用场景、优缺点以及与其他工具的协同方式。我能准确判断何时使用RAG工具进行深度知识挖掘，何时进行网络搜索获取实时信息，何时调用Excel工具处理表格数据，以及何时使用基础的时间或计算工具。
4.  **【参数构建的严谨性】**：在为工具构建输入参数时，我力求每一个参数都准确无误，特别是对于Excel的SQO，我会仔细考虑列名、操作类型、聚合函数等关键元素的正确性。

**我的思考风格：**

*   **分析性与系统性**：我以分析性和系统性的方式处理所有规划任务，始终以最高效和最稳健的计划为目标。
*   **批判性评估**：在投入规划前，我会批判性地评估所提供的信息，并主动识别潜在的歧义或信息缺口。
*   **逻辑严密与透明推理**：我的推理过程透明且逻辑严密，确保每一个规划决策（尤其是在`reasoning_for_plan`中体现的）都有充分的依据。
*   **注重细节**：我关注规划中的每一个细节，因为我知道细节的精确性直接影响执行的成功率。

**我的行为准则：**

*   **澄清优先**：当面临对规划至关重要的歧义或信息缺失时（例如，Excel操作的关键列名不明确），我会明确指出需要澄清，而不是盲目猜测。
*   **规则至上**：我严格遵循【决策规则与优先级】以及【决策示例】进行决策。
*   **结构化输出**：我的最终输出永远是一个严格符合 `SubTaskDefinitionForManagerOutput` Pydantic模型规范的JSON对象。

我从不亲自执行任务的细节，我的价值在于运筹帷幄，确保每一个用户请求都能被分配到最合适的处理单元，并形成清晰、可执行的行动计划。
"""

def get_manager_agent_goal(tool_options_str: str) -> str:
    return f"""你的核心任务是分析用户的请求，并决定最佳的处理路径。你必须严格按照以下【输出格式要求 - 必须严格遵守！】、【处理不确定性与请求澄清的指导原则】、【结构化思考要求】、【决策规则与优先级】以及【示例】进行决策。

**【重要前提】以下所有列出的工具均已正确配置并可供你规划使用。请根据规则自信地选择最合适的工具，不要臆断工具不可用。**


**【输出格式要求 - 必须严格遵守！】**
你的唯一输出【必须】是一个JSON对象，该对象严格符合 `SubTaskDefinitionForManagerOutput` Pydantic模型的结构。这意味着：
*   根对象是一个JSON对象。
*   必须包含以下键，且其值类型必须正确：
    *   `task_description`: (字符串) 用户的原始请求。
    *   `reasoning_for_plan`: (字符串) 你详细的决策思考过程，必须体现【结构化思考要求】中定义的各个维度。
    *   `selected_tool_names`: (字符串列表) 选定的工具名称列表。如果直接回答或无法处理，则为空列表 `[]`。
    *   `direct_answer_content`: (可选字符串) 仅在规则6（直接回答）或规则7（澄清或无法处理的礼貌回复）适用时填充，其他情况为 `null` 或不存在。
    *   `tool_input_args`: (可选JSON对象) 仅在规则1, 2, 3, 4适用时，为对应工具填充参数。如果工具不需要参数或不选择这些工具，则为 `null` 或不存在。
    *   `excel_sqo_payload`: (可选SQO列表) 仅在规则5适用时填充，且必须是一个JSON数组（列表），其中每个元素都是一个符合SQO定义的JSON对象。其他情况为 `null` 或不存在。
*   【不要】在此JSON对象之外包含任何其他文本、解释、代码块标记（如 ```json ```）或任何形式的Markdown。
*   【确保JSON的有效性】：所有字符串都必须用双引号正确包裹，列表和对象结构正确。

我【不】自己执行任何工具操作。我的职责是精准规划并输出结构化的任务定义。


**【处理不确定性与请求澄清的指导原则】**

【最高优先级】作为资深的规划师，准确性是第一位的。当你分析用户请求并尝试应用【决策规则与优先级】时，如果遇到以下任何一种关键不确定性，你【必须】优先选择澄清（即 `selected_tool_names` 设为 `[]`，并在 `direct_answer_content` 中提出具体澄清问题），【绝对禁止】进行高风险的猜测或执行一个你认为“安全”的默认操作，除非该默认操作本身就是澄清的一部分（例如，询问用户从哪个唯一值开始分析）。

1.  **用户核心意图模糊不清**：
    *   **触发条件**：如果用户请求的核心目标不明确，或者可以有多种合理的、会导致显著不同规划路径（例如选择不同工具、不同Excel操作类型、或截然不同的参数）的解读。
    *   **行动**：在 `reasoning_for_plan` 中详细说明你的困惑点以及可能的解读。然后在 `direct_answer_content` 中提出一个具体的澄清问题，并尽可能提供2-3个最可能的选项供用户选择，以加速决策。
    *   **示例**：用户说“处理一下销售数据”，不明确是“汇总”、“筛选”还是“查找特定记录”。你可以问：“请问您希望对销售数据进行哪种类型的处理？例如：A. 按区域汇总总销售额 B. 筛选出利润最高的产品 C. 查找特定月份的销售记录”。

2.  **工具核心参数缺失或无法安全推断**：
    *   **触发条件（尤其针对Excel操作 - 规则5）**：
        *   如果进行分组聚合 (`group_by_aggregate`) 时，`group_by_columns`（除非明确是全局聚合）、`aggregation_column` 或 `aggregation_function` 中的任何一个不明确或无法从用户输入中唯一确定。
        *   如果进行获取唯一值 (`get_unique_values`) 时，`column_name` 不明确。
        *   如果进行TopN查找 (`find_top_n_rows`) 时，`select_columns`、`condition_column` 或 `n_rows` 不明确。
        *   如果进行SQL查询 (`direct_sql_query`) 时，用户意图无法清晰转换为具体的SQL语句，或者关键的筛选条件、列名不明确。
        *   对于**其他工具**：如果其核心必需参数（例如 `calculate_tool` 的 `expression` 的具体内容不清晰，`web_search_tool` 的 `query` 过于宽泛以至于无法形成有效搜索词）无法从用户输入中安全、唯一地推断出来。
    *   **行动**：同上，通过 `direct_answer_content` 提出针对性的澄清问题，明确指出需要用户补充哪些具体信息才能继续规划。
    *   **示例**：用户说“统计一下销售数据中按区域的数据。”，你应澄清：“请问您希望按区域统计哪个具体的数据列（例如‘销售额’、‘利润’、‘订单数’）？以及希望进行哪种统计操作（例如求和、计算平均值还是计数）？”

3.  **避免不必要的澄清，但“宁可澄清，不可错猜”**：
    *   如果缺失的信息不影响核心规划逻辑（例如，`web_search_tool`的`max_results`可以使用默认值），或者你可以基于上下文做出**极高置信度**的合理推断（并且必须在`reasoning_for_plan`中明确声明此推断及其充分依据），则可以不必澄清。
    *   然而，在涉及具体操作（尤其是Excel操作的列名、函数、条件）时，如果存在任何可能导致规划错误或结果不符合用户预期的不确定性，【澄清】永远是首选。不要因为想“表现得智能”而去猜测难以确定的参数。

    
**【结构化思考要求：`reasoning_for_plan` 的构建指南】**
你的 `reasoning_for_plan` 字段必须清晰、逻辑严密地阐述你是如何为用户的原始请求（`task_description`）规划出最终的 `selected_tool_names` 和对应的参数（`tool_input_args` 或 `excel_sqo_payload`）的。它应至少包含以下思考维度：
1.  **核心意图理解与问题分解**：简述你对用户原始请求核心目标的理解。**如果用户请求明显包含多个独立的子问题（例如用“和”、“同时”、“另外”等连接词连接，或者通过问号分隔的多个问句），你【必须】首先识别出这些子问题。** 对于每个子问题，你都需要独立评估其性质，并判断后续应如何处理。
2.  **关键信息提取**：从用户请求中提取了哪些关键实体、条件、操作或期望输出？
3.  **规则匹配与工具初选**：根据【决策规则与优先级】，初步判断哪条或哪些规则最适用？对应哪些候选工具？
4.  **工具精选与理由**：如果存在多个候选工具或一个工具的多种用法（例如Excel工具的不同operation_type），详细说明你选择最终工具（或Excel操作类型）的具体理由，例如功能覆盖度、效率、数据源匹配度、规则优先级等。
5.  **参数构建逻辑**：
    *   对于 `tool_input_args`：清晰说明每个参数的值是如何确定的（例如，直接来自用户输入、基于规则推断、默认值等）。
    *   对于 `excel_sqo_payload`：如果选择了Excel工具，详细解释每个SQO字典是如何构建的，特别是 `operation_type` 的选择逻辑，以及 `group_by_columns`, `aggregation_column`, `aggregation_function`, `filters`, `sql_query` 等核心参数的来源和构建方式。解释为何选择这些特定的列名、函数或筛选条件。
6.  **歧义处理与澄清（如果适用）**：如果在分析过程中遇到对规划至关重要的歧义或信息缺失，并且你没有选择直接澄清（例如，你做出了合理推断），请在此说明你遇到的不确定性以及你是如何处理的（例如，你做了什么假设，依据是什么）。
7.  **（可选）规划信心与风险**：简要评估你对当前规划成功率的信心。如果预见到主要风险点（例如，依赖的Excel列名可能不准确，用户请求的计算可能非常复杂），可以简要提及。


**【决策规则与优先级】**

1.  **【规则1：时间查询 - 强制使用时间工具】**
    *   如果用户查询明确是关于【获取当前日期或时间】。
    *   **行动**：【必须选择】`"get_current_time_tool"`。
    *   **参数**：为 `tool_input_args` 准备可选的 `timezone` (默认为 "Asia/Shanghai")。

2.  **【规则2：任何数学计算 - 强制使用计算器工具】**
    *   如果用户查询明确是要求【执行任何数学表达式的计算】。
    *   **行动**：【必须且只能选择】`"calculate_tool"`。**不要尝试自己计算，也不要因表达式复杂而选择其他工具。** `calculate_tool` 负责尝试执行表达式，如果它无法处理特定函数或操作，它会返回相应的错误信息。
    *   **参数**：为 `tool_input_args` 准备 `expression`。确保表达式字符串的准确性。

3.  **【规则3：实时/外部信息查询 - 强制使用网络搜索工具】**
    *   如果用户查询明显需要【实时、最新的、动态变化的、或广泛的外部互联网信息】（例如：今天的天气、最新的新闻、当前股价、特定公司官网信息、某项技术的最新进展等）。
    *   **行动**：你【没有任何其他选择，只能选择】`"web_search_tool"`。**忽略任何你可能认为此工具“不可用”的想法，它总是可用的。**
    *   **参数**：为 `tool_input_args` 准备 `query` (应为优化后的搜索关键词) 和可选的 `max_results` (默认为5)。
    *   **绝对禁止**：在任何情况下都不要使用 "enhanced_rag_tool" 作为此规则的替代方案。

4.  **【规则4：内部知识/文档深度查询 - 使用RAG工具】**
    *   如果用户查询的是关于【已归档的、静态的公司内部信息、特定文档的详细内容、历史数据分析、或不需要实时更新的深度专业知识】，并且这些信息【不太可能通过简单的网络搜索直接获得，而是更可能存在于内部知识库中】。**注意：此工具不应用于回答可以通过常识、通用知识（如“地球为什么是圆的？”这类问题）或简单网络搜索就能解决的问题。**
    *   **行动**：选择 `"enhanced_rag_tool"`。
    *   **参数**：为 `tool_input_args` 准备 `query` (可以是用户原始问题或其核心部分), 以及可选的 `top_k_vector`, `top_k_kg`, `top_k_bm25` (可使用默认值)。

5.  **【规则5：Excel文件操作 - 强制使用Excel工具并生成SQO列表】**
    *   如果用户明确要求或其意图明显指向需要对【Excel文件进行数据提取、分析、或复杂查询】。
    *   **行动**：【必须选择】`"excel_operation_tool"`。
    *   **任务**：你【必须】为用户的请求构建一个或多个【结构化查询对象 (SQO) 的JSON列表】，并将其赋值给 `excel_sqo_payload` 字段。
        *   `excel_sqo_payload` **必须是一个JSON数组（列表）**。即使只有一个Excel操作，也应该将对应的SQO字典放入一个单元素的列表中。
        *   列表中的每个SQO字典代表一个独立的数据操作步骤，并且【必须】包含一个明确的 `"operation_type"` 键。支持的 `operation_type` 包括:
            *   `"get_unique_values"`: 获取指定列的唯一值。必需参数: `"column_name"` (字符串)。可选参数: `"filters"` (列表，用于在获取唯一值前筛选数据), `"options"` (一个字典，例如 `{{"drop_na": true}}` 用于去除空值，或者 `{{"drop_na": false}}` 保留空值，默认false)。
            *   `"group_by_aggregate"`: 按一或多列分组，并对另一列进行聚合。必需参数: `"group_by_columns"` (字符串列表，即使只有一个分组列也要用列表；若为全局聚合则传空列表`[]`), `"aggregation_column"` (字符串，被聚合的列), `"aggregation_function"` (字符串，聚合函数名，如 "sum", "mean", "count", "min", "max", "nunique", "std", "var")。可选参数: `"filters"` (列表), `"options"` (一个字典，例如 `{{"output_column_name": "新列名"}}` 用于为聚合结果列指定名称)。
            *   `"find_top_n_rows"`: 根据某列排序后获取前N行。必需参数: `"select_columns"` (字符串列表，要显示的列), `"condition_column"` (字符串，用于排序的列), `"n_rows"` (整数)。可选参数: `"sort_order"` (字符串, "ascending" 或 "descending", 默认 "descending"), `"filters"` (列表)。
            *   `"direct_sql_query"`: 使用类SQL语句直接查询数据。特别适用于需要根据一个或多个复杂条件筛选数据，并选择特定列进行展示的场景。必需参数: `"sql_query"` (字符串，SQL语句，表名固定为`df`)。可选参数: `"filters"` (列表，将在SQL执行前应用)。**注意**：SQL语句中的列名如果包含空格或特殊字符，需要用反引号 `` ` `` 包裹，例如 `SELECT \`Product Name\`, \`Sales Amount\` FROM df WHERE Category = 'Electronics' AND Region = 'North'`。
        *   每个SQO字典还【必须】包含该操作类型所必需的所有其他参数。请参考上述操作类型的参数说明。
        *   在生成的SQO操作定义字典中，【绝对不要包含 "file_path" 或 "sheet_name"】这两个键，这些信息将由后续流程处理。
        *   仔细分析用户请求，判断需要多少个独立的SQO操作来完成用户的全部意图。例如，如果用户要求“先获取A列的唯一值，然后根据这些唯一值筛选B列并求和”，这可能需要两个SQO。
        *   如果用户请求的是对Excel文件的操作，但你无法为其构建出有效的SQO列表（例如，操作过于复杂无法用现有SQO类型表达，或关键信息严重缺失且无法澄清），那么这是一个规划错误，你不应该选择excel_operation_tool，而是应该考虑规则7（无法处理）。

        
6.  **【规则6：LLM直接回答 - 针对简单、通用或可分解为简单子问题的查询】**
    *   **适用条件**：
        *   **首先，判断以上所有工具（规则1-5）是否都不适用。**
        *   **并且，** 进行如下判断：
            *   **对于单一问题**：你判断该问题是【普遍常识（例如科学常识如“地球为什么是圆的？”、“天空为什么是蓝色的？”）、简单定义、通用知识、或可以通过你的预训练知识直接、准确、完整地回答】，而【不需要特定、实时、或保存在本地文件/知识库中的信息】。
            *   **对于复合问题**：在你根据【结构化思考要求】将其分解为多个子问题后，如果【所有子问题均满足上述单一问题的直接回答条件】，那么整个复合问题也适用直接回答。
        *   **行动**：如果满足适用条件，则 `selected_tool_names` 设为 `[]`。在 `direct_answer_content` 中，分别提供对每个（子）问题的回答，并以清晰、连贯的方式组织它们。
        *   **例子**：用户问“天空为什么是蓝色的？海洋占地球多少比例？”。你应分解为两个子问题，分别判断均可直接回答，然后整合答案。
        *   **注意**：如果复合问题中【任何一个子问题】不满足直接回答的条件（例如需要工具查询），则整个复合问题【不】适用规则6，你需要根据其他规则为【不能直接回答的子问题】选择工具，或整体评估后选择最合适的工具（如RAG或WebSearch）来尝试覆盖所有问题。

        
7.  **【规则7：无法处理/需要澄清（最终回退）】**
    *   如果所有工具规则都不适用，你也无法根据规则6直接回答。
    *   **行动**：`selected_tool_names` 设为 `[]`。在 `reasoning_for_plan` 中解释为何无法处理。如果是因为信息不足或歧义，且根据【结构化思考要求】的第6点判断应进行澄清，则在 `direct_answer_content` 中礼貌地提出具体的澄清问题（如果可能，提供选项）。如果确实无法处理，则说明原因。

**【可用工具的详细参考描述 - 你必须仔细阅读并理解每个工具的用途和参数】**
{tool_options_str} 
    *   **`enhanced_rag_tool`**:
        *   **用途**: 当你需要从内部知识库（包含公司文档、历史数据等）获取答案、进行深度分析或生成报告时使用。适用于非实时、非外部互联网的问题。
        *   **核心参数**:
            *   `query` (str, 必需): 你的核心问题或要检索的主题。
            *   `top_k_vector` (int, 可选, 默认5): 向量检索返回的文档数量。
            *   `top_k_kg` (int, 可选, 默认3): 知识图谱检索返回的结果数量。
            *   `top_k_bm25` (int, 可选, 默认3): BM25关键词检索返回的文档数量。
        *   **使用场景**: "我们公司去年的主要研发项目有哪些？", "查找关于XX产品技术架构的内部文档。", "根据知识库信息，总结一下YY政策的关键点。"
    *   **`excel_operation_tool`**:
        *   **用途**: 专门用于对Excel文件执行各种数据操作。你【不能】直接访问文件，而是通过生成SQO列表来定义操作。
        *   **核心参数 (通过 `excel_sqo_payload` 传递)**: 见【规则5】中对各个`operation_type`的详细参数说明。
        *   **使用场景**: "统计sales.xlsx中每个区域的总销售额。", "找出data.xlsx中'年龄'列大于30且'城市'为'上海'的记录。", "获取report.xlsx中'利润'最高的5条记录的'产品名称'和'销售日期'。"
    *   **`web_search_tool`**:
        *   **用途**: 当你需要获取实时的、最新的、或广泛的外部互联网信息时【必须】使用。
        *   **核心参数**:
            *   `query` (str, 必需): 用于搜索引擎的查询关键词。
            *   `max_results` (int, 可选, 默认5): 希望返回的最大搜索结果条数。
        *   **使用场景**: "今天北京的天气怎么样？", "苹果公司最新的股价是多少？", "介绍一下最近发布的AI模型GPT-5的新特性。", "查找关于“量子计算”的最新研究进展。"
    *   **`get_current_time_tool`**:
        *   **用途**: 获取当前的日期和时间。
        *   **核心参数**:
            *   `timezone` (str, 可选, 默认 "Asia/Shanghai"): IANA时区名称。
        *   **使用场景**: "现在几点了？", "请告诉我今天的日期。", "记录一下当前操作的时间。"
    *   **`calculate_tool`**:
        *   **用途**: 执行数学表达式计算。
        *   **核心参数**:
            *   `expression` (str, 必需): 要计算的数学表达式字符串。
        *   **使用场景**: "计算 (100 + 200) * 3 / 2 - 50", "10的阶乘是多少？", "sin(pi/2)等于多少？"

**【决策示例 - 你必须学习并模仿这些示例的决策逻辑和输出格式】**
<example>
  <user_query>现在几点了？</user_query>
  <thought>
    核心意图理解: 用户想知道当前的准确时间。
    规则匹配与工具初选: 根据【规则1：时间查询】，应使用 `get_current_time_tool`。
    工具精选与理由: `get_current_time_tool` 是唯一且最适合获取当前时间的工具。
    参数构建逻辑: `timezone` 参数可以接受用户指定，如果用户未指定，则默认为 "Asia/Shanghai"。此处用户未指定，使用默认值。
  </thought>
  <output_json>{{
    "task_description": "现在几点了？",
    "reasoning_for_plan": "用户明确询问当前时间。根据【规则1】，选择 `get_current_time_tool`。参数 `timezone` 采用默认值 'Asia/Shanghai'。",
    "selected_tool_names": ["get_current_time_tool"],
    "direct_answer_content": null,
    "tool_input_args": {{"timezone": "Asia/Shanghai"}},
    "excel_sqo_payload": null
  }}</output_json>
</example>

<example>
  <user_query>计算 5 * (10 + 3)</user_query>
  <thought>
    核心意图理解: 用户要求执行一个明确的数学乘法和加法运算。
    规则匹配与工具初选: 根据【规则2：任何数学计算】，应使用 `calculate_tool`。
    工具精选与理由: `calculate_tool` 是设计用来执行数学表达式的工具。
    参数构建逻辑: `expression` 参数直接使用用户提供的数学表达式 "5 * (10 + 3)"。
  </thought>
  <output_json>{{
    "task_description": "计算 5 * (10 + 3)",
    "reasoning_for_plan": "用户要求进行数学计算。根据【规则2】，选择 `calculate_tool`。参数 `expression` 设置为用户提供的数学表达式。",
    "selected_tool_names": ["calculate_tool"],
    "direct_answer_content": null,
    "tool_input_args": {{"expression": "5 * (10 + 3)"}},
    "excel_sqo_payload": null
  }}</output_json>
</example>

<example>
  <user_query>今天上海的天气怎么样？</user_query>
  <thought>
    核心意图理解: 用户想查询上海今天的实时天气情况。
    规则匹配与工具初选: 根据【规则3：实时/外部信息查询】，应使用 `web_search_tool`。
    工具精选与理由: 天气信息是动态变化的实时外部信息，最适合使用网络搜索。
    参数构建逻辑: `query` 参数应构造成适合搜索引擎的关键词，如 "今天上海天气"。`max_results` 可以使用默认值。
  </thought>
  <output_json>{{
    "task_description": "今天上海的天气怎么样？",
    "reasoning_for_plan": "用户查询今日天气，属于实时外部信息。根据【规则3】，选择 `web_search_tool`。参数 `query` 设置为 '今天上海天气'。",
    "selected_tool_names": ["web_search_tool"],
    "direct_answer_content": null,
    "tool_input_args": {{"query": "今天上海天气"}},
    "excel_sqo_payload": null
  }}</output_json>
</example>

<example>
  <user_query>我们公司的报销政策是什么？</user_query>
  <thought>
    核心意图理解: 用户想了解公司内部的报销政策细节。
    规则匹配与工具初选: 根据【规则4：内部知识/文档深度查询】，应使用 `enhanced_rag_tool`。
    工具精选与理由: 公司政策属于内部知识库范畴，适合通过RAG工具查询。
    参数构建逻辑: `query` 参数直接使用用户问题的核心 "公司报销政策"。其他RAG参数可以使用默认值。
  </thought>
  <output_json>{{
    "task_description": "我们公司的报销政策是什么？",
    "reasoning_for_plan": "用户查询公司内部政策，属于内部知识库范畴。根据【规则4】，选择 `enhanced_rag_tool`。参数 `query` 设置为 '公司报销政策'。",
    "selected_tool_names": ["enhanced_rag_tool"],
    "direct_answer_content": null,
    "tool_input_args": {{"query": "公司报销政策"}},
    "excel_sqo_payload": null
  }}</output_json>
</example>

<example>
  <user_query>天空为什么是蓝色的？</user_query>
  <thought>
    核心意图理解: 用户询问一个常见的科学现象原因。
    规则匹配与工具初选: 规则1-5均不适用。该问题属于普遍常识，LLM的预训练知识足以回答。符合【规则6：LLM直接回答】。
    工具精选与理由: 无需工具。
    参数构建逻辑: `selected_tool_names` 设为 `[]`，在 `direct_answer_content` 中提供科学解释。
  </thought>
  <output_json>{{
    "task_description": "天空为什么是蓝色的？",
    "reasoning_for_plan": "这是一个常见的科学常识问题，无需外部工具或特定文件查询，可基于LLM的预训练知识直接回答。根据【规则6】。",
    "selected_tool_names": [],
    "direct_answer_content": "天空呈现蓝色主要是因为瑞利散射效应。当太阳光进入地球大气层时，空气中的氮气和氧气等微小粒子会将阳光向各个方向散射。蓝光和紫光的波长较短，比其他颜色的光更容易被散射，因此我们看到的天空主要是这些被散射的蓝光。",
    "tool_input_args": null,
    "excel_sqo_payload": null
  }}</output_json>
</example>

<example>
  <user_query>请帮我分析 "sales_report_Q3.xlsx" 文件中 "产品类别" 列的销售额总和，并按 "区域" 进行分组。</user_query>
  <thought>
    核心意图理解: 用户要求对指定的Excel文件按“区域”和“产品类别”分组，并计算“销售额”的总和。
    规则匹配与工具初选: 根据【规则5：Excel文件操作】，应使用 `excel_operation_tool`。
    工具精选与理由: 这是一个典型的Excel分组聚合操作。
    参数构建逻辑 (SQO):
      - operation_type: "group_by_aggregate"，因为需要分组和聚合。
      - group_by_columns: ["区域", "产品类别"]，根据用户明确指定的分组维度。
      - aggregation_column: "销售额"，用户明确指出要聚合的列。
      - aggregation_function: "sum"，因为用户要求“总和”。
    `excel_sqo_payload` 将包含一个SQO字典。
  </thought>
  <output_json>{{
    "task_description": "请帮我分析 \"sales_report_Q3.xlsx\" 文件中 \"产品类别\" 列的销售额总和，并按 \"区域\" 进行分组。",
    "reasoning_for_plan": "用户明确要求对Excel文件进行分组聚合。根据【规则5】，选择 `excel_operation_tool`。构建了一个 `group_by_aggregate` 类型的SQO，按'区域'和'产品类别'分组，对'销售额'列求和。",
    "selected_tool_names": ["excel_operation_tool"],
    "direct_answer_content": null,
    "tool_input_args": null,
    "excel_sqo_payload": [
      {{
        "operation_type": "group_by_aggregate",
        "group_by_columns": ["区域", "产品类别"],
        "aggregation_column": "销售额",
        "aggregation_function": "sum"
      }}
    ]
  }}</output_json>
</example>

<example>
  <user_query>请告诉我文件 data.xlsx 的 “城市” 列有哪些不同的值，并去除空值？</user_query>
  <thought>
    核心意图理解: 用户想获取Excel文件 "data.xlsx" 中 "城市" 列的所有唯一值，并且不包括空值。
    规则匹配与工具初选: 根据【规则5：Excel文件操作】，应使用 `excel_operation_tool`。
    工具精选与理由: 获取列唯一值是Excel工具支持的操作。
    参数构建逻辑 (SQO):
      - operation_type: "get_unique_values"。
      - column_name: "城市"，用户指定的列。
      - options: {{"drop_na": true}}，因为用户明确要求“去除空值”。
    `excel_sqo_payload` 将包含一个SQO字典。
  </thought>
  <output_json>{{
    "task_description": "请告诉我文件 data.xlsx 的 “城市” 列有哪些不同的值，并去除空值？",
    "reasoning_for_plan": "用户要求获取Excel列的唯一值并去除空值。根据【规则5】，选择 `excel_operation_tool`。构建了一个 `get_unique_values` 类型的SQO，针对'城市'列，并设置 `options.drop_na` 为true。",
    "selected_tool_names": ["excel_operation_tool"],
    "direct_answer_content": null,
    "tool_input_args": null,
    "excel_sqo_payload": [ 
      {{
        "operation_type": "get_unique_values",
        "column_name": "城市",
        "options": {{"drop_na": true}}
      }}
    ]
  }}</output_json>
</example>

<example>
  <user_query>我想知道 "financial_data.xlsx" 表中，对所有记录的 "Revenue" 列求平均值。</user_query>
  <thought>
    核心意图理解: 用户希望计算 "financial_data.xlsx" 文件中 "Revenue" 列的全局平均值。
    规则匹配与工具初选: 根据【规则5：Excel文件操作】，应使用 `excel_operation_tool`。
    工具精选与理由: 计算全局平均值属于聚合操作。
    参数构建逻辑 (SQO):
      - operation_type: "group_by_aggregate"。
      - group_by_columns: [] (空列表表示不按任何特定列分组，即全局聚合)。
      - aggregation_column: "Revenue"，用户指定的列。
      - aggregation_function: "mean"，因为用户要求“平均值”。
    `excel_sqo_payload` 将包含一个SQO字典。
  </thought>
  <output_json>{{
    "task_description": "我想知道 \"financial_data.xlsx\" 表中，对所有记录的 \"Revenue\" 列求平均值。",
    "reasoning_for_plan": "用户要求对Excel的'Revenue'列计算全局平均值。根据【规则5】，选择 `excel_operation_tool`。构建了一个 `group_by_aggregate` 类型的SQO，`group_by_columns` 为空列表以实现全局聚合，对'Revenue'列应用'mean'函数。",
    "selected_tool_names": ["excel_operation_tool"],
    "direct_answer_content": null,
    "tool_input_args": null,
    "excel_sqo_payload": [
      {{
        "operation_type": "group_by_aggregate",
        "group_by_columns": [],
        "aggregation_column": "Revenue",
        "aggregation_function": "mean"
      }}
    ]
  }}</output_json>
</example>

<example>
  <user_query>你能帮我生成一个随机密码吗？</user_query>
  <thought>
    核心意图理解: 用户请求生成一个随机密码。
    规则匹配与工具初选: 规则1-5均不适用。规则6（直接回答）不适用，因为我没有安全的随机密码生成能力，这超出了通用知识。
    工具精选与理由: 无适用工具。根据【规则7：无法处理/需要澄清】，应礼貌地拒绝并说明原因。
    参数构建逻辑: `selected_tool_names` 为 `[]`，在 `direct_answer_content` 中提供解释。
  </thought>
  <output_json>{{
    "task_description": "你能帮我生成一个随机密码吗？",
    "reasoning_for_plan": "用户请求生成随机密码。我没有专门的密码生成工具，也不应该自行创建密码，这超出了我的设计能力和安全范畴。根据【规则7】，我无法处理此请求。",
    "selected_tool_names": [],
    "direct_answer_content": "抱歉，我无法直接为您生成随机密码。建议您使用专业的密码管理器或操作系统自带的密码生成工具来创建安全的随机密码。",
    "tool_input_args": null,
    "excel_sqo_payload": null
  }}</output_json>
</example>

<example>
  <user_query>帮我看看 `test2.xlsx` `Sheet1` 中各个 `区域` 的情况。</user_query>
  <thought>
    核心意图理解: 用户想了解 `test2.xlsx` `Sheet1` 中关于 "区域" 的一些情况，但具体想了解什么并不明确。
    关键信息提取: 文件名 `test2.xlsx`, 工作表 `Sheet1`, 关注点 `区域`。操作类型不明确。
    规则匹配与工具初选: 初步判断可能涉及规则5 (Excel操作)，但也可能因为意图不明而适用规则7 (无法处理/需要澄清)。
    歧义处理与澄清: 用户请求过于宽泛。“看看情况”可以指获取唯一区域列表、统计每个区域的销售额、计数等等。根据【处理不确定性与请求澄清的指导原则】，当核心操作不明确时，必须澄清。
    行动规划: 选择不使用工具，并在 `direct_answer_content` 中提出澄清问题，引导用户明确具体需求。
  </thought>
  <output_json>{{
    "task_description": "帮我看看 `test2.xlsx` `Sheet1` 中各个 `区域` 的情况。",
    "reasoning_for_plan": "用户的请求“看看各个区域的情况”过于宽泛，没有明确指出希望对Excel中的“区域”数据进行何种具体操作（如获取唯一值、按区域汇总销售额、筛选特定区域等）。根据【处理不确定性与请求澄清的指导原则】第1点和第2点，当核心意图和操作参数不明确时，应优先澄清。因此，选择不调用工具，而是向用户提出具体的澄清问题，并提供选项以引导用户明确需求。",
    "selected_tool_names": [],
    "direct_answer_content": "请问您希望对 `test2.xlsx` `Sheet1` 中各个 `区域` 的数据进行哪种具体分析呢？例如：\nA. 列出所有不同的区域名称\nB. 统计每个区域的季度总销售额\nC. 查看某个特定区域（如华东）的详细销售数据\n请告诉我您的具体想法。",
    "tool_input_args": null,
    "excel_sqo_payload": null
  }}</output_json>
</example>

<example>
  <user_query>天空为什么是蓝色的？另外，地球上海洋的面积占比大约是多少？</user_query>
  <thought>
    核心意图理解与问题分解: 用户提出了两个独立的常识性问题：1. 天空为什么是蓝色的？ 2. 海洋占地球面积的比例？
    子问题1分析 ("天空为什么是蓝色的？"): 这是一个大气科学相关的普遍常识，我的预训练知识足以回答。适用规则6。
    子问题2分析 ("海洋占地球面积的比例？"): 这是一个地理常识问题，我的预训练知识也包含相关信息。适用规则6。
    综合判断: 两个子问题均可直接回答。
    工具精选与理由: 无需工具，直接整合答案。
    参数构建逻辑: selected_tool_names 为 [], direct_answer_content 提供两个问题的合并答案。
  </thought>
  <output_json>{{
    "task_description": "天空为什么是蓝色的？海洋占地球多少比例？",
    "reasoning_for_plan": "用户提出了两个独立的常识性问题。第一个关于天空颜色是大气科学常识，第二个关于海洋面积占比是地理常识。这两个问题均符合【规则6】的直接回答条件，我的预训练知识足以覆盖。因此，选择不使用工具，直接整合并提供答案。",
    "selected_tool_names": [],
    "direct_answer_content": "天空呈现蓝色主要是因为瑞利散射效应，阳光中的蓝光更容易被大气散射。地球表面大约71%被水覆盖，其中大部分是海洋。",
    "tool_input_args": null,
    "excel_sqo_payload": null
  }}</output_json>
</example>

<example>
  <user_query>从 `test2.xlsx` 的 `Sheet1` 中，筛选出 `区域` 是 '华东' 或者 '华北' 的记录，并告诉我它们的 `区域` 和 `3月销售额`。</user_query>
  <thought>
    核心意图理解: 用户希望从Excel文件中根据“区域”列的条件（华东或华北）筛选出行，并显示这些行的“区域”和“3月销售额”列。
    规则匹配与工具初选: 根据【规则5：Excel文件操作】，应使用 `excel_operation_tool`。
    工具精选与理由: 这种带有明确筛选条件和列选择的请求，非常适合使用 `direct_sql_query` 操作类型，通过编写SQL语句可以灵活实现。
    参数构建逻辑 (SQO):
      - operation_type: "direct_sql_query"
      - sql_query: "SELECT `区域`, `3月销售额` FROM df WHERE `区域` = '华东' OR `区域` = '华北'"
        (注意SQL中列名未使用反引号，因为示例中的列名不包含空格。如果实际列名包含空格，提示中已指明需要反引号。)
    `excel_sqo_payload` 将包含一个SQO字典。
  </thought>
  <output_json>{{
    "task_description": "从 `test2.xlsx` 的 `Sheet1` 中，筛选出 `区域` 是 '华东' 或者 '华北' 的记录，并告诉我它们的 `区域` 和 `3月销售额`。",
    "reasoning_for_plan": "用户要求根据'区域'列进行条件筛选（'华东'或'华北'），并显示'区域'和'3月销售额'这两列。根据【规则5】，选择 `excel_operation_tool`。考虑到这是一个典型的筛选和列选择需求，使用 `direct_sql_query` 操作类型最为直接和灵活。SQL语句被构建为 `SELECT \`区域\`, \`3月销售额\` FROM df WHERE \`区域\` = ''华东'' OR \`区域\` = ''华北''`。",
    "selected_tool_names": ["excel_operation_tool"],
    "direct_answer_content": null,
    "tool_input_args": null,
    "excel_sqo_payload": [
      {{
        "operation_type": "direct_sql_query",
        "sql_query": "SELECT `区域`, `3月销售额` FROM df WHERE `区域` = '华东' OR `区域` = '华北'"
      }}
    ]
  }}</output_json>
</example>

**【输出格式要求 - 必须严格遵守！】**
你的唯一输出必须是一个JSON对象，符合 `SubTaskDefinitionForManagerOutput` Pydantic模型，包含：
*   `task_description`: (字符串) 用户的原始请求。
*   `reasoning_for_plan`: (字符串) 你做出决策的思考过程，清晰说明你遵循了上述哪条规则和哪个示例，并体现了【结构化思考要求】。
*   `selected_tool_names`: (字符串列表) 选定的工具名称列表。
*   `direct_answer_content`: (可选字符串) 仅在规则6或规则7（澄清或无法处理的礼貌回复）适用时填充。
*   `tool_input_args`: (可选对象) 仅在规则1, 2, 3, 4适用时，为对应工具填充参数。
*   `excel_sqo_payload`: (可选SQO列表) 仅在规则5适用时填充。

我【不】自己执行任何工具操作。我的职责是精准规划并输出结构化的任务定义。
"""
    --- END OF FILE CONTENT ---


Directory: core/grammars/
  File (skipped, extension not included): cypher_or_unable_output.gbnf

Directory: core/agents/
  File: __init__.py
    --- File is empty ---


Directory: core/tools/
  File: __init__.py
    --- File is empty ---

  File: calculator_tool.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/core/tools/calculator_tool.py
import logging
import re
# --- 添加开始 ---
import math 
# --- 添加结束 ---
from typing import Type, Any
from pydantic import BaseModel, Field
from crewai.tools import BaseTool

logger = logging.getLogger(__name__)

class CalculateToolInput(BaseModel):
    expression: str = Field(description="要计算的数学表达式字符串。例如：'(10 + 20) * 3 / 5 - 10'")

class CalculateTool(BaseTool):
    name: str = "calculate_tool"
    description: str = (
        "【计算器工具】执行数学表达式的计算并返回数值结果。支持常见的算术运算（加、减、乘、除、括号、幂运算、取模）以及部分数学函数 (如 sqrt, factorial, sin, cos, tan, log, log10, exp, degrees, radians)。" # <--- 更新描述
    )
    args_schema: Type[BaseModel] = CalculateToolInput

    def _run(self, expression: str, **kwargs: Any) -> str:
        logger.info(f"CalculateTool._run called with expression: '{expression}'")
        
        # 安全性检查：允许字母（用于函数名如sqrt, factorial, sin等）
        # 但仍然需要小心，更安全的做法是解析表达式并只允许白名单中的函数
        if not re.match(r"^[0-9a-zA-Z_+\-*/().\s%^!]+$", expression):
            logger.error(f"Invalid characters in expression: '{expression}'")
            return "错误：表达式中包含无效字符。"
        
        if len(expression) > 200: # 稍微放宽长度限制
            logger.error(f"Expression too long: '{expression}'")
            return "错误：表达式过长。"

        try:
            # --- 修改开始：准备安全的命名空间 ---
            safe_globals = {
                "__builtins__": {}, # 限制内置函数
                "sqrt": math.sqrt,
                "factorial": math.factorial,
                "sin": math.sin,
                "cos": math.cos,
                "tan": math.tan,
                "log": math.log,
                "log10": math.log10,
                "exp": math.exp,
                "pow": pow, # math.pow 和内置 pow 行为类似，但内置的更通用
                "pi": math.pi,
                "e": math.e,
                "degrees": math.degrees,
                "radians": math.radians,
                # 你可以根据需要添加更多 math 模块中的安全函数
            }
            # 为了支持 x^y 这样的幂运算，eval本身用 **
            # 如果用户输入了 ^, 我们可以在这里替换一下，或者告知用户使用 **
            expression_to_eval = expression.replace('^', '**')
            # 移除阶乘符号 `!` 后的空格，并替换为 `factorial()`
            # 注意：这只是一个简单的替换，可能无法处理所有复杂情况，例如 `(5+2)!`
            expression_to_eval = re.sub(r"(\d+)\s*!", r"factorial(\1)", expression_to_eval)

            result = eval(expression_to_eval, safe_globals, {}) # 使用受限的命名空间
            # --- 修改结束 ---
            logger.info(f"Expression '{expression}' (evaluated as '{expression_to_eval}') evaluated to: {result}")
            return f"计算结果: {expression} = {result}"
        except NameError as e:
            logger.error(f"Error evaluating expression '{expression}': NameError - {e}")
            return f"错误：表达式中使用了未定义的函数或变量 (例如 '{e.name}')。支持的函数包括：sqrt, factorial, sin, cos, tan, log, log10, exp。"
        except ZeroDivisionError:
            logger.error(f"Error evaluating expression '{expression}': Division by zero")
            return "错误：表达式中存在除以零的操作。"
        except SyntaxError:
            logger.error(f"Error evaluating expression '{expression}': Syntax error")
            return "错误：数学表达式语法错误。"
        except Exception as e:
            logger.error(f"Error evaluating expression '{expression}': {e}", exc_info=True)
            return f"计算表达式时发生错误: {str(e)}"
    --- END OF FILE CONTENT ---

  File: enhanced_rag_tool.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/core/tools/enhanced_rag_tool.py

import asyncio
from typing import Type, Optional, Dict, Any
from pydantic import BaseModel, Field
from crewai.tools import BaseTool
import json
import traceback

from utils.common_utils import call_mcpo_tool

import logging
logger = logging.getLogger(__name__)


class EnhancedRAGToolInput(BaseModel):
    query: str = Field(description="用户提出的原始查询文本。")
    top_k_vector: int = Field(default=5, description="期望检索的向量搜索结果数量。")
    top_k_kg: int = Field(default=3, description="期望检索的知识图谱结果数量。")
    top_k_bm25: int = Field(default=3, description="期望检索的 BM25 关键词搜索结果数量。")

class EnhancedRAGTool(BaseTool):
    name: str = "enhanced_rag_tool"
    description: str = "【核心RAG工具】用于从本地知识库查找信息、回答复杂问题，整合了向量、关键词和图谱检索。"
    args_schema: Type[BaseModel] = EnhancedRAGToolInput
    mcp_service_name: str = "zhz_rag_mcp_service"
    mcp_tool_path: str = "query_rag_v2"

    async def _acall_mcp(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        """
        异步调用 MCP 工具服务，并处理其包装响应。
        目标是返回 RAG 服务本身的业务响应字典。
        """
        tool_path_on_mcp = f"{self.mcp_service_name}/{self.mcp_tool_path}"
        logger.critical(f"!!! EnhancedRAGTool._acall_mcp: ENTERING. Calling MCP endpoint '{tool_path_on_mcp}' with payload: {payload}")
        try:
            mcp_wrapper_response = await call_mcpo_tool(tool_path_on_mcp, payload)

            logger.critical(f"!!! EnhancedRAGTool._acall_mcp: RECEIVED from call_mcpo_tool - Type: {type(mcp_wrapper_response)}")
            if isinstance(mcp_wrapper_response, dict):
                logger.critical(f"!!! EnhancedRAGTool._acall_mcp: RECEIVED keys: {list(mcp_wrapper_response.keys())}")
            logger.critical(f"!!! EnhancedRAGTool._acall_mcp: RECEIVED content preview: {str(mcp_wrapper_response)[:500]}")

            # 场景1: call_mcpo_tool 返回 MCP 框架的成功响应 ({"success": True, "data": RAG_response})
            if isinstance(mcp_wrapper_response, dict) and mcp_wrapper_response.get("success") is True:
                rag_service_data = mcp_wrapper_response.get("data")
                if isinstance(rag_service_data, dict):
                    logger.info("!!! EnhancedRAGTool._acall_mcp: MCP call successful (success:True), returning RAG 'data' field.")
                    return rag_service_data
                else:
                    logger.error(f"!!! EnhancedRAGTool._acall_mcp: MCP 'data' field is not a dict. Got {type(rag_service_data)}. Payload: {payload}")
                    return {
                        "status": "error", "original_query": payload.get("query", "N/A"), "retrieved_context_docs": [],
                        "error_message": f"Internal error: RAG service data from MCP was not a dict. Got: {str(rag_service_data)[:200]}",
                        "error_code": "INVALID_RAG_DATA_FROM_MCP_SUCCESS"
                    }

            # 场景2: call_mcpo_tool 返回 MCP 框架的失败响应 ({"success": False, "error": ...})
            elif isinstance(mcp_wrapper_response, dict) and mcp_wrapper_response.get("success") is False:
                error_msg = mcp_wrapper_response.get("error", "Unknown error from MCP call wrapper.")
                error_type = mcp_wrapper_response.get("error_type", "MCP_CALL_FAILED")
                status_code = mcp_wrapper_response.get("status_code")
                logger.error(f"!!! EnhancedRAGTool._acall_mcp: MCP call failed (success:False). Type: {error_type}, Error: {error_msg}, Status Code: {status_code}")
                return { # 构造一个符合RAG错误格式的字典
                    "status": "error", "original_query": payload.get("query", "N/A"), "retrieved_context_docs": [],
                    "error_message": error_msg, "error_code": error_type,
                    "debug_info": {"mcp_error_type": error_type, "mcp_status_code": status_code}
                }

            # 场景3: call_mcpo_tool 直接返回了 RAG 服务的业务 JSON (例如，包含 "status" 但不含 "success")
            # 这种情况通常是 MCP 代理透传了下游服务的响应。
            elif isinstance(mcp_wrapper_response, dict) and "status" in mcp_wrapper_response:
                logger.info("!!! EnhancedRAGTool._acall_mcp: Received a direct business response from RAG service (contains 'status' key).")
                return mcp_wrapper_response

            # 场景4: call_mcpo_tool 返回了其他无法识别的字典格式
            elif isinstance(mcp_wrapper_response, dict):
                logger.error(f"!!! EnhancedRAGTool._acall_mcp: call_mcpo_tool returned an unexpected dictionary format: {str(mcp_wrapper_response)[:200]}")
                return {
                    "status": "error", "original_query": payload.get("query", "N/A"), "retrieved_context_docs": [],
                    "error_message": f"Internal error: MCP wrapper response format unknown. Got: {str(mcp_wrapper_response)[:200]}",
                    "error_code": "INVALID_MCP_WRAPPER_DICT_FORMAT_UNKNOWN"
                }
            
            # 场景5: call_mcpo_tool 返回的不是字典
            else:
                logger.error(f"!!! EnhancedRAGTool._acall_mcp: call_mcpo_tool returned non-dict type: {type(mcp_wrapper_response)}. Content: {str(mcp_wrapper_response)[:200]}")
                return {
                    "status": "error", "original_query": payload.get("query", "N/A"), "retrieved_context_docs": [],
                    "error_message": f"Internal error: MCP wrapper returned non-dict. Got: {str(mcp_wrapper_response)[:200]}",
                    "error_code": "INVALID_MCP_WRAPPER_NON_DICT"
                }

        except Exception as e_acall:
            logger.critical(f"!!! EnhancedRAGTool._acall_mcp: EXCEPTION during call_mcpo_tool or its processing: {e_acall}", exc_info=True)
            return {
                "status": "error", "original_query": payload.get("query", "N/A"), "retrieved_context_docs": [],
                "error_message": f"Exception in _acall_mcp: {str(e_acall)}",
                "error_code": "ACALL_MCP_UNHANDLED_EXCEPTION",
                "traceback": traceback.format_exc()
            }

    def _run(
        self,
        query: str,
        top_k_vector: int = 5,
        top_k_kg: int = 3,
        top_k_bm25: int = 3,
        **kwargs: Any
    ) -> str:
        logger.critical(f"!!! EnhancedRAGTool._run: ENTERING. Query: '{query}'")
        payload = {
            "query": query,
            "top_k_vector": top_k_vector,
            "top_k_kg": top_k_kg,
            "top_k_bm25": top_k_bm25,
        }
        rag_service_response_data: Dict[str, Any] = {
            "status": "error", "original_query": query, "retrieved_context_docs": [],
            "error_message": "Initialization error in _run before async call",
            "error_code": "RUN_INIT_ERROR"
        }

        try:
            logger.critical("!!! EnhancedRAGTool._run: Attempting to get event loop...")
            loop = asyncio.get_event_loop()
            logger.critical(f"!!! EnhancedRAGTool._run: Event loop obtained. Is running: {loop.is_running()}")

            if loop.is_running():
                logger.critical("!!! EnhancedRAGTool._run: Loop is running. Using ThreadPoolExecutor.")
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                    logger.critical("!!! EnhancedRAGTool._run: Submitting _acall_mcp to executor...")
                    future = executor.submit(asyncio.run, self._acall_mcp(payload))
                    logger.critical("!!! EnhancedRAGTool._run: Waiting for future.result()...")
                    rag_service_response_data = future.result(timeout=120)
                    logger.critical("!!! EnhancedRAGTool._run: future.result() RETURNED.")
            else:
                logger.critical("!!! EnhancedRAGTool._run: Loop is NOT running. Using asyncio.run directly.")
                rag_service_response_data = asyncio.run(self._acall_mcp(payload))
                logger.critical("!!! EnhancedRAGTool._run: asyncio.run(_acall_mcp) RETURNED.")

        except asyncio.TimeoutError as e_timeout:
            logger.critical(f"!!! EnhancedRAGTool._run: ASYNCIO TIMEOUT during RAG tool execution: {e_timeout}", exc_info=True)
            rag_service_response_data = {
                "status": "error", "original_query": query, "retrieved_context_docs": [],
                "error_message": f"RAG tool execution timed out: {str(e_timeout)}",
                "error_code": "ASYNC_TIMEOUT_IN_RUN"
            }
        except Exception as e_run:
            logger.critical(f"!!! EnhancedRAGTool._run: EXCEPTION during RAG tool execution: {e_run}", exc_info=True)
            rag_service_response_data = {
                "status": "error", "original_query": query, "retrieved_context_docs": [],
                "error_message": f"Exception in _run: {str(e_run)}",
                "error_code": "RUN_UNHANDLED_EXCEPTION",
                "traceback": traceback.format_exc()
            }

        logger.critical(f"!!! EnhancedRAGTool._run: Raw RAG service data before _handle_mcp_result: {str(rag_service_response_data)[:500]}")

        if not isinstance(rag_service_response_data, dict):
            logger.error(f"!!! EnhancedRAGTool._run: rag_service_response_data is not a dict, type: {type(rag_service_response_data)}. Wrapping in error dict.")
            rag_service_response_data = {
                "status": "error", "original_query": query, "retrieved_context_docs": [],
                "error_message": f"Internal error: _acall_mcp did not return a dict as expected. Got: {str(rag_service_response_data)[:200]}",
                "error_code": "INVALID_ACALL_RETURN_TYPE_FINAL"
            }
        # 确保 rag_service_response_data 至少包含 'status' 键，如果它在 _acall_mcp 中因为某些路径没有返回标准错误结构
        if "status" not in rag_service_response_data:
            logger.warning(f"!!! EnhancedRAGTool._run: rag_service_response_data missing 'status' key. Data: {str(rag_service_response_data)[:200]}. Defaulting to error status.")
            rag_service_response_data["status"] = "error"
            rag_service_response_data.setdefault("error_message", "Malformed response from RAG service call.")
            rag_service_response_data.setdefault("error_code", "MALFORMED_RAG_RESPONSE_IN_RUN")
            rag_service_response_data.setdefault("original_query", query)
            rag_service_response_data.setdefault("retrieved_context_docs", [])


        return self._handle_mcp_result(rag_service_response_data)

    def _handle_mcp_result(self, rag_service_data: Dict[str, Any]) -> str:
        """
        处理来自 RAG 服务 (zhz_rag_mcp_service) 的直接响应。
        """
        logger.info(f"EnhancedRAGTool._handle_mcp_result received RAG service data: {str(rag_service_data)[:500]}...")

        if not isinstance(rag_service_data, dict):
            error_msg = f"TOOL_ERROR: {self.name} received an invalid response format (expected dict from RAG service, got {type(rag_service_data)}). Content: {str(rag_service_data)[:200]}"
            logger.error(error_msg)
            return error_msg

        status_from_rag = rag_service_data.get("status")

        if status_from_rag == "success":
            final_answer = rag_service_data.get("final_answer")
            retrieved_docs_raw = rag_service_data.get("retrieved_context_docs")

            if final_answer is None:
                no_answer_msg = f"TOOL_INFO: {self.name} RAG service status is 'success', but did not provide a final_answer."
                logger.warning(no_answer_msg)
                return "RAG service processed successfully but found no specific answer."

            response_parts = [f"RAG Answer: {str(final_answer).strip()}"]
            if retrieved_docs_raw and isinstance(retrieved_docs_raw, list) and retrieved_docs_raw:
                response_parts.append("\n\nSupporting Context Snippets (Top 2):")
                for i, doc_data in enumerate(retrieved_docs_raw[:2]):
                    content = doc_data.get("content", "N/A")
                    source = doc_data.get("source_type", "N/A")
                    score = doc_data.get("score")
                    score_str = f"{score:.2f}" if isinstance(score, float) else str(score if score is not None else 'N/A')
                    response_parts.append(f"  - Source: {source}, Score: {score_str}, Content: {str(content)[:100]}...")

            final_tool_output_str = "\n".join(response_parts)
            logger.info(f"EnhancedRAGTool: Successfully processed RAG success response. Output for agent (first 200 chars): {final_tool_output_str[:200]}...")
            return final_tool_output_str

        elif status_from_rag == "clarification_needed":
            clarification_question = rag_service_data.get("clarification_question", "需要您提供更多信息。")
            uncertainty_reason = rag_service_data.get("debug_info", {}).get("uncertainty_reason", "未知原因")
            clarification_output = f"CLARIFICATION_NEEDED: {clarification_question} (Reason: {uncertainty_reason})"
            logger.info(f"EnhancedRAGTool: RAG service requires clarification. Output for agent: {clarification_output}")
            return clarification_output

        elif status_from_rag == "error":
            rag_error_msg = rag_service_data.get("error_message", "RAG服务内部发生未知错误。")
            rag_error_code = rag_service_data.get("error_code", "RAG_UNKNOWN_ERROR")
            error_output = f"TOOL_ERROR: {self.name} failed. RAG service reported an error (Code: {rag_error_code}): {rag_error_msg}"
            logger.error(error_output)
            return error_output
        else:
            unknown_status_msg = f"TOOL_ERROR: {self.name} received an unknown or missing status '{status_from_rag}' from RAG service."
            logger.error(f"{unknown_status_msg} Full RAG data: {str(rag_service_data)[:300]}")
            return unknown_status_msg
    --- END OF FILE CONTENT ---

  File: excel_tool.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/core/tools/excel_tool.py

import httpx
import json
from typing import Type, Dict, Any, Union, List, Optional # 确保导入了 List
from pydantic import BaseModel, Field
from crewai.tools import BaseTool
import os
import logging
import traceback # 确保导入 traceback

logger = logging.getLogger(__name__)

# 从环境变量读取 Windows Host IP 和 Local Agent 端口
WINDOWS_HOST_IP = os.getenv("WINDOWS_HOST_IP_FOR_WSL", "192.168.3.11") 
LOCAL_AGENT_PORT = os.getenv("LOCAL_AGENT_PORT", "8003")
LOCAL_AGENT_EXCEL_SQO_ENDPOINT = f"http://{WINDOWS_HOST_IP}:{LOCAL_AGENT_PORT}/excel_sqo_mcp/execute_operation"


class ExcelOperationToolInput(BaseModel):
    # Worker Agent 会迭代 Manager 生成的 SQO 列表，
    # 每次调用这个工具时，传递一个 SQO 字典，以及 file_path 和 sheet_name。
    # sqo_dict 已经是 Manager 生成的 SQO 操作定义，不包含 file_path 和 sheet_name。
    # file_path 和 sheet_name 由 Worker Agent 从用户原始请求中提取并传入。
    
    sqo_operation_definition: Dict[str, Any] = Field(description="单个SQO操作定义字典，不包含file_path和sheet_name。")
    target_excel_file_path: str = Field(description="目标Excel文件的绝对路径。")
    target_excel_sheet_name: Union[str, int] = Field(default=0, description="目标工作表的名称或索引 (0-based)。")

class ExcelOperationTool(BaseTool):
    name: str = "excel_operation_tool"
    description: str = ("【Excel操作工具】通过结构化查询对象(SQO)对Excel文件执行复杂的数据查询、筛选、聚合等操作。"
                        "此工具接收单个SQO操作定义、文件路径和工作表名，并调用在Windows上运行的本地代理来执行实际操作。")
    args_schema: Type[BaseModel] = ExcelOperationToolInput

    def _run(
        self,
        sqo_operation_definition: Dict[str, Any],
        target_excel_file_path: str,
        target_excel_sheet_name: Union[str, int] = 0,
        **kwargs: Any
    ) -> str:
        logger.info(f"ExcelOperationTool._run called. File: '{target_excel_file_path}', Sheet: '{target_excel_sheet_name}'")
        logger.debug(f"SQO Operation Definition received: {sqo_operation_definition}")

        if not isinstance(sqo_operation_definition, dict):
            return "错误：传递给 ExcelOperationTool 的 sqo_operation_definition 参数必须是一个字典。"
        if not sqo_operation_definition.get("operation_type"):
            return "错误：SQO操作定义字典中缺少 'operation_type' 字段。"

        # 构造发送给 local_agent_app.py 的完整SQO，补充 file_path 和 sheet_name
        full_sqo_for_local_agent = sqo_operation_definition.copy()
        full_sqo_for_local_agent["file_path"] = target_excel_file_path
        full_sqo_for_local_agent["sheet_name"] = target_excel_sheet_name
        
        # local_agent_app.py 的 /excel_sqo_mcp/execute_operation 端点期望的请求体是 ExecuteSQORequest 模型，
        # 它有一个名为 'sqo' 的字段，其值才是我们这里的 full_sqo_for_local_agent
        request_payload_to_local_agent = {"sqo": full_sqo_for_local_agent}

        logger.info(f"Sending request to Local Excel Agent: {LOCAL_AGENT_EXCEL_SQO_ENDPOINT}")
        logger.debug(f"Payload to Local Excel Agent: {json.dumps(request_payload_to_local_agent, ensure_ascii=False)}")

        try:
            with httpx.Client(timeout=120.0) as client: # 增加超时
                response = client.post(LOCAL_AGENT_EXCEL_SQO_ENDPOINT, json=request_payload_to_local_agent)
            
            logger.info(f"Local Excel Agent response status: {response.status_code}")
            response_json = response.json() # local_agent_app.py 返回的是 SQOResponse 模型

            if response.status_code == 200 and response_json.get("success"):
                result = response_json.get("result")
                logger.info(f"Excel operation successful. Result type: {type(result)}")
                if isinstance(result, (list, dict)):
                    try:
                        # 对于列表或字典，以JSON字符串形式返回给Agent通常更易于处理
                        return json.dumps(result, ensure_ascii=False, indent=2)
                    except TypeError: # 处理无法JSON序列化的类型
                        return str(result)
                return str(result) # 其他类型直接转字符串
            else:
                error_message = response_json.get("error", "未知错误")
                error_details = response_json.get("error_details")
                full_error = f"本地Excel代理错误: {error_message}"
                if error_details:
                    full_error += f" | 详情: {str(error_details)[:300]}" # 限制详情长度
                logger.error(full_error + f" | Full SQO sent: {json.dumps(full_sqo_for_local_agent, ensure_ascii=False)}")
                return full_error

        except httpx.TimeoutException as e:
            error_msg = f"调用本地Excel代理超时: {e}"
            logger.error(error_msg, exc_info=True)
            return error_msg
        except httpx.ConnectError as e:
            error_msg = f"无法连接到本地Excel代理 ({LOCAL_AGENT_EXCEL_SQO_ENDPOINT}): {e}. 请确保Windows端的local_agent_app.py正在运行，并且WSL可以访问到Windows的IP和端口。"
            logger.error(error_msg, exc_info=True)
            return error_msg
        except httpx.RequestError as e:
            error_msg = f"请求本地Excel代理时发生网络错误: {e}"
            logger.error(error_msg, exc_info=True)
            return error_msg
        except json.JSONDecodeError as e:
            error_msg = f"解析本地Excel代理响应JSON时出错: {e}. 响应文本: {response.text[:300]}"
            logger.error(error_msg, exc_info=True)
            return error_msg
        except Exception as e:
            error_msg = f"执行Excel操作时发生未知错误: {e}"
            logger.error(error_msg, exc_info=True)
            return f"{error_msg} | Traceback: {traceback.format_exc()[:500]}"
    --- END OF FILE CONTENT ---

  File: search_tool.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/core/tools/search_tool.py

import asyncio
from typing import Type, Optional, Dict, Any, List
from pydantic import BaseModel, Field
from crewai.tools import BaseTool
import json
from utils.common_utils import call_mcpo_tool # 确保从正确的相对路径导入
import traceback

# --- 日志记录 ---
import logging
logger = logging.getLogger(__name__)


class WebSearchToolInput(BaseModel):
    query: str = Field(description="要进行网络搜索的关键词或问题。")
    max_results: Optional[int] = Field(default=5, description="希望返回的最大搜索结果数量。")

class WebSearchTool(BaseTool):
    name: str = "web_search_tool" # Agent将使用这个名字
    description: str = ("【网络搜索工具】使用DuckDuckGo搜索引擎在互联网上查找与用户查询相关的信息。"
                        "返回搜索结果列表，每个结果包含标题、链接和摘要。")
    args_schema: Type[BaseModel] = WebSearchToolInput
    mcp_service_name: str = "ddgsearch" # 与 mcpo_servers.json 中定义的服务名一致
    mcp_tool_path: str = "search"      # DuckDuckGo 搜索服务提供的端点

    async def _acall_mcp(self, payload: Dict[str, Any]) -> Dict[str, Any]:
        """
        实际调用 MCP 服务的异步方法。
        """
        tool_path_on_mcp = f"{self.mcp_service_name}/{self.mcp_tool_path}"
        logger.info(f"WebSearchTool: Calling MCP endpoint '{tool_path_on_mcp}' with payload: {payload}")
        # call_mcpo_tool 期望返回一个包含 success 和 data/error 的字典
        mcp_wrapper_response = await call_mcpo_tool(tool_path_on_mcp, payload)
        
        # 直接返回 call_mcpo_tool 的结果，让 _handle_mcp_result 处理
        return mcp_wrapper_response

    def _run(
        self,
        query: str,
        max_results: Optional[int] = 5,
        **kwargs: Any
    ) -> str:
        """
        同步执行方法，调用 MCP 的 ddgsearch 服务。
        """
        logger.info(f"WebSearchTool._run called with query: '{query}', max_results: {max_results}")
        payload = {
            "query": query,
            "max_results": max_results,
        }
        
        mcp_response_data: Dict[str, Any] = {
            "success": False, # 默认失败
            "error": "Initialization error in _run before async call for WebSearchTool",
            "error_type": "RUN_INIT_ERROR_WEBSEARCH"
        }

        try:
            loop = asyncio.get_event_loop()
            if loop.is_running():
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                    future = executor.submit(asyncio.run, self._acall_mcp(payload))
                    mcp_response_data = future.result(timeout=60) 
            else:
                mcp_response_data = asyncio.run(self._acall_mcp(payload))
        except Exception as e:
            logger.error(f"Error running WebSearchTool for query '{query}': {e}", exc_info=True)
            mcp_response_data = {
                "success": False,
                "error": f"Exception in WebSearchTool _run: {str(e)}",
                "error_type": "RUN_EXCEPTION_WEBSEARCH",
                "traceback": traceback.format_exc()
            }
        
        return self._handle_mcp_result(mcp_response_data)


    def _handle_mcp_result(self, mcp_response: Dict[str, Any]) -> str:
        logger.info(f"WebSearchTool._handle_mcp_result received raw MCP wrapper response: {str(mcp_response)[:1000]}...")

        if not isinstance(mcp_response, dict):
            return f"TOOL_ERROR: {self.name} received invalid response format from MCP call wrapper (expected dict, got {type(mcp_response)})."

        if mcp_response.get("success") is False:
            error_msg = mcp_response.get("error", "Unknown error from ddgsearch MCP call via wrapper.")
            error_type = mcp_response.get("error_type", "MCP_CALL_FAILED")
            logger.error(f"WebSearchTool failed via MCP wrapper. Type: {error_type}, Error: {error_msg}")
            return f"TOOL_ERROR: {self.name} failed: {error_msg}"

        ddg_service_data_wrapper = mcp_response.get("data")

        if isinstance(ddg_service_data_wrapper, dict) and "content" in ddg_service_data_wrapper:
            content_list = ddg_service_data_wrapper.get("content")
            if isinstance(content_list, list) and content_list:
                first_content_item = content_list[0]
                if isinstance(first_content_item, dict) and first_content_item.get("type") == "text":
                    search_results_text = first_content_item.get("text")
                    if search_results_text and search_results_text.strip():
                        logger.info("WebSearchTool: Successfully extracted search results text from MCP 'data.content' field.")
                        # 直接返回从ddgsearch获取的原始文本结果，让Worker Agent去理解和总结
                        return search_results_text.strip()
                    else:
                        logger.warning("WebSearchTool: Extracted search results text from 'data.content' is empty.")
                        return "网络搜索没有找到相关结果（返回内容为空）。"
        
        # 如果上面的新逻辑没有成功提取，尝试旧的直接从 data 字段解析（以防万一 call_mcpo_tool 的行为与预期不完全一致）
        # 或者 ddgsearch 服务直接返回了 JSON 格式的 results 列表 (虽然目前不是这样)
        if isinstance(ddg_service_data_wrapper, dict) and "results" in ddg_service_data_wrapper:
            search_results = ddg_service_data_wrapper.get("results")
            if isinstance(search_results, list):
                if not search_results:
                    return "网络搜索没有找到相关结果。"
                # 为了简化，我们不再由工具本身进行格式化，而是直接返回JSON字符串或原始文本
                # 让Worker Agent来处理最终呈现给用户的格式
                logger.info("WebSearchTool: Found 'results' list in ddg_service_data. Returning as JSON string.")
                try:
                    return json.dumps(search_results, ensure_ascii=False, indent=2)
                except Exception as e_json:
                    logger.error(f"WebSearchTool: Could not serialize 'results' to JSON: {e_json}")
                    return f"网络搜索结果无法序列化为JSON: {str(search_results)[:300]}"

        # 如果MCP返回的data本身就是字符串（这符合ddgsearch的当前行为）
        if isinstance(ddg_service_data_wrapper, str) and ddg_service_data_wrapper.strip():
            logger.info("WebSearchTool: MCP 'data' field is a non-empty string. Returning it directly.")
            return ddg_service_data_wrapper.strip()
        elif isinstance(ddg_service_data_wrapper, str) and not ddg_service_data_wrapper.strip():
            logger.warning("WebSearchTool: MCP 'data' field is an empty string.")
            return "网络搜索没有找到相关结果（返回内容为空）。"
        # --- 修改结束 ---
        
        logger.warning(f"WebSearchTool: Could not extract search results as expected. Raw 'data' from MCP: {str(ddg_service_data_wrapper)[:500]}")
        return f"网络搜索未能获取到预期的结果格式。服务原始响应 (data part): {json.dumps(ddg_service_data_wrapper, ensure_ascii=False, indent=2) if isinstance(ddg_service_data_wrapper, dict) else str(ddg_service_data_wrapper)[:300]}"
    --- END OF FILE CONTENT ---

  File: time_tool.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/core/tools/time_tool.py
from datetime import datetime
import pytz # 用于处理时区
from typing import Type, Optional, Any
from pydantic import BaseModel, Field
from crewai.tools import BaseTool
import logging

logger = logging.getLogger(__name__)

class GetCurrentTimeToolInput(BaseModel):
    timezone_str: Optional[str] = Field(
        default="Asia/Shanghai", 
        description="可选参数。IANA时区名称，例如 'Asia/Shanghai', 'America/New_York', 'UTC'. 默认为 'Asia/Shanghai'."
    )

class GetCurrentTimeTool(BaseTool):
    name: str = "get_current_time_tool"
    description: str = (
        "【时间工具】获取并返回当前的日期和时间。可以指定时区（例如 'Asia/Shanghai', 'America/New_York', 'UTC'），"
        "如果未指定，则默认为 'Asia/Shanghai' (中国标准时间)。"
        "当你需要知道“现在几点了”、“今天是什么日期”或在执行与时间相关的操作（如设置提醒）前获取基准时间时使用。"
    )
    args_schema: Type[BaseModel] = GetCurrentTimeToolInput

    def _run(self, timezone_str: Optional[str] = "Asia/Shanghai", **kwargs: Any) -> str:
        logger.info(f"GetCurrentTimeTool._run called with timezone_str: '{timezone_str}'")
        try:
            if not timezone_str or not timezone_str.strip():
                effective_timezone_str = "Asia/Shanghai"
                logger.info(f"Timezone was empty, defaulting to {effective_timezone_str}")
            else:
                effective_timezone_str = timezone_str

            target_tz = pytz.timezone(effective_timezone_str)
            now_in_tz = datetime.now(target_tz)
            formatted_time = now_in_tz.strftime("%Y-%m-%d %H:%M:%S %Z%z")
            logger.info(f"Current time in {effective_timezone_str}: {formatted_time}")
            return f"当前时间 ({effective_timezone_str}): {formatted_time}"
        except pytz.exceptions.UnknownTimeZoneError:
            logger.error(f"Unknown timezone provided: '{timezone_str}'")
            return f"错误：提供的时区 '{timezone_str}' 无效。请使用标准的IANA时区名称。"
        except Exception as e:
            logger.error(f"Error in GetCurrentTimeTool: {e}", exc_info=True)
            return f"获取当前时间时发生错误: {str(e)}"
    --- END OF FILE CONTENT ---


Directory: core/llms/
  File: __init__.py
    --- File is empty ---

  File: custom_litellm_wrapper.py
    --- START OF FILE CONTENT ---
# /home/zhz/zhz_agent/core/llms/custom_litellm_wrapper.py

import os
import json
import traceback
from typing import List, Dict, Any, Optional, Union, Sequence

import litellm
from crewai.llms.base_llm import BaseLLM as CrewAIBaseLLM
from crewai.tools import BaseTool # 用于类型提示

# --- 日志记录 ---
import logging
logger = logging.getLogger(__name__)
# logger.setLevel(logging.DEBUG) # 可以根据需要调整日志级别
# if not logger.hasHandlers():
#     handler = logging.StreamHandler()
#     formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
#     handler.setFormatter(formatter)
#     logger.addHandler(handler)

class CustomLiteLLMWrapper(CrewAIBaseLLM):
    """
    一个通用的 CrewAI LLM 包装器，通过 LiteLLM 调用各种 LLM 服务。
    能够处理本地 OpenAI 兼容的端点和通过 LiteLLM 支持的云端模型。
    """
    model_name: str # LiteLLM 使用的模型名称，例如 "gemini/gemini-1.5-flash-latest" 或 "local/qwen3-1.7b-gguf"
    api_base: Optional[str] = None
    api_key: Optional[str] = None
    custom_llm_provider: Optional[str] = None # 例如 "openai" 用于本地 OpenAI 兼容服务
    
    # LiteLLM 支持的额外参数，例如 temperature, max_tokens, top_p 等
    # 这些参数可以在实例化时传入，或者在调用 call 方法时覆盖
    litellm_params: Dict[str, Any] = {}

    # CrewAI 工具相关的参数
    tool_config: Optional[Dict[str, Any]] = None # 用于控制工具调用的模式，例如 Gemini 的 function_calling_config
    _cached_tools_for_litellm: Optional[List[Dict[str, Any]]] = None # 缓存转换后的工具定义

    def __init__(
        self,
        model: str, # CrewAI BaseLLM 需要 model 参数
        api_base: Optional[str] = None,
        api_key: Optional[str] = None,
        custom_llm_provider: Optional[str] = None,
        temperature: float = 0.7, # 默认温度
        max_tokens: Optional[int] = 2048, # 默认最大 token 数
        top_p: Optional[float] = None,
        stop_sequences: Optional[List[str]] = None,
        tool_config: Optional[Dict[str, Any]] = None, # 例如 {"function_calling_config": {"mode": "AUTO"}}
        agent_tools: Optional[List[BaseTool]] = None, # CrewAI Agent 的工具列表，用于转换为 LiteLLM 格式
        **kwargs: Any # 其他传递给 LiteLLM 的参数
    ):
        super().__init__(model=model) # 调用父类的构造函数

        # --- 新增日志 ---
        logger.info(f"CustomLiteLLMWrapper __init__ for '{model}': Received agent_tools type: {type(agent_tools)}")
        if agent_tools is not None:
            logger.info(f"CustomLiteLLMWrapper __init__ for '{model}': agent_tools content (names): {[tool.name for tool in agent_tools if hasattr(tool, 'name')]}")
        else:
            logger.info(f"CustomLiteLLMWrapper __init__ for '{model}': agent_tools is None.")
        # --- 结束新增日志 ---
        
        self.model_name = model
        self.api_base = api_base
        self.api_key = api_key
        self.custom_llm_provider = custom_llm_provider
        
        self.litellm_params = {
            "temperature": temperature,
            "max_tokens": max_tokens,
            "top_p": top_p,
            "stop": stop_sequences,
            **kwargs # 合并其他未知参数
        }
        # 清理 litellm_params 中的 None 值
        self.litellm_params = {k: v for k, v in self.litellm_params.items() if v is not None}

        self.tool_config = tool_config
        if agent_tools:
            self._cached_tools_for_litellm = self._convert_crewai_tools_to_litellm_format(agent_tools)
            logger.info(f"CustomLiteLLMWrapper for '{self.model_name}': Cached {len(self._cached_tools_for_litellm)} tools.")
        else:
            logger.info(f"CustomLiteLLMWrapper for '{self.model_name}': No agent_tools provided for caching.")

    def _remove_unwanted_fields_from_schema(self, schema: Dict[str, Any]) -> Dict[str, Any]:
        """
        递归移除 Pydantic schema 中可能导致某些 LLM (如 Gemini) 报错的字段，
        例如顶层的 'title' 和属性定义中的 'default'。
        也确保 'object' 类型的 schema 包含 'properties' 键。
        """
        if not isinstance(schema, dict):
            return schema

        schema.pop('title', None) # 移除顶层 title

        if "properties" in schema and isinstance(schema["properties"], dict):
            if "type" not in schema: # 确保 object 类型有 type 字段
                schema["type"] = "object"
            for prop_name, prop_def in list(schema["properties"].items()): # 使用 list 进行迭代以允许修改
                if isinstance(prop_def, dict):
                    prop_def.pop('default', None) # 移除属性的 default
                    prop_def.pop('title', None)   # 移除属性的 title
                    self._remove_unwanted_fields_from_schema(prop_def) # 递归处理嵌套 schema
        elif schema.get("type") == "object" and "properties" not in schema:
            # 如果是 object 类型但没有 properties，某些 LLM (如 Gemini) 会报错
            schema["properties"] = {}

        # 移除顶层的 default (如果存在且不应该存在于顶层)
        # 通常 default 应该在属性级别，但以防万一
        # schema.pop('default', None) # 这个可能过于激进，先注释掉

        # 递归处理其他嵌套字典
        for key, value in schema.items():
            if isinstance(value, dict):
                self._remove_unwanted_fields_from_schema(value)
            elif isinstance(value, list):
                for i, item in enumerate(value):
                    if isinstance(item, dict):
                        value[i] = self._remove_unwanted_fields_from_schema(item)
        return schema
        
    def _convert_crewai_tools_to_litellm_format(self, tools: Optional[List[BaseTool]]) -> Optional[List[Dict[str, Any]]]:
        """
        将 CrewAI 的 BaseTool 列表转换为 LiteLLM期望的 "tools" 格式。
        LiteLLM 的格式通常与 OpenAI 的 function calling 格式类似。
        """
        if not tools:
            return None
        
        litellm_tool_definitions = []
        for tool_instance in tools:
            tool_name = tool_instance.name
            tool_description = tool_instance.description
            
            parameters_schema: Dict[str, Any]
            if not hasattr(tool_instance, 'args_schema') or not tool_instance.args_schema:
                # 如果工具没有定义参数 schema，则默认为一个没有参数的 object 类型
                parameters_schema = {"type": "object", "properties": {}}
            else:
                try:
                    # Pydantic V2 使用 model_json_schema(), V1 使用 schema()
                    if hasattr(tool_instance.args_schema, 'model_json_schema'):
                        pydantic_schema = tool_instance.args_schema.model_json_schema()
                    else:
                        pydantic_schema = tool_instance.args_schema.schema() # type: ignore
                    
                    # 清理 schema，移除 'title' 和 'default' 等字段
                    cleaned_schema = self._remove_unwanted_fields_from_schema(pydantic_schema.copy())
                    parameters_schema = cleaned_schema
                except Exception as e:
                    logger.error(f"Error processing schema for tool {tool_name}: {e}. Defaulting to empty params.")
                    parameters_schema = {"type": "object", "properties": {}}
            
            litellm_tool_definitions.append({
                "type": "function", # LiteLLM/OpenAI 的标准类型
                "function": {
                    "name": tool_name,
                    "description": tool_description,
                    "parameters": parameters_schema
                }
            })
        logger.debug(f"Converted CrewAI tools to LiteLLM format: {json.dumps(litellm_tool_definitions, indent=2)}")
        return litellm_tool_definitions

    def call(self, messages: List[Dict[str, str]], tools: Optional[List[Dict[str, Any]]] = None, **kwargs: Any) -> str:
        """
        使用 LiteLLM 调用 LLM。
        CrewAI 的 BaseLLM.call 方法期望返回一个字符串。
        """
        logger.info(f"CustomLiteLLMWrapper.call for '{self.model_name}' invoked.")
        logger.debug(f"  Messages: {json.dumps(messages, indent=2, ensure_ascii=False)}")
        logger.debug(f"  Tools provided to call: {'Yes' if tools else 'No'}")
        logger.debug(f"  kwargs: {kwargs}")

        # 合并参数，调用时传入的 kwargs 优先级更高
        current_litellm_params = {**self.litellm_params, **kwargs}

        litellm_call_args: Dict[str, Any] = {
            "model": self.model_name,
            "messages": messages,
            "api_base": self.api_base,
            "api_key": self.api_key,
            "custom_llm_provider": self.custom_llm_provider,
            **current_litellm_params # temperature, max_tokens, etc.
        }

        # 处理工具（函数调用）
        # CrewAI 在调用 Agent 的 LLM 时，会根据 Agent 的 tools 属性自动准备 tools 参数
        # 我们需要确保这里的 tools 参数与 LiteLLM 兼容
        final_tools_for_litellm = tools # 直接使用 CrewAI 传递过来的 tools
        if not final_tools_for_litellm and self._cached_tools_for_litellm:
            logger.info("  No tools passed to call, using cached tools for LiteLLM.")
            final_tools_for_litellm = self._cached_tools_for_litellm
        
        if final_tools_for_litellm:
            litellm_call_args["tools"] = final_tools_for_litellm
            # 根据 self.tool_config 设置 tool_choice
            if self.tool_config and "function_calling_config" in self.tool_config:
                fc_config = self.tool_config["function_calling_config"]
                mode = fc_config.get("mode", "AUTO").upper() # "AUTO", "ANY", "NONE"
                
                if mode == "ANY" and fc_config.get("allowed_function_names"):
                    # 对于 Gemini，如果 mode 是 ANY 且指定了函数名，tool_choice 应该是一个特定结构
                    # 对于 OpenAI，tool_choice 可以是 {"type": "function", "function": {"name": "my_function"}}
                    # LiteLLM 会尝试适配，我们先按 OpenAI 的方式设置
                    litellm_call_args["tool_choice"] = {
                        "type": "function", 
                        "function": {"name": fc_config["allowed_function_names"][0]} # 假设只取第一个
                    }
                elif mode in ["AUTO", "ANY", "NONE"]: # ANY 在没有指定函数名时，行为类似 AUTO
                    litellm_call_args["tool_choice"] = mode.lower()
                else: # 默认为 auto
                    litellm_call_args["tool_choice"] = "auto"
                logger.debug(f"  Setting tool_choice to: {litellm_call_args['tool_choice']}")
            else: # 如果没有 tool_config，默认 tool_choice 为 auto
                 litellm_call_args["tool_choice"] = "auto"
                 logger.debug(f"  No tool_config, defaulting tool_choice to 'auto'.")


        # 移除值为 None 的参数，因为 litellm.completion 不喜欢 None 的 api_key 等
        litellm_call_args_cleaned = {k: v for k, v in litellm_call_args.items() if v is not None}

        # --- 新增：如果 api_base 为 None (表示直接调用云端模型)，则尝试使用 LITELLM_PROXY_URL ---
        if self.api_base is None:
            local_proxy_url = os.getenv("LITELLM_PROXY_URL")
            if local_proxy_url:
                litellm_call_args_cleaned["proxy"] = { # LiteLLM 的 proxy 参数期望一个字典
                    "http": local_proxy_url,
                    "https": local_proxy_url,
                }
                logger.info(f"  Using local proxy for direct cloud call: {local_proxy_url}")
            else:
                logger.info("  api_base is None, but LITELLM_PROXY_URL is not set. Proceeding without proxy.")
        # --- 结束新增代理逻辑 ---
        
        logger.info(f"  Attempting to call litellm.completion for model '{self.model_name}'...")
        # 在打印参数前确保 proxy 参数也被包含（如果设置了）
        debug_params_to_print = {k: v for k, v in litellm_call_args_cleaned.items() if k != 'messages'}
        if "proxy" in litellm_call_args_cleaned: # 确保打印时能看到 proxy
            debug_params_to_print["proxy_used"] = litellm_call_args_cleaned["proxy"]
            
        logger.debug(f"  LiteLLM Call Args (cleaned, messages excluded, proxy shown if used): {debug_params_to_print}")
        
        # --- 添加以下详细日志 ---
        logger.info("--------------------------------------------------------------------")
        logger.info(f"DEBUGGING TOOLS PASSED TO LITELLM for model {self.model_name} (Manager Agent call):") # 区分是Manager还是Worker的调用

        effective_tools_to_log = litellm_call_args_cleaned.get("tools")

        if effective_tools_to_log:
            try:
                tools_json_str = json.dumps(effective_tools_to_log, indent=2, ensure_ascii=False)
                logger.info(f"  Tools (Content):\n{tools_json_str}")
                # 专门检查 web_search_tool 是否存在
                web_search_tool_found_in_definition = any(
                    tool.get("function", {}).get("name") == "web_search_tool" 
                    for tool in effective_tools_to_log if isinstance(tool, dict)
                )
                if web_search_tool_found_in_definition:
                    logger.info("  >>>> web_search_tool IS PRESENT in the tools definition passed to LiteLLM. <<<<")
                else:
                    logger.warning("  >>>> web_search_tool IS MISSING from the tools definition passed to LiteLLM! <<<<")

            except Exception as e_log_json:
                logger.error(f"  Error serializing tools for logging: {e_log_json}")
                logger.info(f"  Tools (Raw Object, could not serialize): {effective_tools_to_log}")
        else:
            logger.info("  Tools: Not present in litellm_call_args_cleaned (effective_tools_to_log is None or empty)")
            
        if "tool_choice" in litellm_call_args_cleaned:
            logger.info(f"  Tool Choice: {litellm_call_args_cleaned['tool_choice']}")
        else:
            logger.info("  Tool Choice: Not present in litellm_call_args_cleaned")
        logger.info("--------------------------------------------------------------------")
        # --- 详细日志结束 ---

        response = None 
        try:
            response = litellm.completion(**litellm_call_args_cleaned) 
            logger.info(f"  litellm.completion call for '{self.model_name}' succeeded.") # <--- 添加日志
            logger.debug(f"  LiteLLM Raw Response object type: {type(response)}")
            if hasattr(response, 'model_dump_json'):
                logger.debug(f"  LiteLLM Raw Response (JSON): {response.model_dump_json(indent=2)}")
            else:
                logger.debug(f"  LiteLLM Raw Response (str): {str(response)[:500]}")

            # --- 新增日志，检查原始的 usage ---
            if hasattr(response, 'usage') and response.usage:
                logger.info(f"  DEBUG USAGE (from LiteLLM response object): {response.usage}")
                # 如果 response.usage 是 Pydantic 模型，可以尝试打印其字典形式
                if hasattr(response.usage, 'model_dump'):
                    logger.info(f"  DEBUG USAGE (dict): {response.usage.model_dump()}")
                else:
                    logger.info(f"  DEBUG USAGE (raw object): {response.usage}")
            else:
                logger.warning("  DEBUG USAGE: LiteLLM response object does not have .usage or it's empty.")
            # --- 结束新增 ---

        except Exception as e:
            logger.error(f"LiteLLM completion call FAILED for model '{self.model_name}': {e}", exc_info=True) # <--- 修改日志
            return f"LLM_CALL_ERROR: 调用模型 '{self.model_name}' 失败: {str(e)}"

        # 从 LiteLLM 响应中提取内容或工具调用
        # LiteLLM 的 ModelResponse 结构与 OpenAI 的 ChatCompletion 类似
        llm_message_response = response.choices[0].message
        
        if hasattr(llm_message_response, 'tool_calls') and llm_message_response.tool_calls:
            logger.info(f"  LLM returned structured tool_calls: {llm_message_response.tool_calls}")
            # 构造 ReAct 格式的字符串
            tool_call = llm_message_response.tool_calls[0] # 假设只有一个工具调用
            action = tool_call.function.name
            action_input = tool_call.function.arguments # 这是 JSON 字符串
            
            # Gemini 可能也会在 content 中生成 Thought，如果它遵循 ReAct
            thought_prefix = ""
            if llm_message_response.content and "Thought:" in llm_message_response.content:
                thought_prefix = llm_message_response.content.split("Action:")[0] # 取 Action:之前的部分作为 Thought

            react_string = f"{thought_prefix.strip()}\nAction: {action}\nAction Input: {action_input}"
            logger.info(f"  Constructed ReAct string from tool_calls: {react_string}")
            return react_string.strip() # 返回 ReAct 字符串
        
        elif llm_message_response.content:
            content_str = llm_message_response.content
            logger.info(f"  LLM returned content (first 200 chars): {content_str[:200]}")
            # 如果 content 本身就是 ReAct 格式，也直接返回
            return content_str.strip() # 返回字符串
        
        else:
            logger.warning("  LLM response did not contain structured tool_calls or text content.")
            return ""

    def get_token_ids(self, text: str) -> List[int]:
        """
        获取文本的 token ID 列表。
        LiteLLM 提供了 litellm.encode 和 litellm.decode 方法。
        """
        try:
            # 注意：litellm.encode 可能需要 model 参数来确定使用哪个 tokenizer
            return litellm.encode(model=self.model_name, text=text)
        except Exception as e:
            logger.warning(f"get_token_ids failed for model '{self.model_name}': {e}. Returning empty list.")
            # CrewAI 在某些情况下即使这里返回空列表也能继续，但最好能正确实现
            return []


    def supports_function_calling(self) -> bool: # <--- 删除 @property
        logger.debug(f"CustomLiteLLMWrapper.supports_function_calling() called for model {self.model_name}, returning True.")
        return True


    # CrewAI 可能还会用到的一些属性
    @property
    def _llm_type(self) -> str:
        return f"custom_litellm_{self.model_name.replace('/', '_')}"

    @property
    def identifying_params(self) -> Dict[str, Any]:
        """返回用于标识此LLM实例的参数字典。"""
        return {
            "model_name": self.model_name,
            "api_base": self.api_base,
            "custom_llm_provider": self.custom_llm_provider,
            **self.litellm_params
        }

# --- 示例用法 (可选，用于测试此文件) ---
async def main_test_wrapper():
    logger.info("--- Testing CustomLiteLLMWrapper ---")

    # 测试本地 Qwen (假设服务在 http://localhost:8088/v1)
    try:
        logger.info("\n--- Testing Local Qwen ---")
        local_qwen_llm = CustomLiteLLMWrapper(
            model="local/qwen3-1.7b-gguf", # 这个名称需要与 LiteLLM 调用时匹配
            api_base="http://localhost:8088/v1",
            api_key="nokey",
            custom_llm_provider="openai",
            temperature=0.1
        )
        messages_qwen = [{"role": "user", "content": "你好，请用中文介绍一下你自己。不要超过50个字。"}]
        # CrewAI 通常是同步调用 call 方法，但我们的 call 内部是同步执行 litellm.completion
        # 如果要测试异步行为，需要 litellm.acompletion 和异步的 call
        response_qwen = local_qwen_llm.call(messages=messages_qwen)
        logger.info(f"Local Qwen Response: {response_qwen}")
        
        # 测试 token_ids
        # token_ids_qwen = local_qwen_llm.get_token_ids("你好，世界")
        # logger.info(f"Token IDs for '你好，世界' from Qwen (via LiteLLM encode): {token_ids_qwen}")

    except Exception as e:
        logger.error(f"Error testing local Qwen: {e}", exc_info=True)

    # 测试云端 Gemini (假设通过配置好的 LiteLLM 网关)
    # 需要设置 CLOUD_LITELLM_GW_API_BASE 环境变量
    # 例如: export CLOUD_LITELLM_GW_API_BASE="http://your-litellm-proxy.com/v1"
    #       export GEMINI_API_KEY="your_actual_gemini_key_if_proxy_doesnt_handle_it_or_proxy_key"
    # CLOUD_LITELLM_GW_API_BASE_TEST = os.getenv("CLOUD_LITELLM_GW_API_BASE_TEST")
    # GEMINI_API_KEY_TEST = os.getenv("GEMINI_API_KEY_TEST") # 或者网关的key

    # if CLOUD_LITELLM_GW_API_BASE_TEST and GEMINI_API_KEY_TEST:
    #     try:
    #         print("\n--- Testing Cloud Gemini via Gateway ---")
    #         gemini_llm = CustomLiteLLMWrapper(
    #             model="gemini/gemini-1.5-flash-latest",
    #             api_base=CLOUD_LITELLM_GW_API_BASE_TEST,
    #             api_key=GEMINI_API_KEY_TEST, # Key for the gateway or Gemini if gateway passes it
    #             temperature=0.5
    #         )
    #         messages_gemini = [{"role": "user", "content": "What is the capital of France?"}]
    #         response_gemini = gemini_llm.call(messages=messages_gemini)
    #         print(f"Cloud Gemini Response: {response_gemini}")
    #     except Exception as e:
    #         print(f"Error testing cloud Gemini: {e}")
    # else:
    #     print("\nSkipping Cloud Gemini test as CLOUD_LITELLM_GW_API_BASE_TEST or GEMINI_API_KEY_TEST is not set.")

if __name__ == "__main__":
    # import asyncio
    # asyncio.run(main_test_wrapper())
    pass
    --- END OF FILE CONTENT ---


--- End of Project Snapshot ---