--- START OF FILE __init__.py ------ END OF FILE __init__.py ---

--- START OF FILE bm25.py ---# zhz_agent/bm25.py

import json
import os
from typing import List, Dict, Any, Optional
import jieba # 用于中文分词
from bm25s import BM25 # 导入bm25s库

# --- [修改] 从项目内部导入pydantic_models -> 改为绝对导入 ---
from zhz_agent.pydantic_models import RetrievedDocument

class BM25Retriever:
    def __init__(self, data_path: str):
        self.data_path = data_path
        self.documents: List[Dict[str, Any]] = [] # 存储原始文档
        self.tokenized_corpus: List[List[str]] = [] # 存储分词后的文档
        self.bm25_model: Optional[BM25] = None # BM25模型实例
        self._load_documents_and_build_index()

    def _load_documents_and_build_index(self):
        """
        从JSON文件加载文档，进行分词，并构建BM25索引。
        """
        file_path = os.path.join(self.data_path, "sample_documents.json")
        if not os.path.exists(file_path):
            print(f"错误: 模拟BM25文档文件不存在: {file_path}")
            return

        with open(file_path, 'r', encoding='utf-8') as f:
            self.documents = json.load(f)

        if not self.documents:
            print("警告: 模拟BM25文档文件为空。")
            return

        # 提取所有文档内容用于分词和索引
        corpus_texts = [doc['content'] for doc in self.documents]

        print("BM25Retriever: 正在对文档进行分词并构建索引...")
        # 使用jieba进行中文分词
        self.tokenized_corpus = [list(jieba.cut(text)) for text in corpus_texts]

        # 初始化BM25模型并构建索引
        self.bm25_model = BM25()
        self.bm25_model.index(self.tokenized_corpus)

        print(f"BM25Retriever: 加载了 {len(self.documents)} 个文档，BM25索引已构建。")

    async def retrieve(self, query: str, top_k: int = 3) -> List[RetrievedDocument]:
        """
        根据查询文本进行关键词检索。
        """
        if not self.bm25_model or not self.documents:
            print("BM25Retriever未初始化或没有文档。")
            return []

        # 对查询进行分词
        query_tokens_single = list(jieba.cut(query)) # 单个查询的分词结果
        query_tokens_batch = [query_tokens_single] # <--- 将单个查询封装成列表的列表

        # 执行BM25检索
        # bm25s的retrieve方法返回文档索引和对应的BM25分数
        doc_indices, doc_scores = self.bm25_model.retrieve(query_tokens_batch, k=top_k, return_as="tuple")

        retrieved_results: List[RetrievedDocument] = []

        # bm25s.retrieve 返回的是一个元组 (doc_indices_batch, doc_scores_batch)
        # 即使是单个查询，它们也是列表的列表，所以我们取第一个元素
        if doc_indices is not None and doc_indices.shape[0] > 0 and doc_indices[0].size > 0: # 检查是否有结果
            # doc_indices[0] 是第一个查询的结果索引数组
            # doc_scores[0] 是第一个查询的结果分数数组

            for i in range(doc_indices[0].size): # 遍历第一个查询的结果
                doc_idx = doc_indices[0][i]
                score = float(doc_scores[0][i]) # 将numpy float转换为Python float

                # 过滤掉得分过低的（BM25分数可能为负，或非常接近0）
                # 这里可以根据实际情况调整阈值，但BM25通常不设严格阈值，而是取top_k
                # 如果score为负，通常表示不相关，可以过滤掉
                if score > 0:
                    doc = self.documents[doc_idx]
                    retrieved_results.append(
                        RetrievedDocument(
                            source_type="keyword_bm25",
                            content=doc['content'],
                            score=score,
                            metadata=doc.get('metadata', {})
                        )
                    )
        print(f"BM25Retriever: 检索到 {len(retrieved_results)} 个BM25结果。")
        return retrieved_results

# 示例用法 (可以在文件末尾添加，用于独立测试)
# async def main():
#     bm25_retriever = BM25Retriever(data_path="data")
#     results = await bm25_retriever.retrieve("公司最新人力资源政策", top_k=2)
#     for res in results:
#         print(f"得分: {res.score:.4f}, 来源: {res.source_type}, 内容: {res.content[:50]}...")
# if __name__ == "__main__":
#     import asyncio
#     asyncio.run(main())--- END OF FILE bm25.py ---

--- START OF FILE custom_crewai_tools.py ---# zhz_agent/custom_crewai_tools.py

import os
import json
import asyncio
import traceback
from typing import Type, List, Dict, Any, Optional, ClassVar
from pydantic import BaseModel, Field
from crewai.tools import BaseTool
import httpx

# 从 zhz_agent.pydantic_models 导入 QueryRequest
from zhz_agent.pydantic_models import QueryRequest # 用于 RAG 工具的输入

# MCPO 代理的基地址
MCPO_BASE_URL = os.getenv("MCPO_BASE_URL", "http://localhost:8006")

class BaseMCPTool(BaseTool):
    mcpo_base_url: str = MCPO_BASE_URL
    _call_mcpo_func: ClassVar[callable] = None

    @classmethod
    def set_mcpo_caller(cls, caller: callable):
        cls._call_mcpo_func = caller

    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    async def _call_mcpo_endpoint(self, service_and_tool_path: str, payload: dict) -> dict | str:
        api_url = f"{self.mcpo_base_url}/{service_and_tool_path}"
        cleaned_payload = {k: v for k, v in payload.items() if v is not None}
        print(f"BaseMCPTool: Calling mcpo endpoint: POST {api_url} with payload: {json.dumps(cleaned_payload, ensure_ascii=False)}")
        
        # --- [修改] 移除 proxies=None 参数 ---
        async with httpx.AsyncClient(trust_env=False) as client:
            response = None
            try:
                headers = {"Content-Type": "application/json"}
                response = await client.post(api_url, json=cleaned_payload, headers=headers, timeout=300.0)
                print(f"BaseMCPTool: mcpo status code for {service_and_tool_path}: {response.status_code}")
                print(f"BaseMCPTool: mcpo response headers for {service_and_tool_path}: {response.headers}") # <--- 新增日志
                # 尝试分块读取响应或提前获取少量内容进行日志记录，以防响应过大卡住 .text 或 .json()
                try:
                    response_text_snippet = await response.aread(num_bytes=1024) # 读取前1KB
                    print(f"BaseMCPTool: mcpo response text snippet (first 1KB) for {service_and_tool_path}: {response_text_snippet.decode(errors='ignore')}")
                except Exception as e_read:
                    print(f"BaseMCPTool: Error reading response snippet: {e_read}")

                if response.status_code == 200:
                    try:
                        # print(f"BaseMCPTool: mcpo raw response text for {service_and_tool_path}: {response.text}") # 如果怀疑内容问题，可以取消注释，但小心大响应
                        return response.json()
                    except json.JSONDecodeError:
                        print(f"BaseMCPTool Warning: mcpo returned status 200 but response is not JSON for '{service_and_tool_path}'. Returning raw text.")
                        return response.text
                else:
                    error_text = f"mcpo call to '{service_and_tool_path}' failed with status {response.status_code}. Response: {response.text[:500]}..."
                    print(f"BaseMCPTool Error: {error_text}")
                    return {"error": error_text, "status_code": response.status_code}
            except httpx.RequestError as exc:
                error_msg = f"BaseMCPTool HTTP RequestError calling mcpo tool '{service_and_tool_path}': {type(exc).__name__} - {exc}"
                print(f"BaseMCPTool Error: {error_msg}"); traceback.print_exc()
                return {"error": error_msg, "exception_type": type(exc).__name__}
            except Exception as exc:
                error_msg = f"BaseMCPTool Unexpected error calling mcpo tool '{service_and_tool_path}': {type(exc).__name__} - {exc}"
                response_text_snippet = response.text[:500] if response and hasattr(response, 'text') else "Response object not available or no text."
                print(f"BaseMCPTool Error: {error_msg}. Response snippet: {response_text_snippet}"); traceback.print_exc()
                return {"error": error_msg, "exception_type": type(exc).__name__}

    def _handle_tool_result(self, result: dict | str, tool_name_for_log: str) -> str:
        print(f"BaseMCPTool DEBUG: {tool_name_for_log} result from mcpo: {str(result)[:500]}...")
        parsed_result = result
        if isinstance(result, str):
            try:
                parsed_result = json.loads(result)
            except json.JSONDecodeError:
                if "error" in result.lower() or "failed" in result.lower() or "traceback" in result.lower():
                    return f"调用 {tool_name_for_log} 失败，返回非JSON错误文本: {result}"
                print(f"BaseMCPTool Info: Result for {tool_name_for_log} is a non-JSON string, returning as is.")
                return result
        if isinstance(parsed_result, dict):
            if "error" in parsed_result and "status_code" in parsed_result:
                return f"调用 {tool_name_for_log} 时发生HTTP错误：{parsed_result.get('error')}"
            if parsed_result.get("status") == "error":
                error_msg = parsed_result.get("error_message", "未知错误")
                error_code = parsed_result.get("error_code", "NO_CODE")
                return f"工具 {tool_name_for_log} 执行失败 (错误码: {error_code})：{error_msg}"
            try:
                return json.dumps(parsed_result, ensure_ascii=False, indent=2)
            except Exception as e:
                print(f"BaseMCPTool Error formatting successful dict result for {tool_name_for_log}: {e}")
                return str(parsed_result)
        print(f"BaseMCPTool Warning: Unexpected result format from {tool_name_for_log} mcpo call: {type(result)}, content: {str(result)[:200]}")
        return f"从 {tool_name_for_log} 服务收到的结果格式不正确或无法处理: {str(result)[:500]}"

    def _run_default_sync_wrapper(self, **kwargs) -> str:
        tool_name = getattr(self, 'name', self.__class__.__name__)
        print(f"BaseMCPTool INFO: Synchronous _run called for {tool_name} with args: {kwargs}.")
        result_str = ""
        try:
            # --- 改进的 asyncio.run 处理 ---
            try:
                loop = asyncio.get_running_loop()
            except RuntimeError:
                loop = None

            async def async_runner():
                return await self._arun(**kwargs)

            if loop and loop.is_running():
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                    future = executor.submit(asyncio.run, async_runner())
                    result = future.result(timeout=120)
            else:
                result = asyncio.run(async_runner())
            result_str = str(result)
        except asyncio.TimeoutError:
            error_message = f"Tool {tool_name} execution timed out after 120 seconds."
            print(f"BaseMCPTool ERROR_TRACE: {error_message}"); result_str = error_message
        except RuntimeError as e:
            if "cannot run event loop while another loop is running" in str(e).lower() or "event loop is already running" in str(e).lower():
                error_message = (f"BaseMCPTool Error in {tool_name} _run: Nested asyncio event loop conflict. Original error: {e}")
            else:
                error_message = f"BaseMCPTool RuntimeError in {tool_name} _run: {type(e).__name__} - {e}"
            print(f"BaseMCPTool ERROR_TRACE: {error_message}"); traceback.print_exc();
            result_str = error_message
        except Exception as e:
            error_message = f"BaseMCPTool General Exception in {tool_name} _run: {type(e).__name__} - {e}"
            print(f"BaseMCPTool ERROR_TRACE: {error_message}"); traceback.print_exc(); result_str = error_message
        return result_str

class HybridRAGTool(BaseMCPTool):
    name: str = "HybridRAGQueryTool"
    description: str = (
        "【核心RAG工具】用于通过执行混合检索增强生成 (RAG) 搜索来回答用户问题。 "
        "该工具整合了向量检索、知识图谱检索和关键词检索，并进行智能融合和重排序。 "
        "当用户需要从知识库中获取信息、回答复杂问题或生成报告时，应调用此工具。"
    )
    args_schema: Type[BaseModel] = QueryRequest
    target_mcp_service_path: str = "zhz_agent_service/query_rag"

    async def _arun(self, query: str, top_k_vector: int, top_k_kg: int, top_k_bm25: int, **kwargs: Any) -> str: # --- 确认有 **kwargs ---
        tool_name_for_log = getattr(self, 'name', "HybridRAGTool")
        print(f"CrewAI Tool DEBUG: {tool_name_for_log}._arun called with query='{query}', top_k_vector={top_k_vector}, top_k_kg={top_k_kg}, top_k_bm25={top_k_bm25}, additional_kwargs={kwargs}")

        security_context = kwargs.get('security_context')
        if security_context:
            print(f"CrewAI Tool INFO: Received security_context (in HybridRAGTool): {str(security_context)[:200]}...")

        payload = {
            "query": query,
            "top_k_vector": top_k_vector,
            "top_k_kg": top_k_kg,
            "top_k_bm25": top_k_bm25
        }
        result = await self._call_mcpo_endpoint(self.target_mcp_service_path, payload)
        return self._handle_tool_result(result, self.name)

    def _run(self, query: str, top_k_vector: int, top_k_kg: int, top_k_bm25: int, **kwargs: Any) -> str: # --- 确认有 **kwargs ---
        return self._run_default_sync_wrapper(query=query, top_k_vector=top_k_vector, top_k_kg=top_k_kg, top_k_bm25=top_k_bm25, **kwargs)--- END OF FILE custom_crewai_tools.py ---

--- START OF FILE custom_llm.py ---#/home/zhz/zhz_agent/custom_llm.py
import os
import json
import httpx
import asyncio
import traceback
from typing import List, Dict, Any, Optional, Union, Sequence, Type 
# --- CrewAI & LiteLLM Imports ---
from crewai.tools import BaseTool
from crewai.llms.base_llm import BaseLLM as CrewAIBaseLLM
import litellm

# --- [修改] Local Imports -> 改为绝对导入 ---
from zhz_agent.llm import call_sglang_llm # For SGLang LLM
from dotenv import load_dotenv

load_dotenv()

# --- SGLang Config ---
SGLANG_API_URL_FOR_LLM = os.getenv("SGLANG_API_URL", "http://localhost:30000/generate")

# --- CustomGeminiLLM (from ceshi/run_agent.py with fixes) ---
class CustomGeminiLLM(CrewAIBaseLLM):
    model_name: str
    api_key: str
    max_tokens: Optional[int] = 2048
    tool_config: Optional[Dict[str, Any]] = None
    stop: Optional[List[str]] = None
    _gemini_tools_cache: Optional[List[Dict[str, Any]]] = None

    def __init__(self, model: str, api_key: str, temperature: float = 0.1, max_tokens: Optional[int] = 2048, tool_config: Optional[Dict[str, Any]] = None, stop: Optional[List[str]] = None, agent_tools: Optional[List[BaseTool]] = None, **kwargs):
        super().__init__(model=model, temperature=temperature)
        self.model_name = model
        self.api_key = api_key
        self.max_tokens = max_tokens
        self.tool_config = tool_config or {"function_calling_config": {"mode": "AUTO"}}
        self.stop = stop
        if agent_tools:
            self._gemini_tools_cache = self._convert_crewai_tools_to_gemini_format(agent_tools)
            print(f"CustomGeminiLLM __init__: Cached {len(self._gemini_tools_cache)} tools.")
        else:
            print("CustomGeminiLLM __init__: No agent_tools provided for caching.")

    def _remove_unwanted_fields(self, schema: Dict[str, Any]) -> Dict[str, Any]:
        if not isinstance(schema, dict):
            return schema

        schema.pop('title', None)

        if "properties" in schema:
            if "type" not in schema:
                schema["type"] = "object"
            for prop_name, prop_def in list(schema["properties"].items()):
                if isinstance(prop_def, dict):
                    prop_def.pop('default', None)
                    prop_def.pop('title', None)
                    self._remove_unwanted_fields(prop_def)
        elif schema.get("type") == "object" and "properties" not in schema:
            schema["properties"] = {}

        keys_to_delete = [k for k, v in schema.items() if k == 'default']
        for k in keys_to_delete:
            del schema[k]

        for k, v in schema.items():
            if isinstance(v, dict):
                self._remove_unwanted_fields(v)
            elif isinstance(v, list):
                for i, item in enumerate(v):
                    if isinstance(item, dict):
                        v[i] = self._remove_unwanted_fields(item)
        return schema

    def _convert_crewai_tools_to_gemini_format(self, tools: Optional[List[BaseTool]]) -> Optional[List[Dict[str, Any]]]:
        if not tools:
            return None
        gemini_tool_declarations = []
        for tool_instance in tools:
            tool_name = tool_instance.name
            tool_description = tool_instance.description
            if not hasattr(tool_instance, 'args_schema') or not tool_instance.args_schema:
                parameters_schema = {"type": "object", "properties": {}}
            else:
                try:
                    if hasattr(tool_instance.args_schema, 'model_json_schema'):
                        pydantic_schema = tool_instance.args_schema.model_json_schema()
                    else:
                        pydantic_schema = tool_instance.args_schema.schema()
                    cleaned_schema = self._remove_unwanted_fields(pydantic_schema.copy())
                    parameters_schema = cleaned_schema
                except Exception as e:
                    print(f"Error processing schema for tool {tool_name}: {e}")
                    parameters_schema = {"type": "object", "properties": {}}
            gemini_tool_declarations.append({
                "name": tool_name,
                "description": tool_description,
                "parameters": parameters_schema
            })
        final_tools_for_litellm = []
        for declaration in gemini_tool_declarations:
            final_tools_for_litellm.append({
                "type": "function",
                "function": declaration
            })
        return final_tools_for_litellm

    def call(self, messages: Union[str, List[Dict[str, str]]], tools: Optional[List[dict]] = None, callbacks: Optional[List[Any]] = None, **kwargs: Any) -> Union[str, Any]:
        print(f"CustomGeminiLLM CALL method invoked.")
        print(f"  CALL - Tools received by CustomLLM.call: {'Yes' if tools else 'No'}")
        print(f"  CALL - Callbacks received by CustomLLM.call: {'Yes' if callbacks else 'No'}")

        if isinstance(messages, str):
            processed_messages = [{"role": "user", "content": messages}]
        else:
            processed_messages = messages

        litellm_params = {
            "model": self.model_name,
            "messages": processed_messages,
            "api_key": self.api_key,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "stop": self.stop
        }

        # --- Proxy Addition ---
        proxy_url = os.getenv("LITELLM_PROXY_URL")
        if proxy_url:
            litellm_params["proxy"] = {
                "http": proxy_url,
                "https": proxy_url,
            }
            print(f"CustomGeminiLLM.call - Using proxy: {proxy_url}")
        else:
            print("CustomGeminiLLM.call - No proxy configured (LITELLM_PROXY_URL not set).")

        # --- Tool Handling (tools: null fix) ---
        final_tools_for_litellm = None
        received_tools_to_process = tools
        if not received_tools_to_process and self._gemini_tools_cache:
            print("  CALL - INFO: Tools argument was None, using cached tools.")
            received_tools_to_process = self._gemini_tools_cache

        if received_tools_to_process:
            cleaned_tools_for_litellm = []
            for tool_dict in received_tools_to_process:
                current_tool_def = tool_dict.copy()
                if current_tool_def.get("type") == "function" and "function" in current_tool_def:
                    func_def = current_tool_def["function"].copy()
                    if "parameters" in func_def:
                        func_def["parameters"] = self._remove_unwanted_fields(func_def["parameters"].copy())
                    current_tool_def["function"] = func_def
                    cleaned_tools_for_litellm.append(current_tool_def)
                else:
                    cleaned_tools_for_litellm.append(tool_dict)
            final_tools_for_litellm = cleaned_tools_for_litellm

        if final_tools_for_litellm:
            litellm_params["tools"] = final_tools_for_litellm
            fc_config = self.tool_config.get("function_calling_config", {})
            mode = fc_config.get("mode", "AUTO").upper()
            allowed_names = fc_config.get("allowed_function_names")

            if mode == "ANY" and allowed_names:
                litellm_params["tool_choice"] = {
                    "type": "function",
                    "function": {"name": allowed_names[0]}
                }
            elif mode in ["AUTO", "ANY", "NONE"]:
                litellm_params["tool_choice"] = mode.lower()
            else:
                litellm_params["tool_choice"] = "auto"
            print(f"CustomGeminiLLM DEBUG: Setting tool_choice to: {litellm_params['tool_choice']}")

        if callbacks:
            litellm_params["callbacks"] = callbacks

        try:
            print(f"CustomGeminiLLM.call - LiteLLM PARAMS (Preview): model={litellm_params['model']}, msgs_count={len(litellm_params['messages'])}, tools={'Yes' if 'tools' in litellm_params else 'No'}, tool_choice={litellm_params.get('tool_choice')}, proxy={'Yes' if 'proxy' in litellm_params else 'No'}")
            response = litellm.completion(**litellm_params)
        except Exception as e:
            print(f"CRITICAL ERROR: LiteLLM completion call failed: {e}")
            if callbacks:
                for handler in callbacks:
                    if hasattr(handler, 'on_llm_error'):
                        try:
                            handler.on_llm_error(error=e, llm=self, **kwargs)
                        except Exception as cb_err:
                            print(f"Error in callback on_llm_error: {cb_err}")
            raise

        llm_message_response = response.choices[0].message
        if hasattr(llm_message_response, 'tool_calls') and llm_message_response.tool_calls:
            print(f"CustomGeminiLLM.call - Detected tool_calls: {llm_message_response.tool_calls}")
            # --- ReAct Format Workaround (AttributeError fix) ---
            tool_call = llm_message_response.tool_calls[0]
            action = tool_call.function.name
            action_input = tool_call.function.arguments
            react_string = f"Action: {action}\nAction Input: {action_input}"
            print(f"CustomGeminiLLM.call - Returning ReAct string: {react_string}")
            return react_string
        else:
            print(f"CustomGeminiLLM.call - Returning text content.")
            return llm_message_response.content or ""

    def get_token_counter_instance(self):
        class GeminiTokenCounter:
            def __init__(self, model_name):
                self.model_name = model_name

            def count_tokens(self, text: Union[str, List[Dict[str,str]]]) -> int:
                try:
                    if isinstance(text, list):
                        return litellm.token_counter(model=self.model_name, messages=text)
                    return litellm.token_counter(model=self.model_name, text=str(text))
                except Exception as e:
                    print(f"Warning: Token counting failed ({e}), falling back to rough estimate.")
                    if isinstance(text, list):
                        return sum(len(str(m.get("content","")).split()) for m in text)
                    return len(str(text).split())
        return GeminiTokenCounter(model_name=self.model_name)


# --- CustomSGLangLLM (from hybrid_rag/custom_llm.py) ---
class CustomSGLangLLM(CrewAIBaseLLM):
    endpoint_url: str = SGLANG_API_URL_FOR_LLM
    model_name: str = "qwen2-3b-instruct"
    temperature: float = 0.1
    max_new_tokens_val: int = 1024

    def __init__(self, endpoint: Optional[str] = None, model: Optional[str] = None, temperature: Optional[float] = None, max_new_tokens: Optional[int] = None, **kwargs: Any):
        super().__init__(**kwargs)
        if endpoint: self.endpoint_url = endpoint
        if model: self.model_name = model
        if temperature is not None: self.temperature = temperature
        if max_new_tokens is not None: self.max_new_tokens_val = max_new_tokens
        print(f"CustomSGLangLLM initialized. Endpoint: {self.endpoint_url}, Model: {self.model_name}, Temp: {self.temperature}, MaxTokens: {self.max_new_tokens_val}")

    def _prepare_sglang_prompt(self, messages: Sequence[Dict[str, str]]) -> str:
        prompt_str = ""
        for message in messages:
            role = message.get("role")
            content = message.get("content")
            if role and content:
                prompt_str += f"<|im_start|>{role}\n{content}<|im_end|>\n"
        prompt_str += "<|im_start|>assistant\n"
        return prompt_str

    def call(self, messages: Sequence[Dict[str, str]], **kwargs: Any) -> str:
        print(f"CustomSGLangLLM.call received messages: {messages}")
        sglang_prompt = self._prepare_sglang_prompt(messages)
        print(f"CustomSGLangLLM.call prepared sglang_prompt (first 200 chars): {sglang_prompt[:200]}...")
        stop_sequences_for_sglang = kwargs.get("stop", ["<|im_end|>", "<|endoftext|>"])

        try:
            try:
                loop = asyncio.get_running_loop()
            except RuntimeError:
                loop = None

            async def async_runner():
                return await call_sglang_llm(
                    prompt=sglang_prompt,
                    temperature=self.temperature,
                    max_new_tokens=self.max_new_tokens_val,
                    stop_sequences=stop_sequences_for_sglang
                )

            if loop and loop.is_running():
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                    future = executor.submit(asyncio.run, async_runner())
                    response_text = future.result(timeout=120)
            else:
                response_text = asyncio.run(async_runner())

        except Exception as e:
            print(f"CustomSGLangLLM.call: Error during SGLang call: {type(e).__name__} - {e}")
            traceback.print_exc()
            return f"LLM_CALL_ERROR: 调用SGLang服务失败 - {str(e)}"

        if response_text is None:
            print("CustomSGLangLLM.call: SGLang returned None.")
            return "LLM_CALL_ERROR: SGLang服务未返回任何文本。"

        print(f"CustomSGLangLLM.call: SGLang returned text (first 200 chars): {response_text[:200]}...")
        return response_text

    def get_token_ids(self, text: str) -> List[int]:
        print("CustomSGLangLLM.get_token_ids: Not implemented, returning empty list.")
        return []

    @property
    def support_function_calling(self) -> bool:
        return False

    @property
    def support_stop_words(self) -> bool:
        return True

    @property
    def available_models(self) -> List[str]:
        return [self.model_name]

    @property
    def context_window(self) -> int:
        return 32768

    @property
    def identifying_params(self) -> Dict[str, Any]:
        return {
            "model": self.model_name,
            "endpoint_url": self.endpoint_url,
            "temperature": self.temperature,
            "max_new_tokens": self.max_new_tokens_val,
        }--- END OF FILE custom_llm.py ---

--- START OF FILE database.py ---# ZHZ_AGENT/database.py
import os
from databases import Database
from sqlalchemy import create_engine
from sqlalchemy.orm import declarative_base
from typing import Optional

# --- APScheduler 相关导入 ---
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.jobstores.sqlalchemy import SQLAlchemyJobStore
# --- [修改] 明确导入并使用 pytz ---
import pytz #

# --- 数据库配置 ---
ZHZ_AGENT_DIR = os.path.dirname(os.path.abspath(__file__))
DB_NAME = "ZHZ_AGENT_tasks.db"
DATABASE_FILE_PATH = os.path.join(ZHZ_AGENT_DIR, DB_NAME)
DATABASE_URL = f"sqlite+aiosqlite:///{DATABASE_FILE_PATH}"

database = Database(DATABASE_URL)
sqlalchemy_engine = create_engine(DATABASE_URL.replace("+aiosqlite", ""))
Base = declarative_base() #

# --- 全局调度器实例定义 ---
scheduler: Optional[AsyncIOScheduler] = None

def get_scheduler() -> AsyncIOScheduler:
    """获取或创建调度器实例，并配置作业存储和 UTC 时区。"""
    global scheduler
    if scheduler is None:
        jobstore_url = f"sqlite:///{DATABASE_FILE_PATH}"
        jobstores = {
            'default': SQLAlchemyJobStore(url=jobstore_url, tablename='apscheduler_jobs_v2') #
        }
        # --- [修复] 明确使用 pytz.utc 设置时区 ---
        scheduler = AsyncIOScheduler(
            jobstores=jobstores,
            timezone=pytz.utc # <--- 强制使用 pytz.utc #
        )
        import logging
        logging.getLogger('apscheduler').setLevel(logging.DEBUG)
        print(f"APScheduler initialized with timezone: {pytz.utc}") # 确认使用 pytz.utc #
    return scheduler #--- END OF FILE database.py ---

--- START OF FILE database_models.py ---# zhz_agent/database_models.py
from sqlalchemy import Column, String, DateTime, Integer, Text, Enum as SQLAlchemyEnum, ForeignKey, Boolean, JSON
from sqlalchemy.sql import func
import uuid

# --- [修改] 从 pydantic_models 导入枚举 -> 改为绝对导入 ---
from zhz_agent.pydantic_models import TaskStatus, ReminderMethod

# --- [修改] 从新的 database.py 导入 Base -> 改为绝对导入 ---
from zhz_agent.database import Base # <--- 确保只从这里导入 Base #

class TaskDB(Base): # 命名为 TaskDB 以区分 Pydantic 的 TaskModel
    __tablename__ = "tasks"

    id = Column(String, primary_key=True, index=True, default=lambda: str(uuid.uuid4()))
    title = Column(String, index=True, nullable=False)
    description = Column(Text, nullable=True) #
    status = Column(SQLAlchemyEnum(TaskStatus), default=TaskStatus.PENDING, nullable=False) #
    created_at = Column(DateTime(timezone=True), server_default=func.now(), nullable=False) #
    updated_at = Column(DateTime(timezone=True), server_default=func.now(), onupdate=func.now(), nullable=False) #
    due_date = Column(DateTime(timezone=True), nullable=True) #
    reminder_time = Column(DateTime(timezone=True), nullable=True) #
    reminder_offset_minutes = Column(Integer, nullable=True) #
    reminder_methods = Column(JSON, default=[ReminderMethod.NOTIFICATION.value], nullable=False) #
    priority = Column(Integer, default=0, nullable=False) #
    tags = Column(JSON, default=[], nullable=False) #
    action_type = Column(String, nullable=True) #
    action_payload = Column(JSON, default={}, nullable=True) #
    execution_result = Column(Text, nullable=True) #
    last_executed_at = Column(DateTime(timezone=True), nullable=True) #

    def __repr__(self):
        return f"<TaskDB(id={self.id}, title='{self.title}', status='{self.status.value}')>"--- END OF FILE database_models.py ---

--- START OF FILE fusion.py ---# zhz_agent/fusion.py
import hashlib
import jieba
import torch
import asyncio
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from typing import List, Dict, Any, Optional
import logging # <--- 新增导入 logging

# 从项目内部导入pydantic_models -> 改为绝对导入
from zhz_agent.pydantic_models import RetrievedDocument
import os

# 获取一个logger实例，可以与rag_service.py中的logger同名或不同名，
# 但如果希望输出到同一个文件，需要在rag_service.py中获取这个logger并配置handler
# 为了简单起见，这里我们让 FusionEngine 接收一个 logger 对象
# 如果没有传递logger，它会自己创建一个基本的控制台logger (或不记录详细步骤)

class FusionEngine:
    _current_script_path = os.path.abspath(__file__)
    _script_directory = os.path.dirname(_current_script_path)
    LOCAL_RERANKER_MODEL_PATH = os.path.join(_script_directory, "local_models", "bge-reranker-base")

    def __init__(self, logger: Optional[logging.Logger] = None): # <--- 修改构造函数以接收logger
        if logger:
            self.logger = logger
        else:
            # 如果没有提供logger，创建一个默认的，或者决定不记录详细步骤
            self.logger = logging.getLogger("FusionEngineLogger")
            if not self.logger.hasHandlers(): # 防止重复添加handler
                self.logger.setLevel(logging.INFO) # 或 DEBUG
                # console_handler = logging.StreamHandler()
                # console_handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(message)s'))
                # self.logger.addHandler(console_handler)
                # self.logger.propagate = False # 可选，如果不想让它也输出到根logger
                self.logger.info("FusionEngine initialized with its own basic logger.")
            else:
                self.logger.info("FusionEngine initialized, re-using existing logger configuration for FusionEngineLogger.")


        self.reranker_tokenizer: Optional[AutoTokenizer] = None
        self.reranker_model: Optional[AutoModelForSequenceClassification] = None
        self.reranker_device = "cuda" if torch.cuda.is_available() else "cpu"
        self._load_reranker_model()

    def _load_reranker_model(self):
        self.logger.info(f"FusionEngine: 正在加载重排序模型从本地路径: {self.LOCAL_RERANKER_MODEL_PATH} 到 {self.reranker_device}...")
        
        if not os.path.exists(self.LOCAL_RERANKER_MODEL_PATH):
            _error_msg_model_path = f"错误：重排序模型本地路径不存在: {self.LOCAL_RERANKER_MODEL_PATH}。请先运行 download_reranker_model.py 下载模型。"
            self.logger.error(_error_msg_model_path)
            raise RuntimeError(_error_msg_model_path)

        try:
            self.reranker_tokenizer = AutoTokenizer.from_pretrained(self.LOCAL_RERANKER_MODEL_PATH)
            self.reranker_model = AutoModelForSequenceClassification.from_pretrained(self.LOCAL_RERANKER_MODEL_PATH)
            self.reranker_model.to(self.reranker_device)

            if self.reranker_device == 'cuda':
                self.reranker_model.half()
                self.logger.info("FusionEngine: 重排序模型已加载到GPU并使用FP16。")
            else:
                self.logger.info("FusionEngine: 重排序模型已加载到CPU。")
            
            self.reranker_model.eval()
            self.logger.info("FusionEngine: 重排序模型加载成功！")
        except Exception as e:
            self.logger.error(f"错误: 重排序模型加载失败: {e}", exc_info=True)
            self.reranker_tokenizer = None
            self.reranker_model = None

    def _rerank_documents_sync(self, query: str, documents: List[RetrievedDocument]) -> List[RetrievedDocument]:
        if not self.reranker_model or not self.reranker_tokenizer:
            self.logger.warning("FusionEngine: 重排序模型未加载。无法进行精细重排序。返回原始文档。")
            return documents

        if not documents:
            self.logger.info("FusionEngine: 没有文档可供精细重排序。")
            return []

        pairs = [[query, doc.content] for doc in documents]
        self.logger.info(f"FusionEngine: 正在对 {len(documents)} 个文档进行精细重排序...")
        
        with torch.no_grad():
            inputs = self.reranker_tokenizer(
                pairs, 
                padding=True, 
                truncation=True, 
                return_tensors='pt', 
                max_length=512
            ).to(self.reranker_device)
            scores = self.reranker_model(**inputs).logits.view(-1).float().cpu().numpy()

        for i, doc in enumerate(documents):
            doc.score = float(scores[i])

        reranked_docs = sorted(documents, key=lambda doc: doc.score, reverse=True)
        self.logger.info(f"FusionEngine: 精细重排序后得到 {len(reranked_docs)} 个结果。")
        return reranked_docs

    def _tokenize_text(self, text: str) -> set[str]:
        return set(jieba.cut(text))

    def _calculate_jaccard_similarity(self, query_tokens: set[str], doc_tokens: set[str]) -> float:
        intersection = len(query_tokens.intersection(doc_tokens))
        union = len(query_tokens.union(doc_tokens))
        return intersection / union if union > 0 else 0.0
        
    async def fuse_results(self, 
                           all_raw_retrievals: List[RetrievedDocument],
                           user_query: str) -> str:
        

        self.logger.info(f"FusionEngine: 正在融合总计 {len(all_raw_retrievals)} 个原始召回结果。")
        
        combined_results: List[RetrievedDocument] = []
        seen_content_hashes = set()

        for doc in all_raw_retrievals:
            content_hash = hashlib.md5(doc.content.encode('utf-8')).hexdigest()
            if content_hash not in seen_content_hashes:
                combined_results.append(doc)
                seen_content_hashes.add(content_hash)
            else:
                self.logger.debug(f"FusionEngine: 发现重复内容，已跳过: {doc.content[:50]}...")

        combined_results.sort(key=lambda doc: (doc.score if doc.score is not None else -1.0, len(doc.content)), reverse=True)
        
        self.logger.debug(f"FusionEngine: 去重和初步排序后得到 {len(combined_results)} 个结果。")
        for i_doc, doc_fused in enumerate(combined_results):
            self.logger.debug(f"  Combined Doc {i_doc}: type={doc_fused.source_type}, score={doc_fused.score}, content='{str(doc_fused.content)[:100]}...'")

        # --- 修改筛选参数 ---
        JACCARD_THRESHOLD = 0.05 
        # 对于知识图谱这类精确但可能简短的结果，可以将最小长度设得很小，或者针对不同source_type设置不同阈值
        # 简单起见，我们先统一降低 MIN_DOC_LENGTH_CHARS
        MIN_DOC_LENGTH_CHARS_KG = 5    # 知识图谱结果的最小长度可以非常短
        MIN_DOC_LENGTH_CHARS_OTHER = 30 # 其他来源文档的最小长度可以保持稍大一些 (原为50)
        MAX_DOC_LENGTH_CHARS = 1000 

        query_tokens_set = self._tokenize_text(user_query)
        
        screened_results: List[RetrievedDocument] = []
        for doc in combined_results:
            current_min_doc_length = MIN_DOC_LENGTH_CHARS_OTHER
            if doc.source_type == "knowledge_graph":
                current_min_doc_length = MIN_DOC_LENGTH_CHARS_KG
            
            if not (current_min_doc_length <= len(doc.content) <= MAX_DOC_LENGTH_CHARS):
                self.logger.debug(f"FusionEngine: 过滤掉长度不符的文档 (长度: {len(doc.content)}, 要求范围: [{current_min_doc_length}-{MAX_DOC_LENGTH_CHARS}]): type={doc.source_type}, content='{doc.content[:50]}...'")
                continue

            # Jaccard相似度过滤可以考虑只对非KG结果应用，或者对KG结果使用不同阈值
            # 为简单起见，暂时对所有类型应用相同Jaccard阈值，但要注意KG结果通常与自然语言查询的词汇重叠度不高
            if doc.source_type != "knowledge_graph": # 或者可以给KG一个更低的Jaccard阈值
                doc_tokens_set = self._tokenize_text(doc.content)
                jaccard_score = self._calculate_jaccard_similarity(query_tokens_set, doc_tokens_set)
                if jaccard_score < JACCARD_THRESHOLD:
                    self.logger.debug(f"FusionEngine: 过滤掉Jaccard相似度过低的文档 (Jaccard: {jaccard_score:.2f}): type={doc.source_type}, content='{doc.content[:50]}...'")
                    continue
            else: # 对于KG结果，我们可以选择跳过Jaccard检查，或者使用一个非常低的阈值
                self.logger.debug(f"FusionEngine: 知识图谱结果 '{doc.content[:50]}...' 跳过Jaccard相似度检查或使用默认通过。")

            screened_results.append(doc)
        
        self.logger.debug(f"FusionEngine: 轻量级初筛后得到 {len(screened_results)} 个结果。")
        for i_doc, doc_screened in enumerate(screened_results):
            self.logger.debug(f"  Screened Doc {i_doc}: type={doc_screened.source_type}, score={doc_screened.score}, jaccard_with_query_approx={self._calculate_jaccard_similarity(query_tokens_set, self._tokenize_text(doc_screened.content)):.2f}, content='{str(doc_screened.content)[:100]}...'")
        
        final_fused_and_reranked_results = await asyncio.to_thread(
            self._rerank_documents_sync,
            query=user_query, 
            documents=screened_results
        )
        
        self.logger.debug(f"FusionEngine: 精细重排序后得到 {len(final_fused_and_reranked_results)} 个结果。")
        for i_doc, doc_reranked in enumerate(final_fused_and_reranked_results):
            self.logger.debug(f"  Reranked Doc {i_doc}: type={doc_reranked.source_type}, score={doc_reranked.score:.4f}, content='{str(doc_reranked.content)[:100]}...'")        
        context_parts = []
        # --- 新增：优先处理KG结果，或者给它们特殊标记 ---
        # 我们可以先将KG结果和其他结果分开，然后有策略地组合
        kg_docs = [doc for doc in final_fused_and_reranked_results if doc.source_type == "knowledge_graph"]
        other_docs = [doc for doc in final_fused_and_reranked_results if doc.source_type != "knowledge_graph"]

        # 优先将KG结果放在前面，或者给它们更强的引导词
        doc_counter = 1
        for doc in kg_docs:
            # 可以考虑增强KG结果的提示，例如：
            context_parts.append(f"【知识图谱精确信息】文档 {doc_counter} (来源: {doc.source_type}, 原始得分: {doc.score:.4f}):\n{doc.content}\n")
            doc_counter += 1
        
        for doc in other_docs:
            context_parts.append(f"【相关上下文】文档 {doc_counter} (来源: {doc.source_type}, 重排序得分: {doc.score:.4f}):\n{doc.content}\n")
            doc_counter += 1
        
        if not context_parts:
            self.logger.info("FusionEngine: 最终未找到相关信息可供生成上下文。")
            return "未在知识库中找到相关信息。"
        
        final_context = "\n".join(context_parts)
        self.logger.info(f"FusionEngine: 最终生成的上下文文本长度: {len(final_context)}。")
        return final_context--- END OF FILE fusion.py ---

--- START OF FILE kg.py ---# zhz_agent/kg.py
import json
import os
from typing import List, Dict, Any, Optional
from neo4j import GraphDatabase, basic_auth
import asyncio
import logging # <--- 导入 logging

# --- [修改] 配置日志 ---
# 获取 kg.py 文件所在的目录
_kg_py_dir = os.path.dirname(os.path.abspath(__file__))
log_file_path = os.path.join(_kg_py_dir, 'kg_retriever.log')

# 获取一个特定的记录器实例
kg_logger = logging.getLogger(__name__) # __name__ 将是 'zhz_agent.kg'
kg_logger.setLevel(logging.DEBUG) # 为此记录器设置级别

# 阻止消息传播到根记录器的处理程序
# 如果应用程序的其他部分配置根记录器进行控制台输出，这一点很重要
kg_logger.propagate = False

# 清除此记录器实例上任何现有的处理程序 (例如，来自交互式会话中的先前运行)
if kg_logger.hasHandlers():
    kg_logger.handlers.clear()

# 创建一个文件处理程序
try:
    file_handler = logging.FileHandler(log_file_path, mode='w') # mode='w' 表示覆盖
    file_handler.setLevel(logging.DEBUG) # 为此处理程序设置级别

    # 创建一个格式化程序并为处理程序设置它
    # 添加了 %(name)s 来显示记录器的名称，有助于调试
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(filename)s:%(lineno)d - %(name)s - %(message)s')
    file_handler.setFormatter(formatter)

    # 将处理程序添加到记录器
    kg_logger.addHandler(file_handler)
    # 使用新的配置记录一条消息
    kg_logger.info("--- KG logging reconfigured to write to kg_retriever.log (dedicated handler) ---")

except Exception as e:
    # 如果记录器设置失败，则回退到打印，以便我们看到错误
    print(f"CRITICAL: Failed to configure file handler for kg_logger: {e}")
    # （可选）如果文件处理程序失败，可以添加一个控制台处理程序作为备用
    # console_handler = logging.StreamHandler()
    # console_handler.setFormatter(formatter)
    # kg_logger.addHandler(console_handler)
    # kg_logger.error(f"Failed to configure file handler, logging to console instead. Error: {e}", exc_info=True)


from zhz_agent.llm import generate_simulated_kg_query_response, generate_cypher_query
from zhz_agent.pydantic_models import RetrievedDocument
from dotenv import load_dotenv
load_dotenv()

class KGRetriever:
    NEO4J_URI = os.getenv("NEO4J_URI")
    NEO4J_USER = os.getenv("NEO4J_USER")
    NEO4J_PASSWORD = os.getenv("NEO4J_PASSWORD")

    def __init__(self, data_path: str, llm_cypher_generator_func: callable):
        self.data_path = data_path
        self.llm_cypher_generator_func = llm_cypher_generator_func
        self.kg_schema_description: str = ""
        self.kg_data_summary: str = ""
        self.kg_facts: List[Dict[str, Any]] = []
        self._load_kg_data()
        self._driver = None
        self._connect_to_neo4j()
        kg_logger.info("KGRetriever initialized.") # 这条日志现在应该只去 kg_retriever.log

    def _connect_to_neo4j(self):
        try:
            self._driver = GraphDatabase.driver(self.NEO4J_URI, auth=basic_auth(self.NEO4J_USER, self.NEO4J_PASSWORD))
            self._driver.verify_connectivity()
            kg_logger.info("成功连接到 Neo4j!")
        except Exception as e:
            kg_logger.error(f"连接 Neo4j 失败: {e}", exc_info=True)
            self._driver = None
    
    def close(self):
        if self._driver:
            self._driver.close()
            kg_logger.info("已关闭 Neo4j 连接。")

    def _load_kg_data(self):
        file_path = os.path.join(self.data_path, "sample_kg.json")
        if not os.path.exists(file_path):
            kg_logger.error(f"模拟知识图谱文件不存在: {file_path}")
            return

        with open(file_path, 'r', encoding='utf-8') as f:
            kg_data = json.load(f)
        
        self.kg_schema_description = kg_data.get("schema_description", "")
        self.kg_data_summary = kg_data.get("data_summary", "")
        self.kg_facts = kg_data.get("facts", [])
        kg_logger.info(f"加载了知识图谱Schema和数据摘要。包含 {len(self.kg_facts)} 条模拟事实。")

    async def retrieve(self, query: str, top_k: int = 2) -> List[RetrievedDocument]:
        kg_logger.info(f"开始执行知识图谱检索，查询: '{query}', top_k: {top_k}")
        if not self._driver:
            kg_logger.warning("Neo4j驱动未初始化或连接失败。无法执行知识图谱查询。")
            return []
        if not self.kg_schema_description:
            kg_logger.warning("知识图谱Schema描述未加载。无法生成Cypher查询。")
            return []

        kg_logger.info(f"正在调用LLM生成Cypher查询，问题: {query}")
        cypher_query = await self.llm_cypher_generator_func(
            user_question=query,
            kg_schema_description=self.kg_schema_description
        )
        
        kg_logger.info(f"LLM生成的Cypher查询:\n---\n{cypher_query}\n---")

        if cypher_query == "无法生成Cypher查询。":
            kg_logger.warning("LLM明确表示无法生成Cypher查询。")
            return []

        retrieved_results: List[RetrievedDocument] = []
        try:
            kg_logger.debug(f"准备在Neo4j中执行查询。")
            # records = await asyncio.to_thread(
            #     self._driver.session(database="neo4j").run, # 在同步代码中运行Neo4j查询
            #     cypher_query
            # )
            # 修改为在异步函数中正确使用同步的 Neo4j 驱动调用
            session = self._driver.session(database="neo4j")
            try:
                result = await asyncio.to_thread(session.run, cypher_query)
                records = await asyncio.to_thread(list, result) # 将结果具体化
            finally:
                await asyncio.to_thread(session.close)

            kg_logger.debug(f"Neo4j查询执行完毕，开始处理结果。")
            
            count = 0
            for record in records: # records 现在是一个列表
                content = self._format_neo4j_record_as_text(record.data())
                if content:
                    retrieved_results.append(
                        RetrievedDocument(
                            source_type="knowledge_graph",
                            content=content,
                            score=1.0,
                            metadata={"cypher_query": cypher_query, "original_query": query}
                        )
                    )
                    count += 1
                    if count >= top_k:
                        break
            kg_logger.info(f"检索到 {len(retrieved_results)} 个知识图谱结果。")
            return retrieved_results

        except Exception as e:
            kg_logger.error(f"执行Cypher查询失败。错误的查询是:\n---\n{cypher_query}\n---")
            kg_logger.error(f"具体的Neo4j错误信息: {e}", exc_info=True)
            return []

    def _format_neo4j_record_as_text(self, record_data: Dict[str, Any]) -> str:
        formatted_parts = []
        for key, value in record_data.items():
            if isinstance(value, dict) and 'name' in value: # 假设节点对象以字典形式返回，并包含name属性
                formatted_parts.append(f"{key}: {value['name']}")
            elif isinstance(value, dict) and 'amount' in value and 'period' in value: # 特殊处理 SalesAmount
                 formatted_parts.append(f"{key}: {value['amount']} ({value['period']})")
            elif isinstance(value, (str, int, float)):
                formatted_parts.append(f"{key}: {value}")
            # 可以根据需要添加更多对不同数据类型的处理
        
        if formatted_parts:
            return ", ".join(formatted_parts)
        return "" # 如果记录为空或无法格式化，则返回空字符串--- END OF FILE kg.py ---

--- START OF FILE llm.py ---# zhz_agent_project/llm.py
import os
import httpx # 用于异步HTTP请求
import json # 用于处理JSON数据
import asyncio # 用于 asyncio.to_thread
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv
import traceback # Ensure traceback is imported
import logging
load_dotenv() # 确保加载.env文件

# --- 为 llm.py 配置一个logger ---
llm_py_logger = logging.getLogger("LLMUtilsLogger") # 给一个独特的名字
llm_py_logger.setLevel(logging.INFO) # 可以设置为 INFO 或 DEBUG

# 防止重复添加handler，如果这个模块被多次导入或初始化
if not llm_py_logger.hasHandlers():
    _llm_console_handler = logging.StreamHandler() # 输出到控制台
    _llm_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
    _llm_console_handler.setFormatter(_llm_formatter)
    llm_py_logger.addHandler(_llm_console_handler)
    llm_py_logger.propagate = False # 通常不希望 utils 模块的日志传播到根

llm_py_logger.info("--- LLMUtilsLogger configured ---")
# --- 结束logger配置 ---


# --- 定义LLM在无法回答时应该输出的精确短语 (不含停止标记) ---
NO_ANSWER_PHRASE_ANSWER_CLEAN = "根据提供的信息，无法回答该问题。"
NO_ANSWER_PHRASE_KG_CLEAN = "从知识图谱中未找到直接相关信息。"

# --- 定义通用的唯一停止标记 ---
UNIQUE_STOP_TOKEN = "<|im_endofunable|>" # 报告建议的独特停止标记

# --- 定义LLM在Prompt中被要求输出的，包含特殊停止标记的短语 ---
NO_ANSWER_PHRASE_ANSWER_WITH_STOP_TOKEN = f"{NO_ANSWER_PHRASE_ANSWER_CLEAN}{UNIQUE_STOP_TOKEN}"
NO_ANSWER_PHRASE_KG_WITH_STOP_TOKEN = f"{NO_ANSWER_PHRASE_KG_CLEAN}{UNIQUE_STOP_TOKEN}"

# --- 配置 SGLang LLM API ---
SGLANG_API_URL = os.getenv("SGLANG_API_URL") 
# 确保SGLang服务正在运行，并且这个URL是正确的

async def call_sglang_llm(prompt: str, 
                          temperature: float = 0.2, 
                          max_new_tokens: Optional[int] = 1024,
                          stop_sequences: Optional[List[str]] = None) -> Optional[str]:
    
    llm_py_logger.debug(f"Attempting to call SGLang LLM. Prompt (first 100 chars): {prompt[:100]}...") # 例如替换

    payload = {
        "text": prompt,
        "sampling_params": {
            "temperature": temperature,
            "max_new_tokens": max_new_tokens,
            "stop": stop_sequences if stop_sequences else [] 
        }
    }
    headers = {"Content-Type": "application/json"}

    try:
        print(f"[LLM_DEBUG] Payload constructed. SGLANG_API_URL: {SGLANG_API_URL}")
        async with httpx.AsyncClient(timeout=120.0) as client: 
            print("[LLM_DEBUG] httpx.AsyncClient created. About to POST...")
            response = await client.post(SGLANG_API_URL, json=payload, headers=headers)
            print(f"[LLM_DEBUG] POST request sent. Status code: {response.status_code}")
            
            response.raise_for_status()
            print("[LLM_DEBUG] response.raise_for_status() passed.")

            response_json = response.json() # 尝试解析JSON
            print("[LLM_DEBUG] response.json() successful.")
            
            llm_output_text = response_json.get("text", "[[TEXT_FIELD_NOT_FOUND]]").strip() # 获取text字段，如果不存在则用特殊标记
            print(f"\n<<<<<<<<<< SGLANG LLM INPUT PROMPT START (call_sglang_llm) >>>>>>>>>>\n{prompt}\n<<<<<<<<<< SGLANG LLM INPUT PROMPT END >>>>>>>>>>\n")
            print(f"\n>>>>>>>>>> SGLANG LLM RAW OUTPUT TEXT START (call_sglang_llm) >>>>>>>>>>\n{llm_output_text}\n>>>>>>>>>> SGLANG LLM RAW OUTPUT TEXT END >>>>>>>>>>\n")
            llm_py_logger.debug(f"Full SGLang raw response JSON: {response.text}") # 例如替换

            meta_info = response_json.get("meta_info", {})
            finish_reason = meta_info.get("finish_reason", {})
            print(f"[LLM_DEBUG] Meta info: {meta_info}, Finish reason: {finish_reason}")

            if finish_reason.get("type") == "stop" and finish_reason.get("matched") == UNIQUE_STOP_TOKEN:
                return llm_output_text.split(UNIQUE_STOP_TOKEN)[0].strip() if llm_output_text and llm_output_text != "[[TEXT_FIELD_NOT_FOUND]]" else NO_ANSWER_PHRASE_ANSWER_CLEAN
            
            return llm_output_text if llm_output_text != "[[TEXT_FIELD_NOT_FOUND]]" else None

    except httpx.HTTPStatusError as e: # 更具体的HTTP错误
        print(f"[LLM_DEBUG] httpx.HTTPStatusError: {e}")
        print(f"[LLM_DEBUG] Response content causing status error: {e.response.text[:500]}")
        traceback.print_exc()
        return None
    except httpx.RequestError as e:
        print(f"[LLM_DEBUG] httpx.RequestError: {e}")
        traceback.print_exc()
        return None
    except json.JSONDecodeError as e:
        response_text_for_debug = "N/A"
        if 'response' in locals() and hasattr(response, 'text'):
            response_text_for_debug = response.text[:500]
        print(f"[LLM_DEBUG] json.JSONDecodeError: {e}. Raw response text: {response_text_for_debug}")
        traceback.print_exc()
        return None
    except Exception as e:
        print(f"[LLM_DEBUG] Unknown error in call_sglang_llm: {type(e).__name__} - {e}")
        traceback.print_exc()
        return None

async def generate_answer_from_context(user_query: str, context: str) -> Optional[str]:
    
    prompt = f"""<|im_start|>system
你是一个精确且忠实的问答助手。你的任务是严格根据用户问题和下面提供的【上下文信息】，生成一个准确、简洁、直接的答案。

**【回答核心准则 - 必须严格遵守！】**

1.  **【忠实于上下文 - 最高优先级】**: 你的所有回答都**必须完全基于**提供的【上下文信息】。**绝对不允许**依赖任何外部知识、个人观点或进行任何形式的推测。如果上下文中没有明确支持的信息，就必须指出信息未知或无法回答。

2.  **【优先采纳精确信息】**: 如果上下文中包含以“【知识图谱精确信息】”标记的片段，这代表了高置信度的结构化事实。请**优先直接采纳这些信息**来回答用户问题的对应部分。忽略该片段的原始得分。

3.  **【处理组合问题/多方面问题】**:
    *   仔细分析用户问题，识别其中是否包含多个子问题或期望获取多个方面的信息（例如，通过“和”、“以及”连接，或一个问句包含多个疑问点）。
    *   对于每个识别出的子问题或信息点，请在【上下文信息】中独立查找答案。
    *   将找到的各个方面的答案清晰地组织起来。如果某个方面的信息在上下文中找不到，**必须明确指出该方面信息未知或未提供**，例如：“关于[问题的某个方面]，上下文中未提供具体信息。” **不要因为部分信息缺失而放弃回答其他能找到信息的部分。**

4.  **【处理数值与计算】**:
    *   如果用户问题需要对上下文中的数字进行计算（如求和、求差、平均等），并且上下文中包含可用于计算的明确数字（注意识别数字和单位），请执行计算并在答案中清晰展示计算结果和原始数据。
    *   例如，如果上下文说“A产品销售额50万元，B产品销售额30万元”，用户问“AB产品总销售额”，你应该回答“A产品销售额为50万元，B产品销售额为30万元，总销售额为80万元。”
    *   如果上下文中的数字不清晰、单位不一致难以换算，或者进行计算的条件不足，请列出原始数据并说明无法精确计算。

5.  **【信息不足的最终判断】**: 如果综合运用以上规则后，【上下文信息】中**完全没有**与用户问题相关的任何信息，或者完全无法回答用户问题的任何一个方面，则**必须直接且完整地**输出：“{NO_ANSWER_PHRASE_ANSWER_WITH_STOP_TOKEN}”

**【输出要求】**: 你的回答必须是纯文本，直接给出答案本身，语气应客观、专业。避免使用对话标记或不必要的解释性语句，除非是为了说明信息来源或缺失情况。
<|im_end|>
<|im_start|>user
用户问题: {user_query}

上下文信息:
{context}
<|im_end|>
<|im_start|>assistant
"""
    stop_sequences = ["<|im_end|>", UNIQUE_STOP_TOKEN] 
    
    # 保持较低的temperature以获取更确定的答案，除非特定任务需要创造性
    return await call_sglang_llm(prompt, temperature=0.05, max_new_tokens=512, stop_sequences=stop_sequences)# 降低温度，鼓励直接提取

async def generate_simulated_kg_query_response(user_query: str, kg_schema_description: str, kg_data_summary_for_prompt: str) -> Optional[str]:
    """
    模拟Text-to-Cypher和知识图谱查询。
    这里我们不实际生成Cypher，而是让LLM直接根据问题和KG描述生成“事实片段”。
    """
    prompt = f"""<|im_start|>system
你是一个知识图谱查询助手。你的任务是根据用户提出的问题、知识图谱Schema描述和图谱中的数据摘要，直接抽取出与问题最相关的1-2个事实片段作为答案。
只输出事实片段，不要解释，不要生成Cypher语句，不要包含任何额外对话或标记。
如果找不到直接相关的事实，请**直接且完整地**回答：“{NO_ANSWER_PHRASE_KG_WITH_STOP_TOKEN}”
<|im_end|>
<|im_start|>user
知识图谱Schema描述:
{kg_schema_description}

知识图谱数据摘要: 
{kg_data_summary_for_prompt}

用户问题: {user_query}
<|im_end|>
<|im_start|>assistant
"""
    stop_sequences = ["<|im_end|>", UNIQUE_STOP_TOKEN]
    
    return await call_sglang_llm(prompt, temperature=0.5, max_new_tokens=256, stop_sequences=stop_sequences)

async def generate_expanded_queries(original_query: str) -> List[str]:
    """
    利用LLM从用户原始查询生成多个语义相关但表述各异的子问题或扩展查询。
    """
    prompt = f"""<|im_start|>system
你是一个专家查询分析师。根据用户提供的查询，生成3个不同但相关的子问题，以探索原始查询的不同方面。这些子问题将用于检索更全面的信息。
你的回答必须是一个JSON数组（列表），其中每个元素是一个字符串（子问题）。
只输出JSON数组，不要包含任何其他解释、对话标记或代码块。

示例:
用户查询: "公司年度财务报告和未来一年的预算规划"
助手:
[
  "公司最近的年度财务报告总结是什么？",
  "未来一年的详细预算规划有哪些主要构成？",
  "对比往年，公司财务状况有何显著变化？"
]
<|im_end|>
<|im_start|>user
原始查询: {original_query}
<|im_end|>
<|im_start|>assistant
"""
    stop_sequences = ["<|im_end|>"]
    
    print(f"调用SGLang LLM API进行查询扩展 (Prompt长度: {len(prompt)} 字符)...")
    llm_output = await call_sglang_llm(
        prompt, 
        temperature=0.7, # 稍高温度，鼓励生成多样性
        max_new_tokens=512, # 允许生成较长的JSON列表
        stop_sequences=stop_sequences
    )
    expanded_queries = []
    if llm_output:
        try:
            json_str = llm_output.strip()
            if json_str.startswith("```json"):
                json_str = json_str[len("```json"):].strip()
                if json_str.endswith("```"):
                    json_str = json_str[:-len("```")].strip()
            
            parsed_queries = json.loads(json_str)
            if isinstance(parsed_queries, list) and all(isinstance(q, str) for q in parsed_queries):
                expanded_queries = parsed_queries
                print(f"LLM成功生成 {len(expanded_queries)} 个扩展查询。")
            else:
                print(f"LLM生成的扩展查询JSON格式不符合预期 (不是字符串列表): {llm_output[:200]}...")
        except json.JSONDecodeError as e:
            print(f"解析LLM扩展查询JSON失败: {e}. 原始输出: {llm_output[:200]}...")
        except Exception as e:
            print(f"处理LLM扩展查询时发生未知错误: {e}. 原始输出: {llm_output[:200]}...")
    else:
        print("LLM未能生成扩展查询。")

    expanded_queries.append(original_query) 
    return expanded_queries

async def generate_cypher_query(user_question: str, kg_schema_description: str) -> Optional[str]:
    """
    利用LLM将自然语言问题转换为Neo4j Cypher查询语句。
    """
    prompt = f"""<|im_start|>system
你是顶级Neo4j Cypher查询生成专家。你的任务是根据用户问题和严格提供的【知识图谱Schema】，生成一个【语法正确】、【逻辑合理】且【高效】的Cypher查询。

**【核心指令与约束 - 必须严格遵守！】**

1.  **【Schema绝对绑定 - 最高优先级】**:
    *   你生成的Cypher查询中所有用到的【节点标签】、【关系类型】、【属性名称】及其对应的【数据类型】，都**必须严格存在于**下面提供的 "知识图谱Schema描述" 中。
    *   在构建查询的每一步，都要反复与Schema核对。**严禁臆断、猜测或使用任何Schema中未明确定义的元素。**
    *   **属性名称的大小写和确切拼写必须与Schema完全一致。**
    *   **关系类型的名称和方向必须与Schema完全一致。** 例如，如果Schema定义为 `(Person)-[:WORKS_ON]->(Project)`，则查询中不能是 `(Project)-[:WORKS_ON]->(Person)`，除非Schema中也定义了反向关系。

2.  **【纯净输出格式 - 严格要求】**:
    *   如果能生成有效查询，你的回答**必须只包含纯粹的Cypher查询语句本身**。
    *   如果根据问题和Schema无法生成有效的Cypher查询（例如，问题超出了Schema表达能力，或问题本身逻辑不通），则**必须只输出固定的短语：“无法生成Cypher查询。”**
    *   **绝对禁止**在有效的Cypher语句前后添加任何前缀（如“Cypher查询: ”）、后缀、解释、注释、markdown标记（如 ```cypher ```）或任何其他多余的文本。

3.  **【属性与值的使用】**:
    *   当在`WHERE`子句中对属性进行匹配时，确保值的类型与Schema中定义的属性类型一致。例如，如果`name`是字符串，则匹配 `name: '张三'`；如果`year`是数字，则匹配 `year: 2023`。
    *   对于数值计算（如`SUM`, `AVG`），**必须**使用Schema中明确指定的数字类型属性（例如，`SalesAmount`节点的 `numeric_amount`）。

4.  **【查询构建逻辑指引】**:
    *   **实体识别**: 准确识别用户问题中的核心实体及其在Schema中对应的节点标签和属性。
    *   **关系路径**: 基于问题和Schema构建清晰的`MATCH`路径。
    *   **条件过滤**: 使用`WHERE`子句添加必要的过滤条件。
    *   **结果返回**: 使用`RETURN`子句指定需要返回的信息，并用`AS`为返回的列指定清晰、合法的别名（字母或下划线开头）。
    *   **多步查询**: 对于需要关联多个信息点的问题，合理使用`WITH`传递中间结果。
    *   **聚合**: 如需统计或汇总，正确使用`COUNT()`, `SUM()`, `COLLECT()`等聚合函数。

**【知识图谱Schema描述】**:
{kg_schema_description}

**【查询示例 - 严格基于上述Schema】**:

*   用户问题: "张三参与了哪个项目？"
    Cypher查询: MATCH (p:Person {{name: '张三'}})-[:WORKS_ON]->(proj:Project) RETURN proj.name AS projectName

*   用户问题: "华东区域2024年第一季度的销售额是多少？"
    Cypher查询: MATCH (r:Region {{name: '华东'}})-[:HAS_SALES_AMOUNT]->(sa:SalesAmount {{period: '2024年第一季度'}}) RETURN sa.numeric_amount AS salesAmount, sa.unit AS salesUnit

*   用户问题: "查询所有产品的名称。"
    Cypher查询: MATCH (prod:Product) RETURN prod.name AS productName

*   用户问题: "项目X有哪些人参与？"
    Cypher查询: MATCH (p:Person)-[:WORKS_ON]->(proj:Project {{name: '项目X'}}) RETURN p.name AS participantName

*   用户问题: "2024年第一季度所有区域的总销售额是多少？"
    Cypher查询: MATCH (r:Region)-[:HAS_SALES_AMOUNT]->(sa:SalesAmount {{period: '2024年第一季度'}}) RETURN sum(sa.numeric_amount) AS totalSales, sa.unit AS commonUnit LIMIT 1 
    (此查询假设所有相关销售额的单位是相同的，并取第一个出现的单位作为代表)

*   用户问题: "与新产品A相关的文档ID是什么？"
    Cypher查询: MATCH (p:Product {{name: '新产品A'}})-[:RELATED_TO]->(d:Document) RETURN d.id AS documentId

*   用户问题: "公司CEO是谁？" (假设Schema中没有CEO信息)
    Cypher查询: 无法生成Cypher查询。

现在，请根据以下用户问题和上述Schema及规则生成Cypher查询。
<|im_end|>
<|im_start|>user
用户问题: {user_question}
<|im_end|>
<|im_start|>assistant
"""
    # ... (后续的LLM调用和后处理逻辑与您当前版本一致) ...
    stop_sequences = ["<|im_end|>", "无法生成Cypher查询。"] 
    
    cypher_query = await call_sglang_llm(
        prompt, 
        temperature=0.0, 
        max_new_tokens=400, # 稍微增加一点，以防复杂查询需要更多空间
        stop_sequences=stop_sequences
    )
    
    if not cypher_query or cypher_query.strip() == "" or cypher_query.strip().lower() == "无法生成cypher查询。" or "无法生成cypher查询" in cypher_query.strip().lower():
        llm_py_logger.warning(f"LLM未能生成有效Cypher查询或明确表示无法生成，原始输出: '{cypher_query}'") # <--- 使用 llm_py_logger
        return "无法生成Cypher查询。" 
    
    processed_query = cypher_query.strip()
    prefixes_to_remove = ["cypher查询:", "cypher query:", "```cypher", "```sql", "```"]
    for prefix in prefixes_to_remove:
        if processed_query.lower().startswith(prefix.lower()):
            processed_query = processed_query[len(prefix):].strip()
    
    if processed_query.endswith("```"):
        processed_query = processed_query[:-len("```")].strip()

    if not processed_query:
        llm_py_logger.warning(f"LLM生成的Cypher查询在移除常见前缀/后缀后为空，原始输出: '{cypher_query}'") # <--- 使用 llm_py_logger

    llm_py_logger.info(f"LLM成功生成Cypher查询 (后处理后): {processed_query}")
    return processed_query

async def generate_clarification_question(original_query: str, uncertainty_reason: str) -> Optional[str]:
    """
    利用LLM根据用户原始查询和不确定性原因，生成一个具体的澄清问题。
    """
    prompt = f"""<|im_start|>system
你是一个智能助手，擅长在理解用户查询时识别歧义并请求澄清。
你的任务是根据用户原始查询和系统检测到的不确定性原因，生成一个简洁、明确的澄清问题。
澄清问题应该帮助用户选择正确的意图，或者提供更多必要的信息。
只输出澄清问题，不要包含任何额外解释、对话标记或代码块。
<|im_end|>
<|im_start|>user
用户原始查询: {original_query}
不确定性原因: {uncertainty_reason}

请生成一个澄清问题:
<|im_end|>
<|im_start|>assistant
"""
    stop_sequences = ["<|im_end|>"]
    print(f"调用SGLang LLM API生成澄清问题 (Prompt长度: {len(prompt)} 字符)...")
    clarification_question = await call_sglang_llm(
        prompt, 
        temperature=0.5, # 适中温度，鼓励生成合理的问题
        max_new_tokens=128, # 澄清问题通常不会太长
        stop_sequences=stop_sequences
    )
    if not clarification_question or clarification_question.strip() == "":
        print("LLM未能生成澄清问题，返回默认提示。")
        return "抱歉，我不太理解您的意思，请您再具体说明一下。"
    print(f"LLM成功生成澄清问题: {clarification_question}")
    return clarification_question

async def generate_clarification_options(original_query: str, uncertainty_reason: str) -> List[str]:
    """
    利用LLM根据用户原始查询和不确定性原因，生成多个具体的澄清选项。
    """
    prompt = f"""<|im_start|>system
你是一个智能助手，擅长根据用户查询的模糊性提供具体的澄清选项。
你的任务是根据用户原始查询和系统检测到的不确定性原因，生成3-5个具体的、可供用户选择的澄清选项。
每个选项都应该是一个简洁的短语或问题，帮助用户明确其意图。
你的回答必须是一个JSON数组（列表），其中每个元素是一个字符串（澄清选项）。
只输出JSON数组，不要包含任何其他解释、对话标记或代码块。

示例:
用户查询: "帮我预定机票。"
不确定性原因: "缺少出发城市、目的地、日期等信息。"
助手:
[
  "请问您想从哪个城市出发？",
  "请问您的目的地是哪里？",
  "请问您希望在哪一天出行？",
  "您有偏好的航空公司或舱位等级吗？"
]
<|im_end|>
<|im_start|>user
用户原始查询: {original_query}
不确定性原因: {uncertainty_reason}

请生成澄清选项:
<|im_end|>
<|im_start|>assistant
"""
    stop_sequences = ["<|im_end|>"]
    print(f"调用SGLang LLM API生成澄清选项 (Prompt长度: {len(prompt)} 字符)...")
    llm_output = await call_sglang_llm(
        prompt,
        temperature=0.7, # 稍高温度，鼓励生成多样性
        max_new_tokens=256,
        stop_sequences=stop_sequences
    )

    options = []
    if llm_output:
        try:
            json_str = llm_output.strip()
            if json_str.startswith("```json"):
                json_str = json_str[len("```json"):].strip()
                if json_str.endswith("```"):
                    json_str = json_str[:-len("```")].strip()
            
            parsed_options = json.loads(json_str)
            if isinstance(parsed_options, list) and all(isinstance(o, str) for o in parsed_options):
                options = parsed_options
                print(f"LLM成功生成 {len(options)} 个澄清选项。")
            else:
                print(f"LLM生成的澄清选项JSON格式不符合预期 (不是字符串列表): {llm_output[:200]}...")
        except json.JSONDecodeError as e:
            print(f"解析LLM澄清选项JSON失败: {e}. 原始输出: {llm_output[:200]}...")
        except Exception as e:
            print(f"处理LLM澄清选项时发生未知错误: {e}. 原始输出: {llm_output[:200]}...")
    else:
        print("LLM未能生成澄清选项。")
    
    if not options:
        options.append("请提供更多详细信息。")
    
    return options

async def generate_intent_classification(user_query: str) -> Dict[str, Any]:
    """
    利用LLM对用户查询进行意图分类，判断是否需要澄清。
    返回一个字典，包含 'clarification_needed' (bool) 和 'reason' (str)。
    """
    prompt = f"""<|im_start|>system
你是一个智能意图分类器。你的任务是分析用户查询，判断该查询是否清晰明确，或者是否存在歧义、信息不足导致需要进一步澄清。
如果查询包含具体的命名实体（如人名“张三”、项目名“项目X”、产品名“新产品A”等），并且问题是关于这些实体的特定信息（例如“张三的职位是什么？”、“项目X的截止日期是哪天？”、“新产品A的功能有哪些？”），则通常认为查询是清晰的，不需要澄清。
只有当查询缺少定位关键信息所必需的核心实体，或者询问的范围过于宽泛无法直接操作时，才需要澄清。

如果查询需要澄清，请说明原因。
你的回答必须是一个JSON对象，包含两个字段：
1. "clarification_needed": 布尔值，如果需要澄清则为 true，否则为 false。
2. "reason": 字符串，如果需要澄清，请简要说明原因；如果不需要，则为空字符串。

示例1 (需要澄清 - 信息不足):
用户查询: "帮我预定明天去上海的机票。"
助手:
{{
  "clarification_needed": true,
  "reason": "缺少出发城市、具体时间（上午/下午/晚上）、舱位等级等信息。"
}}

示例2 (不需要澄清 - 清晰):
用户查询: "公司最新的销售额报告在哪里可以找到？"
助手:
{{
  "clarification_needed": false,
  "reason": ""
}}

示例3 (需要澄清 - 实体不明确):
用户查询: "关于项目进展的文档。"
助手:
{{
  "clarification_needed": true,
  "reason": "项目名称不明确，文档类型（报告、计划、会议纪要等）不明确。"
}}

示例4 (不需要澄清 - 包含具体实体和明确问题):
用户查询: "张三参与了哪个项目？"
助手:
{{
  "clarification_needed": false,
  "reason": ""
}}

示例5 (不需要澄清 - 包含具体实体和明确问题):
用户查询: "华东区域2024年第一季度的销售额是多少？"
助手:
{{
  "clarification_needed": false,
  "reason": ""
}}

示例6 (需要澄清 - “公司”指默认上下文，但其余部分仍模糊):
用户查询: "公司的政策"
助手:
{{
  "clarification_needed": true,
  "reason": "未能明确指出是关于哪方面的公司政策（例如：人力资源、IT安全、财务等）。"
}}
<|im_end|>
<|im_start|>user
用户查询: {user_query}
<|im_end|>
<|im_start|>assistant
"""
    stop_sequences = ["<|im_end|>"]
    print(f"调用SGLang LLM API进行意图分类 (Prompt长度: {len(prompt)} 字符)...")

    llm_output = await call_sglang_llm(
        prompt,
        temperature=0.1, # 较低温度，鼓励生成结构化、确定的答案
        max_new_tokens=256,
        stop_sequences=stop_sequences
    )

    if llm_output:
        try:
            json_str = llm_output.strip()
            if json_str.startswith("```json"):
                json_str = json_str[len("```json"):].strip()
                if json_str.endswith("```"):
                    json_str = json_str[:-len("```")].strip()

            parsed_result = json.loads(json_str)
            if isinstance(parsed_result, dict) and \
               "clarification_needed" in parsed_result and \
               "reason" in parsed_result:
                print(f"LLM成功进行意图分类: {parsed_result}")
                return parsed_result
            else:
                print(f"LLM生成的意图分类JSON格式不符合预期: {llm_output[:200]}...")
        except json.JSONDecodeError as e:
            print(f"解析LLM意图分类JSON失败: {e}. 原始输出: {llm_output[:200]}...")
        except Exception as e:
            print(f"处理LLM意图分类时发生未知错误: {e}. 原始输出: {llm_output[:200]}...")

    print("LLM未能生成有效的意图分类结果，默认不需澄清。")
    return {"clarification_needed": False, "reason": "LLM分类失败或无结果。"}
--- END OF FILE llm.py ---

--- START OF FILE load_neo4j_data.py ---# zhz_agent/load_neo4j_data.py
import json
import os
from neo4j import GraphDatabase, basic_auth
from dotenv import load_dotenv
import traceback

load_dotenv() # 确保加载 .env 文件中的NEO4J凭证

NEO4J_URI = os.getenv("NEO4J_URI", "bolt://localhost:7687")
NEO4J_USER = os.getenv("NEO4J_USER", "neo4j")
NEO4J_PASSWORD = os.getenv("NEO4J_PASSWORD") # 您需要确保这个密码是正确的

DATA_PATH = os.path.join(os.path.dirname(__file__), "data")
SAMPLE_KG_PATH = os.path.join(DATA_PATH, "sample_kg.json")

def clear_database(driver):
    """清除数据库中的所有节点和关系"""
    with driver.session(database="neo4j") as session:
        session.run("MATCH (n) DETACH DELETE n")
        print("Cleared all nodes and relationships from the database.")

def create_constraints(driver):
    """创建一些基本约束，确保节点属性的唯一性（如果适用）"""
    with driver.session(database="neo4j") as session:
        try:
            session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (p:Person) REQUIRE p.name IS UNIQUE")
            session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (pr:Project) REQUIRE pr.name IS UNIQUE")
            session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (r:Region) REQUIRE r.name IS UNIQUE")
            session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (prod:Product) REQUIRE prod.name IS UNIQUE")
            session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (d:Document) REQUIRE d.id IS UNIQUE")
            # SalesAmount 通常不需要唯一约束，因为它可能重复（例如不同区域同一时期的销售）
            print("Ensured constraints are created (or already exist).")
        except Exception as e:
            print(f"Error creating constraints: {e}")


def load_data(driver, kg_data):
    """根据kg_data中的facts加载数据到Neo4j"""
    facts = kg_data.get("facts", [])
    
    with driver.session(database="neo4j") as session:
        entities_to_create = set()
        node_types_from_schema = { # 定义了主要实体的标签和它们的主要标识属性
            "Person": "name", "Project": "name", "Region": "name", 
            "Product": "name", "Document": "id", "Idea": "name" # 新增Idea类型
        }

        for fact in facts:
            subject_name = fact.get("subject")
            object_name = fact.get("object")
            fact_type = fact.get("type", "")

            subject_label = None
            # 基于fact_type或其他逻辑推断subject_label
            if "person_" in fact_type: subject_label = "Person"
            elif "region_" in fact_type: subject_label = "Region"
            elif "product_" in fact_type: subject_label = "Product"
            # ... 其他类型的映射 ...
            
            if subject_label and subject_name:
                prop_name = node_types_from_schema.get(subject_label, "name")
                entities_to_create.add((subject_label, prop_name, subject_name))

            object_label = None
            if not fact_type.endswith("_amount"): # 不是直接的销售额事实
                if "_project" in fact_type: object_label = "Project"
                elif "_product" in fact_type: object_label = "Product"
                elif "_document" in fact_type: object_label = "Document"
                elif "_idea" in fact_type: object_label = "Idea" # 新增对Idea类型的处理
                # ... 其他类型的映射 ...

                if object_label and object_name:
                    prop_name = node_types_from_schema.get(object_label, "name") # Document会用id, Idea会用name
                    entities_to_create.add((object_label, prop_name, object_name))
        
        for label, prop, value in entities_to_create:
            if value is not None:
                query = f"MERGE (n:{label} {{{prop}: $value}})"
                session.run(query, value=value)
                print(f"Merged node: ({label} {{{prop}: '{value}'}})")

        for fact in facts:
            s_name = fact.get("subject")
            rel = fact.get("relation")
            o_name = fact.get("object")
            fact_type = fact.get("type", "")
            period = fact.get("period")

            if fact_type == "region_sales_amount" and period:
                session.run("MERGE (r:Region {name: $s_name})", s_name=s_name)
                try:
                    # ... (销售额解析逻辑不变) ...
                    if isinstance(o_name, str) and '万元' in o_name:
                        numeric_val_str = o_name.replace('万元', '').strip()
                        numeric_val = float(numeric_val_str)
                        unit_val = '万元'
                    # ... (其他单位解析) ...
                    else:
                        numeric_val = float(o_name) # 尝试直接转换
                        unit_val = None 
                    
                    query = """
                    MATCH (r:Region {name: $s_name})
                    CREATE (sa:SalesAmount {numeric_amount: $num_val, period: $period, unit: $unit_val})
                    CREATE (r)-[:HAS_SALES_AMOUNT]->(sa)
                    """
                    session.run(query, s_name=s_name, num_val=numeric_val, period=period, unit_val=unit_val)
                    print(f"Created SalesAmount for {s_name}, {period}: {numeric_val} {unit_val or ''}")
                except ValueError:
                    print(f"Could not parse sales amount: {o_name} for {s_name}, {period}. Skipping this SalesAmount fact.")
                
            elif s_name and rel and o_name: 
                s_label, o_label = None, None
                s_prop, o_prop = "name", "name" 

                # --- 更精确的标签和属性推断 ---
                if fact_type == "person_project" and rel == "WORKS_ON":
                    s_label, o_label = "Person", "Project"
                elif fact_type == "person_idea" and rel == "PROPOSED_IDEA": # 新增
                    s_label, o_label = "Person", "Idea"
                elif fact_type == "region_product" and rel == "HAS_SALES_PRODUCT": # 假设type是 region_product
                    s_label, o_label = "Region", "Product"
                elif fact_type == "product_document" and rel == "RELATED_TO":
                    s_label, o_label = "Product", "Document"
                    o_prop = "id" # Document用id匹配
                # 您可以根据您的fact_type添加更多精确的映射规则

                if s_label and o_label:
                    query = f"""
                    MATCH (s:{s_label} {{{s_prop}: $s_name}})
                    MATCH (o:{o_label} {{{o_prop}: $o_name}})
                    MERGE (s)-[:{rel}]->(o)
                    """
                    session.run(query, s_name=s_name, o_name=o_name)
                    print(f"Merged relationship: ({s_label} {{{s_prop}:'{s_name}'}})-[:{rel}]->({o_label} {{{o_prop}:'{o_name}'}})")
                else:
                    print(f"Could not determine labels for fact: {fact} (s_label: {s_label}, o_label: {o_label}). Relationship not created.")
            else:
                print(f"Skipping incomplete fact: {fact}")


if __name__ == "__main__":
    driver = None
    try:
        driver = GraphDatabase.driver(NEO4J_URI, auth=basic_auth(NEO4J_USER, NEO4J_PASSWORD))
        driver.verify_connectivity()
        print("Successfully connected to Neo4j.")
        
        clear_database(driver) # 清空数据库
        create_constraints(driver) # 创建约束

        with open(SAMPLE_KG_PATH, 'r', encoding='utf-8') as f:
            kg_data_to_load = json.load(f)
        
        load_data(driver, kg_data_to_load)
        
        print("\nData loading process completed.")
        print("You can now verify the data in Neo4j Browser (http://localhost:7474).")
        print("Example query to check SalesAmount:")
        print("MATCH (r:Region)-[:HAS_SALES_AMOUNT]->(sa:SalesAmount) RETURN r.name, sa.numeric_amount, sa.unit, sa.period")
        print("Example query to check Person-Project:")
        print("MATCH (p:Person)-[:WORKS_ON]->(proj:Project) RETURN p.name, proj.name")

    except Exception as e:
        print(f"An error occurred: {e}")
        traceback.print_exc()
    finally:
        if driver:
            driver.close()
            print("Neo4j connection closed.")--- END OF FILE load_neo4j_data.py ---

--- START OF FILE main.py ---# zhz_agent/main.py
from fastapi import FastAPI
from contextlib import asynccontextmanager
import uvicorn
import logging
from typing import Type, List, Dict, Any, Optional, ClassVar

# --- [修改] 导入 -> 改为绝对导入 ---
from zhz_agent.database import database, sqlalchemy_engine, Base, get_scheduler # <--- [修改] 导入 get_scheduler
from zhz_agent.task_manager_service import router as tasks_router
from zhz_agent import database_models # 确保 SQLAlchemy 模型被导入

@asynccontextmanager
async def lifespan(app_instance: FastAPI):
    print("--- Main FastAPI 应用启动 (已集成任务管理API 和 APScheduler) ---") # [修改] 更新了描述

    # --- 数据库初始化 ---
    await database.connect()
    print("数据库已连接。")
    try:
        Base.metadata.create_all(bind=sqlalchemy_engine)
        print("数据库表已执行 create_all。")
        from sqlalchemy import inspect
        inspector = inspect(sqlalchemy_engine)
        if inspector.has_table("tasks"):
            print("'tasks' 表已成功创建/存在于数据库中。")
        else:
            print("警告: 'tasks' 表在 create_all 之后仍未找到！这通常意味着模型没有在 create_all 之前被正确导入。")
            print(f"   已知的表: {inspector.get_table_names()}") # 打印所有实际创建的表
            print(f"   Base.metadata.tables: {Base.metadata.tables.keys()}") # 打印 SQLAlchemy 元数据中已注册的表
    except Exception as e:
        print(f"创建或检查数据库表时出错: {e}")
        import traceback
        traceback.print_exc() # 打印详细的异常堆栈


    # --- [修改] APScheduler 初始化 (使用 get_scheduler) ---
    current_scheduler = get_scheduler() # <--- 获取调度器实例
    try:
        logging.getLogger('apscheduler').setLevel(logging.DEBUG) # 设置为 DEBUG 级别

        if not current_scheduler.running: # <--- 只有在未运行时才启动
            current_scheduler.start()
            print("APScheduler 已启动并使用数据库作业存储。")
        else:
            print("APScheduler 已在运行。")
    except Exception as e:
        print(f"APScheduler 启动失败: {e}")

    print("RAG 组件的初始化和管理在 zhz_agent_mcp_server.py。")
    print("任务管理API已在 /tasks 路径下可用。")

    yield # FastAPI 应用在此运行

    print("--- Main FastAPI 应用关闭 ---")
    current_scheduler_on_shutdown = get_scheduler() # <--- 再次获取以确保是同一个实例
    if current_scheduler_on_shutdown and current_scheduler_on_shutdown.running:
        current_scheduler_on_shutdown.shutdown()
        print("APScheduler 已关闭。")
    await database.disconnect()
    print("数据库已断开连接。")
    print("RAG 组件的清理在 zhz_agent_mcp_server.py。")

# --- App 定义 (保持不变) ---
app = FastAPI(
    title="Hybrid RAG Backend with Task Management",
    description="主 FastAPI 应用，负责接收请求、编排 Agent，并提供任务管理API。",
    version="0.2.1",
    lifespan=lifespan
)

app.include_router(tasks_router)

@app.get("/")
async def read_root():
    return {
        "message": "Welcome to the Hybrid RAG Backend Main App.",
        "available_services": {
            "task_management": "/tasks/docs",
            "rag_via_mcpo": "mcpo proxy at port 8006 (see mcpo_servers.json)"
        }
    }

if __name__ == "__main__":
    print("--- 启动 Main FastAPI 服务器 (包含任务管理API) ---")
    uvicorn.run("zhz_agent.main:app", host="0.0.0.0", port=8000, reload=True) # Ensure correct run command--- END OF FILE main.py ---

--- START OF FILE pydantic_models.py ---# zhz_agent/pydantic_models.py
from pydantic import BaseModel, Field, ConfigDict, model_validator
from typing import List, Dict, Any, Optional
from enum import Enum
from datetime import datetime
import uuid

# --- RAG Models ---
class QueryRequest(BaseModel):
    model_config = ConfigDict(extra='forbid')
    query: str = Field(description="用户提出的原始查询文本。", json_schema_extra=lambda schema: schema.pop('default', None) if schema.get('default') is None else None)
    top_k_vector: int = Field(description="期望检索的向量搜索结果数量。", json_schema_extra=lambda schema: schema.pop('default', None) if schema.get('default') is None else None)
    top_k_kg: int = Field(description="期望检索的知识图谱结果数量。", json_schema_extra=lambda schema: schema.pop('default', None) if schema.get('default') is None else None)
    top_k_bm25: int = Field(description="期望检索的 BM25 关键词搜索结果数量。", json_schema_extra=lambda schema: schema.pop('default', None) if schema.get('default') is None else None)

    @model_validator(mode='before')
    @classmethod
    def remove_internal_params(cls, data: Any) -> Any:
        if isinstance(data, dict):
            print(f"Pydantic DEBUG (QueryRequest before validation): Received data for validation: {str(data)[:500]}")
            removed_security_context = data.pop('security_context', None)
            if removed_security_context:
                print(f"Pydantic INFO (QueryRequest before validation): Removed 'security_context': {str(removed_security_context)[:100]}")
            removed_agent_fingerprint = data.pop('agent_fingerprint', None)
            if removed_agent_fingerprint:
                print(f"Pydantic INFO (QueryRequest before validation): Removed 'agent_fingerprint': {str(removed_agent_fingerprint)[:100]}")
        return data

class RetrievedDocument(BaseModel):
    source_type: str
    content: str
    score: Optional[float] = Field(default=None, json_schema_extra=lambda schema: schema.pop('default', None) if schema.get('default') is None else None)
    metadata: Optional[Dict[str, Any]] = Field(default=None, json_schema_extra=lambda schema: schema.pop('default', None) if schema.get('default') is None else None)

class HybridRAGResponse(BaseModel):
    original_query: str
    answer: str
    retrieved_sources: List[RetrievedDocument]
    debug_info: Optional[Dict[str, Any]] = Field(default=None, json_schema_extra=lambda schema: schema.pop('default', None) if schema.get('default') is None else None)


# --- Task Management Models ---
class TaskStatus(str, Enum):
    PENDING = "pending"      # 待处理 (新创建，尚未到执行时间)
    ACTIVE = "active"        # 活动 (已到执行时间，等待执行或正在执行)
    COMPLETED = "completed"  # 已完成
    CANCELLED = "cancelled"  # 已取消
    FAILED = "failed"        # 执行失败
    REMINDING = "reminding"    # 提醒中 (可选状态)

class ReminderMethod(str, Enum):
    NOTIFICATION = "notification" # 桌面通知

class TaskModel(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid.uuid4()), description="任务的唯一ID (自动生成)")
    title: str = Field(description="任务标题")
    description: Optional[str] = Field(None, description="任务的详细描述")
    status: TaskStatus = Field(default=TaskStatus.PENDING, description="任务当前状态")
    created_at: datetime = Field(default_factory=datetime.utcnow, description="任务创建时间 (UTC)")
    updated_at: datetime = Field(default_factory=datetime.utcnow, description="任务最后更新时间 (UTC)")
    due_date: Optional[datetime] = Field(None, description="任务截止日期或计划执行时间 (UTC)")
    reminder_time: Optional[datetime] = Field(None, description="任务提醒时间 (UTC)")
    reminder_offset_minutes: Optional[int] = Field(None, description="提醒时间相对于due_date的提前分钟数 (例如10分钟前)")
    reminder_methods: List[ReminderMethod] = Field(default=[ReminderMethod.NOTIFICATION], description="提醒方式列表")
    priority: int = Field(default=0, description="任务优先级 (例如 0:普通, 1:重要, 2:紧急)")
    tags: List[str] = Field(default_factory=list, description="任务标签") # 确保默认为空列表
    action_type: Optional[str] = Field(None, description="任务到期时需要执行的动作类型 (例如 'navigate', 'send_message', 'run_report')")
    action_payload: Dict[str, Any] = Field(default_factory=dict, description="执行动作时需要的参数 (例如导航的目的地)") # 确保默认为空字典
    execution_result: Optional[str] = Field(None, description="任务执行后的结果或错误信息")
    last_executed_at: Optional[datetime] = Field(None, description="上次执行时间 (UTC)")

    model_config = ConfigDict(
        use_enum_values=True,
        from_attributes=True,
    )

class CreateTaskRequest(BaseModel):
    title: str
    description: Optional[str] = None
    due_date: Optional[datetime] = None
    reminder_offset_minutes: Optional[int] = None # 例如 "10" 代表提前10分钟
    reminder_methods: Optional[List[ReminderMethod]] = [ReminderMethod.NOTIFICATION]
    priority: Optional[int] = 0
    tags: Optional[List[str]] = None
    action_type: Optional[str] = None
    action_payload: Optional[Dict[str, Any]] = None
    model_config = ConfigDict(extra='forbid')

class UpdateTaskRequest(BaseModel):
    title: Optional[str] = None
    description: Optional[str] = None
    status: Optional[TaskStatus] = None
    due_date: Optional[datetime] = None
    reminder_offset_minutes: Optional[int] = None
    reminder_methods: Optional[List[ReminderMethod]] = None
    priority: Optional[int] = None
    tags: Optional[List[str]] = None
    action_type: Optional[str] = None
    action_payload: Optional[Dict[str, Any]] = None
    model_config = ConfigDict(extra='forbid')--- END OF FILE pydantic_models.py ---

--- START OF FILE rag_service.py ---# zhz_agent/rag_service.py

import os
import json
import asyncio
import traceback
from contextlib import asynccontextmanager
from typing import List, Dict, Any, Optional, AsyncIterator
from dataclasses import dataclass
import time
import logging
import sys # <--- 确保导入 sys

# MCP 框架导入
from mcp.server.fastmcp import FastMCP, Context # 确保从 fastmcp 导入


# --- 配置 rag_service 的专用日志 ---
_rag_service_py_dir = os.path.dirname(os.path.abspath(__file__))
_rag_service_log_file = os.path.join(_rag_service_py_dir, 'rag_service_debug.log')

rag_logger = logging.getLogger("RagServiceLogger") # 给一个独特的名字
rag_logger.setLevel(logging.DEBUG)
rag_logger.propagate = False # 不传递给根记录器

if rag_logger.hasHandlers():
    rag_logger.handlers.clear()

try:
    _file_handler = logging.FileHandler(_rag_service_log_file, mode='w') # 每次覆盖
    _file_handler.setLevel(logging.DEBUG)
    _formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
    _file_handler.setFormatter(_formatter)
    rag_logger.addHandler(_file_handler)
    rag_logger.info("--- RagServiceLogger configured to write to rag_service_debug.log ---")
except Exception as e:
    print(f"CRITICAL: Failed to configure RagServiceLogger: {e}")



# --- 从项目内部导入所有 RAG 模块 ---
from zhz_agent.pydantic_models import QueryRequest, HybridRAGResponse, RetrievedDocument
from zhz_agent.llm import (
    generate_answer_from_context, 
    generate_expanded_queries, 
    generate_cypher_query, 
    generate_clarification_question, 
    generate_intent_classification, 
    generate_clarification_options, 
    NO_ANSWER_PHRASE_ANSWER_CLEAN
)
from zhz_agent.vector import VectorRetriever
from zhz_agent.kg import KGRetriever
from zhz_agent.fusion import FusionEngine # FusionEngine 现在接收 logger
from zhz_agent.bm25 import BM25Retriever


from dotenv import load_dotenv
load_dotenv()

# --- 应用上下文 Dataclass ---
@dataclass
class AppContext:
    vector_retriever: VectorRetriever
    kg_retriever: KGRetriever
    bm25_retriever: BM25Retriever
    fusion_engine: FusionEngine

# --- MCP 服务器生命周期管理 ---
@asynccontextmanager
async def app_lifespan_for_rag_service(server: FastMCP) -> AsyncIterator[AppContext]:
    rag_logger.info("--- RAG Service (FastMCP): 正在初始化 RAG 组件 ---")
    
    current_dir = os.path.dirname(os.path.abspath(__file__))
    data_dir = os.path.join(current_dir, "data")

    vector_retriever_instance: Optional[VectorRetriever] = None
    kg_retriever_instance: Optional[KGRetriever] = None
    bm25_retriever_instance: Optional[BM25Retriever] = None
    fusion_engine_instance: Optional[FusionEngine] = None

    try:
        vector_retriever_instance = VectorRetriever(data_path=data_dir)
        rag_logger.info("RAG Service: VectorRetriever 初始化成功。")
    except Exception as e:
        rag_logger.error(f"RAG Service: VectorRetriever 初始化失败: {e}", exc_info=True)
        raise RuntimeError(f"VectorRetriever 初始化失败: {e}") from e
    
    try:
        kg_retriever_instance = KGRetriever(data_path=data_dir, llm_cypher_generator_func=generate_cypher_query)
        rag_logger.info("RAG Service: KGRetriever 初始化成功。")
    except Exception as e:
        rag_logger.error(f"RAG Service: KGRetriever 初始化失败: {e}", exc_info=True)
        raise RuntimeError(f"KGRetriever 初始化失败: {e}") from e
        
    try:
        bm25_retriever_instance = BM25Retriever(data_path=data_dir)
        rag_logger.info("RAG Service: BM25Retriever 初始化成功。")
    except Exception as e:
        rag_logger.error(f"RAG Service: BM25Retriever 初始化失败: {e}", exc_info=True)
        raise RuntimeError(f"BM25Retriever 初始化失败: {e}") from e
    
    try:
        fusion_engine_instance = FusionEngine(logger=rag_logger) # 传递 logger
        rag_logger.info("RAG Service: FusionEngine 初始化成功 (已传入logger)。")
    except Exception as e:
        rag_logger.error(f"RAG Service: FusionEngine 初始化失败: {e}", exc_info=True)
        raise RuntimeError(f"FusionEngine 初始化失败: {e}") from e

    rag_logger.info("--- RAG Service (FastMCP): RAG 组件初始化完成。---")

    ctx = AppContext(
        vector_retriever=vector_retriever_instance,
        kg_retriever=kg_retriever_instance,
        bm25_retriever=bm25_retriever_instance,
        fusion_engine=fusion_engine_instance
    )
    try:
        yield ctx
    finally:
        rag_logger.info("--- RAG Service (FastMCP): 正在清理资源 ---")
        if kg_retriever_instance:
            kg_retriever_instance.close() 
        rag_logger.info("--- RAG Service (FastMCP): 清理完成 ---")

# --- 初始化 FastMCP 应用 ---
rag_mcp_application = FastMCP(
    name="zhz_agent_service", 
    description="Hybrid RAG 服务，提供多路召回、融合与答案生成功能。",
    lifespan=app_lifespan_for_rag_service,
)

# --- MCP 工具定义 ---
@rag_mcp_application.tool()
async def query_rag(
    ctx: Context,
    query: str,
    top_k_vector: int = 3,
    top_k_kg: int = 2,
    top_k_bm25: int = 3
) -> str: # FastMCP 工具函数期望返回一个字符串 (通常是JSON字符串)
    rag_logger.info(f"\n--- RAG Service: 接收到查询: '{query}' ---")
    rag_logger.info(f"    top_k_vector: {top_k_vector}, top_k_kg: {top_k_kg}, top_k_bm25: {top_k_bm25}")
    start_time_total = time.time()

    app_ctx: AppContext = ctx.request_context.lifespan_context
    response_payload = {} 
    original_query_for_response = query 
    final_json_output = "" # 初始化最终的JSON输出

    try:
        # --- LLM 驱动的意图分类和澄清触发 ---
        rag_logger.info(f"--- RAG Service: [TIME] 开始 LLM 驱动的意图分类 at {time.time() - start_time_total:.2f}s ---")
        start_time_intent = time.time() # 确保start_time_intent已定义
        intent_classification_result = await generate_intent_classification(query)
        rag_logger.info(f"--- RAG Service: [TIME] 结束 LLM 驱动的意图分类, 耗时: {time.time() - start_time_intent:.2f}s ---")
        rag_logger.info(f"--- RAG Service: 意图分类结果: {intent_classification_result}")

        if intent_classification_result.get("clarification_needed"):
            # rag_logger.info(...) # 使用 rag_logger 替代 print
            uncertainty_reason = intent_classification_result.get("reason", "查询可能存在歧义或信息不足。")
            clarification_question_text = await generate_clarification_question(query, uncertainty_reason)
            response_payload = {
                "status": "clarification_needed",
                "clarification_question": clarification_question_text,
                "original_query": original_query_for_response,
                "debug_info": {"uncertainty_reason": uncertainty_reason, "source": "intent_classification"}
            }
            rag_logger.info(f"--- RAG Service: 需要澄清，返回: {response_payload}")
            final_json_output = json.dumps(response_payload, ensure_ascii=False)
            # --- 在所有返回路径前添加 flush ---
            sys.stdout.flush()
            sys.stderr.flush()
            return final_json_output

        # --- 如果不需要澄清，则继续RAG流程 ---
        rag_logger.info(f"--- RAG Service: LLM 意图分类结果：查询清晰，无需澄清。---")
        rag_logger.info(f"--- RAG Service: [TIME] 开始查询扩展 for: {query} at {time.time() - start_time_total:.2f}s ---")
        start_time_expansion = time.time()
        expanded_queries = await generate_expanded_queries(query)
        rag_logger.info(f"--- RAG Service: 扩展查询列表 (共 {len(expanded_queries)} 个): {expanded_queries} ---")
        rag_logger.info(f"--- RAG Service: [TIME] 结束查询扩展, 耗时: {time.time() - start_time_expansion:.2f}s ---")
        
        all_raw_retrievals: List[RetrievedDocument] = []
        tasks = []
        task_names = [] 

        for i_eq, q_ext in enumerate(expanded_queries):
            rag_logger.debug(f"为扩展查询 '{q_ext}' (索引 {i_eq}) 创建召回任务...")
            tasks.append(app_ctx.vector_retriever.retrieve(q_ext, top_k_vector))
            task_names.append(f"VectorRetrieve_EQ{i_eq}")
            tasks.append(app_ctx.kg_retriever.retrieve(q_ext, top_k_kg))
            task_names.append(f"KGRetrieve_EQ{i_eq}")
            tasks.append(app_ctx.bm25_retriever.retrieve(q_ext, top_k_bm25))
            task_names.append(f"BM25Retrieve_EQ{i_eq}")
        
        rag_logger.info(f"--- RAG Service: [TIME] 开始并行召回 (共 {len(tasks)} 个任务) at {time.time() - start_time_total:.2f}s ---")
        start_time_retrieval = time.time()
        all_results_nested = await asyncio.gather(*tasks, return_exceptions=True)
        rag_logger.info(f"--- RAG Service: [TIME] 结束并行召回, 耗时: {time.time() - start_time_retrieval:.2f}s ---")
        
        kg_retrieval_error_message: Optional[str] = None
        rag_logger.debug(f"--- RAG Service: 开始处理 {len(all_results_nested)} 个召回任务的结果 ---")

        for i, result_item in enumerate(all_results_nested):
            task_name = task_names[i] 
            if isinstance(result_item, Exception):
                error_msg = f"RAG Service WARNING: 任务 '{task_name}' 执行失败: {type(result_item).__name__} - {str(result_item)}"
                rag_logger.error(error_msg, exc_info=True)
                if "KGRetrieve" in task_name: 
                    kg_retrieval_error_message = f"知识图谱检索任务 '{task_name}' 失败: {str(result_item)}"
            elif isinstance(result_item, list):
                rag_logger.debug(f"任务 '{task_name}' 成功返回 {len(result_item)} 个结果。内容预览 (最多前2项):")
                for doc_idx, doc_item in enumerate(result_item[:2]):
                    rag_logger.debug(f"  Doc {doc_idx}: source={doc_item.source_type}, content='{str(doc_item.content)[:100]}...'")
                all_raw_retrievals.extend(result_item)
            else:
                rag_logger.warning(f"RAG Service WARNING: 任务 '{task_name}' 返回了预料之外的结果类型: {type(result_item)}")
        
        rag_logger.info(f"RAG Service: 总计从各路召回有效合并的结果数: {len(all_raw_retrievals)}")
        
        rag_logger.debug(f"--- RAG Service: 准备送入 FusionEngine 的 all_raw_retrievals (共 {len(all_raw_retrievals)} 条) ---")
        for i_doc, doc_retrieved in enumerate(all_raw_retrievals):
            rag_logger.debug(f"  Doc {i_doc}: type={doc_retrieved.source_type}, score={doc_retrieved.score}, content='{str(doc_retrieved.content)[:150]}...'")

        if kg_retrieval_error_message and not all_raw_retrievals:
            response_payload = {
                "status": "error",
                "error_code": "KNOWLEDGE_GRAPH_FAILURE_NO_FALLBACK",
                "error_message": kg_retrieval_error_message,
                "original_query": original_query_for_response,
                "debug_info": {"error_source": "knowledge_graph_retrieval", "details": kg_retrieval_error_message}
            }
            rag_logger.error(f"--- RAG Service: 'query_rag' 因KG查询失败且无其他结果，提前返回错误 ---") # 使用 rag_logger
            final_json_output = json.dumps(response_payload, ensure_ascii=False)
            sys.stdout.flush()
            sys.stderr.flush()
            return final_json_output

        # rag_logger.info(...) # 使用 rag_logger 替代 print
        # print(f"RAG Service: 总计原始召回结果数 (可能包含来自失败检索的空列表): {len(all_raw_retrievals)}")

        if not all_raw_retrievals: 
            response_payload = {
                "status": "success", 
                "final_answer": "抱歉，根据您提供的查询，未能从知识库中找到相关信息。",
                "original_query": original_query_for_response,
                "debug_info": {
                    "message": "No documents retrieved from any source.",
                    "kg_retrieval_error": kg_retrieval_error_message if kg_retrieval_error_message else "N/A"
                }
            }
            final_json_output = json.dumps(response_payload, ensure_ascii=False)
            sys.stdout.flush()
            sys.stderr.flush()
            return final_json_output

        rag_logger.info(f"--- RAG Service: [TIME] 开始结果融合 at {time.time() - start_time_total:.2f}s ---")
        start_time_fusion = time.time()
        fused_context_text = await app_ctx.fusion_engine.fuse_results(all_raw_retrievals, query) 
        rag_logger.info(f"--- RAG Service: [TIME] 结束结果融合, 耗时: {time.time() - start_time_fusion:.2f}s ---")
        
        rag_logger.info(f"\n--- RAG Service: FUSED CONTEXT TEXT for generate_answer_from_context ---")
        rag_logger.info(f"{fused_context_text}") 
        rag_logger.info(f"--- END OF FUSED CONTEXT TEXT ---\n")
        
        if fused_context_text == "未在知识库中找到相关信息。":
            final_answer = "根据现有知识，未能找到您查询的相关信息。"
            response_payload = {
                "status": "success",
                "final_answer": final_answer,
                "original_query": original_query_for_response,
                "debug_info": {
                    "message": "No relevant context found after fusion.",
                    "total_raw_retrievals_count": len(all_raw_retrievals),
                    "kg_retrieval_error": kg_retrieval_error_message if kg_retrieval_error_message else "N/A"
                }
            }
        else:
            rag_logger.info(f"--- RAG Service: [TIME] 开始最终答案生成 at {time.time() - start_time_total:.2f}s ---") # 使用 rag_logger
            start_time_answer_gen = time.time()
            final_answer = await generate_answer_from_context(query, fused_context_text)
            rag_logger.info(f"--- RAG Service: [TIME] 结束最终答案生成, 耗时: {time.time() - start_time_answer_gen:.2f}s ---") # 使用 rag_logger

            if not final_answer or final_answer.strip() == NO_ANSWER_PHRASE_ANSWER_CLEAN:
                final_answer = "根据您提供的信息，我暂时无法给出明确的回答。"
            response_payload = {
                "status": "success",
                "final_answer": final_answer,
                "original_query": original_query_for_response,
                "debug_info": {
                    "total_raw_retrievals_count": len(all_raw_retrievals),
                    "kg_retrieval_error": kg_retrieval_error_message if kg_retrieval_error_message else "N/A"
                }
            }
        
        # --- 这里是正常执行完毕的返回路径 ---
        final_json_output = json.dumps(response_payload, ensure_ascii=False) # 赋值给 final_json_output
        rag_logger.info(f"--- RAG Service: 'query_rag' 工具成功执行完毕, 总耗时: {time.time() - start_time_total:.2f}s, 准备返回JSON响应 ---")
        rag_logger.info(f"--- RAG Service: 准备返回的最终JSON (成功): {final_json_output[:500]}...")
        
    except Exception as e:
        # rag_logger.error(...) # 使用 rag_logger 替代 print
        rag_logger.error(f"RAG Service CRITICAL ERROR in 'query_rag' tool: {type(e).__name__} - {str(e)}", exc_info=True)
        # traceback.print_exc() # logging 的 exc_info=True 会处理堆栈

        response_payload = { 
            "status": "error",
            "error_code": "RAG_SERVICE_INTERNAL_ERROR",
            "error_message": f"RAG服务内部发生未预期错误: {str(e)}",
            "original_query": original_query_for_response,
            "debug_info": {"exception_type": type(e).__name__, "traceback_snippet": traceback.format_exc(limit=1)}
        }
        final_json_output = json.dumps(response_payload, ensure_ascii=False) # 赋值给 final_json_output
        rag_logger.info(f"--- RAG Service: 'query_rag' 工具因异常结束, 总耗时: {time.time() - start_time_total:.2f}s, 准备返回错误JSON响应 ---")
        rag_logger.info(f"--- RAG Service: 准备返回的最终JSON (异常): {final_json_output[:500]}...")
    
    # --- 确保在函数末尾，所有路径都有一个明确的返回，并且之前有 flush ---
    sys.stdout.flush()
    sys.stderr.flush()
    return final_json_output

if __name__ == "__main__":
    # 这部分代码在使用 -m zhz_agent.rag_service 时不会被 mcpo 直接执行
    # mcpo 是通过导入并调用 rag_mcp_application 对象来工作的
    # 但如果想独立测试这个文件，可以保留
    rag_logger.info("--- 启动 RAG Service (FastMCP for mcpo) ---") # 使用 rag_logger
    rag_mcp_application.run()--- END OF FILE rag_service.py ---

--- START OF FILE run_agent.py ---# /home/zhz/zhz_agent/run_agent.py

import os
import json
import datetime

from crewai import Agent, Task, Crew, Process

# --- 导入我们自己的项目模块 (使用绝对导入) ---
from zhz_agent.custom_crewai_tools import HybridRAGTool, BaseMCPTool
from zhz_agent.pydantic_models import QueryRequest # 用于 RAG 工具的输入
from zhz_agent.utils import call_mcpo_tool
from zhz_agent.custom_llm import CustomGeminiLLM

# --- 环境配置 ---
from dotenv import load_dotenv
load_dotenv()

# --- CrewAI 基类和事件系统 ---
from crewai.tools import BaseTool
from pydantic import BaseModel, Field
from typing import Any, Dict, List, Optional, Type
from crewai.llms.base_llm import BaseLLM as CrewAIBaseLLM
try:
    from crewai.utilities.events.base_event_listener import BaseEventListener as CrewAIBaseCallbackHandler
    from crewai.utilities.events import LLMCallStartedEvent, LLMCallCompletedEvent
    print("Successfully imported BaseEventListener and Event Types")
except ImportError:
    print("Failed to import BaseEventListener or Event Types, using dummy classes.")
    class CrewAIBaseCallbackHandler: pass
    class LLMCallStartedEvent: pass
    class LLMCallCompletedEvent: pass

# --- LiteLLM ---
import litellm

# --- 定义简单工具以供测试 ---
class SimpleToolInput(BaseModel):
    message: str = Field(description="A simple message string for the tool.")

class MySimpleTestTool(BaseTool):
    name: str = "MySimpleTestTool"
    description: str = "A very simple test tool that takes a message and returns it."
    args_schema: Type[BaseModel] = SimpleToolInput

    def _run(self, message: str) -> str:
        print(f"MySimpleTestTool received: {message}")
        return f"MySimpleTestTool processed: {message}"

# --- 配置 Agent 使用的 LLM 实例 ---
GEMINI_MODEL_NAME = "gemini/gemini-1.5-flash-latest"
GEMINI_API_KEY = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")

if not GEMINI_API_KEY:
    print("CRITICAL ERROR: GOOGLE_API_KEY or GEMINI_API_KEY not set.")
    exit(1)

# --- 定义详细的事件监听器 ---
class MyDetailedLogger(CrewAIBaseCallbackHandler):
    def __init__(self):
        super().__init__()
        print(f"[{datetime.datetime.now()}] MyDetailedLogger: 已初始化。")

    def setup_listeners(self, crewai_event_bus):
        print(f"[{datetime.datetime.now()}] MyDetailedLogger: 正在设置监听器...")

        @crewai_event_bus.on(LLMCallStartedEvent)
        def handle_llm_start(source, event: LLMCallStartedEvent):
            self.on_llm_start_logic(source, event)

        @crewai_event_bus.on(LLMCallCompletedEvent)
        def handle_llm_completed(source, event: LLMCallCompletedEvent):
            self.on_llm_end_logic(source, event)

        print(f"[{datetime.datetime.now()}] MyDetailedLogger: 监听器设置完成。")

    def on_llm_start_logic(self, source, event: LLMCallStartedEvent):
        print(f"\n>>>> LLM 调用开始 (Event Logic) <<<<")
        llm_inputs = getattr(event, 'llm_inputs', {})
        messages = llm_inputs.get('messages')
        tools = llm_inputs.get('tools')
        print(f"来源 (Source): {source}")
        if messages:
            print("消息 (来自 event.llm_inputs):")
            if isinstance(messages, list) and len(messages) > 0:
                first_message = messages[0]
                if isinstance(first_message, dict) and 'content' in first_message:
                    content_snippet = str(first_message.get('content', ''))[:300]
                    print(f"   Role: {first_message.get('role')}, Content Snippet: {content_snippet}...")
                else:
                     print(f"  First message (raw): {first_message}")
            else:
                 print(f"  Messages (raw): {messages}")
        else:
            print("消息 (来自 event.llm_inputs): 无")
        if tools:
            print("工具 (来自 event.llm_inputs):")
            try:
                print(f"  {json.dumps(tools, indent=2, ensure_ascii=False)}")
            except Exception as e:
                print(f"  无法序列化工具为 JSON: {e}. 工具: {tools}")
        else:
            print("工具 (来自 event.llm_inputs): 无")
        print("----------------------------------")

    def on_llm_end_logic(self, source, event: LLMCallCompletedEvent):
        print(f"\n>>>> LLM 调用结束 (Event Logic) <<<<")
        response = getattr(event, 'llm_output', None)
        print(f"来源 (Source): {source}")
        if response:
            if hasattr(response, 'choices') and response.choices:
                choice = response.choices[0]
                if hasattr(choice, 'message') and choice.message:
                    print(f"  消息内容: {choice.message.content}")
                    if hasattr(choice.message, 'tool_calls') and choice.message.tool_calls:
                        print(f"  工具调用: {choice.message.tool_calls}")
                    else:
                        print(f"  工具调用: 无")
            elif hasattr(response, 'content'):
                print(f"  响应内容: {response.content}")
            else:
                print(f"  LLM 响应 (来自 event.llm_output): {str(response)[:500]}...")
        else:
            print("  在 event.llm_output 中未找到响应对象。")
        print("----------------------------------")

# --- 实例化 CustomGeminiLLM ---
custom_llm_tool_config = {"function_calling_config": {"mode": "AUTO"}}
zhz_agent_tool = HybridRAGTool()
researcher_tools = [zhz_agent_tool]

llm_for_agent = CustomGeminiLLM(
    model=GEMINI_MODEL_NAME,
    api_key=GEMINI_API_KEY,
    temperature=0.1,
    max_tokens=2048,
    tool_config=custom_llm_tool_config,
    agent_tools=researcher_tools # 传递工具列表以供缓存
)
print(f"Custom Agent LLM configured: {GEMINI_MODEL_NAME} with custom tool_config")

# --- 设置 BaseMCPTool 的调用器 ---
BaseMCPTool.set_mcpo_caller(call_mcpo_tool)

# --- 定义 Agents ---
researcher_agent = Agent(
    role='信息检索专家',
    goal='准确地回答用户查询，并且只使用提供的工具。',
    backstory=(
        "你是一位高级AI助手，专注于信息检索。"
        "你的专长在于高效地利用工具来查找最相关和最精确的答案来回应用户的查询。"
    ),
    llm=llm_for_agent,
    tools=researcher_tools,
    verbose=True,
    allow_delegation=False,
)

writer_agent = Agent(
    role='报告撰写专家',
    goal='根据提供的信息，撰写清晰、结构良好且富有洞察力的报告。',
    backstory=(
        "您是一位资深的报告撰写专家，拥有出色的分析和写作能力。"
        "您擅长将复杂的信息提炼成易于理解的报告，并能根据不同的输出状态（答案、澄清、错误）"
        "灵活调整报告内容和格式。"
    ),
    llm=llm_for_agent,
    verbose=True,
    allow_delegation=False,
)

# --- 定义 Tasks (包含上下文传递修复) ---
research_task_description = """你收到了来自用户的以下查询：

'{query}'

你应该使用提供的 `HybridRAGQueryTool` 工具来处理这个查询。
如果这个工具需要 `top_k_vector`, `top_k_kg`, 或 `top_k_bm25` 这些参数，请使用以下建议值：
top_k_vector: 5, top_k_kg: 3, top_k_bm25: 5。
在使用完必要的工具后，你的最终输出应该是（使用中文）：'我的最终答案是：' 
后跟你使用的工具返回的精确、完整的JSON字符串输出内容。"""

research_task_expected_output = "短语 '我的最终答案是：' 后跟你使用的工具返回的精确、完整的JSON字符串输出内容。"

research_task = Task(
    description=research_task_description,
    expected_output=research_task_expected_output,
    agent=researcher_agent,
)

report_writing_task = Task(
    description="""根据【前一个任务】（信息检索专家）提供的RAG工具输出（它是一个JSON字符串），生成一份报告或响应。
请仔细分析这个JSON字符串输出，它应该包含一个 'status' 字段。
1. 如果 'status' 是 'success'，则提取 'final_answer' 字段的内容，并基于此答案撰写一份简洁的报告。
2. 如果 'status' 是 'clarification_needed'，则提取 'clarification_question' 字段的内容，并向用户明确指出需要澄清的问题，例如：'系统需要澄清：[澄清问题]'。
3. 如果 'status' 是 'error'，则提取 'error_message' (或 'error') 字段的内容，并向用户报告错误，例如：'RAG服务发生错误：[错误信息]'。
你的最终输出必须是清晰、专业且符合上述情况的报告或响应。""",
    expected_output="一份清晰的报告，或者一个明确的澄清请求，或者一个错误报告。",
    agent=writer_agent,
    context=[research_task],
)

# --- 实例化监听器 ---
my_event_logger = MyDetailedLogger()

# --- 定义 Crew (添加 event_listeners) ---
office_brain_crew = Crew(
    agents=[researcher_agent, writer_agent],
    tasks=[research_task, report_writing_task],
    process=Process.sequential,
    verbose=True,
    event_listeners=[my_event_logger] # <<< --- 激活事件监听器 ---
)

# --- 启动 Crew ---
if __name__ == "__main__":
    print("--- 启动智能助手终端大脑 Crew (使用 CustomGeminiLLM 和事件监听器) ---")
    user_query_input = "公司2024年第一季度在华东和华北的总销售额一共是多少？"
    # --- 修复：kickoff inputs 只包含 query ---
    inputs = {'query': user_query_input}
    result = office_brain_crew.kickoff(inputs=inputs)
    print("\n\n=== 最终报告 ===\n")
    if hasattr(result, 'raw'):
        print(result.raw)
    else:
        print(result)
    print("\n--- Crew 任务完成 ---")--- END OF FILE run_agent.py ---

--- START OF FILE task_jobs.py ---# zhz_agent/task_jobs.py
from datetime import datetime
from typing import Dict, Any
import os
import traceback
import httpx # <--- 确保 httpx 已导入
import json # <--- 确保 json 已导入

# 从 .database 导入 database 对象以便查询任务详情
# 从 .pydantic_models 导入 TaskModel 以便类型转换
# 从 .main 导入 scheduler 以便在需要时重新调度（虽然通常作业函数不直接操作调度器）
# 更好的做法是通过参数传递必要的信息，而不是依赖全局导入
WINDOWS_HOST_IP = os.getenv("WINDOWS_HOST_IP_FOR_WSL", "192.168.3.11") # <--- 请务必替换为您真实的Windows IP
LOCAL_AGENT_PORT = os.getenv("LOCAL_AGENT_PORT", "8003") # 与 local_agent_app.py 中的端口一致

# 如果 WINDOWS_HOST_IP 仍然是占位符，给出提示
if WINDOWS_HOST_IP == "在此处填写您上一步找到的Windows主机IP":
    print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
    print("REMINDER_JOB WARNING: WINDOWS_HOST_IP 未正确设置在 task_jobs.py 中!")
    print("请编辑 task_jobs.py 文件，将 '在此处填写您上一步找到的Windows主机IP' 替换为实际IP地址。")
    print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")

LOCAL_AGENT_NOTIFY_URL = f"http://{WINDOWS_HOST_IP}:{LOCAL_AGENT_PORT}/notify"

async def send_task_reminder(task_id: str, task_title: str, reminder_methods: list):
    """
    实际发送任务提醒的函数。
    """
    print(f"REMINDER_JOB: [{datetime.utcnow()}] 正在为任务 '{task_id}' - '{task_title}' 发送提醒。")
    for method in reminder_methods:
        if method == "notification": # 假设 ReminderMethod.NOTIFICATION.value 是 "notification"
            print(f"  REMINDER_JOB: 尝试通过 Local Agent 发送桌面通知: '{task_title}'")
            try:
                async with httpx.AsyncClient(timeout=10.0) as client:
                    response = await client.post(
                        LOCAL_AGENT_NOTIFY_URL,
                        json={"title": f"任务提醒: {task_title}", "message": f"任务 '{task_title}' 即将到期或需要关注。"}
                    )
                    response.raise_for_status() # Raise an exception for bad status codes
                    print(f"  REMINDER_JOB: 本地代理通知请求发送成功. 状态: {response.status_code}")
            except httpx.RequestError as e:
                print(f"  REMINDER_JOB: 通过本地代理发送通知失败 (RequestError): {e}")
                traceback.print_exc()
            except Exception as e:
                print(f"  REMINDER_JOB: 通过本地代理发送通知失败 (General Error): {e}")
                traceback.print_exc()
        # elif method == "email": #
        #     print(f"  REMINDER_JOB: 模拟发送邮件提醒...")

async def execute_task_action(task_id: str, action_type: str, action_payload: Dict[str, Any]):
    """
    实际执行任务动作的函数。
    """
    print(f"EXECUTION_JOB: [{datetime.utcnow()}] 正在为任务 '{task_id}' 执行动作 '{action_type}'。")
    print(f"  EXECUTION_JOB: 动作参数: {action_payload}")

    final_result = f"动作 '{action_type}' 已模拟执行。"
    success = True

    if action_type == "navigate":
        destination = action_payload.get("destination")
        if destination:
            print(f"  EXECUTION_JOB: 模拟导航到 '{destination}'...")
            final_result = f"已模拟为导航到 '{destination}' 准备好路线。"
        else:
            final_result = "导航动作失败：缺少目的地。"
            success = False
    elif action_type == "log_event":
        event_details = action_payload.get("event_details", "无详情")
        print(f"  EXECUTION_JOB: 记录事件: '{event_details}'")
        final_result = f"事件 '{event_details}' 已记录。"
    else:
        final_result = f"未知的动作类型: {action_type}"
        success = False

    # 更新数据库中的任务状态和结果 (需要访问数据库)
    # 这部分逻辑最好通过API调用或服务层来完成，以避免循环导入和分散DB操作
    # 这里我们只打印信息，实际应用中需要实现DB更新
    print(f"  EXECUTION_JOB: 任务 '{task_id}' 执行完毕。结果: {final_result}, 状态: {'COMPLETED' if success else 'FAILED'}")
--- END OF FILE task_jobs.py ---

--- START OF FILE task_manager_service.py ---# zhz_agent/task_manager_service.py
from fastapi import APIRouter, HTTPException, Depends, Body, Query, Path, status
from typing import List, Optional, Any, cast
from datetime import datetime, timedelta
import uuid
import traceback # 导入 traceback
import pytz

# --- [修改] 从 pydantic_models 导入我们定义的模型 -> 改为绝对导入 ---
from zhz_agent.pydantic_models import TaskModel, CreateTaskRequest, UpdateTaskRequest, TaskStatus, ReminderMethod

# --- [修改] 从 database_models 导入 SQLAlchemy 表模型 -> 改为绝对导入 ---
from zhz_agent.database_models import TaskDB

# --- [修改] 从新的 database.py 导入 database 对象 和 get_scheduler -> 改为绝对导入 ---
from zhz_agent.database import database, get_scheduler

# --- [修改] 从 .task_jobs 导入作业函数 -> 改为绝对导入 ---
from zhz_agent.task_jobs import send_task_reminder, execute_task_action
from apscheduler.triggers.date import DateTrigger # 用于指定精确的运行时间
from apscheduler.jobstores.base import JobLookupError # <--- [修改] 导入 JobLookupError 的正确路径

# APIRouter 实例
router = APIRouter(
    prefix="/tasks",
    tags=["Task Management"],
    responses={404: {"description": "Not found"}},
)

def _ensure_utc(dt: datetime) -> datetime:
    """确保 datetime 对象是 UTC 时区感知的。"""
    if dt.tzinfo is None:
        return pytz.utc.localize(dt) # 如果是朴素时间，假定它是UTC并设为UTC
    return dt.astimezone(pytz.utc) # 如果是其他时区，转换为UTC

def _schedule_task_jobs(task: TaskModel):
    current_scheduler = get_scheduler() # 获取 scheduler 实例
    print(f"DEBUG SCHEDULER: _schedule_task_jobs called. Scheduler instance: {current_scheduler}, Is running: {current_scheduler.running if current_scheduler else 'N/A'}")
    if not current_scheduler or not current_scheduler.running:
        print("SCHEDULER_ERROR: APScheduler 未运行，无法调度作业。")
        return

    # 提醒作业
    if task.reminder_time and task.status == TaskStatus.PENDING:
        reminder_job_id = f"reminder_{task.id}"
        try:
            reminder_methods_list = task.reminder_methods
            reminder_utc = _ensure_utc(task.reminder_time)
            print(f"SCHEDULER DEBUG: Passing reminder_methods to job: {reminder_methods_list}") # 添加日志

            current_scheduler.add_job(
                send_task_reminder,
                trigger=DateTrigger(run_date=reminder_utc),
                args=[task.id, task.title, reminder_methods_list], # <--- [修复] 直接传递列表
                id=reminder_job_id,
                name=f"Reminder for task {task.title[:20]}",
                replace_existing=True
            )
            print(f"SCHEDULER: 已为任务 '{task.id}' 添加/更新提醒作业，运行于 {task.reminder_time}")
        except Exception as e:
            print(f"SCHEDULER_ERROR: 添加提醒作业失败 for task '{task.id}': {e}")
            traceback.print_exc() # 打印详细错误堆栈

    # 执行作业
    if task.due_date and task.status == TaskStatus.PENDING:
        execution_job_id = f"execution_{task.id}"
        try:
            due_utc = _ensure_utc(task.due_date) # <--- [新增] 确保时间是 UTC 感知的
            print(f"SCHEDULER DEBUG: Adding execution job at {due_utc} ({due_utc.tzinfo})") # <--- [新增] 添加时区日志
            current_scheduler.add_job(
                execute_task_action,
                trigger=DateTrigger(run_date=due_utc),
                args=[task.id, task.action_type, task.action_payload],
                id=execution_job_id,
                name=f"Execution for task {task.title[:20]}",
                replace_existing=True
            )
            print(f"SCHEDULER: 已为任务 '{task.id}' 添加/更新执行作业，运行于 {task.due_date}")
        except Exception as e:
            print(f"SCHEDULER_ERROR: 添加执行作业失败 for task '{task.id}': {e}")

def _cancel_task_jobs(task_id: str):
    """从 APScheduler 取消作业"""
    current_scheduler = get_scheduler()
    if not current_scheduler or not current_scheduler.running:
        print("SCHEDULER_ERROR: APScheduler 未运行，无法取消作业。")
        return

    reminder_job_id = f"reminder_{task_id}"
    execution_job_id = f"execution_{task_id}"

    try:
        current_scheduler.remove_job(reminder_job_id)
        print(f"SCHEDULER: 已移除提醒作业 for task '{task_id}'")
    except JobLookupError:
        print(f"SCHEDULER_INFO: 提醒作业 '{reminder_job_id}' 未找到，无需移除。")
    except Exception as e:
        print(f"SCHEDULER_ERROR: 移除提醒作业失败 for task '{task_id}': {e}")

    try:
        current_scheduler.remove_job(execution_job_id)
        print(f"SCHEDULER: 已移除执行作业 for task '{task_id}'")
    except JobLookupError:
        print(f"SCHEDULER_INFO: 执行作业 '{execution_job_id}' 未找到，无需移除。")
    except Exception as e:
        print(f"SCHEDULER_ERROR: 移除执行作业失败 for task '{task_id}': {e}")

@router.post("/", response_model=TaskModel, status_code=status.HTTP_201_CREATED)
async def create_task(task_request: CreateTaskRequest = Body(...)):
    """
    创建一个新任务。
    """
    now = datetime.utcnow()
    task_id = str(uuid.uuid4())

    reminder_time_val = None
    if task_request.due_date and task_request.reminder_offset_minutes is not None:
        reminder_time_val = task_request.due_date - timedelta(minutes=task_request.reminder_offset_minutes)

    reminder_methods_values = [
        method.value if hasattr(method, 'value') else str(method)
        for method in (task_request.reminder_methods or [ReminderMethod.NOTIFICATION])
    ]

    insert_query = TaskDB.__table__.insert().values(
        id=task_id,
        title=task_request.title,
        description=task_request.description,
        status=TaskStatus.PENDING,
        created_at=now,
        updated_at=now,
        due_date=task_request.due_date,
        reminder_time=reminder_time_val,
        reminder_offset_minutes=task_request.reminder_offset_minutes,
        reminder_methods=reminder_methods_values, # <--- 确保存入的是字符串列表
        priority=task_request.priority or 0,
        tags=task_request.tags or [],
        action_type=task_request.action_type,
        action_payload=task_request.action_payload or {}
    )

    try:
        await database.execute(insert_query)
    except Exception as e:
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Failed to create task in database: {e}")

    created_task_db = await database.fetch_one(TaskDB.__table__.select().where(TaskDB.id == task_id))
    if not created_task_db:
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to retrieve task after creation")

    response_task = TaskModel.model_validate(dict(created_task_db))

    response_task.reminder_methods = [
        m.value if hasattr(m, 'value') else str(m)
        for m in response_task.reminder_methods
    ]

    _schedule_task_jobs(response_task)
    print(f"TASK_MANAGER: Created task '{response_task.id}' with title '{response_task.title}' in DB")
    return response_task

@router.get("/", response_model=List[TaskModel])
async def list_tasks(
    status_filter: Optional[TaskStatus] = Query(None, alias="status"),
    priority_filter: Optional[int] = Query(None, alias="priority"),
    skip: int = Query(0, ge=0),
    limit: int = Query(10, ge=1, le=100)
):
    """
    获取任务列表，支持过滤和分页。
    """
    query = TaskDB.__table__.select()
    if status_filter:
        query = query.where(TaskDB.status == status_filter)
    if priority_filter is not None:
        query = query.where(TaskDB.priority == priority_filter)

    query = query.order_by(TaskDB.created_at.desc()).offset(skip).limit(limit)

    db_tasks = await database.fetch_all(query)
    return [TaskModel.model_validate(dict(task)) for task in db_tasks]

@router.get("/{task_id}", response_model=TaskModel)
async def get_task(task_id: str = Path(..., description="要获取的任务ID")):
    """
    根据ID获取单个任务的详细信息。
    """
    query = TaskDB.__table__.select().where(TaskDB.id == task_id)
    db_task = await database.fetch_one(query)
    if not db_task:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Task not found")
    return TaskModel.model_validate(dict(db_task))

@router.put("/{task_id}", response_model=TaskModel)
async def update_task(
    task_id: str = Path(..., description="要更新的任务ID"),
    task_update: UpdateTaskRequest = Body(...)
):
    """
    更新现有任务。
    """
    existing_task_query = TaskDB.__table__.select().where(TaskDB.id == task_id)
    db_task = await database.fetch_one(existing_task_query)
    if not db_task:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Task not found")

    update_data = task_update.model_dump(exclude_unset=True)
    update_data["updated_at"] = datetime.utcnow()

    if "reminder_methods" in update_data and update_data["reminder_methods"] is not None:
        update_data["reminder_methods"] = [
            method.value if hasattr(method, 'value') else str(method)
            for method in update_data["reminder_methods"]
        ]

    current_due_date = update_data.get("due_date", cast(Optional[datetime], db_task.due_date))
    current_offset = update_data.get("reminder_offset_minutes", cast(Optional[int], db_task.reminder_offset_minutes))

    if current_due_date and current_offset is not None:
        update_data["reminder_time"] = current_due_date - timedelta(minutes=current_offset)
    elif "due_date" in update_data and current_offset is None:
         update_data["reminder_time"] = None


    update_query = TaskDB.__table__.update().where(TaskDB.id == task_id).values(**update_data)
    await database.execute(update_query)

    updated_db_task = await database.fetch_one(existing_task_query)
    if not updated_db_task:
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to retrieve task after update")

    response_task = TaskModel.model_validate(dict(updated_db_task))

    response_task.reminder_methods = [
        m.value if hasattr(m, 'value') else str(m)
        for m in response_task.reminder_methods
    ]

    _cancel_task_jobs(task_id)
    _schedule_task_jobs(response_task)
    print(f"TASK_MANAGER: Updated task '{response_task.id}' in DB")
    return response_task

@router.delete("/{task_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_task(task_id: str = Path(..., description="要删除的任务ID")):
    """
    删除一个任务。
    """
    existing_task_query = TaskDB.__table__.select().where(TaskDB.id == task_id)
    db_task = await database.fetch_one(existing_task_query)
    if not db_task:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Task not found")

    delete_query = TaskDB.__table__.delete().where(TaskDB.id == task_id)
    await database.execute(delete_query)

    _cancel_task_jobs(task_id)
    print(f"TASK_MANAGER: Deleted task '{task_id}' from DB")
    return None

@router.post("/{task_id}/complete", response_model=TaskModel)
async def mark_task_as_complete(task_id: str = Path(..., description="要标记为完成的任务ID")):
    """
    将任务标记为已完成。
    """
    existing_task_query = TaskDB.__table__.select().where(TaskDB.id == task_id)
    db_task_row = await database.fetch_one(existing_task_query)
    if not db_task_row:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Task not found")

    db_task = TaskModel.model_validate(dict(db_task_row))
    if db_task.status == TaskStatus.COMPLETED:
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="Task is already completed")

    update_data = {
        "status": TaskStatus.COMPLETED,
        "updated_at": datetime.utcnow()
    }
    update_query = TaskDB.__table__.update().where(TaskDB.id == task_id).values(**update_data)
    await database.execute(update_query)

    completed_db_task = await database.fetch_one(existing_task_query)
    if not completed_db_task:
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to retrieve task after marking complete")

    response_task = TaskModel.model_validate(dict(completed_db_task))
    _cancel_task_jobs(task_id)
    print(f"TASK_MANAGER: Marked task '{response_task.id}' as completed in DB")
    return response_task--- END OF FILE task_manager_service.py ---

--- START OF FILE utils.py ---# zhz_agent/utils.py

import httpx
import json
import traceback
import os
from dotenv import load_dotenv
load_dotenv()

MCPO_BASE_URL = os.getenv("MCPO_BASE_URL", "http://localhost:8006")

async def call_mcpo_tool(tool_name_with_prefix: str, payload: dict):
    """
    异步调用MCP工具服务。
    """
    api_url = f"{MCPO_BASE_URL}/{tool_name_with_prefix}"
    cleaned_payload = {k: v for k, v in payload.items() if v is not None}

    print(f"Calling mcpo endpoint: POST {api_url} with payload: {json.dumps(cleaned_payload, ensure_ascii=False)}")

    async with httpx.AsyncClient() as client:
        response = None  # 初始化response变量
        try:
            headers = {"Content-Type": "application/json"}
            response = await client.post(api_url, json=cleaned_payload, headers=headers, timeout=120.0)
            print(f"mcpo status code: {response.status_code}")

            if response.status_code == 200:
                try:
                    result_data = response.json()
                    if isinstance(result_data, dict) and result_data.get("isError"):
                        error_content_list = result_data.get("content", [{"type": "text", "text": "Unknown error from MCP tool"}])
                        error_text = "Unknown error from MCP tool"
                        for item in error_content_list:
                            if item.get("type") == "text":
                                error_text = item.get("text", error_text)
                                break
                        print(f"MCP Tool execution failed (isError=true): {error_text}")
                        try:
                            parsed_mcp_error = json.loads(error_text)
                            if isinstance(parsed_mcp_error, dict) and "error" in parsed_mcp_error:
                                return {"error": f"Tool '{tool_name_with_prefix}' failed via MCP: {parsed_mcp_error['error']}"}
                        except json.JSONDecodeError:
                            pass # 不是JSON，直接使用error_text
                        return {"error": f"Tool '{tool_name_with_prefix}' failed via MCP: {error_text}"}
                    return result_data
                except json.JSONDecodeError:
                    print(f"Warning: mcpo returned status 200 but response is not JSON for '{tool_name_with_prefix}'. Returning raw text.")
                    return {"content": [{"type": "text", "text": response.text}]} # 包装成MCP期望的格式之一
            else:
                error_text = f"mcpo call to '{tool_name_with_prefix}' failed with status {response.status_code}. Response: {response.text[:500]}..."
                print(error_text)
                return {"error": error_text}

        except httpx.RequestError as exc: # 更具体的网络请求错误
            error_msg = f"HTTP RequestError calling mcpo tool '{tool_name_with_prefix}': {type(exc).__name__} - {exc}"
            print(error_msg)
            traceback.print_exc()
            return {"error": error_msg}
        except Exception as exc: # 捕获其他所有异常
            error_msg = f"Unexpected error calling mcpo tool '{tool_name_with_prefix}': {type(exc).__name__} - {exc}"
            response_text_snippet = response.text[:500] if response and hasattr(response, 'text') else "Response object not available or no text."
            print(f"{error_msg}. Response snippet: {response_text_snippet}")
            traceback.print_exc()
            return {"error": error_msg}
--- END OF FILE utils.py ---

--- START OF FILE vector.py ---# zhz_agent/vector.py
import json
import os
from typing import List, Dict, Any, Optional
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# --- [修改] 导入 Pydantic 模型 -> 改为绝对导入 ---
from zhz_agent.pydantic_models import RetrievedDocument

class VectorRetriever:
    def __init__(self, data_path: str):
        """
        初始化向量检索器。
        
        参数:
            data_path (str): 包含文档的目录路径。
        """
        self.data_path = data_path
        self.documents: List[Dict[str, Any]] = []  # 存储加载的文档列表
        self.vectorizer: Optional[TfidfVectorizer] = None  # TF-IDF向量化器实例
        self.document_vectors: Optional[np.ndarray] = None  # 文档的向量表示
        self._load_documents()  # 加载文档并初始化向量化器

    def _load_documents(self):
        """
        从JSON文件加载文档并初始化TF-IDF向量化器。
        """
        file_path = os.path.join(self.data_path, "sample_documents.json")
        if not os.path.exists(file_path):
            print(f"错误: 模拟向量文档文件不存在: {file_path}")
            return

        with open(file_path, 'r', encoding='utf-8') as f:
            self.documents = json.load(f)
        
        if not self.documents:
            print("警告: 模拟向量文档文件为空。")
            return

        corpus = [doc['content'] for doc in self.documents]
        
        self.vectorizer = TfidfVectorizer(max_features=100)
        self.document_vectors = self.vectorizer.fit_transform(corpus)
        print(f"VectorRetriever: 加载了 {len(self.documents)} 个文档，TF-IDF模型已训练。")

    async def retrieve(self, query: str, top_k: int = 3) -> List[RetrievedDocument]:
        """
        根据查询文本进行语义检索。
        
        参数:
            query (str): 用户输入的查询文本。
            top_k (int): 返回的最相关文档数量，默认为3。
        
        返回:
            List[RetrievedDocument]: 包含最相关文档的列表。
        """
        if not self.vectorizer or self.document_vectors is None:
            print("VectorRetriever未初始化或没有文档。")
            return []

        query_vector = self.vectorizer.transform([query])
        similarities = cosine_similarity(query_vector, self.document_vectors).flatten()
        top_indices = similarities.argsort()[-top_k:][::-1] # 降序排列，取top_k
        
        retrieved_results: List[RetrievedDocument] = []
        for idx in top_indices:
            doc = self.documents[idx]
            score = float(similarities[idx]) # 将numpy float转换为Python float
            
            if score >= 0.0: # 暂时设为0，确保能返回结果
                retrieved_results.append(
                    RetrievedDocument(
                        source_type="vector_search",
                        content=doc['content'],
                        score=score,
                        metadata=doc.get('metadata', {})
                    )
                )
        print(f"VectorRetriever: 检索到 {len(retrieved_results)} 个结果。")
        return retrieved_results--- END OF FILE vector.py ---

