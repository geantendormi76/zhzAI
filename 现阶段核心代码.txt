--- START OF FILE __init__.py ------ END OF FILE __init__.py ---

--- START OF FILE bm25.py ---# zhz_agent/bm25.py

import json
import os
from typing import List, Dict, Any, Optional
import jieba # 用于中文分词
from bm25s import BM25 # 导入bm25s库

# --- [修改] 从项目内部导入pydantic_models -> 改为绝对导入 ---
from zhz_agent.pydantic_models import RetrievedDocument

class BM25Retriever:
    def __init__(self, data_path: str):
        self.data_path = data_path
        self.documents: List[Dict[str, Any]] = [] # 存储原始文档
        self.tokenized_corpus: List[List[str]] = [] # 存储分词后的文档
        self.bm25_model: Optional[BM25] = None # BM25模型实例
        self._load_documents_and_build_index()

    def _load_documents_and_build_index(self):
        """
        从JSON文件加载文档，进行分词，并构建BM25索引。
        """
        file_path = os.path.join(self.data_path, "sample_documents.json")
        if not os.path.exists(file_path):
            print(f"错误: 模拟BM25文档文件不存在: {file_path}")
            return

        with open(file_path, 'r', encoding='utf-8') as f:
            self.documents = json.load(f)

        if not self.documents:
            print("警告: 模拟BM25文档文件为空。")
            return

        # 提取所有文档内容用于分词和索引
        corpus_texts = [doc['content'] for doc in self.documents]

        print("BM25Retriever: 正在对文档进行分词并构建索引...")
        # 使用jieba进行中文分词
        self.tokenized_corpus = [list(jieba.cut(text)) for text in corpus_texts]

        # 初始化BM25模型并构建索引
        self.bm25_model = BM25()
        self.bm25_model.index(self.tokenized_corpus)

        print(f"BM25Retriever: 加载了 {len(self.documents)} 个文档，BM25索引已构建。")

    async def retrieve(self, query: str, top_k: int = 3) -> List[RetrievedDocument]:
        """
        根据查询文本进行关键词检索。
        """
        if not self.bm25_model or not self.documents:
            print("BM25Retriever未初始化或没有文档。")
            return []

        # 对查询进行分词
        query_tokens_single = list(jieba.cut(query)) # 单个查询的分词结果
        query_tokens_batch = [query_tokens_single] # <--- 将单个查询封装成列表的列表

        # 执行BM25检索
        # bm25s的retrieve方法返回文档索引和对应的BM25分数
        doc_indices, doc_scores = self.bm25_model.retrieve(query_tokens_batch, k=top_k, return_as="tuple")

        retrieved_results: List[RetrievedDocument] = []

        # bm25s.retrieve 返回的是一个元组 (doc_indices_batch, doc_scores_batch)
        # 即使是单个查询，它们也是列表的列表，所以我们取第一个元素
        if doc_indices is not None and doc_indices.shape[0] > 0 and doc_indices[0].size > 0: # 检查是否有结果
            # doc_indices[0] 是第一个查询的结果索引数组
            # doc_scores[0] 是第一个查询的结果分数数组

            for i in range(doc_indices[0].size): # 遍历第一个查询的结果
                doc_idx = doc_indices[0][i]
                score = float(doc_scores[0][i]) # 将numpy float转换为Python float

                # 过滤掉得分过低的（BM25分数可能为负，或非常接近0）
                # 这里可以根据实际情况调整阈值，但BM25通常不设严格阈值，而是取top_k
                # 如果score为负，通常表示不相关，可以过滤掉
                if score > 0:
                    doc = self.documents[doc_idx]
                    retrieved_results.append(
                        RetrievedDocument(
                            source_type="keyword_bm25",
                            content=doc['content'],
                            score=score,
                            metadata=doc.get('metadata', {})
                        )
                    )
        print(f"BM25Retriever: 检索到 {len(retrieved_results)} 个BM25结果。")
        return retrieved_results

# 示例用法 (可以在文件末尾添加，用于独立测试)
# async def main():
#     bm25_retriever = BM25Retriever(data_path="data")
#     results = await bm25_retriever.retrieve("公司最新人力资源政策", top_k=2)
#     for res in results:
#         print(f"得分: {res.score:.4f}, 来源: {res.source_type}, 内容: {res.content[:50]}...")
# if __name__ == "__main__":
#     import asyncio
#     asyncio.run(main())--- END OF FILE bm25.py ---

--- START OF FILE chromadb_retriever.py ---from typing import List, Dict, Any, Optional
from sentence_transformers import SentenceTransformer
import chromadb
import logging


# 配置ChromaDBRetriever的日志记录器
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

class ChromaDBRetriever:
    def __init__(
        self,
        collection_name: str = "rag_documents",
        persist_directory: str = "/home/zhz/dagster_home/chroma_data",
        embedding_model_name_or_path: str = "/home/zhz/models/bge-small-zh-v1.5",
    ):
        """
        初始化ChromaDBRetriever。

        Args:
            collection_name (str): 要查询的ChromaDB集合名称。
            persist_directory (str): ChromaDB数据持久化的目录。
            embedding_model_name_or_path (str): 用于查询向量化的SentenceTransformer模型名称或路径。
            device (str): 模型运行的设备 (例如 "cpu", "cuda")。
        """
        self.collection_name = collection_name
        self.persist_directory = persist_directory
        self.embedding_model_name_or_path = embedding_model_name_or_path
        self._client: Optional[chromadb.Client] = None
        self._collection: Optional[chromadb.Collection] = None
        self._embedding_model: Optional[SentenceTransformer] = None

        self._initialize_retriever()

    def _initialize_retriever(self):
        """
        初始化ChromaDB客户端、集合和嵌入模型。
        """
        try:
            logger.info(f"Initializing ChromaDB client from path: {self.persist_directory}")
            self._client = chromadb.PersistentClient(path=self.persist_directory)
            logger.info(f"Getting ChromaDB collection: {self.collection_name}")
            self._collection = self._client.get_collection(name=self.collection_name)
            if self._collection.count() == 0:
                logger.warning(f"ChromaDB collection '{self.collection_name}' is empty!")
            else:
                logger.info(f"ChromaDB collection '{self.collection_name}' loaded. Item count: {self._collection.count()}")
        except Exception as e:
            logger.error(f"Failed to initialize ChromaDB client or collection: {e}")
            raise

        try:
            logger.info(f"Loading SentenceTransformer model: {self.embedding_model_name_or_path}")
            self._embedding_model = SentenceTransformer(self.embedding_model_name_or_path)
            logger.info("SentenceTransformer model loaded successfully.")
        except Exception as e:
            logger.error(f"Failed to load SentenceTransformer model: {e}")
            raise
            
    def retrieve(self, query_text: str, n_results: int = 5, include_fields: Optional[List[str]] = None) -> List[Dict[str, Any]]:
        """
        根据查询文本从ChromaDB检索相似的文档块。

        Args:
            query_text (str): 用户查询的文本。
            n_results (int): 希望返回的最大结果数量。
            include_fields (Optional[List[str]]): 希望从ChromaDB返回的字段列表，
                                               例如 ["metadatas", "documents", "distances"]。
                                               如果为None，则ChromaDB通常会返回默认字段。
                                               我们存储时，chunk_text在metadatas里。

        Returns:
            List[Dict[str, Any]]: 检索到的文档块列表。每个字典通常包含：
                                   'id' (chunk_id), 
                                   'text' (chunk_text from metadata), 
                                   'metadata' (原始元数据),
                                   'distance' (相似度距离，越小越相似)
                                   具体结构取决于ChromaDB的返回和我们的处理。
        """
        if self._collection is None or self._embedding_model is None:
            logger.error("Retriever is not properly initialized.")
            # 尝试重新初始化，或者直接返回错误/空列表
            try:
                self._initialize_retriever()
                if self._collection is None or self._embedding_model is None: # 再次检查
                    return []
            except Exception as e:
                logger.error(f"Failed to re-initialize retriever during retrieve call: {e}")
                return []

        logger.info(f"Retrieving documents for query: '{query_text[:100]}...' with n_results={n_results}")
        
        try:
            # 1. 将查询文本向量化并归一化
            query_embedding_np = self._embedding_model.encode(
                query_text, 
                convert_to_tensor=False, 
                normalize_embeddings=True # <--- 关键：归一化查询嵌入
            )
            query_embedding = query_embedding_np.tolist()
            
            # 2. 在ChromaDB中查询 (include_fields_query 的逻辑不变)
            if include_fields is None:
                include_fields_query = ["metadatas", "distances"] 
            else:
                include_fields_query = include_fields
            results = self._collection.query(
                query_embeddings=[query_embedding], 
                n_results=n_results,
                include=include_fields_query 
            )
            # 3. 处理并格式化结果
            retrieved_docs = []
            if results and results.get("ids") and results.get("ids")[0]:
                ids_list = results["ids"][0]
                metadatas_list = results.get("metadatas", [[]])[0] 
                distances_list = results.get("distances", [[]])[0] 

                for i in range(len(ids_list)):
                    # ... (提取 chunk_id, metadata, distance 的代码不变) ...
                    chunk_id = ids_list[i]
                    metadata = metadatas_list[i] if metadatas_list and i < len(metadatas_list) else {}
                    distance = distances_list[i] if distances_list and i < len(distances_list) else float('inf')
                    chunk_text_content = metadata.get("chunk_text", "[Chunk text not found in metadata]")

                    # 计算相似度分数
                    # 如果ChromaDB使用cosine距离 (范围0-2, 0表示最相似)
                    # 相似度 = 1 - (distance / 2)  => 范围 0-1, 1表示最相似
                    # 或者直接用 cosine_similarity = 1 - distance (如果distance是 1-cos_sim)
                    # ChromaDB的cosine距离是 sqrt(2-2*cos_sim) 的平方，即 2-2*cos_sim (如果向量已归一化)
                    # 所以 cos_sim = 1 - distance / 2
                    # 我们希望分数越大越好
                    score = (1 - distance / 2.0) if distance != float('inf') and distance <=2.0 else 0.0 
                    # 确保分数在合理范围，如果distance意外地大于2，则score为0

                    retrieved_docs.append({
                        "id": chunk_id,
                        "text": chunk_text_content,
                        "score": score, # <--- 更新了分数计算
                        "distance": distance, 
                        "metadata": metadata 
                    })
                
                logger.info(f"Retrieved {len(retrieved_docs)} documents from ChromaDB.")
            else:
                logger.info("No documents retrieved from ChromaDB for the query.")

            return retrieved_docs

        except Exception as e:
            logger.error(f"Error during ChromaDB retrieval: {e}")
            return []

if __name__ == '__main__':
    # 简单的测试代码
    logger.info("--- ChromaDBRetriever Test ---")
    
    # 确保您的ChromaDB数据库中已经有通过Dagster流水线存入的数据
    # 并且模型路径正确
    try:
        retriever = ChromaDBRetriever()
        
        # 测试查询
        test_query = "人工智能的应用有哪些？" 
        # 根据您doc1.txt "这是第一个测试文档，关于人工智能和机器学习。" 应该能召回一些
        
        retrieved_results = retriever.retrieve(test_query, n_results=3)
        
        if retrieved_results:
            print(f"\n--- Retrieved Results for query: '{test_query}' ---")
            for i, doc in enumerate(retrieved_results):
                print(f"Result {i+1}:")
                print(f"  ID: {doc['id']}")
                print(f"  Text (first 100 chars): {doc['text'][:100]}...")
                print(f"  Score: {doc['score']:.4f} (Distance: {doc['distance']:.4f})")
                print(f"  Metadata: {doc['metadata']}")
                print("-" * 20)
        else:
            print(f"\nNo results retrieved for query: '{test_query}'")
            
        # 测试一个可能没有结果的查询
        # test_query_no_results = "恐龙为什么会灭绝？"
        # retrieved_no_results = retriever.retrieve(test_query_no_results, n_results=3)
        # if not retrieved_no_results:
        #     print(f"\nCorrectly retrieved no results for query: '{test_query_no_results}'")

    except Exception as e:
        print(f"An error occurred during the test: {e}")
--- END OF FILE chromadb_retriever.py ---

--- START OF FILE custom_crewai_tools.py ---# zhz_agent/custom_crewai_tools.py

import os
import json
import asyncio
import traceback
from typing import Type, List, Dict, Any, Optional, ClassVar
from pydantic import BaseModel, Field
from crewai.tools import BaseTool
import httpx

# 从 zhz_agent.pydantic_models 导入 QueryRequest
from zhz_agent.pydantic_models import QueryRequest # 用于 RAG 工具的输入

# MCPO 代理的基地址
MCPO_BASE_URL = os.getenv("MCPO_BASE_URL", "http://localhost:8006")

class BaseMCPTool(BaseTool):
    mcpo_base_url: str = MCPO_BASE_URL
    _call_mcpo_func: ClassVar[callable] = None

    @classmethod
    def set_mcpo_caller(cls, caller: callable):
        cls._call_mcpo_func = caller

    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    async def _call_mcpo_endpoint(self, service_and_tool_path: str, payload: dict) -> dict | str:
        api_url = f"{self.mcpo_base_url}/{service_and_tool_path}"
        cleaned_payload = {k: v for k, v in payload.items() if v is not None}
        print(f"BaseMCPTool: Calling mcpo endpoint: POST {api_url} with payload: {json.dumps(cleaned_payload, ensure_ascii=False)}")
        
        # --- [修改] 移除 proxies=None 参数 ---
        async with httpx.AsyncClient(trust_env=False) as client:
            response = None
            try:
                headers = {"Content-Type": "application/json"}
                response = await client.post(api_url, json=cleaned_payload, headers=headers, timeout=300.0)
                print(f"BaseMCPTool: mcpo status code for {service_and_tool_path}: {response.status_code}")
                print(f"BaseMCPTool: mcpo response headers for {service_and_tool_path}: {response.headers}") # <--- 新增日志
                # 尝试分块读取响应或提前获取少量内容进行日志记录，以防响应过大卡住 .text 或 .json()
                try:
                    response_text_snippet = await response.aread(num_bytes=1024) # 读取前1KB
                    print(f"BaseMCPTool: mcpo response text snippet (first 1KB) for {service_and_tool_path}: {response_text_snippet.decode(errors='ignore')}")
                except Exception as e_read:
                    print(f"BaseMCPTool: Error reading response snippet: {e_read}")

                if response.status_code == 200:
                    try:
                        # print(f"BaseMCPTool: mcpo raw response text for {service_and_tool_path}: {response.text}") # 如果怀疑内容问题，可以取消注释，但小心大响应
                        return response.json()
                    except json.JSONDecodeError:
                        print(f"BaseMCPTool Warning: mcpo returned status 200 but response is not JSON for '{service_and_tool_path}'. Returning raw text.")
                        return response.text
                else:
                    error_text = f"mcpo call to '{service_and_tool_path}' failed with status {response.status_code}. Response: {response.text[:500]}..."
                    print(f"BaseMCPTool Error: {error_text}")
                    return {"error": error_text, "status_code": response.status_code}
            except httpx.RequestError as exc:
                error_msg = f"BaseMCPTool HTTP RequestError calling mcpo tool '{service_and_tool_path}': {type(exc).__name__} - {exc}"
                print(f"BaseMCPTool Error: {error_msg}"); traceback.print_exc()
                return {"error": error_msg, "exception_type": type(exc).__name__}
            except Exception as exc:
                error_msg = f"BaseMCPTool Unexpected error calling mcpo tool '{service_and_tool_path}': {type(exc).__name__} - {exc}"
                response_text_snippet = response.text[:500] if response and hasattr(response, 'text') else "Response object not available or no text."
                print(f"BaseMCPTool Error: {error_msg}. Response snippet: {response_text_snippet}"); traceback.print_exc()
                return {"error": error_msg, "exception_type": type(exc).__name__}

    def _handle_tool_result(self, result: dict | str, tool_name_for_log: str) -> str:
        print(f"BaseMCPTool DEBUG: {tool_name_for_log} result from mcpo: {str(result)[:500]}...")
        parsed_result = result
        if isinstance(result, str):
            try:
                parsed_result = json.loads(result)
            except json.JSONDecodeError:
                if "error" in result.lower() or "failed" in result.lower() or "traceback" in result.lower():
                    return f"调用 {tool_name_for_log} 失败，返回非JSON错误文本: {result}"
                print(f"BaseMCPTool Info: Result for {tool_name_for_log} is a non-JSON string, returning as is.")
                return result
        if isinstance(parsed_result, dict):
            if "error" in parsed_result and "status_code" in parsed_result:
                return f"调用 {tool_name_for_log} 时发生HTTP错误：{parsed_result.get('error')}"
            if parsed_result.get("status") == "error":
                error_msg = parsed_result.get("error_message", "未知错误")
                error_code = parsed_result.get("error_code", "NO_CODE")
                return f"工具 {tool_name_for_log} 执行失败 (错误码: {error_code})：{error_msg}"
            try:
                return json.dumps(parsed_result, ensure_ascii=False, indent=2)
            except Exception as e:
                print(f"BaseMCPTool Error formatting successful dict result for {tool_name_for_log}: {e}")
                return str(parsed_result)
        print(f"BaseMCPTool Warning: Unexpected result format from {tool_name_for_log} mcpo call: {type(result)}, content: {str(result)[:200]}")
        return f"从 {tool_name_for_log} 服务收到的结果格式不正确或无法处理: {str(result)[:500]}"

    def _run_default_sync_wrapper(self, **kwargs) -> str:
        tool_name = getattr(self, 'name', self.__class__.__name__)
        print(f"BaseMCPTool INFO: Synchronous _run called for {tool_name} with args: {kwargs}.")
        result_str = ""
        try:
            # --- 改进的 asyncio.run 处理 ---
            try:
                loop = asyncio.get_running_loop()
            except RuntimeError:
                loop = None

            async def async_runner():
                return await self._arun(**kwargs)

            if loop and loop.is_running():
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                    future = executor.submit(asyncio.run, async_runner())
                    result = future.result(timeout=120)
            else:
                result = asyncio.run(async_runner())
            result_str = str(result)
        except asyncio.TimeoutError:
            error_message = f"Tool {tool_name} execution timed out after 120 seconds."
            print(f"BaseMCPTool ERROR_TRACE: {error_message}"); result_str = error_message
        except RuntimeError as e:
            if "cannot run event loop while another loop is running" in str(e).lower() or "event loop is already running" in str(e).lower():
                error_message = (f"BaseMCPTool Error in {tool_name} _run: Nested asyncio event loop conflict. Original error: {e}")
            else:
                error_message = f"BaseMCPTool RuntimeError in {tool_name} _run: {type(e).__name__} - {e}"
            print(f"BaseMCPTool ERROR_TRACE: {error_message}"); traceback.print_exc();
            result_str = error_message
        except Exception as e:
            error_message = f"BaseMCPTool General Exception in {tool_name} _run: {type(e).__name__} - {e}"
            print(f"BaseMCPTool ERROR_TRACE: {error_message}"); traceback.print_exc(); result_str = error_message
        return result_str

class HybridRAGTool(BaseMCPTool):
    name: str = "HybridRAGQueryTool"
    description: str = (
        "【核心RAG工具】用于通过执行混合检索增强生成 (RAG) 搜索来回答用户问题。 "
        "该工具整合了向量检索、知识图谱检索和关键词检索，并进行智能融合和重排序。 "
        "当用户需要从知识库中获取信息、回答复杂问题或生成报告时，应调用此工具。"
    )
    args_schema: Type[BaseModel] = QueryRequest
    target_mcp_service_path: str = "zhz_agent_service/query_rag"

    async def _arun(self, query: str, top_k_vector: int, top_k_kg: int, top_k_bm25: int, **kwargs: Any) -> str: # --- 确认有 **kwargs ---
        tool_name_for_log = getattr(self, 'name', "HybridRAGTool")
        print(f"CrewAI Tool DEBUG: {tool_name_for_log}._arun called with query='{query}', top_k_vector={top_k_vector}, top_k_kg={top_k_kg}, top_k_bm25={top_k_bm25}, additional_kwargs={kwargs}")

        security_context = kwargs.get('security_context')
        if security_context:
            print(f"CrewAI Tool INFO: Received security_context (in HybridRAGTool): {str(security_context)[:200]}...")

        payload = {
            "query": query,
            "top_k_vector": top_k_vector,
            "top_k_kg": top_k_kg,
            "top_k_bm25": top_k_bm25
        }
        result = await self._call_mcpo_endpoint(self.target_mcp_service_path, payload)
        return self._handle_tool_result(result, self.name)

    def _run(self, query: str, top_k_vector: int, top_k_kg: int, top_k_bm25: int, **kwargs: Any) -> str: # --- 确认有 **kwargs ---
        return self._run_default_sync_wrapper(query=query, top_k_vector=top_k_vector, top_k_kg=top_k_kg, top_k_bm25=top_k_bm25, **kwargs)--- END OF FILE custom_crewai_tools.py ---

--- START OF FILE custom_llm.py ---#/home/zhz/zhz_agent/custom_llm.py
import os
import json
import httpx
import asyncio
import traceback
from typing import List, Dict, Any, Optional, Union, Sequence, Type 
# --- CrewAI & LiteLLM Imports ---
from crewai.tools import BaseTool
from crewai.llms.base_llm import BaseLLM as CrewAIBaseLLM
import litellm

# --- [修改] Local Imports -> 改为绝对导入 ---
from zhz_agent.llm import call_sglang_llm # For SGLang LLM
from dotenv import load_dotenv

load_dotenv()

# --- SGLang Config ---
SGLANG_API_URL_FOR_LLM = os.getenv("SGLANG_API_URL", "http://localhost:30000/generate")

# --- CustomGeminiLLM (from ceshi/run_agent.py with fixes) ---
class CustomGeminiLLM(CrewAIBaseLLM):
    model_name: str
    api_key: str
    max_tokens: Optional[int] = 2048
    tool_config: Optional[Dict[str, Any]] = None
    stop: Optional[List[str]] = None
    _gemini_tools_cache: Optional[List[Dict[str, Any]]] = None

    def __init__(self, model: str, api_key: str, temperature: float = 0.1, max_tokens: Optional[int] = 2048, tool_config: Optional[Dict[str, Any]] = None, stop: Optional[List[str]] = None, agent_tools: Optional[List[BaseTool]] = None, **kwargs):
        super().__init__(model=model, temperature=temperature)
        self.model_name = model
        self.api_key = api_key
        self.max_tokens = max_tokens
        self.tool_config = tool_config or {"function_calling_config": {"mode": "AUTO"}}
        self.stop = stop
        if agent_tools:
            self._gemini_tools_cache = self._convert_crewai_tools_to_gemini_format(agent_tools)
            print(f"CustomGeminiLLM __init__: Cached {len(self._gemini_tools_cache)} tools.")
        else:
            print("CustomGeminiLLM __init__: No agent_tools provided for caching.")

    def _remove_unwanted_fields(self, schema: Dict[str, Any]) -> Dict[str, Any]:
        if not isinstance(schema, dict):
            return schema

        schema.pop('title', None)

        if "properties" in schema:
            if "type" not in schema:
                schema["type"] = "object"
            for prop_name, prop_def in list(schema["properties"].items()):
                if isinstance(prop_def, dict):
                    prop_def.pop('default', None)
                    prop_def.pop('title', None)
                    self._remove_unwanted_fields(prop_def)
        elif schema.get("type") == "object" and "properties" not in schema:
            schema["properties"] = {}

        keys_to_delete = [k for k, v in schema.items() if k == 'default']
        for k in keys_to_delete:
            del schema[k]

        for k, v in schema.items():
            if isinstance(v, dict):
                self._remove_unwanted_fields(v)
            elif isinstance(v, list):
                for i, item in enumerate(v):
                    if isinstance(item, dict):
                        v[i] = self._remove_unwanted_fields(item)
        return schema

    def _convert_crewai_tools_to_gemini_format(self, tools: Optional[List[BaseTool]]) -> Optional[List[Dict[str, Any]]]:
        if not tools:
            return None
        gemini_tool_declarations = []
        for tool_instance in tools:
            tool_name = tool_instance.name
            tool_description = tool_instance.description
            if not hasattr(tool_instance, 'args_schema') or not tool_instance.args_schema:
                parameters_schema = {"type": "object", "properties": {}}
            else:
                try:
                    if hasattr(tool_instance.args_schema, 'model_json_schema'):
                        pydantic_schema = tool_instance.args_schema.model_json_schema()
                    else:
                        pydantic_schema = tool_instance.args_schema.schema()
                    cleaned_schema = self._remove_unwanted_fields(pydantic_schema.copy())
                    parameters_schema = cleaned_schema
                except Exception as e:
                    print(f"Error processing schema for tool {tool_name}: {e}")
                    parameters_schema = {"type": "object", "properties": {}}
            gemini_tool_declarations.append({
                "name": tool_name,
                "description": tool_description,
                "parameters": parameters_schema
            })
        final_tools_for_litellm = []
        for declaration in gemini_tool_declarations:
            final_tools_for_litellm.append({
                "type": "function",
                "function": declaration
            })
        return final_tools_for_litellm

    def call(self, messages: Union[str, List[Dict[str, str]]], tools: Optional[List[dict]] = None, callbacks: Optional[List[Any]] = None, **kwargs: Any) -> Union[str, Any]:
        print(f"CustomGeminiLLM CALL method invoked.")
        print(f"  CALL - Tools received by CustomLLM.call: {'Yes' if tools else 'No'}")
        print(f"  CALL - Callbacks received by CustomLLM.call: {'Yes' if callbacks else 'No'}")

        if isinstance(messages, str):
            processed_messages = [{"role": "user", "content": messages}]
        else:
            processed_messages = messages

        litellm_params = {
            "model": self.model_name,
            "messages": processed_messages,
            "api_key": self.api_key,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "stop": self.stop
        }

        # --- Proxy Addition ---
        proxy_url = os.getenv("LITELLM_PROXY_URL")
        if proxy_url:
            litellm_params["proxy"] = {
                "http": proxy_url,
                "https": proxy_url,
            }
            print(f"CustomGeminiLLM.call - Using proxy: {proxy_url}")
        else:
            print("CustomGeminiLLM.call - No proxy configured (LITELLM_PROXY_URL not set).")

        # --- Tool Handling (tools: null fix) ---
        final_tools_for_litellm = None
        received_tools_to_process = tools
        if not received_tools_to_process and self._gemini_tools_cache:
            print("  CALL - INFO: Tools argument was None, using cached tools.")
            received_tools_to_process = self._gemini_tools_cache

        if received_tools_to_process:
            cleaned_tools_for_litellm = []
            for tool_dict in received_tools_to_process:
                current_tool_def = tool_dict.copy()
                if current_tool_def.get("type") == "function" and "function" in current_tool_def:
                    func_def = current_tool_def["function"].copy()
                    if "parameters" in func_def:
                        func_def["parameters"] = self._remove_unwanted_fields(func_def["parameters"].copy())
                    current_tool_def["function"] = func_def
                    cleaned_tools_for_litellm.append(current_tool_def)
                else:
                    cleaned_tools_for_litellm.append(tool_dict)
            final_tools_for_litellm = cleaned_tools_for_litellm

        if final_tools_for_litellm:
            litellm_params["tools"] = final_tools_for_litellm
            fc_config = self.tool_config.get("function_calling_config", {})
            mode = fc_config.get("mode", "AUTO").upper()
            allowed_names = fc_config.get("allowed_function_names")

            if mode == "ANY" and allowed_names:
                litellm_params["tool_choice"] = {
                    "type": "function",
                    "function": {"name": allowed_names[0]}
                }
            elif mode in ["AUTO", "ANY", "NONE"]:
                litellm_params["tool_choice"] = mode.lower()
            else:
                litellm_params["tool_choice"] = "auto"
            print(f"CustomGeminiLLM DEBUG: Setting tool_choice to: {litellm_params['tool_choice']}")

        if callbacks:
            litellm_params["callbacks"] = callbacks

        try:
            print(f"CustomGeminiLLM.call - LiteLLM PARAMS (Preview): model={litellm_params['model']}, msgs_count={len(litellm_params['messages'])}, tools={'Yes' if 'tools' in litellm_params else 'No'}, tool_choice={litellm_params.get('tool_choice')}, proxy={'Yes' if 'proxy' in litellm_params else 'No'}")
            response = litellm.completion(**litellm_params)
        except Exception as e:
            print(f"CRITICAL ERROR: LiteLLM completion call failed: {e}")
            if callbacks:
                for handler in callbacks:
                    if hasattr(handler, 'on_llm_error'):
                        try:
                            handler.on_llm_error(error=e, llm=self, **kwargs)
                        except Exception as cb_err:
                            print(f"Error in callback on_llm_error: {cb_err}")
            raise

        llm_message_response = response.choices[0].message
        if hasattr(llm_message_response, 'tool_calls') and llm_message_response.tool_calls:
            print(f"CustomGeminiLLM.call - Detected tool_calls: {llm_message_response.tool_calls}")
            # --- ReAct Format Workaround (AttributeError fix) ---
            tool_call = llm_message_response.tool_calls[0]
            action = tool_call.function.name
            action_input = tool_call.function.arguments
            react_string = f"Action: {action}\nAction Input: {action_input}"
            print(f"CustomGeminiLLM.call - Returning ReAct string: {react_string}")
            return react_string
        else:
            print(f"CustomGeminiLLM.call - Returning text content.")
            return llm_message_response.content or ""

    def get_token_counter_instance(self):
        class GeminiTokenCounter:
            def __init__(self, model_name):
                self.model_name = model_name

            def count_tokens(self, text: Union[str, List[Dict[str,str]]]) -> int:
                try:
                    if isinstance(text, list):
                        return litellm.token_counter(model=self.model_name, messages=text)
                    return litellm.token_counter(model=self.model_name, text=str(text))
                except Exception as e:
                    print(f"Warning: Token counting failed ({e}), falling back to rough estimate.")
                    if isinstance(text, list):
                        return sum(len(str(m.get("content","")).split()) for m in text)
                    return len(str(text).split())
        return GeminiTokenCounter(model_name=self.model_name)


# --- CustomSGLangLLM (from hybrid_rag/custom_llm.py) ---
class CustomSGLangLLM(CrewAIBaseLLM):
    endpoint_url: str = SGLANG_API_URL_FOR_LLM
    model_name: str = "qwen2-3b-instruct"
    temperature: float = 0.1
    max_new_tokens_val: int = 1024

    def __init__(self, endpoint: Optional[str] = None, model: Optional[str] = None, temperature: Optional[float] = None, max_new_tokens: Optional[int] = None, **kwargs: Any):
        super().__init__(**kwargs)
        if endpoint: self.endpoint_url = endpoint
        if model: self.model_name = model
        if temperature is not None: self.temperature = temperature
        if max_new_tokens is not None: self.max_new_tokens_val = max_new_tokens
        print(f"CustomSGLangLLM initialized. Endpoint: {self.endpoint_url}, Model: {self.model_name}, Temp: {self.temperature}, MaxTokens: {self.max_new_tokens_val}")

    def _prepare_sglang_prompt(self, messages: Sequence[Dict[str, str]]) -> str:
        prompt_str = ""
        for message in messages:
            role = message.get("role")
            content = message.get("content")
            if role and content:
                prompt_str += f"<|im_start|>{role}\n{content}<|im_end|>\n"
        prompt_str += "<|im_start|>assistant\n"
        return prompt_str

    def call(self, messages: Sequence[Dict[str, str]], **kwargs: Any) -> str:
        print(f"CustomSGLangLLM.call received messages: {messages}")
        sglang_prompt = self._prepare_sglang_prompt(messages)
        print(f"CustomSGLangLLM.call prepared sglang_prompt (first 200 chars): {sglang_prompt[:200]}...")
        stop_sequences_for_sglang = kwargs.get("stop", ["<|im_end|>", "<|endoftext|>"])

        try:
            try:
                loop = asyncio.get_running_loop()
            except RuntimeError:
                loop = None

            async def async_runner():
                return await call_sglang_llm(
                    prompt=sglang_prompt,
                    temperature=self.temperature,
                    max_new_tokens=self.max_new_tokens_val,
                    stop_sequences=stop_sequences_for_sglang
                )

            if loop and loop.is_running():
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                    future = executor.submit(asyncio.run, async_runner())
                    response_text = future.result(timeout=120)
            else:
                response_text = asyncio.run(async_runner())

        except Exception as e:
            print(f"CustomSGLangLLM.call: Error during SGLang call: {type(e).__name__} - {e}")
            traceback.print_exc()
            return f"LLM_CALL_ERROR: 调用SGLang服务失败 - {str(e)}"

        if response_text is None:
            print("CustomSGLangLLM.call: SGLang returned None.")
            return "LLM_CALL_ERROR: SGLang服务未返回任何文本。"

        print(f"CustomSGLangLLM.call: SGLang returned text (first 200 chars): {response_text[:200]}...")
        return response_text

    def get_token_ids(self, text: str) -> List[int]:
        print("CustomSGLangLLM.get_token_ids: Not implemented, returning empty list.")
        return []

    @property
    def support_function_calling(self) -> bool:
        return False

    @property
    def support_stop_words(self) -> bool:
        return True

    @property
    def available_models(self) -> List[str]:
        return [self.model_name]

    @property
    def context_window(self) -> int:
        return 32768

    @property
    def identifying_params(self) -> Dict[str, Any]:
        return {
            "model": self.model_name,
            "endpoint_url": self.endpoint_url,
            "temperature": self.temperature,
            "max_new_tokens": self.max_new_tokens_val,
        }--- END OF FILE custom_llm.py ---

--- START OF FILE database.py ---# ZHZ_AGENT/database.py
import os
from databases import Database
from sqlalchemy import create_engine
from sqlalchemy.orm import declarative_base
from typing import Optional

# --- APScheduler 相关导入 ---
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.jobstores.sqlalchemy import SQLAlchemyJobStore
# --- [修改] 明确导入并使用 pytz ---
import pytz #

# --- 数据库配置 ---
ZHZ_AGENT_DIR = os.path.dirname(os.path.abspath(__file__))
DB_NAME = "ZHZ_AGENT_tasks.db"
DATABASE_FILE_PATH = os.path.join(ZHZ_AGENT_DIR, DB_NAME)
DATABASE_URL = f"sqlite+aiosqlite:///{DATABASE_FILE_PATH}"

database = Database(DATABASE_URL)
sqlalchemy_engine = create_engine(DATABASE_URL.replace("+aiosqlite", ""))
Base = declarative_base() #

# --- 全局调度器实例定义 ---
scheduler: Optional[AsyncIOScheduler] = None

def get_scheduler() -> AsyncIOScheduler:
    """获取或创建调度器实例，并配置作业存储和 UTC 时区。"""
    global scheduler
    if scheduler is None:
        jobstore_url = f"sqlite:///{DATABASE_FILE_PATH}"
        jobstores = {
            'default': SQLAlchemyJobStore(url=jobstore_url, tablename='apscheduler_jobs_v2') #
        }
        # --- [修复] 明确使用 pytz.utc 设置时区 ---
        scheduler = AsyncIOScheduler(
            jobstores=jobstores,
            timezone=pytz.utc # <--- 强制使用 pytz.utc #
        )
        import logging
        logging.getLogger('apscheduler').setLevel(logging.DEBUG)
        print(f"APScheduler initialized with timezone: {pytz.utc}") # 确认使用 pytz.utc #
    return scheduler #--- END OF FILE database.py ---

--- START OF FILE database_models.py ---# zhz_agent/database_models.py
from sqlalchemy import Column, String, DateTime, Integer, Text, Enum as SQLAlchemyEnum, ForeignKey, Boolean, JSON
from sqlalchemy.sql import func
import uuid

# --- [修改] 从 pydantic_models 导入枚举 -> 改为绝对导入 ---
from zhz_agent.pydantic_models import TaskStatus, ReminderMethod

# --- [修改] 从新的 database.py 导入 Base -> 改为绝对导入 ---
from zhz_agent.database import Base # <--- 确保只从这里导入 Base #

class TaskDB(Base): # 命名为 TaskDB 以区分 Pydantic 的 TaskModel
    __tablename__ = "tasks"

    id = Column(String, primary_key=True, index=True, default=lambda: str(uuid.uuid4()))
    title = Column(String, index=True, nullable=False)
    description = Column(Text, nullable=True) #
    status = Column(SQLAlchemyEnum(TaskStatus), default=TaskStatus.PENDING, nullable=False) #
    created_at = Column(DateTime(timezone=True), server_default=func.now(), nullable=False) #
    updated_at = Column(DateTime(timezone=True), server_default=func.now(), onupdate=func.now(), nullable=False) #
    due_date = Column(DateTime(timezone=True), nullable=True) #
    reminder_time = Column(DateTime(timezone=True), nullable=True) #
    reminder_offset_minutes = Column(Integer, nullable=True) #
    reminder_methods = Column(JSON, default=[ReminderMethod.NOTIFICATION.value], nullable=False) #
    priority = Column(Integer, default=0, nullable=False) #
    tags = Column(JSON, default=[], nullable=False) #
    action_type = Column(String, nullable=True) #
    action_payload = Column(JSON, default={}, nullable=True) #
    execution_result = Column(Text, nullable=True) #
    last_executed_at = Column(DateTime(timezone=True), nullable=True) #

    def __repr__(self):
        return f"<TaskDB(id={self.id}, title='{self.title}', status='{self.status.value}')>"--- END OF FILE database_models.py ---

--- START OF FILE file_bm25_retriever.py ---from typing import List, Dict, Any, Optional, Tuple
import jieba
import bm25s # 我们确认使用 bm25s
import pickle
import os
import logging
import numpy as np

# 配置日志记录器
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

class FileBM25Retriever:
    def __init__(
        self,
        index_directory_path: str = "/home/zhz/dagster_home/bm25_index_data/", # 与Dagster中配置一致
        # user_dict_path: Optional[str] = None 
    ):
        self.index_directory_path = index_directory_path
        # self.user_dict_path = user_dict_path

        self._bm25_model: Optional[bm25s.BM25] = None
        self._doc_ids: Optional[List[str]] = None 
        # self._tokenizer: Optional[bm25s.Tokenizer] = None # bm25s.tokenize 是一个函数，或者可以用Tokenizer类

        self._initialize_retriever()

    def _initialize_retriever(self):
        # if self.user_dict_path and os.path.exists(self.user_dict_path):
        #     # ... (加载jieba用户词典的逻辑不变) ...
        
        if not os.path.isdir(self.index_directory_path): # <--- 修改：检查目录是否存在
            logger.error(f"BM25 index directory not found at: {self.index_directory_path}")
            raise FileNotFoundError(f"BM25 index directory not found: {self.index_directory_path}")

        try:
            logger.info(f"Loading BM25 model from directory: {self.index_directory_path}")
            # 使用bm25s的load类方法
            # load_corpus=False 因为我们不期望在模型文件中包含原始语料库文本
            # mmap=False (默认) 先不使用内存映射，除非索引非常大
            self._bm25_model = bm25s.BM25.load(
                self.index_directory_path,
                load_corpus=False, # 通常我们不需要在这里加载原始语料库
                # mmap=False 
            )
            
            if self._bm25_model is None: # load 失败通常会抛异常，但以防万一
                logger.error("Failed to load BM25 model (returned None).")
                raise ValueError("Failed to load BM25 model.")
            logger.info("BM25 model loaded successfully.")

            # 单独加载 document_ids 列表
            doc_ids_path = os.path.join(self.index_directory_path, "doc_ids.pkl")
            if not os.path.exists(doc_ids_path):
                logger.error(f"doc_ids.pkl not found in {self.index_directory_path}")
                raise FileNotFoundError(f"doc_ids.pkl not found in {self.index_directory_path}")
            
            with open(doc_ids_path, 'rb') as f_in:
                self._doc_ids = pickle.load(f_in)
            
            if self._doc_ids is None: # pickle 加载空文件可能返回None
                 logger.warning(f"doc_ids.pkl loaded, but it was empty or invalid.")
                 self._doc_ids = [] # 设为空列表以避免后续错误
            logger.info(f"Document IDs loaded successfully. Number of indexed documents: {len(self._doc_ids)}")

        except Exception as e:
            logger.error(f"Failed to load BM25 index or document IDs: {e}")
            raise
            
    def retrieve(self, query_text: str, n_results: int = 5) -> List[Dict[str, Any]]:
        # ... (初始化检查和空索引检查不变) ...
        if self._bm25_model is None or self._doc_ids is None:
            logger.error("BM25 Retriever is not properly initialized.")
            # ... (尝试重新初始化或返回空的逻辑不变) ...
            return []
        
        if not self._doc_ids: 
            logger.info("BM25 index is empty, no results to retrieve.")
            return []

        logger.info(f"Retrieving documents with BM25 for query: '{query_text[:100]}...' with n_results={n_results}")

        try:
            # 1. 将查询文本分词
            # bm25s 有自己的 tokenize 函数，它会处理停用词、词干提取（如果配置了）并返回模型期望的格式
            # 我们需要确保查询时的分词方式与索引时一致。
            # 如果索引时用了jieba，查询时也应该用jieba。
            # bm25s.BM25 对象在加载后，其内部应该已经有了词汇表 (vocab_dict)，
            # 它的 get_scores 方法期望的是与词汇表ID对应的输入，或者它能自己处理分词后的文本列表。
            
            query_tokenized_jieba = list(jieba.cut_for_search(query_text))
            logger.debug(f"Tokenized query (jieba): {query_tokenized_jieba}")

            # 2. 使用BM25模型进行查询
            # bm25s 的 retrieve 方法可以直接返回文档索引和分数
            # 它接受分词后的查询 (list of str)
            # results 是文档索引 (numpy array), scores 是对应的分数 (numpy array)
            # 它们都是 (n_queries, k) 的形状，我们只有一个查询，所以是 (1, k)
            
            # 确保 k 不超过实际文档数
            actual_k = min(n_results, len(self._doc_ids))
            if actual_k == 0: # 如果索引中没有文档
                return []

            results_indices, results_scores = self._bm25_model.get_top_n(
                query_tokenized_jieba, 
                corpus=None, # 我们不需要在这里提供原始语料库，它返回的是索引
                n=actual_k
            )
            # get_top_n 返回的是一个列表（每个查询一个结果列表），我们只有一个查询
            # 每个结果列表中的元素是 (doc_index, score) 吗？还是直接是doc_index?
            # 查阅 bm25s 文档：get_top_n(query_tokens, documents, n=5)
            #   - query_tokens: list of tokens for the query.
            #   - documents: list of documents (list of tokens).
            #   - n: number of top documents to retrieve.
            # Returns: list of top n documents.
            # 这看起来是返回文档本身，不是我们想要的。

            # 让我们回到使用 get_scores 然后自己排序的方式，这更可控
            all_scores = self._bm25_model.get_scores(query_tokenized_jieba)
            
            top_n_indices = np.argsort(all_scores)[-actual_k:][::-1] # 降序取前N

            retrieved_docs = []
            for doc_corpus_index in top_n_indices:
                doc_id = self._doc_ids[doc_corpus_index] # 从0-based索引映射到我们的chunk_id
                score = float(all_scores[doc_corpus_index]) 
                retrieved_docs.append({
                    "id": doc_id,
                    "score": score
                })
            
            logger.info(f"Retrieved {len(retrieved_docs)} documents using BM25.")
            return retrieved_docs

        except Exception as e:
            logger.error(f"Error during BM25 retrieval: {e}")
            return []
            
    def retrieve(self, query_text: str, n_results: int = 5) -> List[Dict[str, Any]]:
        """
        根据查询文本使用BM25检索相关的文档块ID和分数。

        Args:
            query_text (str): 用户查询的文本。
            n_results (int): 希望返回的最大结果数量。

        Returns:
            List[Dict[str, Any]]: 检索到的文档块列表。每个字典包含：
                                   'id' (chunk_id), 
                                   'score' (BM25分数)
        """
        if self._bm25_model is None or self._doc_ids is None:
            logger.error("BM25 Retriever is not properly initialized.")
            try:
                self._initialize_retriever()
                if self._bm25_model is None or self._doc_ids is None:
                    return []
            except Exception as e:
                logger.error(f"Failed to re-initialize BM25 retriever during retrieve call: {e}")
                return []
        
        if not self._doc_ids: # 如果索引为空
            logger.info("BM25 index is empty, no results to retrieve.")
            return []

        logger.info(f"Retrieving documents with BM25 for query: '{query_text[:100]}...' with n_results={n_results}")

        try:
            # 1. 将查询文本分词
            query_tokenized = list(jieba.cut_for_search(query_text))
            logger.debug(f"Tokenized query: {query_tokenized}")

            # 2. 使用BM25模型进行查询
            # bm25s 的 get_scores 方法返回所有文档的分数
            # bm25s 的 get_batch_results (或类似名称) 可能更适合获取top-N
            # 我们需要查阅 bm25s 的API来获取top-N的文档索引和分数
            # 假设它有一个类似 get_top_n 的方法，或者我们需要自己处理 get_scores 的结果

            # 查阅 bm25s 文档，它通常使用 `bm25_model.get_scores(query_tokenized)` 得到所有分数
            # 然后我们需要自己排序并取top N
            # 或者，bm25s.BM25 可能有更直接的方法，例如 `search` 或 `query`
            # 经过快速查阅，bm25s 似乎没有直接的 top_n 方法，但其设计是为了快速计算所有分数。
            # `bm25_model.get_scores(query_tokenized)` 返回一个numpy数组，包含每个文档的分数。

            all_scores = self._bm25_model.get_scores(query_tokenized)
            
            # 获取分数最高的n_results个文档的索引
            # 注意：如果实际文档数少于n_results，则取实际数量
            actual_n_results = min(n_results, len(self._doc_ids))
            
            # 使用numpy的argsort来获取排序后的索引，然后取最后N个（因为argsort默认升序）
            # 或者取负数再取前N个
            top_n_indices = np.argsort(all_scores)[-actual_n_results:][::-1] # 降序取前N

            retrieved_docs = []
            for index in top_n_indices:
                doc_id = self._doc_ids[index]
                score = float(all_scores[index]) # 转换为Python float
                # 我们只返回ID和分数，文本内容由上层逻辑获取
                retrieved_docs.append({
                    "id": doc_id,
                    "score": score
                })
            
            logger.info(f"Retrieved {len(retrieved_docs)} documents using BM25.")
            return retrieved_docs

        except Exception as e:
            logger.error(f"Error during BM25 retrieval: {e}")
            return []

if __name__ == '__main__':
    logger.info("--- FileBM25Retriever Test ---")
    
    # 确保您的BM25索引文件已通过Dagster流水线创建
    # (例如 /home/zhz/dagster_home/bm25_index/rag_bm25_index.pkl)
    try:
        retriever = FileBM25Retriever()
        
        # 测试查询 (与ChromaDBRetriever使用相同的查询，以便后续比较和融合)
        test_query = "人工智能的应用有哪些？" 
        
        retrieved_results = retriever.retrieve(test_query, n_results=3)
        
        if retrieved_results:
            print(f"\n--- BM25 Retrieved Results for query: '{test_query}' ---")
            for i, doc in enumerate(retrieved_results):
                print(f"Result {i+1}:")
                print(f"  ID: {doc['id']}")
                print(f"  Score: {doc['score']:.4f}")
                # 我们这里不获取文本，由rag_service负责
                print("-" * 20)
        else:
            print(f"\nNo results retrieved with BM25 for query: '{test_query}'")

    except Exception as e:
        print(f"An error occurred during the BM25 test: {e}")--- END OF FILE file_bm25_retriever.py ---

--- START OF FILE fusion.py ---# zhz_agent/fusion.py
import hashlib
import jieba
import torch
import asyncio
from transformers import AutoTokenizer, AutoModelForSequenceClassification
from typing import List, Dict, Any, Optional
import logging 
import os

# 从项目内部导入pydantic_models
from zhz_agent.pydantic_models import RetrievedDocument

class FusionEngine:
    _current_script_path = os.path.abspath(__file__)
    _script_directory = os.path.dirname(_current_script_path)

    # 确保您的模型路径指向正确的位置，如果不在 local_models/bge-reranker-base
    LOCAL_RERANKER_MODEL_PATH = os.getenv(
        "RERANKER_MODEL_PATH", 
        "/home/zhz/models/bge-reranker-base" # <--- 直接指定新的、统一管理后的模型路径
    )

    def __init__(self, logger: Optional[logging.Logger] = None):
        if logger:
            self.logger = logger
        else:
            self.logger = logging.getLogger("FusionEngineLogger")
            if not self.logger.hasHandlers():
                self.logger.setLevel(logging.INFO)
                self.logger.info("FusionEngine initialized with its own basic logger (no handlers configured by default).")
            else:
                self.logger.info("FusionEngine initialized, re-using existing logger configuration for FusionEngineLogger.")

        self.reranker_tokenizer: Optional[AutoTokenizer] = None
        self.reranker_model: Optional[AutoModelForSequenceClassification] = None
        self.reranker_device = "cuda" if torch.cuda.is_available() else "cpu"
        self._load_reranker_model()

    def _load_reranker_model(self):
        self.logger.info(f"FusionEngine: Loading reranker model from: {self.LOCAL_RERANKER_MODEL_PATH} to {self.reranker_device}...")
        
        if not os.path.isdir(self.LOCAL_RERANKER_MODEL_PATH): # 检查是否是目录
            _error_msg_model_path = f"Error: Reranker model local path does not exist or is not a directory: {self.LOCAL_RERANKER_MODEL_PATH}."
            self.logger.error(_error_msg_model_path)
            # 在实际应用中，这里可能应该抛出异常，或者让服务无法启动
            # 为了测试，我们先允许模型为空，后续调用会检查
            self.reranker_model = None
            self.reranker_tokenizer = None
            return # 提前返回

        try:
            self.reranker_tokenizer = AutoTokenizer.from_pretrained(self.LOCAL_RERANKER_MODEL_PATH)
            self.reranker_model = AutoModelForSequenceClassification.from_pretrained(self.LOCAL_RERANKER_MODEL_PATH)
            self.reranker_model.to(self.reranker_device)

            if self.reranker_device == 'cuda' and hasattr(self.reranker_model, 'half'): # 检查是否有half方法
                self.reranker_model.half()
                self.logger.info("FusionEngine: Reranker model loaded to GPU and using FP16.")
            else:
                self.logger.info(f"FusionEngine: Reranker model loaded to {self.reranker_device}.")
            
            self.reranker_model.eval()
            self.logger.info("FusionEngine: Reranker model loading successful!")
        except Exception as e:
            self.logger.error(f"Error: Reranker model loading failed: {e}", exc_info=True)
            self.reranker_tokenizer = None
            self.reranker_model = None

    def _rerank_documents_sync(self, query: str, documents: List[RetrievedDocument]) -> List[RetrievedDocument]:
        if not self.reranker_model or not self.reranker_tokenizer:
            self.logger.warning("FusionEngine: Reranker model not loaded. Cannot perform fine-grained reranking. Returning documents as is (or after basic sort if any).")
            # 可以选择返回原始顺序，或者按原始分数（如果可比）排序
            return sorted(documents, key=lambda doc: doc.score if doc.score is not None else -float('inf'), reverse=True)


        if not documents:
            self.logger.info("FusionEngine: No documents to rerank.")
            return []

        # 确保文档内容是字符串
        pairs = []
        valid_documents_for_reranking = []
        for doc in documents:
            if isinstance(doc.content, str):
                pairs.append([query, doc.content])
                valid_documents_for_reranking.append(doc)
            else:
                self.logger.warning(f"FusionEngine: Document with non-string content skipped for reranking. ID: {doc.metadata.get('chunk_id', 'N/A')}, Type: {type(doc.content)}")
        
        if not pairs:
            self.logger.info("FusionEngine: No valid document pairs for reranking after content check.")
            return []

        self.logger.info(f"FusionEngine: Reranking {len(valid_documents_for_reranking)} documents with CrossEncoder...")
        
        try:
            with torch.no_grad():
                inputs = self.reranker_tokenizer(
                    pairs, 
                    padding=True, 
                    truncation=True, 
                    return_tensors='pt', 
                    max_length=512 # BGE Reranker通常是512
                ).to(self.reranker_device)
                
                logits = self.reranker_model(**inputs).logits
                # CrossEncoder通常直接输出一个分数，而不是需要sigmoid/softmax
                # BGE-Reranker输出的是logit，可以直接用作分数，或者通过sigmoid转为概率（但不必要）
                scores = logits.view(-1).float().cpu().numpy()

            for i, doc in enumerate(valid_documents_for_reranking):
                doc.score = float(scores[i]) # 更新文档的score为reranker的打分

            reranked_docs = sorted(valid_documents_for_reranking, key=lambda doc: doc.score, reverse=True)
            self.logger.info(f"FusionEngine: Reranking complete. {len(reranked_docs)} documents sorted.")
            return reranked_docs
        except Exception as e_rerank_detail:
            self.logger.error(f"FusionEngine: Detailed error during reranking with CrossEncoder: {e_rerank_detail}", exc_info=True)
            # 如果重排序失败，返回按原始分数（如果可比）排序的文档，或者简单返回valid_documents_for_reranking
            return sorted(valid_documents_for_reranking, key=lambda d: d.score if d.score is not None else -float('inf'), reverse=True)


    def _tokenize_text(self, text: str) -> set[str]:
        if not isinstance(text, str): # 添加类型检查
            self.logger.warning(f"FusionEngine: _tokenize_text received non-string input: {type(text)}. Returning empty set.")
            return set()
        return set(jieba.cut(text))

    def _calculate_jaccard_similarity(self, query_tokens: set[str], doc_tokens: set[str]) -> float:
        if not query_tokens or not doc_tokens: # 处理空集合的情况
            return 0.0
        intersection = len(query_tokens.intersection(doc_tokens))
        union = len(query_tokens.union(doc_tokens))
        return intersection / union if union > 0 else 0.0
        
    async def fuse_results(
        self,
        all_raw_retrievals: List[RetrievedDocument],
        user_query: str,
        top_n_final: int = 3
    ) -> List[RetrievedDocument]:

        self.logger.info(f"FusionEngine: Fusing {len(all_raw_retrievals)} raw retrieved documents for query: '{user_query}'. Target top_n_final: {top_n_final}")

        if not all_raw_retrievals:
            self.logger.info("FusionEngine: No documents to fuse.")
            return []

        # 1. 去重 (基于内容的哈希值)
        unique_docs_map: Dict[str, RetrievedDocument] = {}
        for doc in all_raw_retrievals:
            if not isinstance(doc.content, str) or not doc.content.strip():
                self.logger.debug(f"FusionEngine: Skipping doc with invalid content for hashing: {doc.metadata.get('chunk_id', 'N/A') if doc.metadata else 'N/A'}")
                continue
            content_hash = hashlib.md5(doc.content.encode('utf-8')).hexdigest()
            if content_hash not in unique_docs_map:
                unique_docs_map[content_hash] = doc
            else:
                # 如果内容重复，可以保留分数较高的一个（如果分数可比且来自不同召回源）
                # 这里简化处理，保留第一个遇到的，或者可以根据source_type和score进行更复杂的选择
                if doc.score is not None and unique_docs_map[content_hash].score is not None:
                    if doc.score > unique_docs_map[content_hash].score: # type: ignore
                        unique_docs_map[content_hash] = doc # 保留分数更高的
                elif doc.score is not None: # 当前文档有分数，已存的没有
                     unique_docs_map[content_hash] = doc
                self.logger.debug(f"FusionEngine: Duplicate content hash found. Doc with score {doc.score} vs existing {unique_docs_map[content_hash].score}. Content: {doc.content[:50]}...")
        
        unique_docs = list(unique_docs_map.values())
        self.logger.info(f"FusionEngine: After deduplication (content hash): {len(unique_docs)} documents.")

        if not unique_docs:
            return []

        # 2. 初步筛选 (基于长度和Jaccard相似度)
        # 定义阈值
        JACCARD_THRESHOLD = 0.05  # Jaccard相似度阈值，低于此则可能被过滤
        MIN_DOC_LENGTH_CHARS_KG = 10    # 知识图谱结果的最小字符长度
        MIN_DOC_LENGTH_CHARS_OTHER = 10 # 其他来源（向量、BM25）的最小字符长度
        # Reranker (如BGE-Reranker) 通常处理的token上限是512。
        # 一个中文字符大致对应1-3个token，英文单词大致对应1个token。
        # 为安全起见，可以设置一个字符上限，例如 1000-1500 字符，避免超长输入给Reranker。
        # 如果Reranker的tokenizer有max_length参数，它会自动截断，但预先过滤可以减少不必要的处理。
        MAX_DOC_LENGTH_CHARS = 1500 # 文档的最大字符长度，防止过长输入给reranker

        query_tokens_set = self._tokenize_text(user_query)
        screened_results: List[RetrievedDocument] = []
        
        self.logger.info(f"FusionEngine: Starting light screening for {len(unique_docs)} unique documents.")
        for doc_idx, doc in enumerate(unique_docs):
            doc_content_str = str(doc.content) # 确保是字符串
            doc_length = len(doc_content_str)
            
            # 长度筛选
            min_len_chars = MIN_DOC_LENGTH_CHARS_KG if doc.source_type == "knowledge_graph" else MIN_DOC_LENGTH_CHARS_OTHER
            if not (min_len_chars <= doc_length <= MAX_DOC_LENGTH_CHARS):
                self.logger.debug(f"  Screening: Doc {doc_idx} (ID: {doc.metadata.get('chunk_id', 'N/A') if doc.metadata else 'N/A'}) failed length check. Length: {doc_length}, Expected: [{min_len_chars}-{MAX_DOC_LENGTH_CHARS}], Type: {doc.source_type}. Content: '{doc_content_str[:50]}...'")
                continue

            # Jaccard相似度筛选 (可选，如果query_tokens_set为空则跳过)
            if query_tokens_set: # 只有当查询分词结果非空时才进行Jaccard计算
                doc_tokens_set = self._tokenize_text(doc_content_str)
                if not doc_tokens_set: # 如果文档分词结果为空，Jaccard为0
                    jaccard_sim = 0.0
                else:
                    jaccard_sim = self._calculate_jaccard_similarity(query_tokens_set, doc_tokens_set)
                
                if jaccard_sim < JACCARD_THRESHOLD:
                    self.logger.debug(f"  Screening: Doc {doc_idx} (ID: {doc.metadata.get('chunk_id', 'N/A') if doc.metadata else 'N/A'}) failed Jaccard check. Similarity: {jaccard_sim:.4f} < {JACCARD_THRESHOLD}. Content: '{doc_content_str[:50]}...'")
                    continue
            else:
                self.logger.debug(f"  Screening: Doc {doc_idx} (ID: {doc.metadata.get('chunk_id', 'N/A') if doc.metadata else 'N/A'}) - Query tokens empty, skipping Jaccard check.")


            screened_results.append(doc)
            self.logger.debug(f"  Screening: Doc {doc_idx} (ID: {doc.metadata.get('chunk_id', 'N/A') if doc.metadata else 'N/A'}) passed light screening.")
        
        self.logger.info(f"FusionEngine: After light screening: {len(screened_results)} documents remain.")
        
        if not screened_results:
            self.logger.info("FusionEngine: No documents remain after light screening. Returning empty list.")
            return []
        
        # 如果初筛后文档数量仍然很多，可以考虑再根据原始分数进行一次粗排和截断
        # 例如，如果 screened_results 数量 > top_n_final * 10，则取分数最高的前 top_n_final * 10 个
        # 这需要确保原始分数具有一定的可比性，或者对不同来源的分数进行大致的归一化
        # 当前我们先不加这一步，假设上游召回和初步筛选已将数量控制在合理范围
        docs_for_reranking = screened_results

        # 3. 使用Cross-Encoder进行精细重排序
        # _rerank_documents_sync 是同步函数，在异步函数中调用需要用 asyncio.to_thread
        final_fused_and_reranked_results = await asyncio.to_thread(
            self._rerank_documents_sync,
            query=user_query,
            documents=docs_for_reranking # 使用筛选后的文档
        )

        self.logger.info(f"FusionEngine: After reranking: {len(final_fused_and_reranked_results)} documents.")
        for i_doc, doc_reranked in enumerate(final_fused_and_reranked_results[:top_n_final+5]): # 日志多打几条看看分数
            self.logger.debug(f"  Reranked Doc {i_doc}: type={doc_reranked.source_type}, new_score={doc_reranked.score:.4f}, content='{str(doc_reranked.content)[:100]}...'")

        # 4. 根据 top_n_final 截取最终结果
        final_output_documents = final_fused_and_reranked_results[:top_n_final]

        self.logger.info(f"FusionEngine: Returning final top {len(final_output_documents)} documents.")
        return final_output_documents--- END OF FILE fusion.py ---

--- START OF FILE kg.py ---# zhz_agent/kg.py
import json
import os
from typing import List, Dict, Any, Optional
from neo4j import GraphDatabase, basic_auth, Result, Record 
from neo4j.graph import Node, Relationship, Path
import asyncio
import logging

# --- 日志配置 (保持您之前的优秀配置) ---
_kg_py_dir = os.path.dirname(os.path.abspath(__file__))
log_file_path = os.path.join(_kg_py_dir, 'kg_retriever.log')
kg_logger = logging.getLogger(__name__) 
kg_logger.setLevel(logging.DEBUG) 
kg_logger.propagate = False
if kg_logger.hasHandlers():
    kg_logger.handlers.clear()
try:
    file_handler = logging.FileHandler(log_file_path, mode='w') 
    file_handler.setLevel(logging.DEBUG) 
    formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(filename)s:%(lineno)d - %(name)s - %(message)s')
    file_handler.setFormatter(formatter)
    kg_logger.addHandler(file_handler)
    kg_logger.info("--- KG logging reconfigured to write to kg_retriever.log (dedicated handler) ---")
except Exception as e:
    print(f"CRITICAL: Failed to configure file handler for kg_logger: {e}")

from zhz_agent.llm import generate_cypher_query
from zhz_agent.pydantic_models import RetrievedDocument
from dotenv import load_dotenv
load_dotenv()

NEW_KG_SCHEMA_DESCRIPTION = """
你的任务是根据用户的问题，严格利用以下【知识图谱Schema信息】生成一个或多个Cypher查询。

**【知识图谱Schema信息】**

1.  **节点 (Nodes):**
    *   **绝对核心规则：在生成的Cypher查询中，所有节点匹配时必须且只能使用 `:ExtractedEntity` 这个统一标签。严禁在MATCH模式中使用例如 :Person, :Organization, :Task 等更具体的标签名。节点的具体类型通过其 `label` 属性进行区分和筛选（例如，`(n:ExtractedEntity {label: 'PERSON'})`）。**
    *   每个 `:ExtractedEntity` 节点有且仅有以下两个核心属性:
        *   `text`: 字符串 (String)，表示实体的原始文本内容。
        *   `label`: 字符串 (String)，表示实体的类型。目前已知的实体类型包括: "PERSON", "ORGANIZATION", "TASK"。 (注意：虽然理论上可以有 "LOCATION" 等其他类型，但当前已定义的关系主要涉及这三者。)

2.  **关系 (Relationships):**
    *   目前仅支持以下两种关系类型，它们严格连接特定标签的 `:ExtractedEntity` 节点：
        *   关系名称: `:WORKS_AT`
            *   方向和类型: `(:ExtractedEntity {label:"PERSON"}) -[:WORKS_AT]-> (:ExtractedEntity {label:"ORGANIZATION"})`
            *   描述: 表示一个 PERSON 在一个 ORGANIZATION 工作。**此关系严格用于表示工作单位，目标节点必须是 `label:"ORGANIZATION"` 的 `:ExtractedEntity`。如果问题中提及“地点”但明显指代公司或机构，请查询 `ORGANIZATION` 类型的实体。**
            *   示例: `(person:ExtractedEntity {label:"PERSON", text:"张三"})-[:WORKS_AT]->(org:ExtractedEntity {label:"ORGANIZATION", text:"谷歌"})`
        *   关系名称: `:ASSIGNED_TO`
            *   方向和类型: `(:ExtractedEntity {label:"TASK"}) -[:ASSIGNED_TO]-> (:ExtractedEntity {label:"PERSON"})`
            *   描述: 表示一个任务分配给了一个人。
            *   示例: `(task:ExtractedEntity {label:"TASK", text:"项目Alpha的文档编写任务"})-[:ASSIGNED_TO]->(person:ExtractedEntity {label:"PERSON", text:"张三"})`
    *   **重要约束**：生成Cypher查询时，**必须且只能**使用上述明确定义的关系类型 (`:WORKS_AT`, `:ASSIGNED_TO`) 和节点属性 (`text`, `label`)。严禁使用任何未在此处定义的其他关系类型或节点属性。

**【Cypher查询生成规则】**

1.  **严格遵循Schema**:
    *   你的查询**必须完全基于**上面提供的【知识图谱Schema信息】。
    *   **节点标签必须固定为 `:ExtractedEntity`。例如，匹配一个“张三”这个人时，应写为 `(p:ExtractedEntity {label: 'PERSON', text: '张三'})`，绝对不能写成 `(p:Person {text: '张三'})`。**
    *   节点属性只能使用 `text` 和 `label`。
    *   关系类型只能使用 `:WORKS_AT` 和 `:ASSIGNED_TO`，并严格遵守其定义的方向和连接的实体类型。

2.  **匹配逻辑**:
    *   当用户问题中提及具体实体名称时，优先使用该实体的 `text` 属性进行精确匹配。
    *   同时，根据问题上下文或实体类型提示，使用 `label` 属性进行辅助筛选。

3.  **输出格式**:
    *   如果能生成有效查询，你的回答**必须只包含纯粹的Cypher查询语句本身**。
    *   如果根据问题和Schema无法生成有效的Cypher查询（例如，问题超出了Schema的表达能力，问题本身逻辑不通，或涉及未定义的关系/属性），**或者问题的核心查询意图（例如询问某个实体的一个特定但Schema中未定义的属性，或寻找一个Schema中未定义的关系类型来连接实体）无法通过已定义的节点属性或关系类型来精确满足，则必须只输出固定的短语：“无法生成Cypher查询。”不要试图通过返回实体本身的其他已知属性或已知的相关实体来“部分回答”该核心意图。如果一个问题询问某个任务的“具体内容”或“要求”，而Schema中没有为TASK实体定义这些属性或相关关系，那么就应该返回“无法生成Cypher查询。”**
    *   **绝对禁止**在有效的Cypher语句前后添加任何前缀、后缀、解释、注释或markdown标记。

**【查询示例 - 严格基于上述Schema和规则】**:

*   用户问题: "张三在哪里工作？"
    Cypher查询: MATCH (p:ExtractedEntity {text: '张三', label: 'PERSON'})-[:WORKS_AT]->(org:ExtractedEntity {label: 'ORGANIZATION'}) RETURN org.text AS organizationName

*   用户问题: "项目Alpha的文档编写任务分配给了谁？"
    Cypher查询: MATCH (task:ExtractedEntity {text: '项目Alpha的文档编写任务', label: 'TASK'})-[:ASSIGNED_TO]->(person:ExtractedEntity {label: 'PERSON'}) RETURN person.text AS personName

*   用户问题: "列出所有在谷歌工作的人。"
    Cypher查询: MATCH (p:ExtractedEntity {label: 'PERSON'})-[:WORKS_AT]->(org:ExtractedEntity {text: '谷歌', label: 'ORGANIZATION'}) RETURN p.text AS employeeName

*   用户问题: "张三负责哪些任务？"
    Cypher查询: MATCH (task:ExtractedEntity {label: 'TASK'})-[:ASSIGNED_TO]->(p:ExtractedEntity {text: '张三', label: 'PERSON'}) RETURN task.text AS taskName

*   用户问题: "谷歌公司有哪些员工？"
    Cypher查询: MATCH (p:ExtractedEntity {label: 'PERSON'})-[:WORKS_AT]->(org:ExtractedEntity {text: '谷歌', label: 'ORGANIZATION'}) RETURN p.text AS employeeName

*   用户问题: "查询所有任务及其负责人。"
    Cypher查询: MATCH (task:ExtractedEntity {label: 'TASK'})-[:ASSIGNED_TO]->(person:ExtractedEntity {label: 'PERSON'}) RETURN task.text AS taskName, person.text AS assignedPerson

*   用户问题: "百度的CEO是谁？" (此问题超出现有Schema表达能力)
    Cypher查询: 无法生成Cypher查询。

*   用户问题: "项目Alpha文档编写任务的具体内容是什么？" (核心意图是查询“具体内容”，但Schema中没有为TASK实体定义这些属性或相关关系，所以无法生成查询)
    Cypher查询: 无法生成Cypher查询。

*   用户问题: "张三目前的工作地点是哪个城市？" (Schema中 :WORKS_AT 指向 ORGANIZATION，没有直接的城市地点关系，且ORGANIZATION节点也没有城市属性)
    Cypher查询: 无法生成Cypher查询。

*   用户问题: "张三最近一次的工作变动是什么时候？" (此问题涉及Schema未定义的属性如日期)
    Cypher查询: 无法生成Cypher查询。

现在，请根据以下用户问题和上述Schema及规则生成Cypher查询。
"""

class KGRetriever:
    NEO4J_URI = os.getenv("NEO4J_URI", "bolt://localhost:7687")
    NEO4J_USER = os.getenv("NEO4J_USER", "neo4j")
    NEO4J_PASSWORD = os.getenv("NEO4J_PASSWORD", "zhz199276")
    NEO4J_DATABASE = os.getenv("NEO4J_DATABASE", "neo4j")

    def __init__(self, llm_cypher_generator_func: callable = generate_cypher_query):
        self.llm_cypher_generator_func = llm_cypher_generator_func
        self._driver: Optional[GraphDatabase.driver] = None
                # --- 在连接前打印出将要使用的连接参数 ---
        kg_logger.info(f"KGRetriever attempting to connect with URI: {self.NEO4J_URI}, User: {self.NEO4J_USER}, DB: {self.NEO4J_DATABASE}")
        
        self._connect_to_neo4j()
        kg_logger.info(f"KGRetriever initialized. Connected to Neo4j: {self._driver is not None}")

    def _connect_to_neo4j(self):
        if self._driver is not None:
            try:
                self._driver.verify_connectivity()
                kg_logger.info("Neo4j connection already active and verified.")
                return
            except Exception:
                kg_logger.warning("Existing Neo4j driver failed connectivity test, attempting to reconnect.")
                try:
                    self._driver.close()
                except: pass
                self._driver = None
        try:
            self._driver = GraphDatabase.driver(self.NEO4J_URI, auth=basic_auth(self.NEO4J_USER, self.NEO4J_PASSWORD))
            with self._driver.session(database=self.NEO4J_DATABASE) as session:
                session.run("RETURN 1").consume()
            kg_logger.info(f"Successfully connected to Neo4j at {self.NEO4J_URI} on database '{self.NEO4J_DATABASE}'.")
        except Exception as e:
            kg_logger.error(f"Failed to connect to Neo4j: {e}", exc_info=True)
            self._driver = None
    
    def close(self):
        if self._driver:
            self._driver.close()
            kg_logger.info("Closed Neo4j connection.")
            self._driver = None

    def _convert_neo4j_value_to_json_serializable(self, value: Any) -> Any: # <--- 覆盖整个方法
        """
        递归地将Neo4j返回的各种值转换为JSON可序列化的Python原生类型。
        """
        if isinstance(value, Node):
            # Node对象可以直接通过dict(node)获取其所有属性
            # element_id 是推荐的唯一标识符
            return {"_element_id": value.element_id, "_labels": list(value.labels), "properties": dict(value)}
        
        elif isinstance(value, Relationship):
            # Relationship对象也可以通过dict(relationship)获取其属性
            return {
                "_element_id": value.element_id,
                "_type": value.type,
                "_start_node_element_id": value.start_node.element_id,
                "_end_node_element_id": value.end_node.element_id,
                "properties": dict(value) # 关系的属性
            }
            
        elif isinstance(value, Path):
            # Path对象包含一系列交替的节点和关系
            return {
                "nodes": [self._convert_neo4j_value_to_json_serializable(n) for n in value.nodes],
                "relationships": [self._convert_neo4j_value_to_json_serializable(r) for r in value.relationships]
            }
            
        elif isinstance(value, list) or isinstance(value, tuple) or isinstance(value, set):
            return [self._convert_neo4j_value_to_json_serializable(item) for item in value]
            
        elif isinstance(value, dict):
            # 检查是否已经是我们转换后的格式，避免重复处理和无限递归
            if "_element_id" in value and ("_labels" in value or "_type" in value): 
                return value
            return {k: self._convert_neo4j_value_to_json_serializable(v) for k, v in value.items()}
            
        elif isinstance(value, (str, int, float, bool)) or value is None:
            return value
            
        else:
            kg_logger.warning(f"KGRetriever: Encountered an unhandled Neo4j type ({type(value)}), attempting to convert to string: {str(value)}")
            try:
                return str(value)
            except Exception as e_str:
                kg_logger.error(f"KGRetriever: Failed to convert unhandled type {type(value)} to string: {e_str}")
                return f"[Unserializable object: {type(value)}]"

    def execute_cypher_query_sync(self, query: str, parameters: Optional[Dict[str, Any]] = None) -> List[Dict[str, Any]]:
        if not self._driver:
            kg_logger.error("Neo4j driver not initialized in execute_cypher_query_sync.")
            return []        
        kg_logger.info(f"--- Executing SYNC Cypher ---") # <--- 修改日志级别为INFO，确保能看到
        kg_logger.info(f"Query: {query}")
        kg_logger.info(f"Params: {parameters}")
        results_list: List[Dict[str, Any]] = []
        try:
            with self._driver.session(database=self.NEO4J_DATABASE) as session:
                result_obj: Result = session.run(query, parameters)
                raw_records = list(result_obj) # 将迭代器具体化为列表
                kg_logger.info(f"Neo4j raw_records count: {len(raw_records)}")

                for record_instance in raw_records: 
                    record_as_dict = {}
                    for key, value in record_instance.items():
                        record_as_dict[key] = self._convert_neo4j_value_to_json_serializable(value)
                    results_list.append(record_as_dict)

            # --- 打印转换后的结果数量和前几条 ---
            kg_logger.info(f"SYNC Cypher executed. Converted records count: {len(results_list)}")
            if results_list:
                kg_logger.debug(f"First converted record (sample): {json.dumps(results_list[0], ensure_ascii=False, indent=2)}")
            else:
                kg_logger.debug("No records converted.")
                
        except Exception as e:
            kg_logger.error(f"Failed to execute SYNC Cypher query: '{query}' with params: {parameters}. Error: {e}", exc_info=True)
        return results_list

    def _format_neo4j_record_for_retrieval(self, record_data: Dict[str, Any]) -> str:
        """
        将单条已转换为纯Python字典的Neo4j记录格式化为一段描述性文本。
        """
        parts = []
        for key, value in record_data.items():
            if isinstance(value, dict):
                # 尝试提取节点的text和label属性，或关系的type
                if '_labels' in value and 'text' in value: # 假设是转换后的Node
                    parts.append(f"{key}({value['text']}:{'/'.join(value['_labels'])})")
                elif '_type' in value: # 假设是转换后的Relationship
                    rel_props_str = ", ".join([f"{k_prop}: {v_prop}" for k_prop, v_prop in value.items() if not k_prop.startswith('_')])
                    parts.append(f"{key}(TYPE={value['_type']}{', PROPS=[' + rel_props_str + ']' if rel_props_str else ''})")
                else: # 其他字典
                    # 为了避免过长的输出，可以限制value的打印长度或深度
                    value_str = json.dumps(value, ensure_ascii=False, indent=None)
                    if len(value_str) > 100: value_str = value_str[:100] + "..."
                    parts.append(f"{key}: {value_str}")
            elif value is not None:
                parts.append(f"{key}: {str(value)}")
        
        return " | ".join(parts) if parts else "No specific details found in this record."

    async def retrieve_with_llm_cypher(self, query: str, top_k: int = 3) -> List[RetrievedDocument]:
        kg_logger.info(f"Starting KG retrieval with LLM-generated Cypher for query: '{query}', top_k: {top_k}")
        if not self._driver:
            kg_logger.warning("Neo4j driver not initialized. Cannot perform KG query.")
            return []

        kg_logger.info(f"Calling LLM to generate Cypher query using new schema...")
        cypher_query = await self.llm_cypher_generator_func(
            user_question=query,
            kg_schema_description=NEW_KG_SCHEMA_DESCRIPTION 
        )
        kg_logger.info(f"LLM generated Cypher query:\n---\n{cypher_query}\n---")

        if not cypher_query or cypher_query == "无法生成Cypher查询。":
            kg_logger.warning("LLM could not generate a valid Cypher query.")
            return []

        results = self.execute_cypher_query_sync(cypher_query)
        
        retrieved_docs = []
        for record_dict in results[:top_k]:
            content = self._format_neo4j_record_for_retrieval(record_dict)
            retrieved_docs.append(
                RetrievedDocument(
                    source_type="knowledge_graph",
                    content=content,
                    score=1.0, 
                    metadata={"cypher_query": cypher_query, "original_query": query}
                )
            )
        kg_logger.info(f"Retrieved {len(retrieved_docs)} documents from KG using LLM-generated Cypher.")
        return retrieved_docs

    def get_entity_details_manual(self, entity_text: str, entity_type_attr: Optional[str] = None) -> List[Dict[str, Any]]:
        if entity_type_attr:
            cypher = "MATCH (e:ExtractedEntity {text: $text, label: $label_attr}) RETURN e"
            params = {"text": entity_text, "label_attr": entity_type_attr.upper()}
        else:
            cypher = "MATCH (e:ExtractedEntity {text: $text}) RETURN e"
            params = {"text": entity_text}
        return self.execute_cypher_query_sync(cypher, params)

    def get_relations_manual(self, entity_text: str, entity_type_attr: str, relation_type: Optional[str] = None, direction: str = "BOTH") -> List[Dict[str, Any]]:
        rel_type_cypher = f":{relation_type.upper()}" if relation_type else "r" # 关系类型转大写
        
        if direction.upper() == "OUT":
            rel_clause = f"-[{rel_type_cypher}]->"
        elif direction.upper() == "IN":
            rel_clause = f"<-[{rel_type_cypher}]-"
        else: # BOTH
            rel_clause = f"-[{rel_type_cypher}]-"
            
        cypher = (
            f"MATCH (e:ExtractedEntity {{text: $text, label: $label_attr}}){rel_clause}(neighbor:ExtractedEntity) "
            f"RETURN e as entity, {rel_type_cypher if not relation_type else 'r'} as relationship, neighbor as related_entity"
        )
        # 如果 relation_type 为 None, Cypher 中 r 会匹配任何关系类型，但我们返回时仍用 'r' 作为键
        # 如果 relation_type 指定了，Cypher 中会用具体的类型，返回时也用 'r' 作为键
        # 为了统一，我们让 RETURN 语句中的关系变量总是 'r'
        if relation_type: # 如果指定了关系类型，确保返回的变量是 r
             cypher = (
                f"MATCH (e:ExtractedEntity {{text: $text, label: $label_attr}})-[r:{relation_type.upper()}]"
                f"{'->' if direction.upper() == 'OUT' else '<-' if direction.upper() == 'IN' else '-'}"
                f"(neighbor:ExtractedEntity) "
                f"RETURN e as entity, r as relationship, neighbor as related_entity"
            )
        else: # 如果没有指定关系类型，匹配任何关系
            cypher = (
                f"MATCH (e:ExtractedEntity {{text: $text, label: $label_attr}})-[r]-"
                f"{'(neighbor:ExtractedEntity)' if direction.upper() != 'BOTH' else '(neighbor:ExtractedEntity)'} " # 确保neighbor被定义
                f"RETURN e as entity, r as relationship, neighbor as related_entity"
            )


        params = {"text": entity_text, "label_attr": entity_type_attr.upper()}
        return self.execute_cypher_query_sync(cypher, params)

async def main_test():
    logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(name)s - %(message)s')
    kg_logger.info("--- KGRetriever New Test ---")
    try:
        kg_retriever = KGRetriever() 
        if kg_retriever._driver is None:
            print("Failed to connect to Neo4j. Exiting test.")
            return 

        print("\n--- Test 1: Get entity details for '张三' (PERSON) ---")
        zhang_san_details = kg_retriever.get_entity_details_manual("张三", "PERSON")
        if zhang_san_details:
            print(json.dumps(zhang_san_details, indent=2, ensure_ascii=False))
        else:
            print("'张三' (PERSON) not found.")

        print("\n--- Test 2: Get 'WORKS_AT' relations for '张三' (PERSON) ---")
        zhang_san_relations = kg_retriever.get_relations_manual("张三", "PERSON", relation_type="WORKS_AT", direction="OUT")
        if zhang_san_relations:
            print(f"Found {len(zhang_san_relations)} WORKS_AT relations for '张三':")
            for item in zhang_san_relations:
                print(json.dumps(item, indent=2, ensure_ascii=False))
        else:
            print("No WORKS_AT relations found for '张三'.")

        print("\n--- Test 3: Retrieve with LLM-generated Cypher for '张三在哪里工作？' ---")
        llm_results_zs = await kg_retriever.retrieve_with_llm_cypher("张三在哪里工作？")
        if llm_results_zs:
            print(f"LLM query results for '张三在哪里工作？':")
            for doc in llm_results_zs:
                print(f"  Content: {doc.content}")
                print(f"  Metadata: {doc.metadata}")
        else:
            print("LLM query for '张三在哪里工作？' returned no results or failed.")

        print("\n--- Test 4: Retrieve with LLM-generated Cypher for '项目Alpha的文档编写任务分配给了谁？' ---")
        llm_results_task = await kg_retriever.retrieve_with_llm_cypher("项目Alpha的文档编写任务分配给了谁？")
        if llm_results_task:
            print(f"LLM query results for '项目Alpha的文档编写任务分配给了谁？':")
            for doc in llm_results_task:
                print(f"  Content: {doc.content}")
                print(f"  Metadata: {doc.metadata}")
        else:
            print("LLM query for '项目Alpha的文档编写任务分配给了谁？' returned no results or failed.")

        kg_retriever.close()
    except Exception as e:
        print(f"An error occurred during the KGRetriever test: {e}")

if __name__ == '__main__':
    asyncio.run(main_test())--- END OF FILE kg.py ---

--- START OF FILE llm.py ---# zhz_agent_project/llm.py
import os
import httpx # 用于异步HTTP请求
import json # 用于处理JSON数据
import asyncio # 用于 asyncio.to_thread
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv
import traceback # Ensure traceback is imported
import logging
load_dotenv() # 确保加载.env文件

# --- 为 llm.py 配置一个logger ---
llm_py_logger = logging.getLogger("LLMUtilsLogger") # 给一个独特的名字
llm_py_logger.setLevel(logging.INFO) # 可以设置为 INFO 或 DEBUG

# 防止重复添加handler，如果这个模块被多次导入或初始化
if not llm_py_logger.hasHandlers():
    _llm_console_handler = logging.StreamHandler() # 输出到控制台
    _llm_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
    _llm_console_handler.setFormatter(_llm_formatter)
    llm_py_logger.addHandler(_llm_console_handler)
    llm_py_logger.propagate = False # 通常不希望 utils 模块的日志传播到根

llm_py_logger.info("--- LLMUtilsLogger configured ---")
# --- 结束logger配置 ---


# --- 定义LLM在无法回答时应该输出的精确短语 (不含停止标记) ---
NO_ANSWER_PHRASE_ANSWER_CLEAN = "根据提供的信息，无法回答该问题。"
NO_ANSWER_PHRASE_KG_CLEAN = "从知识图谱中未找到直接相关信息。"

# --- 定义通用的唯一停止标记 ---
UNIQUE_STOP_TOKEN = "<|im_endofunable|>" # 报告建议的独特停止标记

# --- 定义LLM在Prompt中被要求输出的，包含特殊停止标记的短语 ---
NO_ANSWER_PHRASE_ANSWER_WITH_STOP_TOKEN = f"{NO_ANSWER_PHRASE_ANSWER_CLEAN}{UNIQUE_STOP_TOKEN}"
NO_ANSWER_PHRASE_KG_WITH_STOP_TOKEN = f"{NO_ANSWER_PHRASE_KG_CLEAN}{UNIQUE_STOP_TOKEN}"

# --- 配置 SGLang LLM API ---
SGLANG_API_URL = os.getenv("SGLANG_API_URL") 
# 确保SGLang服务正在运行，并且这个URL是正确的

async def call_sglang_llm(prompt: str, 
                          temperature: float = 0.2, 
                          max_new_tokens: Optional[int] = 1024,
                          stop_sequences: Optional[List[str]] = None) -> Optional[str]:
    
    llm_py_logger.debug(f"Attempting to call SGLang LLM. Prompt (first 100 chars): {prompt[:100]}...") # 例如替换

    payload = {
        "text": prompt,
        "sampling_params": {
            "temperature": temperature,
            "max_new_tokens": max_new_tokens,
            "stop": stop_sequences if stop_sequences else [] 
        }
    }
    headers = {"Content-Type": "application/json"}

    try:
        print(f"[LLM_DEBUG] Payload constructed. SGLANG_API_URL: {SGLANG_API_URL}")
        async with httpx.AsyncClient(timeout=120.0) as client: 
            print("[LLM_DEBUG] httpx.AsyncClient created. About to POST...")
            response = await client.post(SGLANG_API_URL, json=payload, headers=headers)
            print(f"[LLM_DEBUG] POST request sent. Status code: {response.status_code}")
            
            response.raise_for_status()
            print("[LLM_DEBUG] response.raise_for_status() passed.")

            response_json = response.json() # 尝试解析JSON
            print("[LLM_DEBUG] response.json() successful.")
            
            llm_output_text = response_json.get("text", "[[TEXT_FIELD_NOT_FOUND]]").strip() # 获取text字段，如果不存在则用特殊标记
            print(f"\n<<<<<<<<<< SGLANG LLM INPUT PROMPT START (call_sglang_llm) >>>>>>>>>>\n{prompt}\n<<<<<<<<<< SGLANG LLM INPUT PROMPT END >>>>>>>>>>\n")
            print(f"\n>>>>>>>>>> SGLANG LLM RAW OUTPUT TEXT START (call_sglang_llm) >>>>>>>>>>\n{llm_output_text}\n>>>>>>>>>> SGLANG LLM RAW OUTPUT TEXT END >>>>>>>>>>\n")
            llm_py_logger.debug(f"Full SGLang raw response JSON: {response.text}") # 例如替换

            meta_info = response_json.get("meta_info", {})
            finish_reason = meta_info.get("finish_reason", {})
            print(f"[LLM_DEBUG] Meta info: {meta_info}, Finish reason: {finish_reason}")

            if finish_reason.get("type") == "stop" and finish_reason.get("matched") == UNIQUE_STOP_TOKEN:
                return llm_output_text.split(UNIQUE_STOP_TOKEN)[0].strip() if llm_output_text and llm_output_text != "[[TEXT_FIELD_NOT_FOUND]]" else NO_ANSWER_PHRASE_ANSWER_CLEAN
            
            return llm_output_text if llm_output_text != "[[TEXT_FIELD_NOT_FOUND]]" else None

    except httpx.HTTPStatusError as e: # 更具体的HTTP错误
        print(f"[LLM_DEBUG] httpx.HTTPStatusError: {e}")
        print(f"[LLM_DEBUG] Response content causing status error: {e.response.text[:500]}")
        traceback.print_exc()
        return None
    except httpx.RequestError as e:
        print(f"[LLM_DEBUG] httpx.RequestError: {e}")
        traceback.print_exc()
        return None
    except json.JSONDecodeError as e:
        response_text_for_debug = "N/A"
        if 'response' in locals() and hasattr(response, 'text'):
            response_text_for_debug = response.text[:500]
        print(f"[LLM_DEBUG] json.JSONDecodeError: {e}. Raw response text: {response_text_for_debug}")
        traceback.print_exc()
        return None
    except Exception as e:
        print(f"[LLM_DEBUG] Unknown error in call_sglang_llm: {type(e).__name__} - {e}")
        traceback.print_exc()
        return None

async def generate_answer_from_context(user_query: str, context: str) -> Optional[str]:
    
    prompt = f"""<|im_start|>system
你是一个精确且忠实的问答助手。你的任务是严格根据用户问题和下面提供的【上下文信息】，生成一个准确、简洁、直接的答案。

**【回答核心准则 - 必须严格遵守！】**

1.  **【忠实于上下文 - 最高优先级】**: 你的所有回答都**必须完全基于**提供的【上下文信息】。**绝对不允许**依赖任何外部知识、个人观点或进行任何形式的推测。如果上下文中没有明确支持的信息，就必须指出信息未知或无法回答。

2.  **【优先采纳精确信息】**: 如果上下文中包含以“【知识图谱精确信息】”标记的片段，这代表了高置信度的结构化事实。请**优先直接采纳这些信息**来回答用户问题的对应部分。忽略该片段的原始得分。

3.  **【处理组合问题/多方面问题】**:
    *   仔细分析用户问题，识别其中是否包含多个子问题或期望获取多个方面的信息（例如，通过“和”、“以及”连接，或一个问句包含多个疑问点）。
    *   对于每个识别出的子问题或信息点，请在【上下文信息】中独立查找答案。
    *   将找到的各个方面的答案清晰地组织起来。如果某个方面的信息在上下文中找不到，**必须明确指出该方面信息未知或未提供**，例如：“关于[问题的某个方面]，上下文中未提供具体信息。” **不要因为部分信息缺失而放弃回答其他能找到信息的部分。**

4.  **【处理数值与计算】**:
    *   如果用户问题需要对上下文中的数字进行计算（如求和、求差、平均等），并且上下文中包含可用于计算的明确数字（注意识别数字和单位），请执行计算并在答案中清晰展示计算结果和原始数据。
    *   例如，如果上下文说“A产品销售额50万元，B产品销售额30万元”，用户问“AB产品总销售额”，你应该回答“A产品销售额为50万元，B产品销售额为30万元，总销售额为80万元。”
    *   如果上下文中的数字不清晰、单位不一致难以换算，或者进行计算的条件不足，请列出原始数据并说明无法精确计算。

5.  **【信息不足的最终判断】**: 如果综合运用以上规则后，【上下文信息】中**完全没有**与用户问题相关的任何信息，或者完全无法回答用户问题的任何一个方面，则**必须直接且完整地**输出：“{NO_ANSWER_PHRASE_ANSWER_WITH_STOP_TOKEN}”

**【输出要求】**: 你的回答必须是纯文本，直接给出答案本身，语气应客观、专业。避免使用对话标记或不必要的解释性语句，除非是为了说明信息来源或缺失情况。
<|im_end|>
<|im_start|>user
用户问题: {user_query}

上下文信息:
{context}
<|im_end|>
<|im_start|>assistant
"""
    stop_sequences = ["<|im_end|>", UNIQUE_STOP_TOKEN] 
    
    # 保持较低的temperature以获取更确定的答案，除非特定任务需要创造性
    return await call_sglang_llm(prompt, temperature=0.05, max_new_tokens=512, stop_sequences=stop_sequences)# 降低温度，鼓励直接提取

async def generate_simulated_kg_query_response(user_query: str, kg_schema_description: str, kg_data_summary_for_prompt: str) -> Optional[str]:
    """
    模拟Text-to-Cypher和知识图谱查询。
    这里我们不实际生成Cypher，而是让LLM直接根据问题和KG描述生成“事实片段”。
    """
    prompt = f"""<|im_start|>system
你是一个知识图谱查询助手。你的任务是根据用户提出的问题、知识图谱Schema描述和图谱中的数据摘要，直接抽取出与问题最相关的1-2个事实片段作为答案。
只输出事实片段，不要解释，不要生成Cypher语句，不要包含任何额外对话或标记。
如果找不到直接相关的事实，请**直接且完整地**回答：“{NO_ANSWER_PHRASE_KG_WITH_STOP_TOKEN}”
<|im_end|>
<|im_start|>user
知识图谱Schema描述:
{kg_schema_description}

知识图谱数据摘要: 
{kg_data_summary_for_prompt}

用户问题: {user_query}
<|im_end|>
<|im_start|>assistant
"""
    stop_sequences = ["<|im_end|>", UNIQUE_STOP_TOKEN]
    
    return await call_sglang_llm(prompt, temperature=0.5, max_new_tokens=256, stop_sequences=stop_sequences)

async def generate_expanded_queries(original_query: str) -> List[str]:
    """
    利用LLM从用户原始查询生成多个语义相关但表述各异的子问题或扩展查询。
    """
    prompt = f"""<|im_start|>system
你是一个专家查询分析师。根据用户提供的查询，生成3个不同但相关的子问题，以探索原始查询的不同方面。这些子问题将用于检索更全面的信息。
你的回答必须是一个JSON数组（列表），其中每个元素是一个字符串（子问题）。
只输出JSON数组，不要包含任何其他解释、对话标记或代码块。

示例:
用户查询: "公司年度财务报告和未来一年的预算规划"
助手:
[
  "公司最近的年度财务报告总结是什么？",
  "未来一年的详细预算规划有哪些主要构成？",
  "对比往年，公司财务状况有何显著变化？"
]
<|im_end|>
<|im_start|>user
原始查询: {original_query}
<|im_end|>
<|im_start|>assistant
"""
    stop_sequences = ["<|im_end|>"]
    
    print(f"调用SGLang LLM API进行查询扩展 (Prompt长度: {len(prompt)} 字符)...")
    llm_output = await call_sglang_llm(
        prompt, 
        temperature=0.7, # 稍高温度，鼓励生成多样性
        max_new_tokens=512, # 允许生成较长的JSON列表
        stop_sequences=stop_sequences
    )
    expanded_queries = []
    if llm_output:
        try:
            json_str = llm_output.strip()
            if json_str.startswith("```json"):
                json_str = json_str[len("```json"):].strip()
                if json_str.endswith("```"):
                    json_str = json_str[:-len("```")].strip()
            
            parsed_queries = json.loads(json_str)
            if isinstance(parsed_queries, list) and all(isinstance(q, str) for q in parsed_queries):
                expanded_queries = parsed_queries
                print(f"LLM成功生成 {len(expanded_queries)} 个扩展查询。")
            else:
                print(f"LLM生成的扩展查询JSON格式不符合预期 (不是字符串列表): {llm_output[:200]}...")
        except json.JSONDecodeError as e:
            print(f"解析LLM扩展查询JSON失败: {e}. 原始输出: {llm_output[:200]}...")
        except Exception as e:
            print(f"处理LLM扩展查询时发生未知错误: {e}. 原始输出: {llm_output[:200]}...")
    else:
        print("LLM未能生成扩展查询。")

    expanded_queries.append(original_query) 
    return expanded_queries

async def generate_cypher_query(user_question: str, kg_schema_description: str) -> Optional[str]:
    """
    利用LLM将自然语言问题转换为Neo4j Cypher查询语句。
    """
    prompt = f"""<|im_start|>system
你是顶级Neo4j Cypher查询生成专家。你的任务是根据用户问题和严格提供的【知识图谱Schema】，生成一个【语法正确】、【逻辑合理】且【高效】的Cypher查询。

**【核心指令与约束 - 必须严格遵守！】**

1.  **【Schema绝对绑定 - 最高优先级】**:
    *   你生成的Cypher查询中所有用到的【节点标签】、【关系类型】、【属性名称】及其对应的【数据类型】，都**必须严格存在于**下面提供的 "知识图谱Schema描述" 中。
    *   在构建查询的每一步，都要反复与Schema核对。**严禁臆断、猜测或使用任何Schema中未明确定义的元素。**
    *   **属性名称的大小写和确切拼写必须与Schema完全一致。**
    *   **关系类型的名称和方向必须与Schema完全一致。** 例如，如果Schema定义为 `(Person)-[:WORKS_ON]->(Project)`，则查询中不能是 `(Project)-[:WORKS_ON]->(Person)`，除非Schema中也定义了反向关系。

2.  **【纯净输出格式 - 严格要求】**:
    *   如果能生成有效查询，你的回答**必须只包含纯粹的Cypher查询语句本身**。
    *   如果根据问题和Schema无法生成有效的Cypher查询（例如，问题超出了Schema表达能力，或问题本身逻辑不通），则**必须只输出固定的短语：“无法生成Cypher查询。”**
    *   **绝对禁止**在有效的Cypher语句前后添加任何前缀（如“Cypher查询: ”）、后缀、解释、注释、markdown标记（如 ```cypher ```）或任何其他多余的文本。

3.  **【属性与值的使用】**:
    *   当在`WHERE`子句中对属性进行匹配时，确保值的类型与Schema中定义的属性类型一致。例如，如果`name`是字符串，则匹配 `name: '张三'`；如果`year`是数字，则匹配 `year: 2023`。
    *   对于数值计算（如`SUM`, `AVG`），**必须**使用Schema中明确指定的数字类型属性（例如，`SalesAmount`节点的 `numeric_amount`）。

4.  **【查询构建逻辑指引】**:
    *   **实体识别**: 准确识别用户问题中的核心实体及其在Schema中对应的节点标签和属性。
    *   **关系路径**: 基于问题和Schema构建清晰的`MATCH`路径。
    *   **条件过滤**: 使用`WHERE`子句添加必要的过滤条件。
    *   **结果返回**: 使用`RETURN`子句指定需要返回的信息，并用`AS`为返回的列指定清晰、合法的别名（字母或下划线开头）。
    *   **多步查询**: 对于需要关联多个信息点的问题，合理使用`WITH`传递中间结果。
    *   **聚合**: 如需统计或汇总，正确使用`COUNT()`, `SUM()`, `COLLECT()`等聚合函数。

**【知识图谱Schema描述】**:
{kg_schema_description}

**【查询示例 - 严格基于上述Schema】**:

*   用户问题: "张三参与了哪个项目？"
    Cypher查询: MATCH (p:Person {{name: '张三'}})-[:WORKS_ON]->(proj:Project) RETURN proj.name AS projectName

*   用户问题: "华东区域2024年第一季度的销售额是多少？"
    Cypher查询: MATCH (r:Region {{name: '华东'}})-[:HAS_SALES_AMOUNT]->(sa:SalesAmount {{period: '2024年第一季度'}}) RETURN sa.numeric_amount AS salesAmount, sa.unit AS salesUnit

*   用户问题: "查询所有产品的名称。"
    Cypher查询: MATCH (prod:Product) RETURN prod.name AS productName

*   用户问题: "项目X有哪些人参与？"
    Cypher查询: MATCH (p:Person)-[:WORKS_ON]->(proj:Project {{name: '项目X'}}) RETURN p.name AS participantName

*   用户问题: "2024年第一季度所有区域的总销售额是多少？"
    Cypher查询: MATCH (r:Region)-[:HAS_SALES_AMOUNT]->(sa:SalesAmount {{period: '2024年第一季度'}}) RETURN sum(sa.numeric_amount) AS totalSales, sa.unit AS commonUnit LIMIT 1 
    (此查询假设所有相关销售额的单位是相同的，并取第一个出现的单位作为代表)

*   用户问题: "与新产品A相关的文档ID是什么？"
    Cypher查询: MATCH (p:Product {{name: '新产品A'}})-[:RELATED_TO]->(d:Document) RETURN d.id AS documentId

*   用户问题: "公司CEO是谁？" (假设Schema中没有CEO信息)
    Cypher查询: 无法生成Cypher查询。

现在，请根据以下用户问题和上述Schema及规则生成Cypher查询。
<|im_end|>
<|im_start|>user
用户问题: {user_question}
<|im_end|>
<|im_start|>assistant
"""
    # ... (后续的LLM调用和后处理逻辑与您当前版本一致) ...
    stop_sequences = ["<|im_end|>", "无法生成Cypher查询。"] 
    
    cypher_query = await call_sglang_llm(
        prompt, 
        temperature=0.0, 
        max_new_tokens=400, # 稍微增加一点，以防复杂查询需要更多空间
        stop_sequences=stop_sequences
    )
    
    if not cypher_query or cypher_query.strip() == "" or cypher_query.strip().lower() == "无法生成cypher查询。" or "无法生成cypher查询" in cypher_query.strip().lower():
        llm_py_logger.warning(f"LLM未能生成有效Cypher查询或明确表示无法生成，原始输出: '{cypher_query}'") # <--- 使用 llm_py_logger
        return "无法生成Cypher查询。" 
    
    processed_query = cypher_query.strip()
    prefixes_to_remove = ["cypher查询:", "cypher query:", "```cypher", "```sql", "```"]
    for prefix in prefixes_to_remove:
        if processed_query.lower().startswith(prefix.lower()):
            processed_query = processed_query[len(prefix):].strip()
    
    if processed_query.endswith("```"):
        processed_query = processed_query[:-len("```")].strip()

    if not processed_query:
        llm_py_logger.warning(f"LLM生成的Cypher查询在移除常见前缀/后缀后为空，原始输出: '{cypher_query}'") # <--- 使用 llm_py_logger

    llm_py_logger.info(f"LLM成功生成Cypher查询 (后处理后): {processed_query}")
    return processed_query

async def generate_clarification_question(original_query: str, uncertainty_reason: str) -> Optional[str]:
    """
    利用LLM根据用户原始查询和不确定性原因，生成一个具体的澄清问题。
    """
    prompt = f"""<|im_start|>system
你是一个智能助手，擅长在理解用户查询时识别歧义并请求澄清。
你的任务是根据用户原始查询和系统检测到的不确定性原因，生成一个简洁、明确的澄清问题。
澄清问题应该帮助用户选择正确的意图，或者提供更多必要的信息。
只输出澄清问题，不要包含任何额外解释、对话标记或代码块。
<|im_end|>
<|im_start|>user
用户原始查询: {original_query}
不确定性原因: {uncertainty_reason}

请生成一个澄清问题:
<|im_end|>
<|im_start|>assistant
"""
    stop_sequences = ["<|im_end|>"]
    print(f"调用SGLang LLM API生成澄清问题 (Prompt长度: {len(prompt)} 字符)...")
    clarification_question = await call_sglang_llm(
        prompt, 
        temperature=0.5, # 适中温度，鼓励生成合理的问题
        max_new_tokens=128, # 澄清问题通常不会太长
        stop_sequences=stop_sequences
    )
    if not clarification_question or clarification_question.strip() == "":
        print("LLM未能生成澄清问题，返回默认提示。")
        return "抱歉，我不太理解您的意思，请您再具体说明一下。"
    print(f"LLM成功生成澄清问题: {clarification_question}")
    return clarification_question

async def generate_clarification_options(original_query: str, uncertainty_reason: str) -> List[str]:
    """
    利用LLM根据用户原始查询和不确定性原因，生成多个具体的澄清选项。
    """
    prompt = f"""<|im_start|>system
你是一个智能助手，擅长根据用户查询的模糊性提供具体的澄清选项。
你的任务是根据用户原始查询和系统检测到的不确定性原因，生成3-5个具体的、可供用户选择的澄清选项。
每个选项都应该是一个简洁的短语或问题，帮助用户明确其意图。
你的回答必须是一个JSON数组（列表），其中每个元素是一个字符串（澄清选项）。
只输出JSON数组，不要包含任何其他解释、对话标记或代码块。

示例:
用户查询: "帮我预定机票。"
不确定性原因: "缺少出发城市、目的地、日期等信息。"
助手:
[
  "请问您想从哪个城市出发？",
  "请问您的目的地是哪里？",
  "请问您希望在哪一天出行？",
  "您有偏好的航空公司或舱位等级吗？"
]
<|im_end|>
<|im_start|>user
用户原始查询: {original_query}
不确定性原因: {uncertainty_reason}

请生成澄清选项:
<|im_end|>
<|im_start|>assistant
"""
    stop_sequences = ["<|im_end|>"]
    print(f"调用SGLang LLM API生成澄清选项 (Prompt长度: {len(prompt)} 字符)...")
    llm_output = await call_sglang_llm(
        prompt,
        temperature=0.7, # 稍高温度，鼓励生成多样性
        max_new_tokens=256,
        stop_sequences=stop_sequences
    )

    options = []
    if llm_output:
        try:
            json_str = llm_output.strip()
            if json_str.startswith("```json"):
                json_str = json_str[len("```json"):].strip()
                if json_str.endswith("```"):
                    json_str = json_str[:-len("```")].strip()
            
            parsed_options = json.loads(json_str)
            if isinstance(parsed_options, list) and all(isinstance(o, str) for o in parsed_options):
                options = parsed_options
                print(f"LLM成功生成 {len(options)} 个澄清选项。")
            else:
                print(f"LLM生成的澄清选项JSON格式不符合预期 (不是字符串列表): {llm_output[:200]}...")
        except json.JSONDecodeError as e:
            print(f"解析LLM澄清选项JSON失败: {e}. 原始输出: {llm_output[:200]}...")
        except Exception as e:
            print(f"处理LLM澄清选项时发生未知错误: {e}. 原始输出: {llm_output[:200]}...")
    else:
        print("LLM未能生成澄清选项。")
    
    if not options:
        options.append("请提供更多详细信息。")
    
    return options

async def generate_intent_classification(user_query: str) -> Dict[str, Any]:
    """
    利用LLM对用户查询进行意图分类，判断是否需要澄清。
    返回一个字典，包含 'clarification_needed' (bool) 和 'reason' (str)。
    """
    prompt = f"""<|im_start|>system
你是一个智能意图分类器。你的任务是分析用户查询，判断该查询是否清晰明确，或者是否存在歧义、信息不足导致需要进一步澄清。
如果查询包含具体的命名实体（如人名“张三”、项目名“项目X”、产品名“新产品A”等），并且问题是关于这些实体的特定信息（例如“张三的职位是什么？”、“项目X的截止日期是哪天？”、“新产品A的功能有哪些？”），则通常认为查询是清晰的，不需要澄清。
只有当查询缺少定位关键信息所必需的核心实体，或者询问的范围过于宽泛无法直接操作时，才需要澄清。

如果查询需要澄清，请说明原因。
你的回答必须是一个JSON对象，包含两个字段：
1. "clarification_needed": 布尔值，如果需要澄清则为 true，否则为 false。
2. "reason": 字符串，如果需要澄清，请简要说明原因；如果不需要，则为空字符串。

示例1 (需要澄清 - 信息不足):
用户查询: "帮我预定明天去上海的机票。"
助手:
{{
  "clarification_needed": true,
  "reason": "缺少出发城市、具体时间（上午/下午/晚上）、舱位等级等信息。"
}}

示例2 (不需要澄清 - 清晰):
用户查询: "公司最新的销售额报告在哪里可以找到？"
助手:
{{
  "clarification_needed": false,
  "reason": ""
}}

示例3 (需要澄清 - 实体不明确):
用户查询: "关于项目进展的文档。"
助手:
{{
  "clarification_needed": true,
  "reason": "项目名称不明确，文档类型（报告、计划、会议纪要等）不明确。"
}}

示例4 (不需要澄清 - 包含具体实体和明确问题):
用户查询: "张三参与了哪个项目？"
助手:
{{
  "clarification_needed": false,
  "reason": ""
}}

示例5 (不需要澄清 - 包含具体实体和明确问题):
用户查询: "华东区域2024年第一季度的销售额是多少？"
助手:
{{
  "clarification_needed": false,
  "reason": ""
}}

示例6 (需要澄清 - “公司”指默认上下文，但其余部分仍模糊):
用户查询: "公司的政策"
助手:
{{
  "clarification_needed": true,
  "reason": "未能明确指出是关于哪方面的公司政策（例如：人力资源、IT安全、财务等）。"
}}
<|im_end|>
<|im_start|>user
用户查询: {user_query}
<|im_end|>
<|im_start|>assistant
"""
    stop_sequences = ["<|im_end|>"]
    print(f"调用SGLang LLM API进行意图分类 (Prompt长度: {len(prompt)} 字符)...")

    llm_output = await call_sglang_llm(
        prompt,
        temperature=0.1, # 较低温度，鼓励生成结构化、确定的答案
        max_new_tokens=256,
        stop_sequences=stop_sequences
    )

    if llm_output:
        try:
            json_str = llm_output.strip()
            if json_str.startswith("```json"):
                json_str = json_str[len("```json"):].strip()
                if json_str.endswith("```"):
                    json_str = json_str[:-len("```")].strip()

            parsed_result = json.loads(json_str)
            if isinstance(parsed_result, dict) and \
               "clarification_needed" in parsed_result and \
               "reason" in parsed_result:
                print(f"LLM成功进行意图分类: {parsed_result}")
                return parsed_result
            else:
                print(f"LLM生成的意图分类JSON格式不符合预期: {llm_output[:200]}...")
        except json.JSONDecodeError as e:
            print(f"解析LLM意图分类JSON失败: {e}. 原始输出: {llm_output[:200]}...")
        except Exception as e:
            print(f"处理LLM意图分类时发生未知错误: {e}. 原始输出: {llm_output[:200]}...")

    print("LLM未能生成有效的意图分类结果，默认不需澄清。")
    return {"clarification_needed": False, "reason": "LLM分类失败或无结果。"}
--- END OF FILE llm.py ---

--- START OF FILE load_neo4j_data.py ---# zhz_agent/load_neo4j_data.py
import json
import os
from neo4j import GraphDatabase, basic_auth
from dotenv import load_dotenv
import traceback

load_dotenv() # 确保加载 .env 文件中的NEO4J凭证

NEO4J_URI = os.getenv("NEO4J_URI", "bolt://localhost:7687")
NEO4J_USER = os.getenv("NEO4J_USER", "neo4j")
NEO4J_PASSWORD = os.getenv("NEO4J_PASSWORD") # 您需要确保这个密码是正确的

DATA_PATH = os.path.join(os.path.dirname(__file__), "data")
SAMPLE_KG_PATH = os.path.join(DATA_PATH, "sample_kg.json")

def clear_database(driver):
    """清除数据库中的所有节点和关系"""
    with driver.session(database="neo4j") as session:
        session.run("MATCH (n) DETACH DELETE n")
        print("Cleared all nodes and relationships from the database.")

def create_constraints(driver):
    """创建一些基本约束，确保节点属性的唯一性（如果适用）"""
    with driver.session(database="neo4j") as session:
        try:
            session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (p:Person) REQUIRE p.name IS UNIQUE")
            session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (pr:Project) REQUIRE pr.name IS UNIQUE")
            session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (r:Region) REQUIRE r.name IS UNIQUE")
            session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (prod:Product) REQUIRE prod.name IS UNIQUE")
            session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (d:Document) REQUIRE d.id IS UNIQUE")
            # SalesAmount 通常不需要唯一约束，因为它可能重复（例如不同区域同一时期的销售）
            print("Ensured constraints are created (or already exist).")
        except Exception as e:
            print(f"Error creating constraints: {e}")


def load_data(driver, kg_data):
    """根据kg_data中的facts加载数据到Neo4j"""
    facts = kg_data.get("facts", [])
    
    with driver.session(database="neo4j") as session:
        entities_to_create = set()
        node_types_from_schema = { # 定义了主要实体的标签和它们的主要标识属性
            "Person": "name", "Project": "name", "Region": "name", 
            "Product": "name", "Document": "id", "Idea": "name" # 新增Idea类型
        }

        for fact in facts:
            subject_name = fact.get("subject")
            object_name = fact.get("object")
            fact_type = fact.get("type", "")

            subject_label = None
            # 基于fact_type或其他逻辑推断subject_label
            if "person_" in fact_type: subject_label = "Person"
            elif "region_" in fact_type: subject_label = "Region"
            elif "product_" in fact_type: subject_label = "Product"
            # ... 其他类型的映射 ...
            
            if subject_label and subject_name:
                prop_name = node_types_from_schema.get(subject_label, "name")
                entities_to_create.add((subject_label, prop_name, subject_name))

            object_label = None
            if not fact_type.endswith("_amount"): # 不是直接的销售额事实
                if "_project" in fact_type: object_label = "Project"
                elif "_product" in fact_type: object_label = "Product"
                elif "_document" in fact_type: object_label = "Document"
                elif "_idea" in fact_type: object_label = "Idea" # 新增对Idea类型的处理
                # ... 其他类型的映射 ...

                if object_label and object_name:
                    prop_name = node_types_from_schema.get(object_label, "name") # Document会用id, Idea会用name
                    entities_to_create.add((object_label, prop_name, object_name))
        
        for label, prop, value in entities_to_create:
            if value is not None:
                query = f"MERGE (n:{label} {{{prop}: $value}})"
                session.run(query, value=value)
                print(f"Merged node: ({label} {{{prop}: '{value}'}})")

        for fact in facts:
            s_name = fact.get("subject")
            rel = fact.get("relation")
            o_name = fact.get("object")
            fact_type = fact.get("type", "")
            period = fact.get("period")

            if fact_type == "region_sales_amount" and period:
                session.run("MERGE (r:Region {name: $s_name})", s_name=s_name)
                try:
                    # ... (销售额解析逻辑不变) ...
                    if isinstance(o_name, str) and '万元' in o_name:
                        numeric_val_str = o_name.replace('万元', '').strip()
                        numeric_val = float(numeric_val_str)
                        unit_val = '万元'
                    # ... (其他单位解析) ...
                    else:
                        numeric_val = float(o_name) # 尝试直接转换
                        unit_val = None 
                    
                    query = """
                    MATCH (r:Region {name: $s_name})
                    CREATE (sa:SalesAmount {numeric_amount: $num_val, period: $period, unit: $unit_val})
                    CREATE (r)-[:HAS_SALES_AMOUNT]->(sa)
                    """
                    session.run(query, s_name=s_name, num_val=numeric_val, period=period, unit_val=unit_val)
                    print(f"Created SalesAmount for {s_name}, {period}: {numeric_val} {unit_val or ''}")
                except ValueError:
                    print(f"Could not parse sales amount: {o_name} for {s_name}, {period}. Skipping this SalesAmount fact.")
                
            elif s_name and rel and o_name: 
                s_label, o_label = None, None
                s_prop, o_prop = "name", "name" 

                # --- 更精确的标签和属性推断 ---
                if fact_type == "person_project" and rel == "WORKS_ON":
                    s_label, o_label = "Person", "Project"
                elif fact_type == "person_idea" and rel == "PROPOSED_IDEA": # 新增
                    s_label, o_label = "Person", "Idea"
                elif fact_type == "region_product" and rel == "HAS_SALES_PRODUCT": # 假设type是 region_product
                    s_label, o_label = "Region", "Product"
                elif fact_type == "product_document" and rel == "RELATED_TO":
                    s_label, o_label = "Product", "Document"
                    o_prop = "id" # Document用id匹配
                # 您可以根据您的fact_type添加更多精确的映射规则

                if s_label and o_label:
                    query = f"""
                    MATCH (s:{s_label} {{{s_prop}: $s_name}})
                    MATCH (o:{o_label} {{{o_prop}: $o_name}})
                    MERGE (s)-[:{rel}]->(o)
                    """
                    session.run(query, s_name=s_name, o_name=o_name)
                    print(f"Merged relationship: ({s_label} {{{s_prop}:'{s_name}'}})-[:{rel}]->({o_label} {{{o_prop}:'{o_name}'}})")
                else:
                    print(f"Could not determine labels for fact: {fact} (s_label: {s_label}, o_label: {o_label}). Relationship not created.")
            else:
                print(f"Skipping incomplete fact: {fact}")


if __name__ == "__main__":
    driver = None
    try:
        driver = GraphDatabase.driver(NEO4J_URI, auth=basic_auth(NEO4J_USER, NEO4J_PASSWORD))
        driver.verify_connectivity()
        print("Successfully connected to Neo4j.")
        
        clear_database(driver) # 清空数据库
        create_constraints(driver) # 创建约束

        with open(SAMPLE_KG_PATH, 'r', encoding='utf-8') as f:
            kg_data_to_load = json.load(f)
        
        load_data(driver, kg_data_to_load)
        
        print("\nData loading process completed.")
        print("You can now verify the data in Neo4j Browser (http://localhost:7474).")
        print("Example query to check SalesAmount:")
        print("MATCH (r:Region)-[:HAS_SALES_AMOUNT]->(sa:SalesAmount) RETURN r.name, sa.numeric_amount, sa.unit, sa.period")
        print("Example query to check Person-Project:")
        print("MATCH (p:Person)-[:WORKS_ON]->(proj:Project) RETURN p.name, proj.name")

    except Exception as e:
        print(f"An error occurred: {e}")
        traceback.print_exc()
    finally:
        if driver:
            driver.close()
            print("Neo4j connection closed.")--- END OF FILE load_neo4j_data.py ---

--- START OF FILE main.py ---# zhz_agent/main.py
from fastapi import FastAPI
from contextlib import asynccontextmanager
import uvicorn
import logging
from typing import Type, List, Dict, Any, Optional, ClassVar

# --- [修改] 导入 -> 改为绝对导入 ---
from zhz_agent.database import database, sqlalchemy_engine, Base, get_scheduler # <--- [修改] 导入 get_scheduler
from zhz_agent.task_manager_service import router as tasks_router
from zhz_agent import database_models # 确保 SQLAlchemy 模型被导入

@asynccontextmanager
async def lifespan(app_instance: FastAPI):
    print("--- Main FastAPI 应用启动 (已集成任务管理API 和 APScheduler) ---") # [修改] 更新了描述

    # --- 数据库初始化 ---
    await database.connect()
    print("数据库已连接。")
    try:
        Base.metadata.create_all(bind=sqlalchemy_engine)
        print("数据库表已执行 create_all。")
        from sqlalchemy import inspect
        inspector = inspect(sqlalchemy_engine)
        if inspector.has_table("tasks"):
            print("'tasks' 表已成功创建/存在于数据库中。")
        else:
            print("警告: 'tasks' 表在 create_all 之后仍未找到！这通常意味着模型没有在 create_all 之前被正确导入。")
            print(f"   已知的表: {inspector.get_table_names()}") # 打印所有实际创建的表
            print(f"   Base.metadata.tables: {Base.metadata.tables.keys()}") # 打印 SQLAlchemy 元数据中已注册的表
    except Exception as e:
        print(f"创建或检查数据库表时出错: {e}")
        import traceback
        traceback.print_exc() # 打印详细的异常堆栈


    # --- [修改] APScheduler 初始化 (使用 get_scheduler) ---
    current_scheduler = get_scheduler() # <--- 获取调度器实例
    try:
        logging.getLogger('apscheduler').setLevel(logging.DEBUG) # 设置为 DEBUG 级别

        if not current_scheduler.running: # <--- 只有在未运行时才启动
            current_scheduler.start()
            print("APScheduler 已启动并使用数据库作业存储。")
        else:
            print("APScheduler 已在运行。")
    except Exception as e:
        print(f"APScheduler 启动失败: {e}")

    print("RAG 组件的初始化和管理在 zhz_agent_mcp_server.py。")
    print("任务管理API已在 /tasks 路径下可用。")

    yield # FastAPI 应用在此运行

    print("--- Main FastAPI 应用关闭 ---")
    current_scheduler_on_shutdown = get_scheduler() # <--- 再次获取以确保是同一个实例
    if current_scheduler_on_shutdown and current_scheduler_on_shutdown.running:
        current_scheduler_on_shutdown.shutdown()
        print("APScheduler 已关闭。")
    await database.disconnect()
    print("数据库已断开连接。")
    print("RAG 组件的清理在 zhz_agent_mcp_server.py。")

# --- App 定义 (保持不变) ---
app = FastAPI(
    title="Hybrid RAG Backend with Task Management",
    description="主 FastAPI 应用，负责接收请求、编排 Agent，并提供任务管理API。",
    version="0.2.1",
    lifespan=lifespan
)

app.include_router(tasks_router)

@app.get("/")
async def read_root():
    return {
        "message": "Welcome to the Hybrid RAG Backend Main App.",
        "available_services": {
            "task_management": "/tasks/docs",
            "rag_via_mcpo": "mcpo proxy at port 8006 (see mcpo_servers.json)"
        }
    }

if __name__ == "__main__":
    print("--- 启动 Main FastAPI 服务器 (包含任务管理API) ---")
    uvicorn.run("zhz_agent.main:app", host="0.0.0.0", port=8000, reload=True) # Ensure correct run command--- END OF FILE main.py ---

--- START OF FILE pydantic_models.py ---# zhz_agent/pydantic_models.py
from pydantic import BaseModel, Field, ConfigDict, model_validator
from typing import List, Dict, Any, Optional
from enum import Enum
from datetime import datetime
import uuid

# --- RAG Models ---
class QueryRequest(BaseModel):
    model_config = ConfigDict(extra='forbid')
    query: str = Field(description="用户提出的原始查询文本。", json_schema_extra=lambda schema: schema.pop('default', None) if schema.get('default') is None else None)
    top_k_vector: int = Field(description="期望检索的向量搜索结果数量。", json_schema_extra=lambda schema: schema.pop('default', None) if schema.get('default') is None else None)
    top_k_kg: int = Field(description="期望检索的知识图谱结果数量。", json_schema_extra=lambda schema: schema.pop('default', None) if schema.get('default') is None else None)
    top_k_bm25: int = Field(description="期望检索的 BM25 关键词搜索结果数量。", json_schema_extra=lambda schema: schema.pop('default', None) if schema.get('default') is None else None)

    @model_validator(mode='before')
    @classmethod
    def remove_internal_params(cls, data: Any) -> Any:
        if isinstance(data, dict):
            print(f"Pydantic DEBUG (QueryRequest before validation): Received data for validation: {str(data)[:500]}")
            removed_security_context = data.pop('security_context', None)
            if removed_security_context:
                print(f"Pydantic INFO (QueryRequest before validation): Removed 'security_context': {str(removed_security_context)[:100]}")
            removed_agent_fingerprint = data.pop('agent_fingerprint', None)
            if removed_agent_fingerprint:
                print(f"Pydantic INFO (QueryRequest before validation): Removed 'agent_fingerprint': {str(removed_agent_fingerprint)[:100]}")
        return data

class RetrievedDocument(BaseModel):
    source_type: str
    content: str
    score: Optional[float] = Field(default=None, json_schema_extra=lambda schema: schema.pop('default', None) if schema.get('default') is None else None)
    metadata: Optional[Dict[str, Any]] = Field(default=None, json_schema_extra=lambda schema: schema.pop('default', None) if schema.get('default') is None else None)

class HybridRAGResponse(BaseModel):
    original_query: str
    answer: str
    retrieved_sources: List[RetrievedDocument]
    debug_info: Optional[Dict[str, Any]] = Field(default=None, json_schema_extra=lambda schema: schema.pop('default', None) if schema.get('default') is None else None)


# --- Task Management Models ---
class TaskStatus(str, Enum):
    PENDING = "pending"      # 待处理 (新创建，尚未到执行时间)
    ACTIVE = "active"        # 活动 (已到执行时间，等待执行或正在执行)
    COMPLETED = "completed"  # 已完成
    CANCELLED = "cancelled"  # 已取消
    FAILED = "failed"        # 执行失败
    REMINDING = "reminding"    # 提醒中 (可选状态)

class ReminderMethod(str, Enum):
    NOTIFICATION = "notification" # 桌面通知

class TaskModel(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid.uuid4()), description="任务的唯一ID (自动生成)")
    title: str = Field(description="任务标题")
    description: Optional[str] = Field(None, description="任务的详细描述")
    status: TaskStatus = Field(default=TaskStatus.PENDING, description="任务当前状态")
    created_at: datetime = Field(default_factory=datetime.utcnow, description="任务创建时间 (UTC)")
    updated_at: datetime = Field(default_factory=datetime.utcnow, description="任务最后更新时间 (UTC)")
    due_date: Optional[datetime] = Field(None, description="任务截止日期或计划执行时间 (UTC)")
    reminder_time: Optional[datetime] = Field(None, description="任务提醒时间 (UTC)")
    reminder_offset_minutes: Optional[int] = Field(None, description="提醒时间相对于due_date的提前分钟数 (例如10分钟前)")
    reminder_methods: List[ReminderMethod] = Field(default=[ReminderMethod.NOTIFICATION], description="提醒方式列表")
    priority: int = Field(default=0, description="任务优先级 (例如 0:普通, 1:重要, 2:紧急)")
    tags: List[str] = Field(default_factory=list, description="任务标签") # 确保默认为空列表
    action_type: Optional[str] = Field(None, description="任务到期时需要执行的动作类型 (例如 'navigate', 'send_message', 'run_report')")
    action_payload: Dict[str, Any] = Field(default_factory=dict, description="执行动作时需要的参数 (例如导航的目的地)") # 确保默认为空字典
    execution_result: Optional[str] = Field(None, description="任务执行后的结果或错误信息")
    last_executed_at: Optional[datetime] = Field(None, description="上次执行时间 (UTC)")

    model_config = ConfigDict(
        use_enum_values=True,
        from_attributes=True,
    )

class CreateTaskRequest(BaseModel):
    title: str
    description: Optional[str] = None
    due_date: Optional[datetime] = None
    reminder_offset_minutes: Optional[int] = None # 例如 "10" 代表提前10分钟
    reminder_methods: Optional[List[ReminderMethod]] = [ReminderMethod.NOTIFICATION]
    priority: Optional[int] = 0
    tags: Optional[List[str]] = None
    action_type: Optional[str] = None
    action_payload: Optional[Dict[str, Any]] = None
    model_config = ConfigDict(extra='forbid')

class UpdateTaskRequest(BaseModel):
    title: Optional[str] = None
    description: Optional[str] = None
    status: Optional[TaskStatus] = None
    due_date: Optional[datetime] = None
    reminder_offset_minutes: Optional[int] = None
    reminder_methods: Optional[List[ReminderMethod]] = None
    priority: Optional[int] = None
    tags: Optional[List[str]] = None
    action_type: Optional[str] = None
    action_payload: Optional[Dict[str, Any]] = None
    model_config = ConfigDict(extra='forbid')--- END OF FILE pydantic_models.py ---

--- START OF FILE rag_service.py ---# zhz_agent/rag_service.py

import os
import json
import asyncio
import traceback
from contextlib import asynccontextmanager
from typing import List, Dict, Any, Optional, AsyncIterator
from dataclasses import dataclass, field # 确保导入 field
import time
import logging
import sys

# MCP 框架导入
from mcp.server.fastmcp import FastMCP, Context

# --- 配置 rag_service 的专用日志 ---
_rag_service_py_dir = os.path.dirname(os.path.abspath(__file__))
_rag_service_log_file = os.path.join(_rag_service_py_dir, 'rag_service_debug.log')

rag_logger = logging.getLogger("RagServiceLogger")
rag_logger.setLevel(logging.DEBUG)
rag_logger.propagate = False

if rag_logger.hasHandlers():
    rag_logger.handlers.clear()

try:
    _file_handler = logging.FileHandler(_rag_service_log_file, mode='w')
    _file_handler.setLevel(logging.DEBUG)
    _formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
    _file_handler.setFormatter(_formatter)
    rag_logger.addHandler(_file_handler)
    rag_logger.info("--- RagServiceLogger configured to write to rag_service_debug.log ---")
except Exception as e:
    print(f"CRITICAL: Failed to configure RagServiceLogger: {e}")


# --- 从项目内部导入所有 RAG 模块 ---
from zhz_agent.pydantic_models import QueryRequest, HybridRAGResponse, RetrievedDocument # QueryRequest 和 HybridRAGResponse 可能需要定义或调整
from zhz_agent.llm import (
    generate_answer_from_context, 
    generate_expanded_queries, 
    generate_cypher_query, # KGRetriever 会用
    generate_clarification_question, 
    generate_intent_classification, 
    # generate_clarification_options, # 似乎未使用
    NO_ANSWER_PHRASE_ANSWER_CLEAN
)
# from zhz_agent.vector import VectorRetriever # 不再使用旧的
from zhz_agent.kg import KGRetriever
from zhz_agent.fusion import FusionEngine 
# from zhz_agent.bm25 import BM25Retriever # 不再使用旧的

# --- 导入新的检索器 ---
from zhz_agent.chromadb_retriever import ChromaDBRetriever
from zhz_agent.file_bm25_retriever import FileBM25Retriever


from dotenv import load_dotenv
# 加载 .env 文件，通常在项目根目录
# __file__ 是当前 rag_service.py 的路径
# os.path.join(os.path.dirname(__file__), '..', '.env') 假设 .env 在 zhz_agent 包的上一级目录
dotenv_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), '..', '.env')
if os.path.exists(dotenv_path):
    load_dotenv(dotenv_path=dotenv_path)
    rag_logger.info(f"Loaded .env file from: {dotenv_path}")
else:
    rag_logger.warning(f".env file not found at {dotenv_path}, will rely on environment variables or defaults.")
    # 尝试加载当前目录的 .env (如果 rag_service.py 被直接运行且.env在同级)
    load_dotenv()


# --- 应用上下文 Dataclass ---
@dataclass
class AppContext:
    # vector_retriever: VectorRetriever # 旧的
    chroma_retriever: Optional[ChromaDBRetriever] = None # 新的
    kg_retriever: Optional[KGRetriever] = None
    # bm25_retriever: BM25Retriever # 旧的
    file_bm25_retriever: Optional[FileBM25Retriever] = None # 新的
    fusion_engine: Optional[FusionEngine] = None
    # llm_generator: Optional[Any] = None # LLMGenerator在您的代码中没有被实例化并放入AppContext

# --- MCP 服务器生命周期管理 ---
@asynccontextmanager
async def app_lifespan_for_rag_service(server: FastMCP) -> AsyncIterator[AppContext]:
    rag_logger.info("--- RAG Service (FastMCP): 正在初始化 RAG 组件 (新版) ---")
    
    chroma_retriever_instance: Optional[ChromaDBRetriever] = None
    kg_retriever_instance: Optional[KGRetriever] = None
    file_bm25_retriever_instance: Optional[FileBM25Retriever] = None
    fusion_engine_instance: Optional[FusionEngine] = None

    # 初始化 ChromaDB Retriever
    try:
        # 这些路径和名称应该与Dagster流水线中配置的一致
        # 优先从环境变量读取，如果不存在则使用默认值（如果适用）
        chroma_persist_dir = os.getenv("CHROMA_PERSIST_DIRECTORY", "/home/zhz/dagster_home/chroma_data")
        chroma_collection_name = os.getenv("CHROMA_COLLECTION_NAME", "rag_documents")
        embedding_model_path = os.getenv("EMBEDDING_MODEL_PATH", "/home/zhz/models/bge-small-zh-v1.5")

        if not os.path.isdir(chroma_persist_dir):
             rag_logger.warning(f"ChromaDB persist directory '{chroma_persist_dir}' not found. Retrieval may fail or use an in-memory DB if ChromaDBRetriever handles this.")
        if not os.path.exists(embedding_model_path): # embedding_model_path 应该是目录
            rag_logger.warning(f"Embedding model path '{embedding_model_path}' not found. ChromaDBRetriever initialization might fail.")

        chroma_retriever_instance = ChromaDBRetriever(
            collection_name=chroma_collection_name,
            persist_directory=chroma_persist_dir,
            embedding_model_name_or_path=embedding_model_path
        )
        rag_logger.info("RAG Service: ChromaDBRetriever 初始化成功。")
    except Exception as e:
        rag_logger.error(f"RAG Service: ChromaDBRetriever 初始化失败: {e}", exc_info=True)
        # 不在此处抛出异常，允许服务在部分组件失败时仍能启动（如果设计如此）
    
    # 初始化 File BM25 Retriever
    try:
        bm25_index_dir = os.getenv("BM25_INDEX_DIRECTORY", "/home/zhz/dagster_home/bm25_index_data/")
        if not os.path.isdir(bm25_index_dir):
            rag_logger.warning(f"BM25 index directory '{bm25_index_dir}' not found. FileBM25Retriever initialization might fail.")
            
        file_bm25_retriever_instance = FileBM25Retriever(
            index_directory_path=bm25_index_dir
        )
        rag_logger.info("RAG Service: FileBM25Retriever 初始化成功。")
    except Exception as e:
        rag_logger.error(f"RAG Service: FileBM25Retriever 初始化失败: {e}", exc_info=True)

    # 初始化 KG Retriever
    try:
        # KGRetriever 内部会从环境变量读取NEO4J配置
        # generate_cypher_query 是从 zhz_agent.llm 导入的
        kg_retriever_instance = KGRetriever(llm_cypher_generator_func=generate_cypher_query)
        rag_logger.info("RAG Service: KGRetriever 初始化成功。")
    except Exception as e:
        rag_logger.error(f"RAG Service: KGRetriever 初始化失败: {e}", exc_info=True)
        if kg_retriever_instance and hasattr(kg_retriever_instance, 'close'): # 确保在失败前尝试关闭
            kg_retriever_instance.close()
            
    # 初始化 Fusion Engine
    try:
        fusion_engine_instance = FusionEngine(logger=rag_logger)
        rag_logger.info("RAG Service: FusionEngine 初始化成功。")
    except Exception as e:
        rag_logger.error(f"RAG Service: FusionEngine 初始化失败: {e}", exc_info=True)

    rag_logger.info("--- RAG Service (FastMCP): RAG 组件初始化尝试完成。---")

    ctx = AppContext(
        chroma_retriever=chroma_retriever_instance,
        kg_retriever=kg_retriever_instance,
        file_bm25_retriever=file_bm25_retriever_instance,
        fusion_engine=fusion_engine_instance
    )
    try:
        yield ctx
    finally:
        rag_logger.info("--- RAG Service (FastMCP): 正在清理资源 ---")
        if kg_retriever_instance: # 确保只在成功初始化后才调用close
            kg_retriever_instance.close() 
        rag_logger.info("--- RAG Service (FastMCP): 清理完成 ---")

# --- 初始化 FastMCP 应用 ---
rag_mcp_application = FastMCP(
    name="zhz_agent_rag_service", # 修改了服务名称以区分
    description="Upgraded Hybrid RAG 服务，使用持久化知识库。",
    lifespan=app_lifespan_for_rag_service,
)

# --- MCP 工具定义 ---
@rag_mcp_application.tool()
async def query_rag_v2( # 重命名工具函数以避免与旧的混淆 (如果需要)
    ctx: Context,
    query: str, # 直接使用 query 作为输入，而不是 QueryRequest 对象
    top_k_vector: int = 3,
    top_k_kg: int = 2,
    top_k_bm25: int = 3,
    top_k_final: int = 3 # 最终融合后返回的文档数
) -> str: 
    rag_logger.info(f"\n--- RAG Service (query_rag_v2): 接收到查询: '{query}' ---")
    rag_logger.info(f"    Params: top_k_vector={top_k_vector}, top_k_kg={top_k_kg}, top_k_bm25={top_k_bm25}, top_k_final={top_k_final}")
    start_time_total = time.time()

    app_ctx: AppContext = ctx.request_context.lifespan_context
    response_payload = {} 
    original_query_for_response = query 
    final_json_output = ""

    try:
        # --- 1. LLM 驱动的意图分类和澄清触发 (保持不变) ---
        rag_logger.info(f"--- [TIME] 开始意图分类 at {time.time() - start_time_total:.2f}s ---")
        start_time_intent = time.time()
        intent_classification_result = await generate_intent_classification(query)
        rag_logger.info(f"--- [TIME] 结束意图分类, 耗时: {time.time() - start_time_intent:.2f}s. Result: {intent_classification_result}")

        if intent_classification_result.get("clarification_needed"):
            uncertainty_reason = intent_classification_result.get("reason", "查询可能存在歧义或信息不足。")
            clarification_question_text = await generate_clarification_question(query, uncertainty_reason)
            response_payload = {
                "status": "clarification_needed",
                "clarification_question": clarification_question_text,
                "original_query": original_query_for_response,
                "debug_info": {"uncertainty_reason": uncertainty_reason, "source": "intent_classification"}
            }
            rag_logger.info(f"--- 需要澄清，返回: {response_payload}")
            final_json_output = json.dumps(response_payload, ensure_ascii=False)
            sys.stdout.flush(); sys.stderr.flush()
            return final_json_output

        # --- 2. 查询扩展 (保持不变) ---
        rag_logger.info(f"--- 查询清晰，无需澄清。开始查询扩展 for: {query} ---")
        start_time_expansion = time.time()
        expanded_queries = await generate_expanded_queries(query) # 假设返回 List[str]
        rag_logger.info(f"--- 扩展查询列表 (共 {len(expanded_queries)} 个): {expanded_queries}. 耗时: {time.time() - start_time_expansion:.2f}s ---")
        
        all_raw_retrievals: List[RetrievedDocument] = []
        
        # --- 3. 并行多路召回 (使用新的检索器) ---
        # 我们将为原始查询和每个扩展查询都执行三路召回
        queries_to_process = [query] + expanded_queries 
        # 或者，如果觉得扩展查询过多，可以只用原始查询或选择性使用扩展查询
        # queries_to_process = [query] # 简化：仅使用原始查询进行召回

        rag_logger.info(f"--- [TIME] 开始并行召回 for {len(queries_to_process)} queries at {time.time() - start_time_total:.2f}s ---")
        start_time_retrieval = time.time()

        for current_query_text in queries_to_process:
            rag_logger.info(f"Processing retrievals for query: '{current_query_text}'")
            
            # 向量检索 (ChromaDB)
            if app_ctx.chroma_retriever:
                try:
                    chroma_docs_raw = app_ctx.chroma_retriever.retrieve(query_text=current_query_text, n_results=top_k_vector)
                    for doc_raw in chroma_docs_raw:
                        all_raw_retrievals.append(
                            RetrievedDocument(
                                source_type="vector_chroma",
                                content=doc_raw.get("text", ""),
                                score=doc_raw.get("score", 0.0),
                                metadata={**doc_raw.get("metadata", {}), "original_query_part": current_query_text} # 添加原始查询部分
                            )
                        )
                    rag_logger.info(f"  ChromaDB for '{current_query_text}': found {len(chroma_docs_raw)} docs.")
                except Exception as e_chroma:
                    rag_logger.error(f"  Error during ChromaDB retrieval for '{current_query_text}': {e_chroma}", exc_info=True)
            
            # 关键词检索 (BM25)
            if app_ctx.file_bm25_retriever:
                try:
                    bm25_docs_raw = app_ctx.file_bm25_retriever.retrieve(query_text=current_query_text, n_results=top_k_bm25)
                    # BM25只返回ID和分数，我们需要补充文本。
                    # 策略：尝试从已有的ChromaDB召回结果中匹配ID补充文本，或标记为待补充。
                    for doc_raw_bm25 in bm25_docs_raw:
                        bm25_chunk_id = doc_raw_bm25.get("id")
                        text_content_for_bm25 = f"[BM25: Text for ID {bm25_chunk_id} pending]"
                        # 简单的补充逻辑：
                        found_in_chroma = False
                        for chroma_doc in all_raw_retrievals: # 检查已有的（主要是chroma的）
                            if chroma_doc.metadata.get("chunk_id") == bm25_chunk_id or chroma_doc.metadata.get("id") == bm25_chunk_id : # ChromaDBRetriever的id是chunk_id
                                text_content_for_bm25 = chroma_doc.content
                                found_in_chroma = True
                                break
                        if not found_in_chroma and app_ctx.chroma_retriever: # 如果没找到，尝试从ChromaDB单独获取
                            try:
                                # 假设ChromaDB存储时ID就是chunk_id
                                # get()方法返回更完整的文档信息
                                specific_chroma_doc = app_ctx.chroma_retriever._collection.get(ids=[bm25_chunk_id], include=["metadatas"])
                                if specific_chroma_doc and specific_chroma_doc.get("metadatas") and specific_chroma_doc.get("metadatas")[0]:
                                    text_content_for_bm25 = specific_chroma_doc["metadatas"][0].get("chunk_text", text_content_for_bm25)
                            except Exception as e_chroma_get:
                                rag_logger.warning(f"  Failed to get text for BM25 ID {bm25_chunk_id} from Chroma: {e_chroma_get}")
                                
                        all_raw_retrievals.append(
                            RetrievedDocument(
                                source_type="keyword_bm25s",
                                content=text_content_for_bm25,
                                score=doc_raw_bm25.get("score", 0.0),
                                metadata={"chunk_id": bm25_chunk_id, "original_query_part": current_query_text}
                            )
                        )
                    rag_logger.info(f"  BM25s for '{current_query_text}': found {len(bm25_docs_raw)} potential docs.")
                except Exception as e_bm25:
                    rag_logger.error(f"  Error during BM25 retrieval for '{current_query_text}': {e_bm25}", exc_info=True)

            # 知识图谱检索 (Neo4j)
            if app_ctx.kg_retriever:
                try:
                    rag_logger.info(f"  Performing KG retrieval for query: '{current_query_text}'") # 添加日志
                    kg_docs = await app_ctx.kg_retriever.retrieve_with_llm_cypher(
                        query=current_query_text, # <--- 修改这里
                        top_k=top_k_kg
                    )
                    # retrieve_with_llm_cypher 已经返回 List[RetrievedDocument]
                    for kg_doc in kg_docs: # 添加原始查询部分到元数据
                        if kg_doc.metadata:
                            kg_doc.metadata["original_query_part"] = current_query_text
                        else:
                            kg_doc.metadata = {"original_query_part": current_query_text}
                    all_raw_retrievals.extend(kg_docs)
                    rag_logger.info(f"  KG Retrieval for '{current_query_text}': found {len(kg_docs)} results.")
                except Exception as e_kg:
                    rag_logger.error(f"  Error during KG retrieval for '{current_query_text}': {e_kg}", exc_info=True)
        
        rag_logger.info(f"--- [TIME] 结束所有召回, 耗时: {time.time() - start_time_retrieval:.2f}s ---")
        rag_logger.info(f"--- 总计从各路召回（所有查询处理后）的结果数: {len(all_raw_retrievals)} ---")
        for i_doc, doc_retrieved in enumerate(all_raw_retrievals[:10]): # 日志只打印前10条
            rag_logger.debug(f"  Raw Doc {i_doc}: type={doc_retrieved.source_type}, score={doc_retrieved.score}, content='{str(doc_retrieved.content)[:100]}...'")

        if not all_raw_retrievals: 
            # ... (无召回结果的处理，与您之前的代码类似) ...
            response_payload = {
                "status": "success", 
                "final_answer": "抱歉，根据您提供的查询，未能从知识库中找到相关信息。",
                "original_query": original_query_for_response,
                "debug_info": {"message": "No documents retrieved from any source."}
            }
            final_json_output = json.dumps(response_payload, ensure_ascii=False)
            sys.stdout.flush(); sys.stderr.flush()
            return final_json_output

        # --- 4. 结果融合与重排序 (使用FusionEngine) ---
        rag_logger.info(f"--- [TIME] 开始结果融合与重排序 at {time.time() - start_time_total:.2f}s ---")
        start_time_fusion = time.time()
        if not app_ctx.fusion_engine:
            rag_logger.error("FusionEngine not available! Skipping fusion and reranking.")
            # 如果没有融合引擎，直接使用原始召回结果（可能需要截断和简单排序）
            # 这里简化处理：直接取 all_raw_retrievals，按分数初排（如果分数可比）
            # 或者只用向量检索结果
            # 为了演示，我们假设至少需要向量结果，或者返回错误
            final_context_docs = sorted(all_raw_retrievals, key=lambda d: d.score, reverse=True)[:top_k_final]
        else:
            # FusionEngine的 fuse_results 方法在您的代码中是异步的
            final_context_docs = await app_ctx.fusion_engine.fuse_results(
                all_raw_retrievals, 
                original_query_for_response, # 传递原始查询给融合引擎
                top_n_final=top_k_final # 传递最终需要的文档数
            ) 
        rag_logger.info(f"--- [TIME] 结束结果融合与重排序, 耗时: {time.time() - start_time_fusion:.2f}s. Final context docs: {len(final_context_docs)} ---")
        
        # --- 5. 准备上下文并生成答案 (与您之前的代码类似) ---
        # 注意：您的FusionEngine.fuse_results 返回的是融合后的文本字符串，而不是RetrievedDocument列表
        # 我们需要调整这里，或者调整FusionEngine使其返回RetrievedDocument列表
        # 假设FusionEngine返回的是RetrievedDocument列表 (需要修改FusionEngine)
        
        if not final_context_docs: # 如果融合后没有文档
            fused_context_text_for_llm = "未在知识库中找到相关信息。"
            final_answer_from_llm = "根据现有知识，未能找到您查询的相关信息。"
            response_payload = {
                "status": "success",
                "final_answer": final_answer_from_llm,
                "original_query": original_query_for_response,
                "debug_info": {"message": "No relevant context found after fusion."}
            }
        else:
            # 假设 final_context_docs 是 List[RetrievedDocument]
            context_strings_for_llm = [
                f"Source Type: {doc.source_type}, Score: {doc.score:.4f}\nContent: {doc.content}" 
                for doc in final_context_docs
            ]
            fused_context_text_for_llm = "\n\n---\n\n".join(context_strings_for_llm)

            rag_logger.info(f"\n--- FUSED CONTEXT for LLM (length: {len(fused_context_text_for_llm)} chars) ---")
            rag_logger.info(f"{fused_context_text_for_llm[:1000]}...") # 日志打印部分上下文
            rag_logger.info(f"--- END OF FUSED CONTEXT ---\n")

            rag_logger.info(f"--- [TIME] 开始最终答案生成 at {time.time() - start_time_total:.2f}s ---")
            start_time_answer_gen = time.time()
            final_answer_from_llm = await generate_answer_from_context(query, fused_context_text_for_llm)
            rag_logger.info(f"--- [TIME] 结束最终答案生成, 耗时: {time.time() - start_time_answer_gen:.2f}s ---")

            if not final_answer_from_llm or final_answer_from_llm.strip() == NO_ANSWER_PHRASE_ANSWER_CLEAN:
                final_answer_from_llm = "根据您提供的信息，我暂时无法给出明确的回答。"
            
            response_payload = {
                "status": "success",
                "final_answer": final_answer_from_llm,
                "original_query": original_query_for_response,
                "retrieved_context_docs": [doc.model_dump() for doc in final_context_docs], # 返回用于生成答案的文档
                "debug_info": {"total_raw_retrievals_count": len(all_raw_retrievals)}
            }
        
        final_json_output = json.dumps(response_payload, ensure_ascii=False)
        rag_logger.info(f"--- 'query_rag_v2' 成功执行完毕, 总耗时: {time.time() - start_time_total:.2f}s. 返回JSON响应 ---")
        
    except Exception as e:
        # ... (异常处理与您之前的代码类似) ...
        rag_logger.error(f"RAG Service CRITICAL ERROR in 'query_rag_v2': {type(e).__name__} - {str(e)}", exc_info=True)
        response_payload = { 
            "status": "error",
            "error_code": "RAG_SERVICE_INTERNAL_ERROR",
            "error_message": f"RAG服务内部发生未预期错误: {str(e)}",
            "original_query": original_query_for_response,
            "debug_info": {"exception_type": type(e).__name__}
        }
        final_json_output = json.dumps(response_payload, ensure_ascii=False)
    
    sys.stdout.flush(); sys.stderr.flush()
    return final_json_output

# --- 用于本地独立测试的 main 部分 ---
async def local_rag_test():
    rag_logger.info("--- Starting Local RAG Test ---")
    # 确保所有服务（ChromaDB数据存在, BM25索引文件存在, Neo4j运行, SGLang运行）都准备好
    
    # 模拟FastMCP的Context和AppContext
    class MockLifespanContext:
        def __init__(self):
            self.chroma_retriever = AppContext().chroma_retriever
            self.file_bm25_retriever = AppContext().file_bm25_retriever
            self.kg_retriever = AppContext().kg_retriever
            self.fusion_engine = AppContext().fusion_engine
            # self.llm_generator = AppContext().llm_generator # 不在AppContext中

    class MockRequestContext:
        def __init__(self):
            self.lifespan_context = MockLifespanContext()

    class MockContext: # 模拟FastMCP的Context
        def __init__(self):
            self.request_context = MockRequestContext()
            self.tool_name = "query_rag_v2" # 假设
            self.call_id = "local_test_call"
            # logger 可以设为 rag_logger
            # self.logger = rag_logger 
            # 但工具函数内部的 app_ctx: AppContext = ctx.request_context.lifespan_context
            # 我们需要确保 lifespan_context 正确填充了检索器

    # 实际的app_lifespan_for_rag_service 会在FastMCP启动时填充AppContext
    # 为了本地测试，我们需要手动模拟这个填充过程，或者直接使用全局初始化的检索器
    # 更简单的方式是，让 query_rag_v2 直接使用全局初始化的检索器（如果它们在模块级别）
    # 但FastMCP的推荐做法是通过lifespan管理上下文。

    # 为了本地测试能跑通，我们先假设app_lifespan_for_rag_service在模块加载时已执行
    # 并填充了全局的检索器实例（虽然这不是FastMCP的典型用法）
    # 或者，我们直接在测试函数内调用 app_lifespan_for_rag_service
    
    async with app_lifespan_for_rag_service(None) as app_context_instance: # 传入None作为server参数
        
        # 构建一个模拟的 request_context，它需要有一个 lifespan_context 属性
        class DummyRequestContext:
            def __init__(self, lifespan_ctx):
                self.lifespan_context = lifespan_ctx
        
        mock_req_context = DummyRequestContext(app_context_instance)

        # 创建 FastMCP Context 实例时，直接传入所有必需的参数
        mock_mcp_context = Context(
            request_context=mock_req_context, # <--- 在这里传入
            tool_name="query_rag_v2",
            call_id="local_test_call",
            logger=rag_logger # 使用我们已配置的 rag_logger
        )
        # 不再需要下面这行，因为它会导致 AttributeError
        # mock_mcp_context.request_context = DummyRequestContext() 
        # mock_mcp_context.request_context.lifespan_context = app_context_instance


        test_queries = [
            "张三目前的工作地点是哪里？",
            "项目Alpha文档编写任务的具体内容是什么？",
            "张三最近一次的工作变动是什么时候？"
        ]
        # ... (后续的测试循环代码保持不变) ...
        for t_query in test_queries:
            print(f"\n\n>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>")
            print(f"EXECUTING LOCAL TEST FOR QUERY: {t_query}")
            print(f">>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>>\n")
            response_str = await query_rag_v2(mock_mcp_context, query=t_query, top_k_final=2)
            
            print("\n--- RAG Service Local Test Response ---")
            try:
                response_data = json.loads(response_str)
                print(f"Query: {response_data.get('original_query')}")
                print(f"Answer: {response_data.get('final_answer')}")
                print(f"Status: {response_data.get('status')}")
                if response_data.get('status') == 'success' and response_data.get('retrieved_context_docs'):
                    print("\nRetrieved & Reranked Documents for Context:")
                    for i, doc_dict in enumerate(response_data.get('retrieved_context_docs', [])):
                        # 确保doc_dict是字典才解包
                        if isinstance(doc_dict, dict):
                            doc_obj = RetrievedDocument(**doc_dict) 
                            print(f"  Doc {i+1} (Source: {doc_obj.source_type}, Score: {doc_obj.score:.4f}):")
                            print(f"    Content: {doc_obj.content[:150]}...")
                        else:
                            print(f"  Doc {i+1}: (Unexpected format: {type(doc_dict)}) - {str(doc_dict)[:150]}...")
                elif response_data.get('status') == 'clarification_needed':
                     print(f"Clarification Question: {response_data.get('clarification_question')}")

            except json.JSONDecodeError:
                print("Error decoding JSON response from RAG service.")
                print(f"Raw response string: {response_str}")
            print("---------------------------------------\n")

if __name__ == "__main__":
    if os.getenv("RUN_RAG_SERVICE_LOCAL_TEST") == "true":
        asyncio.run(local_rag_test())
    else:
        rag_logger.info("--- Starting RAG Service (FastMCP for mcpo via direct run) ---")
        rag_mcp_application.run() # 这会启动FastAPI Uvicorn服务器--- END OF FILE rag_service.py ---

--- START OF FILE run_agent.py ---# /home/zhz/zhz_agent/run_agent.py

import os
import json
import datetime

from crewai import Agent, Task, Crew, Process

# --- 导入我们自己的项目模块 (使用绝对导入) ---
from zhz_agent.custom_crewai_tools import HybridRAGTool, BaseMCPTool
from zhz_agent.pydantic_models import QueryRequest # 用于 RAG 工具的输入
from zhz_agent.utils import call_mcpo_tool
from zhz_agent.custom_llm import CustomGeminiLLM

# --- 环境配置 ---
from dotenv import load_dotenv
load_dotenv()

# --- CrewAI 基类和事件系统 ---
from crewai.tools import BaseTool
from pydantic import BaseModel, Field
from typing import Any, Dict, List, Optional, Type
from crewai.llms.base_llm import BaseLLM as CrewAIBaseLLM
try:
    from crewai.utilities.events.base_event_listener import BaseEventListener as CrewAIBaseCallbackHandler
    from crewai.utilities.events import LLMCallStartedEvent, LLMCallCompletedEvent
    print("Successfully imported BaseEventListener and Event Types")
except ImportError:
    print("Failed to import BaseEventListener or Event Types, using dummy classes.")
    class CrewAIBaseCallbackHandler: pass
    class LLMCallStartedEvent: pass
    class LLMCallCompletedEvent: pass

# --- LiteLLM ---
import litellm

# --- 定义简单工具以供测试 ---
class SimpleToolInput(BaseModel):
    message: str = Field(description="A simple message string for the tool.")

class MySimpleTestTool(BaseTool):
    name: str = "MySimpleTestTool"
    description: str = "A very simple test tool that takes a message and returns it."
    args_schema: Type[BaseModel] = SimpleToolInput

    def _run(self, message: str) -> str:
        print(f"MySimpleTestTool received: {message}")
        return f"MySimpleTestTool processed: {message}"

# --- 配置 Agent 使用的 LLM 实例 ---
GEMINI_MODEL_NAME = "gemini/gemini-1.5-flash-latest"
GEMINI_API_KEY = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")

if not GEMINI_API_KEY:
    print("CRITICAL ERROR: GOOGLE_API_KEY or GEMINI_API_KEY not set.")
    exit(1)

# --- 定义详细的事件监听器 ---
class MyDetailedLogger(CrewAIBaseCallbackHandler):
    def __init__(self):
        super().__init__()
        print(f"[{datetime.datetime.now()}] MyDetailedLogger: 已初始化。")

    def setup_listeners(self, crewai_event_bus):
        print(f"[{datetime.datetime.now()}] MyDetailedLogger: 正在设置监听器...")

        @crewai_event_bus.on(LLMCallStartedEvent)
        def handle_llm_start(source, event: LLMCallStartedEvent):
            self.on_llm_start_logic(source, event)

        @crewai_event_bus.on(LLMCallCompletedEvent)
        def handle_llm_completed(source, event: LLMCallCompletedEvent):
            self.on_llm_end_logic(source, event)

        print(f"[{datetime.datetime.now()}] MyDetailedLogger: 监听器设置完成。")

    def on_llm_start_logic(self, source, event: LLMCallStartedEvent):
        print(f"\n>>>> LLM 调用开始 (Event Logic) <<<<")
        llm_inputs = getattr(event, 'llm_inputs', {})
        messages = llm_inputs.get('messages')
        tools = llm_inputs.get('tools')
        print(f"来源 (Source): {source}")
        if messages:
            print("消息 (来自 event.llm_inputs):")
            if isinstance(messages, list) and len(messages) > 0:
                first_message = messages[0]
                if isinstance(first_message, dict) and 'content' in first_message:
                    content_snippet = str(first_message.get('content', ''))[:300]
                    print(f"   Role: {first_message.get('role')}, Content Snippet: {content_snippet}...")
                else:
                     print(f"  First message (raw): {first_message}")
            else:
                 print(f"  Messages (raw): {messages}")
        else:
            print("消息 (来自 event.llm_inputs): 无")
        if tools:
            print("工具 (来自 event.llm_inputs):")
            try:
                print(f"  {json.dumps(tools, indent=2, ensure_ascii=False)}")
            except Exception as e:
                print(f"  无法序列化工具为 JSON: {e}. 工具: {tools}")
        else:
            print("工具 (来自 event.llm_inputs): 无")
        print("----------------------------------")

    def on_llm_end_logic(self, source, event: LLMCallCompletedEvent):
        print(f"\n>>>> LLM 调用结束 (Event Logic) <<<<")
        response = getattr(event, 'llm_output', None)
        print(f"来源 (Source): {source}")
        if response:
            if hasattr(response, 'choices') and response.choices:
                choice = response.choices[0]
                if hasattr(choice, 'message') and choice.message:
                    print(f"  消息内容: {choice.message.content}")
                    if hasattr(choice.message, 'tool_calls') and choice.message.tool_calls:
                        print(f"  工具调用: {choice.message.tool_calls}")
                    else:
                        print(f"  工具调用: 无")
            elif hasattr(response, 'content'):
                print(f"  响应内容: {response.content}")
            else:
                print(f"  LLM 响应 (来自 event.llm_output): {str(response)[:500]}...")
        else:
            print("  在 event.llm_output 中未找到响应对象。")
        print("----------------------------------")

# --- 实例化 CustomGeminiLLM ---
custom_llm_tool_config = {"function_calling_config": {"mode": "AUTO"}}
zhz_agent_tool = HybridRAGTool()
researcher_tools = [zhz_agent_tool]

llm_for_agent = CustomGeminiLLM(
    model=GEMINI_MODEL_NAME,
    api_key=GEMINI_API_KEY,
    temperature=0.1,
    max_tokens=2048,
    tool_config=custom_llm_tool_config,
    agent_tools=researcher_tools # 传递工具列表以供缓存
)
print(f"Custom Agent LLM configured: {GEMINI_MODEL_NAME} with custom tool_config")

# --- 设置 BaseMCPTool 的调用器 ---
BaseMCPTool.set_mcpo_caller(call_mcpo_tool)

# --- 定义 Agents ---
researcher_agent = Agent(
    role='信息检索专家',
    goal='准确地回答用户查询，并且只使用提供的工具。',
    backstory=(
        "你是一位高级AI助手，专注于信息检索。"
        "你的专长在于高效地利用工具来查找最相关和最精确的答案来回应用户的查询。"
    ),
    llm=llm_for_agent,
    tools=researcher_tools,
    verbose=True,
    allow_delegation=False,
)

writer_agent = Agent(
    role='报告撰写专家',
    goal='根据提供的信息，撰写清晰、结构良好且富有洞察力的报告。',
    backstory=(
        "您是一位资深的报告撰写专家，拥有出色的分析和写作能力。"
        "您擅长将复杂的信息提炼成易于理解的报告，并能根据不同的输出状态（答案、澄清、错误）"
        "灵活调整报告内容和格式。"
    ),
    llm=llm_for_agent,
    verbose=True,
    allow_delegation=False,
)

# --- 定义 Tasks (包含上下文传递修复) ---
research_task_description = """你收到了来自用户的以下查询：

'{query}'

你应该使用提供的 `HybridRAGQueryTool` 工具来处理这个查询。
如果这个工具需要 `top_k_vector`, `top_k_kg`, 或 `top_k_bm25` 这些参数，请使用以下建议值：
top_k_vector: 5, top_k_kg: 3, top_k_bm25: 5。
在使用完必要的工具后，你的最终输出应该是（使用中文）：'我的最终答案是：' 
后跟你使用的工具返回的精确、完整的JSON字符串输出内容。"""

research_task_expected_output = "短语 '我的最终答案是：' 后跟你使用的工具返回的精确、完整的JSON字符串输出内容。"

research_task = Task(
    description=research_task_description,
    expected_output=research_task_expected_output,
    agent=researcher_agent,
)

report_writing_task = Task(
    description="""根据【前一个任务】（信息检索专家）提供的RAG工具输出（它是一个JSON字符串），生成一份报告或响应。
请仔细分析这个JSON字符串输出，它应该包含一个 'status' 字段。
1. 如果 'status' 是 'success'，则提取 'final_answer' 字段的内容，并基于此答案撰写一份简洁的报告。
2. 如果 'status' 是 'clarification_needed'，则提取 'clarification_question' 字段的内容，并向用户明确指出需要澄清的问题，例如：'系统需要澄清：[澄清问题]'。
3. 如果 'status' 是 'error'，则提取 'error_message' (或 'error') 字段的内容，并向用户报告错误，例如：'RAG服务发生错误：[错误信息]'。
你的最终输出必须是清晰、专业且符合上述情况的报告或响应。""",
    expected_output="一份清晰的报告，或者一个明确的澄清请求，或者一个错误报告。",
    agent=writer_agent,
    context=[research_task],
)

# --- 实例化监听器 ---
my_event_logger = MyDetailedLogger()

# --- 定义 Crew (添加 event_listeners) ---
office_brain_crew = Crew(
    agents=[researcher_agent, writer_agent],
    tasks=[research_task, report_writing_task],
    process=Process.sequential,
    verbose=True,
    event_listeners=[my_event_logger] # <<< --- 激活事件监听器 ---
)

# --- 启动 Crew ---
if __name__ == "__main__":
    print("--- 启动智能助手终端大脑 Crew (使用 CustomGeminiLLM 和事件监听器) ---")
    user_query_input = "公司2024年第一季度在华东和华北的总销售额一共是多少？"
    # --- 修复：kickoff inputs 只包含 query ---
    inputs = {'query': user_query_input}
    result = office_brain_crew.kickoff(inputs=inputs)
    print("\n\n=== 最终报告 ===\n")
    if hasattr(result, 'raw'):
        print(result.raw)
    else:
        print(result)
    print("\n--- Crew 任务完成 ---")--- END OF FILE run_agent.py ---

--- START OF FILE task_jobs.py ---# zhz_agent/task_jobs.py
from datetime import datetime
from typing import Dict, Any
import os
import traceback
import httpx # <--- 确保 httpx 已导入
import json # <--- 确保 json 已导入

# 从 .database 导入 database 对象以便查询任务详情
# 从 .pydantic_models 导入 TaskModel 以便类型转换
# 从 .main 导入 scheduler 以便在需要时重新调度（虽然通常作业函数不直接操作调度器）
# 更好的做法是通过参数传递必要的信息，而不是依赖全局导入
WINDOWS_HOST_IP = os.getenv("WINDOWS_HOST_IP_FOR_WSL", "192.168.3.11") # <--- 请务必替换为您真实的Windows IP
LOCAL_AGENT_PORT = os.getenv("LOCAL_AGENT_PORT", "8003") # 与 local_agent_app.py 中的端口一致

# 如果 WINDOWS_HOST_IP 仍然是占位符，给出提示
if WINDOWS_HOST_IP == "在此处填写您上一步找到的Windows主机IP":
    print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
    print("REMINDER_JOB WARNING: WINDOWS_HOST_IP 未正确设置在 task_jobs.py 中!")
    print("请编辑 task_jobs.py 文件，将 '在此处填写您上一步找到的Windows主机IP' 替换为实际IP地址。")
    print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")

LOCAL_AGENT_NOTIFY_URL = f"http://{WINDOWS_HOST_IP}:{LOCAL_AGENT_PORT}/notify"

async def send_task_reminder(task_id: str, task_title: str, reminder_methods: list):
    """
    实际发送任务提醒的函数。
    """
    print(f"REMINDER_JOB: [{datetime.utcnow()}] 正在为任务 '{task_id}' - '{task_title}' 发送提醒。")
    for method in reminder_methods:
        if method == "notification": # 假设 ReminderMethod.NOTIFICATION.value 是 "notification"
            print(f"  REMINDER_JOB: 尝试通过 Local Agent 发送桌面通知: '{task_title}'")
            try:
                async with httpx.AsyncClient(timeout=10.0) as client:
                    response = await client.post(
                        LOCAL_AGENT_NOTIFY_URL,
                        json={"title": f"任务提醒: {task_title}", "message": f"任务 '{task_title}' 即将到期或需要关注。"}
                    )
                    response.raise_for_status() # Raise an exception for bad status codes
                    print(f"  REMINDER_JOB: 本地代理通知请求发送成功. 状态: {response.status_code}")
            except httpx.RequestError as e:
                print(f"  REMINDER_JOB: 通过本地代理发送通知失败 (RequestError): {e}")
                traceback.print_exc()
            except Exception as e:
                print(f"  REMINDER_JOB: 通过本地代理发送通知失败 (General Error): {e}")
                traceback.print_exc()
        # elif method == "email": #
        #     print(f"  REMINDER_JOB: 模拟发送邮件提醒...")

async def execute_task_action(task_id: str, action_type: str, action_payload: Dict[str, Any]):
    """
    实际执行任务动作的函数。
    """
    print(f"EXECUTION_JOB: [{datetime.utcnow()}] 正在为任务 '{task_id}' 执行动作 '{action_type}'。")
    print(f"  EXECUTION_JOB: 动作参数: {action_payload}")

    final_result = f"动作 '{action_type}' 已模拟执行。"
    success = True

    if action_type == "navigate":
        destination = action_payload.get("destination")
        if destination:
            print(f"  EXECUTION_JOB: 模拟导航到 '{destination}'...")
            final_result = f"已模拟为导航到 '{destination}' 准备好路线。"
        else:
            final_result = "导航动作失败：缺少目的地。"
            success = False
    elif action_type == "log_event":
        event_details = action_payload.get("event_details", "无详情")
        print(f"  EXECUTION_JOB: 记录事件: '{event_details}'")
        final_result = f"事件 '{event_details}' 已记录。"
    else:
        final_result = f"未知的动作类型: {action_type}"
        success = False

    # 更新数据库中的任务状态和结果 (需要访问数据库)
    # 这部分逻辑最好通过API调用或服务层来完成，以避免循环导入和分散DB操作
    # 这里我们只打印信息，实际应用中需要实现DB更新
    print(f"  EXECUTION_JOB: 任务 '{task_id}' 执行完毕。结果: {final_result}, 状态: {'COMPLETED' if success else 'FAILED'}")
--- END OF FILE task_jobs.py ---

--- START OF FILE task_manager_service.py ---# zhz_agent/task_manager_service.py
from fastapi import APIRouter, HTTPException, Depends, Body, Query, Path, status
from typing import List, Optional, Any, cast
from datetime import datetime, timedelta
import uuid
import traceback # 导入 traceback
import pytz

# --- [修改] 从 pydantic_models 导入我们定义的模型 -> 改为绝对导入 ---
from zhz_agent.pydantic_models import TaskModel, CreateTaskRequest, UpdateTaskRequest, TaskStatus, ReminderMethod

# --- [修改] 从 database_models 导入 SQLAlchemy 表模型 -> 改为绝对导入 ---
from zhz_agent.database_models import TaskDB

# --- [修改] 从新的 database.py 导入 database 对象 和 get_scheduler -> 改为绝对导入 ---
from zhz_agent.database import database, get_scheduler

# --- [修改] 从 .task_jobs 导入作业函数 -> 改为绝对导入 ---
from zhz_agent.task_jobs import send_task_reminder, execute_task_action
from apscheduler.triggers.date import DateTrigger # 用于指定精确的运行时间
from apscheduler.jobstores.base import JobLookupError # <--- [修改] 导入 JobLookupError 的正确路径

# APIRouter 实例
router = APIRouter(
    prefix="/tasks",
    tags=["Task Management"],
    responses={404: {"description": "Not found"}},
)

def _ensure_utc(dt: datetime) -> datetime:
    """确保 datetime 对象是 UTC 时区感知的。"""
    if dt.tzinfo is None:
        return pytz.utc.localize(dt) # 如果是朴素时间，假定它是UTC并设为UTC
    return dt.astimezone(pytz.utc) # 如果是其他时区，转换为UTC

def _schedule_task_jobs(task: TaskModel):
    current_scheduler = get_scheduler() # 获取 scheduler 实例
    print(f"DEBUG SCHEDULER: _schedule_task_jobs called. Scheduler instance: {current_scheduler}, Is running: {current_scheduler.running if current_scheduler else 'N/A'}")
    if not current_scheduler or not current_scheduler.running:
        print("SCHEDULER_ERROR: APScheduler 未运行，无法调度作业。")
        return

    # 提醒作业
    if task.reminder_time and task.status == TaskStatus.PENDING:
        reminder_job_id = f"reminder_{task.id}"
        try:
            reminder_methods_list = task.reminder_methods
            reminder_utc = _ensure_utc(task.reminder_time)
            print(f"SCHEDULER DEBUG: Passing reminder_methods to job: {reminder_methods_list}") # 添加日志

            current_scheduler.add_job(
                send_task_reminder,
                trigger=DateTrigger(run_date=reminder_utc),
                args=[task.id, task.title, reminder_methods_list], # <--- [修复] 直接传递列表
                id=reminder_job_id,
                name=f"Reminder for task {task.title[:20]}",
                replace_existing=True
            )
            print(f"SCHEDULER: 已为任务 '{task.id}' 添加/更新提醒作业，运行于 {task.reminder_time}")
        except Exception as e:
            print(f"SCHEDULER_ERROR: 添加提醒作业失败 for task '{task.id}': {e}")
            traceback.print_exc() # 打印详细错误堆栈

    # 执行作业
    if task.due_date and task.status == TaskStatus.PENDING:
        execution_job_id = f"execution_{task.id}"
        try:
            due_utc = _ensure_utc(task.due_date) # <--- [新增] 确保时间是 UTC 感知的
            print(f"SCHEDULER DEBUG: Adding execution job at {due_utc} ({due_utc.tzinfo})") # <--- [新增] 添加时区日志
            current_scheduler.add_job(
                execute_task_action,
                trigger=DateTrigger(run_date=due_utc),
                args=[task.id, task.action_type, task.action_payload],
                id=execution_job_id,
                name=f"Execution for task {task.title[:20]}",
                replace_existing=True
            )
            print(f"SCHEDULER: 已为任务 '{task.id}' 添加/更新执行作业，运行于 {task.due_date}")
        except Exception as e:
            print(f"SCHEDULER_ERROR: 添加执行作业失败 for task '{task.id}': {e}")

def _cancel_task_jobs(task_id: str):
    """从 APScheduler 取消作业"""
    current_scheduler = get_scheduler()
    if not current_scheduler or not current_scheduler.running:
        print("SCHEDULER_ERROR: APScheduler 未运行，无法取消作业。")
        return

    reminder_job_id = f"reminder_{task_id}"
    execution_job_id = f"execution_{task_id}"

    try:
        current_scheduler.remove_job(reminder_job_id)
        print(f"SCHEDULER: 已移除提醒作业 for task '{task_id}'")
    except JobLookupError:
        print(f"SCHEDULER_INFO: 提醒作业 '{reminder_job_id}' 未找到，无需移除。")
    except Exception as e:
        print(f"SCHEDULER_ERROR: 移除提醒作业失败 for task '{task_id}': {e}")

    try:
        current_scheduler.remove_job(execution_job_id)
        print(f"SCHEDULER: 已移除执行作业 for task '{task_id}'")
    except JobLookupError:
        print(f"SCHEDULER_INFO: 执行作业 '{execution_job_id}' 未找到，无需移除。")
    except Exception as e:
        print(f"SCHEDULER_ERROR: 移除执行作业失败 for task '{task_id}': {e}")

@router.post("/", response_model=TaskModel, status_code=status.HTTP_201_CREATED)
async def create_task(task_request: CreateTaskRequest = Body(...)):
    """
    创建一个新任务。
    """
    now = datetime.utcnow()
    task_id = str(uuid.uuid4())

    reminder_time_val = None
    if task_request.due_date and task_request.reminder_offset_minutes is not None:
        reminder_time_val = task_request.due_date - timedelta(minutes=task_request.reminder_offset_minutes)

    reminder_methods_values = [
        method.value if hasattr(method, 'value') else str(method)
        for method in (task_request.reminder_methods or [ReminderMethod.NOTIFICATION])
    ]

    insert_query = TaskDB.__table__.insert().values(
        id=task_id,
        title=task_request.title,
        description=task_request.description,
        status=TaskStatus.PENDING,
        created_at=now,
        updated_at=now,
        due_date=task_request.due_date,
        reminder_time=reminder_time_val,
        reminder_offset_minutes=task_request.reminder_offset_minutes,
        reminder_methods=reminder_methods_values, # <--- 确保存入的是字符串列表
        priority=task_request.priority or 0,
        tags=task_request.tags or [],
        action_type=task_request.action_type,
        action_payload=task_request.action_payload or {}
    )

    try:
        await database.execute(insert_query)
    except Exception as e:
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Failed to create task in database: {e}")

    created_task_db = await database.fetch_one(TaskDB.__table__.select().where(TaskDB.id == task_id))
    if not created_task_db:
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to retrieve task after creation")

    response_task = TaskModel.model_validate(dict(created_task_db))

    response_task.reminder_methods = [
        m.value if hasattr(m, 'value') else str(m)
        for m in response_task.reminder_methods
    ]

    _schedule_task_jobs(response_task)
    print(f"TASK_MANAGER: Created task '{response_task.id}' with title '{response_task.title}' in DB")
    return response_task

@router.get("/", response_model=List[TaskModel])
async def list_tasks(
    status_filter: Optional[TaskStatus] = Query(None, alias="status"),
    priority_filter: Optional[int] = Query(None, alias="priority"),
    skip: int = Query(0, ge=0),
    limit: int = Query(10, ge=1, le=100)
):
    """
    获取任务列表，支持过滤和分页。
    """
    query = TaskDB.__table__.select()
    if status_filter:
        query = query.where(TaskDB.status == status_filter)
    if priority_filter is not None:
        query = query.where(TaskDB.priority == priority_filter)

    query = query.order_by(TaskDB.created_at.desc()).offset(skip).limit(limit)

    db_tasks = await database.fetch_all(query)
    return [TaskModel.model_validate(dict(task)) for task in db_tasks]

@router.get("/{task_id}", response_model=TaskModel)
async def get_task(task_id: str = Path(..., description="要获取的任务ID")):
    """
    根据ID获取单个任务的详细信息。
    """
    query = TaskDB.__table__.select().where(TaskDB.id == task_id)
    db_task = await database.fetch_one(query)
    if not db_task:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Task not found")
    return TaskModel.model_validate(dict(db_task))

@router.put("/{task_id}", response_model=TaskModel)
async def update_task(
    task_id: str = Path(..., description="要更新的任务ID"),
    task_update: UpdateTaskRequest = Body(...)
):
    """
    更新现有任务。
    """
    existing_task_query = TaskDB.__table__.select().where(TaskDB.id == task_id)
    db_task = await database.fetch_one(existing_task_query)
    if not db_task:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Task not found")

    update_data = task_update.model_dump(exclude_unset=True)
    update_data["updated_at"] = datetime.utcnow()

    if "reminder_methods" in update_data and update_data["reminder_methods"] is not None:
        update_data["reminder_methods"] = [
            method.value if hasattr(method, 'value') else str(method)
            for method in update_data["reminder_methods"]
        ]

    current_due_date = update_data.get("due_date", cast(Optional[datetime], db_task.due_date))
    current_offset = update_data.get("reminder_offset_minutes", cast(Optional[int], db_task.reminder_offset_minutes))

    if current_due_date and current_offset is not None:
        update_data["reminder_time"] = current_due_date - timedelta(minutes=current_offset)
    elif "due_date" in update_data and current_offset is None:
         update_data["reminder_time"] = None


    update_query = TaskDB.__table__.update().where(TaskDB.id == task_id).values(**update_data)
    await database.execute(update_query)

    updated_db_task = await database.fetch_one(existing_task_query)
    if not updated_db_task:
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to retrieve task after update")

    response_task = TaskModel.model_validate(dict(updated_db_task))

    response_task.reminder_methods = [
        m.value if hasattr(m, 'value') else str(m)
        for m in response_task.reminder_methods
    ]

    _cancel_task_jobs(task_id)
    _schedule_task_jobs(response_task)
    print(f"TASK_MANAGER: Updated task '{response_task.id}' in DB")
    return response_task

@router.delete("/{task_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_task(task_id: str = Path(..., description="要删除的任务ID")):
    """
    删除一个任务。
    """
    existing_task_query = TaskDB.__table__.select().where(TaskDB.id == task_id)
    db_task = await database.fetch_one(existing_task_query)
    if not db_task:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Task not found")

    delete_query = TaskDB.__table__.delete().where(TaskDB.id == task_id)
    await database.execute(delete_query)

    _cancel_task_jobs(task_id)
    print(f"TASK_MANAGER: Deleted task '{task_id}' from DB")
    return None

@router.post("/{task_id}/complete", response_model=TaskModel)
async def mark_task_as_complete(task_id: str = Path(..., description="要标记为完成的任务ID")):
    """
    将任务标记为已完成。
    """
    existing_task_query = TaskDB.__table__.select().where(TaskDB.id == task_id)
    db_task_row = await database.fetch_one(existing_task_query)
    if not db_task_row:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Task not found")

    db_task = TaskModel.model_validate(dict(db_task_row))
    if db_task.status == TaskStatus.COMPLETED:
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="Task is already completed")

    update_data = {
        "status": TaskStatus.COMPLETED,
        "updated_at": datetime.utcnow()
    }
    update_query = TaskDB.__table__.update().where(TaskDB.id == task_id).values(**update_data)
    await database.execute(update_query)

    completed_db_task = await database.fetch_one(existing_task_query)
    if not completed_db_task:
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to retrieve task after marking complete")

    response_task = TaskModel.model_validate(dict(completed_db_task))
    _cancel_task_jobs(task_id)
    print(f"TASK_MANAGER: Marked task '{response_task.id}' as completed in DB")
    return response_task--- END OF FILE task_manager_service.py ---

--- START OF FILE utils.py ---# zhz_agent/utils.py

import httpx
import json
import traceback
import os
from dotenv import load_dotenv
load_dotenv()

MCPO_BASE_URL = os.getenv("MCPO_BASE_URL", "http://localhost:8006")

async def call_mcpo_tool(tool_name_with_prefix: str, payload: dict):
    """
    异步调用MCP工具服务。
    """
    api_url = f"{MCPO_BASE_URL}/{tool_name_with_prefix}"
    cleaned_payload = {k: v for k, v in payload.items() if v is not None}

    print(f"Calling mcpo endpoint: POST {api_url} with payload: {json.dumps(cleaned_payload, ensure_ascii=False)}")

    async with httpx.AsyncClient() as client:
        response = None  # 初始化response变量
        try:
            headers = {"Content-Type": "application/json"}
            response = await client.post(api_url, json=cleaned_payload, headers=headers, timeout=120.0)
            print(f"mcpo status code: {response.status_code}")

            if response.status_code == 200:
                try:
                    result_data = response.json()
                    if isinstance(result_data, dict) and result_data.get("isError"):
                        error_content_list = result_data.get("content", [{"type": "text", "text": "Unknown error from MCP tool"}])
                        error_text = "Unknown error from MCP tool"
                        for item in error_content_list:
                            if item.get("type") == "text":
                                error_text = item.get("text", error_text)
                                break
                        print(f"MCP Tool execution failed (isError=true): {error_text}")
                        try:
                            parsed_mcp_error = json.loads(error_text)
                            if isinstance(parsed_mcp_error, dict) and "error" in parsed_mcp_error:
                                return {"error": f"Tool '{tool_name_with_prefix}' failed via MCP: {parsed_mcp_error['error']}"}
                        except json.JSONDecodeError:
                            pass # 不是JSON，直接使用error_text
                        return {"error": f"Tool '{tool_name_with_prefix}' failed via MCP: {error_text}"}
                    return result_data
                except json.JSONDecodeError:
                    print(f"Warning: mcpo returned status 200 but response is not JSON for '{tool_name_with_prefix}'. Returning raw text.")
                    return {"content": [{"type": "text", "text": response.text}]} # 包装成MCP期望的格式之一
            else:
                error_text = f"mcpo call to '{tool_name_with_prefix}' failed with status {response.status_code}. Response: {response.text[:500]}..."
                print(error_text)
                return {"error": error_text}

        except httpx.RequestError as exc: # 更具体的网络请求错误
            error_msg = f"HTTP RequestError calling mcpo tool '{tool_name_with_prefix}': {type(exc).__name__} - {exc}"
            print(error_msg)
            traceback.print_exc()
            return {"error": error_msg}
        except Exception as exc: # 捕获其他所有异常
            error_msg = f"Unexpected error calling mcpo tool '{tool_name_with_prefix}': {type(exc).__name__} - {exc}"
            response_text_snippet = response.text[:500] if response and hasattr(response, 'text') else "Response object not available or no text."
            print(f"{error_msg}. Response snippet: {response_text_snippet}")
            traceback.print_exc()
            return {"error": error_msg}
--- END OF FILE utils.py ---

--- START OF FILE vector.py ---# zhz_agent/vector.py
import json
import os
from typing import List, Dict, Any, Optional
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity
import numpy as np

# --- [修改] 导入 Pydantic 模型 -> 改为绝对导入 ---
from zhz_agent.pydantic_models import RetrievedDocument

class VectorRetriever:
    def __init__(self, data_path: str):
        """
        初始化向量检索器。
        
        参数:
            data_path (str): 包含文档的目录路径。
        """
        self.data_path = data_path
        self.documents: List[Dict[str, Any]] = []  # 存储加载的文档列表
        self.vectorizer: Optional[TfidfVectorizer] = None  # TF-IDF向量化器实例
        self.document_vectors: Optional[np.ndarray] = None  # 文档的向量表示
        self._load_documents()  # 加载文档并初始化向量化器

    def _load_documents(self):
        """
        从JSON文件加载文档并初始化TF-IDF向量化器。
        """
        file_path = os.path.join(self.data_path, "sample_documents.json")
        if not os.path.exists(file_path):
            print(f"错误: 模拟向量文档文件不存在: {file_path}")
            return

        with open(file_path, 'r', encoding='utf-8') as f:
            self.documents = json.load(f)
        
        if not self.documents:
            print("警告: 模拟向量文档文件为空。")
            return

        corpus = [doc['content'] for doc in self.documents]
        
        self.vectorizer = TfidfVectorizer(max_features=100)
        self.document_vectors = self.vectorizer.fit_transform(corpus)
        print(f"VectorRetriever: 加载了 {len(self.documents)} 个文档，TF-IDF模型已训练。")

    async def retrieve(self, query: str, top_k: int = 3) -> List[RetrievedDocument]:
        """
        根据查询文本进行语义检索。
        
        参数:
            query (str): 用户输入的查询文本。
            top_k (int): 返回的最相关文档数量，默认为3。
        
        返回:
            List[RetrievedDocument]: 包含最相关文档的列表。
        """
        if not self.vectorizer or self.document_vectors is None:
            print("VectorRetriever未初始化或没有文档。")
            return []

        query_vector = self.vectorizer.transform([query])
        similarities = cosine_similarity(query_vector, self.document_vectors).flatten()
        top_indices = similarities.argsort()[-top_k:][::-1] # 降序排列，取top_k
        
        retrieved_results: List[RetrievedDocument] = []
        for idx in top_indices:
            doc = self.documents[idx]
            score = float(similarities[idx]) # 将numpy float转换为Python float
            
            if score >= 0.0: # 暂时设为0，确保能返回结果
                retrieved_results.append(
                    RetrievedDocument(
                        source_type="vector_search",
                        content=doc['content'],
                        score=score,
                        metadata=doc.get('metadata', {})
                    )
                )
        print(f"VectorRetriever: 检索到 {len(retrieved_results)} 个结果。")
        return retrieved_results--- END OF FILE vector.py ---

