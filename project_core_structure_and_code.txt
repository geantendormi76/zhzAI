# Core Directories Structure and Code for Project: /home/zhz/zhz_agent

--- Structure and Code for: zhz_rag_pipeline_dagster/ ---
|-- zhz_rag_pipeline/
|-- zhz_rag_pipeline_dagster_project.egg-info/
|-- pyproject.toml
|-- setup.py

``` py
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/setup.py
from setuptools import find_packages, setup

setup(
    name="zhz_rag_pipeline_dagster_project", # 给一个包名
    version="0.0.1",
    packages=find_packages(), # 会找到 zhz_rag_pipeline 这个包
    install_requires=[
        "dagster",
        "dagster-webserver",
        # 添加其他 zhz_rag_pipeline_dagster 项目直接依赖的库
        # 例如 kuzu, dagster-pydantic (如果之后还要用) 等
        # 但核心的 zhz_rag 包的依赖不在这里列出，它应该是独立安装的
    ],
)
```

    |-- parsers/
    |-- __init__.py

``` py
# zhz_rag_pipeline/__init__.py
# This file makes Python treat the directory as a package.
# You can also define a __version__ or import key modules here if needed.
```

    |-- custom_io_managers.py

``` py
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/zhz_rag_pipeline/custom_io_managers.py
import json
import os
from typing import List, Type, Union, get_args, get_origin, Any, Optional 
from dagster import UPathIOManager, InputContext, OutputContext, DagsterInvariantViolationError
from pydantic import BaseModel as PydanticBaseModel
from upath import UPath

class PydanticListJsonIOManager(UPathIOManager):
    extension: str = ".jsonl"

    def __init__(self, base_dir: Optional[str] = None): # Changed base_path to base_dir for clarity
        resolved_base_dir: UPath
        if base_dir:
            resolved_base_dir = UPath(base_dir).resolve() # Resolve to absolute path
        else:
            # Default to <DAGSTER_HOME>/storage/pydantic_jsonl_io
            # DAGSTER_HOME defaults to ~/.dagster, but can be overridden by env var
            dagster_home_str = os.getenv("DAGSTER_HOME", os.path.join(os.path.expanduser("~"), ".dagster"))
            resolved_base_dir = UPath(dagster_home_str) / "storage" / "pydantic_jsonl_io"
        
        # Ensure the directory exists
        try:
            resolved_base_dir.mkdir(parents=True, exist_ok=True)
        except Exception as e:
            # Log this error appropriately, perhaps using a direct print if logger isn't set up yet
            # or re-raise as a Dagster-specific error.
            print(f"[PydanticListJsonIOManager __init__] ERROR: Could not create bafef __init__(self, base_dir: Optse directory {resolved_base_dir}: {e}")
            # Depending on Dagster's init sequence, context.log might not be available here.
            # It's safer to let UPathIOManager handle its own base_path or ensure dir exists before.
            # For now, we proceed, UPathIOManager might handle it or fail later.

        super().__init__(base_path=resolved_base_dir)
        # Log the final base path used by the UPathIOManager instance
        # self.log available after super().__init__() in ConfigurableIOManager context
        # For direct instantiation, we might need to pass a logger or use a global one.
        # print(f"[PydanticListJsonIOManager __init__] Initialized with resolved base_path: {self.base_path}")


    def dump_to_path(self, context: OutputContext, obj: List[PydanticBaseModel], path: UPath):
        context.log.info(f"[PydanticListJsonIOManager dump_to_path] Attempting to dump to resolved path: {path.resolve()}")
        
        if not isinstance(obj, list):
            msg = f"Expected a list of Pydantic models, got {type(obj)}"
            context.log.error(msg)
            raise TypeError(msg)
        
        # Optional: More robust type checking for list items if needed, using context.dagster_type
        # For now, assume obj is List[PydanticBaseModel] based on upstream asset's type hint.

        try:
            with path.open("w", encoding="utf-8") as f:
                for model_instance in obj:
                    if not isinstance(model_instance, PydanticBaseModel):
                        context.log.warning(f"Item in list is not a Pydantic model: {type(model_instance)}. Skipping.")
                        continue
                    json_str = model_instance.json() # Pydantic V1
                    f.write(json_str + "\n")
            context.log.info(f"[PydanticListJsonIOManager dump_to_path] Successfully dumped {len(obj)} items to {path.resolve()}")
        except Exception as e:
            context.log.error(f"[PydanticListJsonIOManager dump_to_path] Failed to dump object to {path.resolve()}: {e}", exc_info=True)
            raise

    def load_from_path(self, context: InputContext, path: UPath) -> List[PydanticBaseModel]:
        context.log.info(f"[PydanticListJsonIOManager load_from_path] Attempting to load from resolved path: {path.resolve()}")
        
        list_typing_type = context.dagster_type.typing_type
        origin = get_origin(list_typing_type)
        args = get_args(list_typing_type)

        if not (origin is list and args and issubclass(args[0], PydanticBaseModel)):
            msg = (
                f"PydanticListJsonIOManager can only handle inputs of type List[PydanticModel], "
                f"but got {list_typing_type} for input '{context.name}'."
            )
            context.log.error(msg)
            raise DagsterInvariantViolationError(msg) # Use Dagster specific error
        
        model_type: Type[PydanticBaseModel] = args[0]
        context.log.info(f"[PydanticListJsonIOManager load_from_path] Target model type for list items: {model_type.__name__}")

        loaded_models: List[PydanticBaseModel] = []
        if not path.exists():
            context.log.warning(f"[PydanticListJsonIOManager load_from_path] File not found at {path.resolve()}, returning empty list for input '{context.name}'.")
            return loaded_models

        try:
            with path.open("r", encoding="utf-8") as f:
                for line_number, line in enumerate(f, 1):
                    line_content = line.strip()
                    if not line_content:
                        continue
                    try:
                        model_instance = model_type.parse_raw(line_content) # Pydantic V1
                        loaded_models.append(model_instance)
                    except Exception as e_parse:
                        context.log.error(
                            f"[PydanticListJsonIOManager load_from_path] Failed to parse JSON line {line_number} "
                            f"into {model_type.__name__} from {path.resolve()}: {e_parse}. "
                            f"Line content (first 100 chars): '{line_content[:100]}...'",
                            exc_info=True
                        )
                        # Optionally re-raise or decide to skip problematic lines
                        # For now, we'll skip
            context.log.info(f"[PydanticListJsonIOManager load_from_path] Successfully loaded {len(loaded_models)} instances of {model_type.__name__} from {path.resolve()}")
        except Exception as e_read:
            context.log.error(f"[PydanticListJsonIOManager load_from_path] Failed to read or process file {path.resolve()}: {e_read}", exc_info=True)
            raise # Re-raise if file reading itself fails catastrophically
            
        return loaded_models
```

    |-- definitions.py

``` py
import dagster as dg
import os
from dagster import Definitions

from zhz_rag_pipeline_dagster.zhz_rag_pipeline.ingestion_assets import all_ingestion_assets
from zhz_rag_pipeline_dagster.zhz_rag_pipeline.processing_assets import all_processing_assets

from zhz_rag_pipeline_dagster.zhz_rag_pipeline.resources import (
    GGUFEmbeddingResource,
    ChromaDBResource,
    LocalLLMAPIResource,
    DuckDBResource,
    GeminiAPIResource,
    SystemResource
)
from zhz_rag_pipeline_dagster.zhz_rag_pipeline.custom_io_managers import PydanticListJsonIOManager

# --- 根据官方文档，将所有资产组合在一起 ---
all_assets = all_ingestion_assets + all_processing_assets

# --- 根据官方文档，定义一个包含所有资源的字典 ---
# Dagster 会自动为每个资产提供它所需要的资源
all_resources = {
    # IO 管理器，键名必须是 "io_manager" 才能被默认使用
    "io_manager": PydanticListJsonIOManager(),
    
    # 其他应用级资源
    "embedder": GGUFEmbeddingResource(
        api_url=os.getenv("EMBEDDING_API_URL", "http://127.0.0.1:8089")
    ),
    "chroma_db": ChromaDBResource(
        collection_name=os.getenv("CHROMA_COLLECTION_NAME", "zhz_rag_collection"),
        persist_directory=os.path.join(os.getenv("ZHZ_AGENT_PROJECT_ROOT", "/home/zhz/zhz_agent"), "zhz_rag", "stored_data", "chromadb_index")
    ),
    "LocalLLM_api": LocalLLMAPIResource(
        api_url="http://127.0.0.1:8088/v1/chat/completions",
        default_temperature=0.1,
        default_max_new_tokens=2048
    ),
    "duckdb_kg": DuckDBResource(
        db_file_path=os.path.join(os.getenv("ZHZ_AGENT_PROJECT_ROOT", "/home/zhz/zhz_agent"), "zhz_rag", "stored_data", "duckdb_knowledge_graph.db")
    ),
    "gemini_api": GeminiAPIResource(
        model_name="gemini/gemini-1.5-flash-latest",
        proxy_url=os.getenv("LITELLM_PROXY_URL"),
        default_temperature=0.1,
        default_max_tokens=2048
    ),
    "system_resource": SystemResource()
}

# --- 创建最终的、简洁的 Definitions 对象 ---
defs = Definitions(
    assets=all_assets,
    resources=all_resources
)
```

    |-- document_parsers.py

``` py
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/zhz_rag_pipeline/document_parsers.py
import os
from markdown_it import MarkdownIt
import logging
from typing import List, Dict, Any, Optional, Union, Literal 
from bs4 import BeautifulSoup
import pandas as pd

# --- 添加 Unstructured 的导入 ---
try:
    from unstructured.partition.docx import partition_docx
    from unstructured.documents.elements import Element as UnstructuredElement, Title, NarrativeText, ListItem, Table, Image, Header, Footer
    _UNSTRUCTURED_AVAILABLE = True
    print("INFO (document_parsers.py): Successfully imported Unstructured for DOCX.")
except ImportError:
    print("WARNING (document_parsers.py): Unstructured library not found. DOCX parsing will be a placeholder.")
    _UNSTRUCTURED_AVAILABLE = False
    class UnstructuredElement: pass # Dummy
# --- 结束添加 ---

# --- 导入我们定义的Pydantic模型 ---
# 假设这个文件和 pydantic_models_dagster.py 在同一个包下或能通过PYTHONPATH找到
try:
    from .pydantic_models_dagster import ( # 使用相对导入
        ParsedDocumentOutput,
        DocumentElementType, 
        TitleElement,
        NarrativeTextElement,
        ListItemElement,
        TableElement,
        CodeBlockElement,
        PageBreakElement,
        DocumentElementMetadata 
    )
    _PYDANTIC_MODELS_AVAILABLE_PARSERS = True
except ImportError:
    print("WARNING (document_parsers.py): Could not import Pydantic models. Using fallback Any/dict.")
    _PYDANTIC_MODELS_AVAILABLE_PARSERS = False
    class BaseModel: pass
    class DocumentElementMetadata(BaseModel): page_number: Optional[int] = None
    class ParsedDocumentOutput(BaseModel): parsed_text: str; elements: list; original_metadata: dict; summary: Optional[str] = None
    class TitleElement(BaseModel): element_type:str="title"; text:str; level:int; metadata: Optional[DocumentElementMetadata] = None
    class NarrativeTextElement(BaseModel): element_type:str="narrative_text"; text:str; metadata: Optional[DocumentElementMetadata] = None
    class ListItemElement(BaseModel): element_type:str="list_item"; text:str; level:int=0; ordered:bool=False; item_number:Optional[Union[int, str]]=None; metadata: Optional[DocumentElementMetadata] = None
    class TableElement(BaseModel): element_type:str="table"; markdown_representation:Optional[str]=None; metadata: Optional[DocumentElementMetadata] = None
    class CodeBlockElement(BaseModel): element_type:str="code_block"; code:str; language:Optional[str]=None; metadata: Optional[DocumentElementMetadata] = None
    class PageBreakElement(BaseModel): element_type:str="page_break"; metadata: Optional[DocumentElementMetadata] = None
    DocumentElementType = Any


logger = logging.getLogger(__name__) # 每个模块用自己的logger

# --- Markdown 解析逻辑 (从 poc_md_markdown_it.py 迁移并封装) ---



def _get_text_from_md_inline(inline_tokens: Optional[List[Any]]) -> str:
    # (这里是 get_text_from_inline_tokens 函数的完整代码)
    text_content = ""
    if inline_tokens is None: return ""
    for token in inline_tokens:
        if token.type == 'text':
            text_content += token.content
        elif token.type == 'code_inline':
            text_content += f"`{token.content}`"
        elif token.type == 'softbreak':
            text_content += ' ' 
        elif token.type == 'hardbreak':
            text_content += '\n'
        elif token.children: 
            text_content += _get_text_from_md_inline(token.children)
    return text_content

def _convert_md_tokens_to_elements_internal(tokens: list) -> List[Any]:
    # (这里是 convert_md_tokens_to_elements 函数的完整代码，但将其重命名为内部函数)
    # (并确保它在 _PYDANTIC_MODELS_AVAILABLE_PARSERS 为True时创建Pydantic实例，否则创建字典)
    elements: List[Any] = []
    idx = 0
    list_level_stack = [] 

    while idx < len(tokens):
        token = tokens[idx]

        if token.type == 'heading_open':
            level = int(token.tag[1:])
            idx_content = idx + 1
            text = ""
            if idx_content < len(tokens) and tokens[idx_content].type == 'inline':
                text = _get_text_from_md_inline(tokens[idx_content].children).strip()
            if _PYDANTIC_MODELS_AVAILABLE_PARSERS: elements.append(TitleElement(text=text, level=level))
            else: elements.append({"element_type": "title", "text": text, "level": level})
            idx = idx_content + 2 
            continue

        elif token.type == 'paragraph_open':
            is_list_item_para = False
            if list_level_stack and token.level >= list_level_stack[-1]["level"]:
                pass 
            if not is_list_item_para or not list_level_stack: 
                idx_content = idx + 1
                text = ""
                if idx_content < len(tokens) and tokens[idx_content].type == 'inline':
                    text = _get_text_from_md_inline(tokens[idx_content].children).strip()
                if text:
                    if _PYDANTIC_MODELS_AVAILABLE_PARSERS: elements.append(NarrativeTextElement(text=text))
                    else: elements.append({"element_type": "narrative_text", "text": text})
            idx = idx + 2 
            if idx < len(tokens) and tokens[idx-1].type == 'inline': 
                idx +=1 
            continue
        
        elif token.type == 'bullet_list_open':
            list_level_stack.append({"ordered": False, "level": token.level})
            idx += 1
            continue
        elif token.type == 'ordered_list_open':
            start_num = token.attrs.get('start', 1)
            list_level_stack.append({"ordered": True, "current_num": start_num, "level": token.level})
            idx += 1
            continue
        
        elif token.type == 'list_item_open':
            item_text = ""
            li_level = token.level
            next_token_idx = idx + 1
            if next_token_idx < len(tokens):
                next_token = tokens[next_token_idx]
                if next_token.type == 'paragraph_open' and next_token.level == li_level + 1 :
                    inline_idx = next_token_idx + 1
                    if inline_idx < len(tokens) and tokens[inline_idx].type == 'inline':
                        item_text = _get_text_from_md_inline(tokens[inline_idx].children).strip()
                elif next_token.type == 'inline' and next_token.level == li_level +1 :
                    item_text = _get_text_from_md_inline(next_token.children).strip()
            
            if list_level_stack:
                list_info = list_level_stack[-1]
                item_num_val = None
                if list_info["ordered"]:
                    item_num_val = list_info["current_num"]
                    list_info["current_num"] += 1
                
                if _PYDANTIC_MODELS_AVAILABLE_PARSERS:
                    elements.append(ListItemElement(
                        text=item_text, level=token.level, ordered=list_info["ordered"],
                        item_number=str(item_num_val) if item_num_val is not None else None))
                else:
                    elements.append({"element_type": "list_item", "text": item_text, "level":token.level, 
                                     "ordered":list_info["ordered"], "item_number":str(item_num_val) if item_num_val is not None else None})

            temp_idx = idx + 1; nesting_count = 0
            while temp_idx < len(tokens):
                if tokens[temp_idx].type == 'list_item_open' and tokens[temp_idx].level == li_level:
                    if nesting_count == 0: idx = temp_idx; break
                if tokens[temp_idx].type == 'list_item_open': nesting_count +=1
                if tokens[temp_idx].type == 'list_item_close':
                    if nesting_count == 0 and tokens[temp_idx].level == li_level: idx = temp_idx + 1; break
                    nesting_count -=1
                temp_idx += 1
            else: idx = temp_idx
            continue

        elif token.type in ['bullet_list_close', 'ordered_list_close']:
            if list_level_stack: list_level_stack.pop()
            idx += 1
            continue

        elif token.type == 'table_open':
            header_content = []; body_rows_cells = []; current_row_cells = []; in_thead = False
            temp_idx = idx + 1
            while temp_idx < len(tokens) and tokens[temp_idx].type != 'table_close':
                t_token = tokens[temp_idx]
                if t_token.type == 'thead_open': in_thead = True
                elif t_token.type == 'thead_close': in_thead = False
                elif t_token.type == 'tr_open': current_row_cells = []
                elif t_token.type in ['th_open', 'td_open']:
                    content_idx = temp_idx + 1
                    if content_idx < len(tokens) and tokens[content_idx].type == 'inline':
                        current_row_cells.append(_get_text_from_md_inline(tokens[content_idx].children).strip())
                elif t_token.type == 'tr_close':
                    if current_row_cells:
                        if in_thead or (not header_content and not body_rows_cells): header_content.append(list(current_row_cells))
                        else: body_rows_cells.append(list(current_row_cells))
                temp_idx += 1
            md_table_str = ""
            if header_content:
                md_table_str += "| " + " | ".join(header_content[0]) + " |\n"
                md_table_str += "| " + " | ".join(["---"] * len(header_content[0])) + " |\n"
            for row_data_list in body_rows_cells: md_table_str += "| " + " | ".join(row_data_list) + " |\n"
            if _PYDANTIC_MODELS_AVAILABLE_PARSERS: elements.append(TableElement(markdown_representation=md_table_str.strip()))
            else: elements.append({"element_type": "table", "markdown_representation": md_table_str.strip()})
            idx = temp_idx + 1 
            continue

        elif token.type == 'fence' or token.type == 'code_block':
            code_content = token.content.strip(); lang = token.info.strip() if token.info else None
            if _PYDANTIC_MODELS_AVAILABLE_PARSERS: elements.append(CodeBlockElement(code=code_content, language=lang))
            else: elements.append({"element_type": "code_block", "code": code_content, "language": lang})
            idx += 1
            continue
        
        elif token.type == 'hr':
            if _PYDANTIC_MODELS_AVAILABLE_PARSERS: elements.append(PageBreakElement())
            else: elements.append({"element_type": "page_break"})
            idx += 1
            continue
        
        elif token.type == 'blockquote_open':
            blockquote_text_parts = []; temp_idx = idx + 1; start_level = token.level
            while temp_idx < len(tokens):
                bq_token = tokens[temp_idx]
                if bq_token.type == 'blockquote_close' and bq_token.level == start_level: idx = temp_idx; break
                if bq_token.type == 'paragraph_open':
                    para_content_idx = temp_idx + 1
                    if para_content_idx < len(tokens) and tokens[para_content_idx].type == 'inline':
                        blockquote_text_parts.append(_get_text_from_md_inline(tokens[para_content_idx].children).strip())
                    temp_idx = para_content_idx + 1 
                    if temp_idx < len(tokens) and tokens[temp_idx].type == 'paragraph_close': temp_idx +=1
                    else: temp_idx -=1
                temp_idx +=1
            else: idx = temp_idx
            if blockquote_text_parts:
                full_text = "\n".join(blockquote_text_parts)
                if _PYDANTIC_MODELS_AVAILABLE_PARSERS: elements.append(NarrativeTextElement(text=full_text)) 
                else: elements.append({"element_type": "narrative_text", "text": full_text, "_is_blockquote": True})
            idx +=1
            continue
        idx += 1 
    return elements

def _generate_parsed_text_from_elements_internal(elements: List[Any]) -> str:
    # (这里是 generate_parsed_text_from_elements 函数的完整代码)
    # (确保它在 _PYDANTIC_MODELS_AVAILABLE_PARSERS 为True时能处理Pydantic实例，否则处理字典)
    text_parts = []
    for el_data_any in elements:
        el_data = {}
        if _PYDANTIC_MODELS_AVAILABLE_PARSERS and hasattr(el_data_any, 'model_dump'):
            el_data = el_data_any.model_dump() 
        elif isinstance(el_data_any, dict):
            el_data = el_data_any
        else: continue

        el_type = el_data.get("element_type")
        if el_type == "title": text_parts.append(f"\n{'#' * el_data.get('level',1)} {el_data.get('text','')}\n")
        elif el_type == "narrative_text": text_parts.append(el_data.get('text','') + "\n")
        elif el_type == "list_item":
            prefix = f"{el_data.get('item_number','')}. " if el_data.get('ordered') and el_data.get('item_number') else "- "
            indent = "  " * el_data.get('level',0)
            text_parts.append(f"{indent}{prefix}{el_data.get('text','')}\n")
        elif el_type == "table":
            if el_data.get('markdown_representation'): text_parts.append(f"\n[Table: {el_data.get('caption','Unnamed Table')}]\n{el_data['markdown_representation']}\n")
            elif el_data.get('text_representation'): text_parts.append(f"\n[Table: {el_data.get('caption','Unnamed Table')}]\n{el_data['text_representation']}\n")
        elif el_type == "code_block":
            lang = el_data.get('language', "") or ""
            text_parts.append(f"\n```{lang}\n{el_data.get('code','')}\n```\n")
        elif el_type == "page_break": text_parts.append("\n---\n")
        text_parts.append("\n") 
    return "".join(text_parts).strip().replace("\n\n\n", "\n\n").replace("\n\n\n", "\n\n")

def _convert_unstructured_to_pydantic(elements: List[UnstructuredElement]) -> List[DocumentElementType]:  # type: ignore
    """Converts a list of Unstructured elements to our internal Pydantic models."""
    pydantic_elements = []
    for el in elements:
        meta = DocumentElementMetadata(page_number=getattr(el.metadata, 'page_number', None))
        
        if isinstance(el, Title):
            pydantic_elements.append(TitleElement(text=el.text, level=getattr(el.metadata, 'category_depth', 1), metadata=meta))
        elif isinstance(el, NarrativeText):
            pydantic_elements.append(NarrativeTextElement(text=el.text, metadata=meta))
        elif isinstance(el, ListItem):
            pydantic_elements.append(ListItemElement(text=el.text, metadata=meta))
        elif isinstance(el, Table):
            # Unstructured v0.12+ has built-in markdown conversion
            pydantic_elements.append(TableElement(markdown_representation=getattr(el, 'text_as_html', str(el)), metadata=meta))
        elif isinstance(el, (Header, Footer, Image)):
             # We can choose to ignore headers, footers, and images for now
             continue
        else:
            # Fallback for any other element types
            if el.text.strip():
                 pydantic_elements.append(NarrativeTextElement(text=el.text, metadata=meta))
                 
    return pydantic_elements


def parse_markdown_to_structured_output(md_content_str: str, original_metadata: Dict[str, Any]) -> Optional[ParsedDocumentOutput]:
    """
    Top-level function to parse markdown string and return ParsedDocumentOutput.
    """
    logger.info(f"Parsing Markdown content (length: {len(md_content_str)} chars)...")
    try:
        md_parser = MarkdownIt("commonmark").enable("table") # Removed "breaks":True based on last log
        tokens = md_parser.parse(md_content_str)
        
        structured_elements = _convert_md_tokens_to_elements_internal(tokens)
        linear_text = _generate_parsed_text_from_elements_internal(structured_elements)

        if _PYDANTIC_MODELS_AVAILABLE_PARSERS:
            return ParsedDocumentOutput(
                parsed_text=linear_text,
                elements=structured_elements,
                original_metadata=original_metadata
            )
        else: # Fallback if Pydantic models aren't available (e.g. PoC context)
            return {
                 "parsed_text": linear_text,
                 "elements": structured_elements,
                 "original_metadata": original_metadata
            } # type: ignore 
    except Exception as e:
        logger.error(f"Error in parse_markdown_to_structured_output: {e}", exc_info=True)
        return None

# --- Placeholder for other parsers ---
def parse_docx_to_structured_output(file_path: str, original_metadata: Dict[str, Any]) -> Optional[ParsedDocumentOutput]:
    """
    Parses a DOCX file using Unstructured.io, extracts rich metadata,
    and converts elements to our internal Pydantic models.
    """
    if not _UNSTRUCTURED_AVAILABLE:
        logger.error("Unstructured library is not available. Cannot parse DOCX files.")
        return None
        
    logger.info(f"Parsing DOCX with Unstructured: {file_path}")
    
    try:
        # 使用 Unstructured 解析文档，设置策略以获取更干净的数据
        unstructured_elements = partition_docx(
            filename=file_path,
            strategy="hi_res",  # 使用高分辨率策略以更好地处理布局
            infer_table_structure=True, # 开启表格结构推断
        )
        
        # --- 关键：合并 Unstructured 提取的元数据 ---
        # partition_docx 返回的第一个元素通常包含文档级别的元数据
        doc_level_meta = {}
        if unstructured_elements:
             # Unstructured v0.12+ 将元数据附加到每个元素上
             # 我们从第一个元素获取通用元数据
             doc_level_meta = unstructured_elements[0].metadata.to_dict()

        # 将 Unstructured 的元数据与我们传入的原始元数据合并
        # Unstructured 的元数据优先级更高，因为它更具体
        combined_metadata = {**original_metadata, **doc_level_meta}
        # 移除一些不需要的内部键
        combined_metadata.pop('parent_id', None)
        combined_metadata.pop('category_depth', None)

        # 将 Unstructured 元素转换为我们自己的 Pydantic 模型
        pydantic_elements = _convert_unstructured_to_pydantic(unstructured_elements)
        
        if not pydantic_elements:
            logger.warning(f"No content elements were extracted from DOCX file: {file_path}")
            return None

        # 从转换后的元素生成线性文本表示
        linear_text = _generate_parsed_text_from_elements_internal(pydantic_elements)

        if _PYDANTIC_MODELS_AVAILABLE_PARSERS:
            return ParsedDocumentOutput(
                parsed_text=linear_text,
                elements=pydantic_elements,
                original_metadata=combined_metadata # <--- 使用合并后的元数据
            )
        else:
            # Fallback for non-pydantic environment (should not happen in production)
            return {
                "parsed_text": linear_text,
                "elements": pydantic_elements,
                "original_metadata": combined_metadata
            } # type: ignore
            
    except Exception as e:
        logger.error(f"Error parsing DOCX file '{file_path}' with Unstructured: {e}", exc_info=True)
        return None
    
def parse_pdf_to_structured_output(file_path: str, original_metadata: Dict[str, Any]) -> Optional[ParsedDocumentOutput]:
    logger.info(f"Parsing PDF: {file_path} (Not yet fully implemented in document_parsers.py)")
    # Here you would integrate the PyMuPDF logic from your PoC
    text_content = f"[Placeholder: PDF content for {os.path.basename(file_path)}]"
    elements = []
    if _PYDANTIC_MODELS_AVAILABLE_PARSERS:
        elements.append(NarrativeTextElement(text=text_content))
        return ParsedDocumentOutput(parsed_text=text_content, elements=elements, original_metadata=original_metadata)
    else:
        elements.append({"element_type":"narrative_text", "text":text_content})
        return {"parsed_text":text_content, "elements":elements, "original_metadata":original_metadata} # type: ignore

def parse_xlsx_to_structured_output(file_path: str, original_metadata: Dict[str, Any]) -> Optional[ParsedDocumentOutput]:
    """
    Parses an XLSX file using pandas, converts each sheet to a Markdown table,
    and enriches metadata with filename and sheet name.
    """
    logger.info(f"Parsing XLSX with pandas: {file_path}")
    
    try:
        xls = pd.ExcelFile(file_path)
        all_elements: List[DocumentElementType] = [] # type: ignore
        full_text_representation = ""

        # --- 核心修正：确保filename在元数据中 ---
        # 无论上游是否提供，我们在这里都以文件路径为准，强制添加/覆盖
        base_metadata = original_metadata.copy()
        base_metadata['filename'] = os.path.basename(file_path)

        for sheet_name in xls.sheet_names:
            try:
                df = pd.read_excel(xls, sheet_name=sheet_name)

                # --- 新增：在处理前，清洗所有字符串类型单元格的前后空格 ---
                for col in df.columns:
                    if df[col].dtype == 'object':
                        df[col] = df[col].str.strip()
                # --- 新增结束 ---

                # 过滤掉完全为空的行和列，避免无效的Markdown输出
                df.dropna(how='all', axis=0, inplace=True)
                df.dropna(how='all', axis=1, inplace=True)

                if df.empty:
                    logger.info(f"  Skipping empty sheet: {sheet_name}")
                    continue
                
                # 将DataFrame转换为Markdown格式的字符串
                markdown_table = df.to_markdown(index=False)
                
                # 为每个工作表创建一个标题和一个表格元素
                sheet_title_text = f"Sheet: {sheet_name}"
                full_text_representation += f"## {sheet_title_text}\n\n{markdown_table}\n\n"
                
                # 为工作表标题创建元素
                all_elements.append(TitleElement(
                    text=sheet_title_text,
                    level=2,
                    metadata=DocumentElementMetadata(page_number=xls.sheet_names.index(sheet_name) + 1)
                ))
                
                # 为表格本身创建元素
                sheet_meta = base_metadata.copy()
                sheet_meta['sheet_name'] = sheet_name
                
                all_elements.append(TableElement(
                    markdown_representation=markdown_table,
                    metadata=DocumentElementMetadata(**sheet_meta) # 传递特定于工作表的元数据
                ))
                logger.info(f"  Successfully parsed sheet: {sheet_name}")

            except Exception as e_sheet:
                logger.error(f"  Failed to parse sheet '{sheet_name}' in file '{file_path}': {e_sheet}", exc_info=True)
                continue

        if not all_elements:
            logger.warning(f"No data parsed from any sheet in XLSX file: {file_path}")
            return None
        
        # 返回包含所有工作表内容的单个 ParsedDocumentOutput
        return ParsedDocumentOutput(
            parsed_text=full_text_representation,
            elements=all_elements,
            original_metadata=base_metadata # 返回包含正确filename的文档级元数据
        )

    except Exception as e:
        logger.error(f"Error parsing XLSX file '{file_path}': {e}", exc_info=True)
        return None

def parse_html_to_structured_output(html_content_str: str, original_metadata: Dict[str, Any]) -> Optional[ParsedDocumentOutput]:
    """
    Parses an HTML string using BeautifulSoup, extracts meaningful elements,
    and converts them to our internal Pydantic models.
    """
    logger.info(f"Parsing HTML content with BeautifulSoup (length: {len(html_content_str)} chars)...")
    if not html_content_str.strip():
        return None

    try:
        soup = BeautifulSoup(html_content_str, 'lxml')
        
        for script_or_style in soup(["script", "style"]):
            script_or_style.decompose()

        elements: List[DocumentElementType] = [] # type: ignore
        
        for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'div', 'li', 'pre']):
            text = tag.get_text(separator=' ', strip=True)
            if text:
                if tag.name.startswith('h'):
                    level = int(tag.name[1:])
                    elements.append(TitleElement(text=text, level=level))
                elif tag.name == 'li':
                    elements.append(ListItemElement(text=text))
                elif tag.name == 'pre':
                    elements.append(CodeBlockElement(code=text))
                else:
                    elements.append(NarrativeTextElement(text=text))
        
        for table in soup.find_all('table'):
            header = [th.get_text(strip=True) for th in table.find_all('th')]
            rows = []
            for tr in table.find_all('tr'):
                cells = [td.get_text(strip=True) for td in tr.find_all('td')]
                if cells:
                    rows.append(cells)
            
            if header or rows:
                md_table_parts = []
                if header:
                    md_table_parts.append(f"| {' | '.join(header)} |")
                    md_table_parts.append(f"|{'---|' * len(header)}")
                for row in rows:
                    md_table_parts.append(f"| {' | '.join(row)} |")
                
                elements.append(TableElement(markdown_representation='\n'.join(md_table_parts)))

        if not elements:
            logger.warning("No structured elements found in HTML content after parsing.")
            return None

        linear_text = _generate_parsed_text_from_elements_internal(elements)

        return ParsedDocumentOutput(
            parsed_text=linear_text,
            elements=elements,
            original_metadata=original_metadata
        )
    except Exception as e:
        logger.error(f"Error parsing HTML content with BeautifulSoup: {e}", exc_info=True)
        return None
```

    |-- evaluation_assets.py

``` py
import dagster as dg
import os
from typing import Dict, List, Any # Optional 可能之后会用到

# 从项目中导入我们重构的批量评估函数和相关工具/常量
from zhz_rag.evaluation.batch_eval_cypher import run_cypher_batch_evaluation
from zhz_rag.evaluation.batch_eval_answer import run_answer_batch_evaluation
from zhz_rag.evaluation.analyze_cypher import perform_cypher_evaluation_analysis
from zhz_rag.evaluation.analyze_answer import perform_answer_evaluation_analysis
from zhz_rag.utils.common_utils import (
    find_latest_rag_interaction_log,
    RAG_INTERACTION_LOGS_DIR,
    EVALUATION_RESULTS_LOGS_DIR,
    get_evaluation_result_log_filepath
)
# 导入 GeminiAPIResource 以声明资源依赖
from zhz_rag_pipeline_dagster.zhz_rag_pipeline.resources import GeminiAPIResource

# --- 资产定义 ---

@dg.asset(
    name="latest_rag_interaction_log_for_evaluation",
    description="Provides the filepath of the latest RAG interaction log to be used for evaluation.",
    group_name="evaluation_pipeline",
    compute_kind="python" # 可选，指明计算类型
)
def latest_rag_interaction_log_for_evaluation_asset(context: dg.AssetExecutionContext) -> str:
    """
    Finds and returns the path to the latest RAG interaction log file.
    """
    log_filepath = find_latest_rag_interaction_log(RAG_INTERACTION_LOGS_DIR)
    if not log_filepath or not os.path.exists(log_filepath):
        error_msg = f"No RAG interaction log file found in directory: {RAG_INTERACTION_LOGS_DIR}"
        context.log.error(error_msg)
        raise dg.Failure(description=error_msg)
    
    context.log.info(f"Using RAG interaction log for evaluation: {log_filepath}")
    context.add_output_metadata({"log_filepath": log_filepath, "filename": os.path.basename(log_filepath)})
    return log_filepath

@dg.asset(
    name="batch_cypher_evaluations_log", # 资产名称最好能反映它产出的是日志文件
    description="Runs batch evaluation of Cypher queries and produces an evaluation log file.",
    group_name="evaluation_pipeline",
    compute_kind="python",
    # deps=[latest_rag_interaction_log_for_evaluation_asset] # 通过函数参数自动推断依赖
)
async def batch_cypher_evaluation_log_asset(
    context: dg.AssetExecutionContext,
    gemini_api: GeminiAPIResource,
    latest_rag_interaction_log_for_evaluation: str # <--- 修改参数名
) -> dg.Output[str]:
    context.log.info(f"Starting batch Cypher evaluation using log file: {latest_rag_interaction_log_for_evaluation}") # <--- 使用新参数名
    
    # 从 Dagster 配置中获取参数，或使用默认/环境变量
    # 这里我们先用之前脚本中的方式，未来可以转为 Dagster run_config
    app_version = os.getenv("APP_VERSION_TAG", "dagster_cypher_eval_0.2")
    # 对于 use_simulated_api，在 Dagster 中通常会通过资源配置或 op_config 来控制，
    # 而不是直接依赖环境变量，这样更灵活。但为了保持与脚本一致，暂时保留。
    use_simulated = os.getenv("USE_SIMULATED_GEMINI_CYPHER_EVAL", "false").lower() == "true"
    api_delay = float(os.getenv("GEMINI_API_CALL_DELAY_SECONDS", "4.1"))

    if use_simulated:
        context.log.warning("Cypher evaluation asset is using SIMULATED Gemini API calls.")

    # 调用我们重构的、现在接受 gemini_resource 的批量评估函数
    eval_stats = await run_cypher_batch_evaluation(
        gemini_resource_for_evaluator=gemini_api, # 传递注入的 Dagster 资源
        rag_interaction_log_filepath=latest_rag_interaction_log_for_evaluation,
        app_version=app_version,
        use_simulated_api=use_simulated, # 这个参数现在由 run_cypher_batch_evaluation 内部处理
        api_call_delay=api_delay
    )
    context.log.info(f"Batch Cypher evaluation completed. Statistics: {eval_stats}")

    # 确定输出的评估结果日志文件名 (与 evaluator.py 中一致)
    output_log_filepath = get_evaluation_result_log_filepath(evaluation_name="cypher_gemini_flash")
    
    # 确保目录存在 (get_evaluation_result_log_filepath 内部的 log_interaction_data 会处理)
    # 但这里我们也可以提前确保，或者依赖 log_interaction_data
    os.makedirs(os.path.dirname(output_log_filepath), exist_ok=True)
            
    metadata = {"evaluation_stats": eval_stats, "output_filepath": output_log_filepath}
    if eval_stats.get("cypher_queries_evaluated", 0) == 0:
        metadata["warning"] = "No Cypher queries were evaluated. Output log might be empty."
        context.log.warning(metadata["warning"])

    return dg.Output(output_log_filepath, metadata=metadata)


@dg.asset(
    name="batch_answer_evaluations_log", # 资产名称
    description="Runs batch evaluation of generated answers from RAG logs using Gemini.",
    group_name="evaluation_pipeline",
    compute_kind="python",
    # deps=[latest_rag_interaction_log_for_evaluation_asset] # 通过函数参数自动推断依赖
)
async def batch_answer_evaluation_log_asset(
    context: dg.AssetExecutionContext,
    gemini_api: GeminiAPIResource,
    latest_rag_interaction_log_for_evaluation: str # <--- 修改参数名
) -> dg.Output[str]:
    context.log.info(f"Starting batch Answer evaluation using log file: {latest_rag_interaction_log_for_evaluation}") # <--- 使用新参数名
    app_version = os.getenv("APP_VERSION_TAG", "dagster_answer_eval_0.2")
    use_simulated = os.getenv("USE_SIMULATED_GEMINI_ANSWER_EVAL", "false").lower() == "true"
    api_delay = float(os.getenv("GEMINI_API_CALL_DELAY_SECONDS", "4.1"))

    if use_simulated:
        context.log.warning("Answer evaluation asset is using SIMULATED Gemini API calls.")

    eval_stats = await run_answer_batch_evaluation(
        gemini_resource_for_evaluator=gemini_api, # 传递注入的 Dagster 资源
        rag_interaction_log_filepath=latest_rag_interaction_log_for_evaluation,
        app_version=app_version,
        use_simulated_api=use_simulated, # 这个参数现在由 run_answer_batch_evaluation 内部处理
        api_call_delay=api_delay
    )
    context.log.info(f"Batch Answer evaluation completed. Statistics: {eval_stats}")

    output_log_filepath = get_evaluation_result_log_filepath(evaluation_name="answer_gemini_flash")
    os.makedirs(os.path.dirname(output_log_filepath), exist_ok=True)

    metadata = {"evaluation_stats": eval_stats, "output_filepath": output_log_filepath}
    if eval_stats.get("answers_evaluated", 0) == 0:
        metadata["warning"] = "No answers were evaluated. Output log might be empty."
        context.log.warning(metadata["warning"])
        
    return dg.Output(output_log_filepath, metadata=metadata)

@dg.asset(
    name="cypher_evaluation_analysis_report", # 资产名称
    description="Generates a CSV analysis report from Cypher evaluation results.",
    group_name="evaluation_pipeline",
    compute_kind="python",
    # deps=[batch_cypher_evaluation_log_asset] # 通过函数参数自动推断依赖
)
def cypher_analysis_report_asset(
    context: dg.AssetExecutionContext,
    batch_cypher_evaluations_log: str # 上游资产的输出 (即 cypher 评估日志文件的路径)
) -> dg.Output[str]: # 输出 CSV 报告文件的路径
    """
    Analyzes Cypher evaluation logs and produces a CSV report.
    """
    context.log.info(f"Starting Cypher evaluation analysis using log file: {batch_cypher_evaluations_log}")

    if not os.path.exists(batch_cypher_evaluations_log):
        error_msg = f"Input Cypher evaluation log file not found: {batch_cypher_evaluations_log}"
        context.log.error(error_msg)
        raise dg.Failure(description=error_msg)

    # 构建输出CSV文件的路径
    # 我们希望CSV文件也存储在 EVALUATION_RESULTS_LOGS_DIR 目录下
    # 文件名可以基于输入日志名或固定一个模式
    base_input_log_name = os.path.basename(batch_cypher_evaluations_log)
    # 从 "eval_results_cypher_gemini_flash_YYYYMMDD.jsonl" 生成 "analysis_cypher_gemini_flash_YYYYMMDD.csv"
    if base_input_log_name.startswith("eval_results_") and base_input_log_name.endswith(".jsonl"):
        analysis_file_name = "analysis_" + base_input_log_name[len("eval_results_"):-len(".jsonl")] + ".csv"
    else: # Fallback naming
        analysis_file_name = f"analysis_cypher_report_{context.run_id[:8]}.csv"
    
    output_csv_filepath = os.path.join(EVALUATION_RESULTS_LOGS_DIR, analysis_file_name)
    
    success = perform_cypher_evaluation_analysis(
        evaluation_log_filepath=batch_cypher_evaluations_log,
        output_csv_filepath=output_csv_filepath
    )

    if success:
        context.log.info(f"Cypher evaluation analysis report generated: {output_csv_filepath}")
        return dg.Output(output_csv_filepath, metadata={"output_csv_filepath": output_csv_filepath, "source_log": base_input_log_name})
    else:
        error_msg = f"Cypher evaluation analysis failed for log file: {batch_cypher_evaluations_log}"
        context.log.error(error_msg)
        raise dg.Failure(description=error_msg)


@dg.asset(
    name="answer_evaluation_analysis_report", # 资产名称
    description="Generates a CSV analysis report from Answer evaluation results.",
    group_name="evaluation_pipeline",
    compute_kind="python",
    # deps=[batch_answer_evaluations_log_asset] # 通过函数参数自动推断依赖
)
def answer_analysis_report_asset(
    context: dg.AssetExecutionContext,
    batch_answer_evaluations_log: str # 上游资产的输出 (即 answer 评估日志文件的路径)
) -> dg.Output[str]: # 输出 CSV 报告文件的路径
    """
    Analyzes Answer evaluation logs and produces a CSV report.
    """
    context.log.info(f"Starting Answer evaluation analysis using log file: {batch_answer_evaluations_log}")

    if not os.path.exists(batch_answer_evaluations_log):
        error_msg = f"Input Answer evaluation log file not found: {batch_answer_evaluations_log}"
        context.log.error(error_msg)
        raise dg.Failure(description=error_msg)

    base_input_log_name = os.path.basename(batch_answer_evaluations_log)
    if base_input_log_name.startswith("eval_results_") and base_input_log_name.endswith(".jsonl"):
        analysis_file_name = "analysis_" + base_input_log_name[len("eval_results_"):-len(".jsonl")] + ".csv"
    else: # Fallback naming
        analysis_file_name = f"analysis_answer_report_{context.run_id[:8]}.csv"
        
    output_csv_filepath = os.path.join(EVALUATION_RESULTS_LOGS_DIR, analysis_file_name)

    success = perform_answer_evaluation_analysis(
        evaluation_log_filepath=batch_answer_evaluations_log,
        output_csv_filepath=output_csv_filepath
    )

    if success:
        context.log.info(f"Answer evaluation analysis report generated: {output_csv_filepath}")
        return dg.Output(output_csv_filepath, metadata={"output_csv_filepath": output_csv_filepath, "source_log": base_input_log_name})
    else:
        error_msg = f"Answer evaluation analysis failed for log file: {batch_answer_evaluations_log}"
        context.log.error(error_msg)
        raise dg.Failure(description=error_msg)

# 将所有评估相关的资产收集到一个列表中，方便在 definitions.py 中引用
all_evaluation_assets = [
    latest_rag_interaction_log_for_evaluation_asset,
    batch_cypher_evaluation_log_asset,
    batch_answer_evaluation_log_asset,
    cypher_analysis_report_asset, # <--- 新增
    answer_analysis_report_asset, # <--- 新增
]
```

    |-- ingestion_assets.py

``` py
# zhz_rag_pipeline/ingestion_assets.py
import dagster as dg
import os
from typing import List, Dict, Any, Union, Optional
from datetime import datetime, timezone

# --- 修改：导入分发器并设置Pydantic可用性标志 ---
# 尝试导入Pydantic模型，并设置一个标志，以便在模型不可用时代码可以优雅地降级。
try:
    from .pydantic_models_dagster import LoadedDocumentOutput, ParsedDocumentOutput, NarrativeTextElement
    _PYDANTIC_AVAILABLE = True
except ImportError:
    LoadedDocumentOutput = dict  # type: ignore
    ParsedDocumentOutput = dict  # type: ignore
    NarrativeTextElement = dict  # type: ignore
    _PYDANTIC_AVAILABLE = False

from .parsers import dispatch_parsing # <--- 修改导入路径
# --- 修改结束 ---

class LoadDocumentsConfig(dg.Config):
    documents_directory: str = "/home/zhz/zhz_agent/data/raw_documents/" # 更新后的原始文档目录
    allowed_extensions: List[str] = [".txt", ".md", ".docx", ".pdf", ".xlsx", ".html", ".htm"] # 扩大允许范围以测试所有解析器

@dg.asset(
    name="raw_documents",
    description="Loads raw documents from a specified directory.",
    group_name="ingestion"
)
def load_documents_asset(
    context: dg.AssetExecutionContext, 
    config: LoadDocumentsConfig
) -> List[LoadedDocumentOutput]:  # type: ignore
    
    loaded_docs: List[LoadedDocumentOutput] = []  # type: ignore
    target_directory = config.documents_directory
    allowed_exts = tuple(config.allowed_extensions) 

    context.log.info(f"Scanning directory: {target_directory} for files with extensions: {allowed_exts}")

    if not os.path.isdir(target_directory):
        context.log.error(f"Directory not found: {target_directory}")
        return loaded_docs

    for filename in os.listdir(target_directory):
        file_path = os.path.join(target_directory, filename)
        if os.path.isfile(file_path):
            file_name_lower = filename.lower()
            file_extension = os.path.splitext(file_name_lower)[1]

            if file_extension in allowed_exts:
                context.log.info(f"Found matching file: {file_path}")
                try:
                    file_stat = os.stat(file_path)
                    
                    # --- VITAL FIX: Do not pass raw_content ---
                    # The downstream parser will handle reading the file from the path.
                    doc_output_data = {
                        "document_path": file_path,
                        "file_type": file_extension,
                        # raw_content is intentionally omitted
                        "metadata": {
                            "filename": filename,
                            "source_directory": target_directory,
                            "size_bytes": file_stat.st_size,
                            "creation_time_utc": datetime.fromtimestamp(file_stat.st_ctime, tz=timezone.utc).isoformat(),
                            "modified_time_utc": datetime.fromtimestamp(file_stat.st_mtime, tz=timezone.utc).isoformat()
                        }
                    }

                    if _PYDANTIC_AVAILABLE:
                        loaded_docs.append(LoadedDocumentOutput(**doc_output_data))
                    else:
                        loaded_docs.append(doc_output_data)

                    context.log.info(f"Successfully created LoadedDocumentOutput for: {file_path}")
                except Exception as e:
                    context.log.error(f"Failed to process file {file_path}: {e}")
            else:
                context.log.debug(f"Skipping file with non-allowed extension: {file_path}")
        else:
            context.log.debug(f"Skipping non-file item: {file_path}")
            
    if not loaded_docs:
        context.log.warning(f"No matching documents found in {target_directory}")

    if loaded_docs:
        first_doc_path = loaded_docs[0].document_path if _PYDANTIC_AVAILABLE and loaded_docs else "N/A"
        context.add_output_metadata(
            metadata={
                "num_documents_loaded": len(loaded_docs),
                "first_document_path": first_doc_path
            }
        )
    return loaded_docs



@dg.asset(
    name="parsed_documents",
    description="Parses loaded documents into text and extracts basic structure using a dispatcher.",
    group_name="ingestion"
)
def parse_document_asset(
    context: dg.AssetExecutionContext,
    raw_documents: List[LoadedDocumentOutput] # type: ignore
) -> List[ParsedDocumentOutput]: # type: ignore
    
    parsed_docs_output_list: List[ParsedDocumentOutput] = [] # type: ignore
    context.log.info(f"Received {len(raw_documents)} documents to parse.")

    for doc_input in raw_documents:
        doc_path = doc_input.document_path
        file_ext = doc_input.file_type.lower()
        original_metadata = doc_input.metadata.copy()
        original_metadata["source_file_path"] = doc_path

        context.log.info(f"Attempting to parse document: {doc_path} (Type: {file_ext})")

        try:
            # --- VITAL REFACTOR ---
            # 直接将文件路径传递给解析器，不再处理字节内容
            # 对于文本文件，解析器内部自己会用 'rt' 模式读取
            # 对于二进制文件(pdf, docx, xlsx)，解析器会用 'rb' 模式或相应库读取
            parsed_output = dispatch_parsing(file_ext, doc_path, original_metadata)

            if not parsed_output:
                context.log.warning(f"Parser for '{file_ext}' returned no output for {doc_path}. Creating a fallback.")
                fallback_text = f"[Content Not Parsed by Specific Parser: {doc_path}]"
                elements = [NarrativeTextElement(text=fallback_text)]
                parsed_output = ParsedDocumentOutput(
                    parsed_text=fallback_text,
                    elements=elements,
                    original_metadata=original_metadata
                )

            # 确保输出总是 Pydantic 模型
            if isinstance(parsed_output, dict):
                parsed_output = ParsedDocumentOutput(**parsed_output)
            
            parsed_docs_output_list.append(parsed_output)
            context.log.info(f"Successfully processed: {doc_path}")

        except Exception as e:
            context.log.error(f"Critical error during parsing asset for {doc_path}: {e}", exc_info=True)
            error_text = f"[Critical Parsing Exception for {doc_path}: {str(e)}]"
            elements = [NarrativeTextElement(text=error_text)]
            error_output = ParsedDocumentOutput(
                parsed_text=error_text,
                elements=elements,
                original_metadata=original_metadata
            )
            parsed_docs_output_list.append(error_output)

    if parsed_docs_output_list:
        context.add_output_metadata(
            metadata={
                "num_documents_processed_for_parsing": len(raw_documents),
                "num_parsed_document_outputs_generated": len(parsed_docs_output_list),
            }
        )
    return parsed_docs_output_list



all_ingestion_assets = [load_documents_asset, parse_document_asset]
```

    |-- processing_assets.py

``` py
#  文件: zhz_rag_pipeline_dagster/zhz_rag_pipeline/processing_assets.py

import json
import asyncio
import re
import dagster as dg
from typing import List, Dict, Any, Optional, Union
import uuid
from langchain_text_splitters import RecursiveCharacterTextSplitter
import hashlib
import pandas as pd
from zhz_rag.utils.common_utils import normalize_text_for_id
from zhz_rag_pipeline_dagster.zhz_rag_pipeline.pydantic_models_dagster import (
    ChunkOutput,
    ParsedDocumentOutput,
    EmbeddingOutput,
    KGTripleSetOutput,
    ExtractedEntity,
    ExtractedRelation,
    # --- 添加导入我们需要的元素类型 ---
    TitleElement,
    NarrativeTextElement,
    ListItemElement,
    TableElement,
    CodeBlockElement,
    ImageElement,
    PageBreakElement,
    HeaderElement,
    FooterElement,
    DocumentElementMetadata
    # --- 结束添加 ---
)
from zhz_rag_pipeline_dagster.zhz_rag_pipeline.resources import (
    GGUFEmbeddingResource,
    ChromaDBResource,
    DuckDBResource,
    LocalLLMAPIResource,
    SystemResource  # <--- 添加这一行以导入 SystemResource
)
import jieba
import bm25s
import pickle
import numpy as np
import os
from zhz_rag.utils.common_utils import normalize_text_for_id

_PYDANTIC_AVAILABLE = False
try:
    from .pydantic_models_dagster import ( # 使用相对导入
        ChunkOutput,
        ParsedDocumentOutput,
        EmbeddingOutput,
        KGTripleSetOutput,
        ExtractedEntity,
        ExtractedRelation,
        TitleElement,
        NarrativeTextElement,
        ListItemElement,
        TableElement,
        CodeBlockElement,
        ImageElement,
        PageBreakElement,
        HeaderElement,
        FooterElement,
        DocumentElementMetadata # <--- 确保这里导入了 DocumentElementMetadata
    )
    _PYDANTIC_AVAILABLE = True
    # 如果 Pydantic 可用，我们也可以直接从模型中获取 DocumentElementType
    # from .pydantic_models_dagster import DocumentElementType # 如果需要更精确的类型提示
except ImportError:
    # 定义占位符
    class BaseModel: pass
    class ChunkOutput(BaseModel): pass
    class ParsedDocumentOutput(BaseModel): pass
    class EmbeddingOutput(BaseModel): pass
    class KGTripleSetOutput(BaseModel): pass
    class ExtractedEntity(BaseModel): pass
    class ExtractedRelation(BaseModel): pass
    class TitleElement(BaseModel): pass
    class NarrativeTextElement(BaseModel): pass
    class ListItemElement(BaseModel): pass
    class TableElement(BaseModel): pass
    class CodeBlockElement(BaseModel): pass
    class ImageElement(BaseModel): pass
    class PageBreakElement(BaseModel): pass
    class HeaderElement(BaseModel): pass
    class FooterElement(BaseModel): pass
    class DocumentElementMetadata(BaseModel): pass # <--- 定义占位符
    DocumentElementType = Any # type: ignore
# --- 结束 Pydantic 模型导入 ---

import logging
logger = logging.getLogger(__name__)
if not logger.handlers:
    handler = logging.StreamHandler()
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    logger.setLevel(logging.DEBUG) # <--- 确保是 DEBUG
    logger.info(f"Logger for {__name__} (processing_assets) configured with DEBUG level.")


class TextChunkerConfig(dg.Config):
    chunk_size: int = 1000 
    chunk_overlap: int = 100
    max_element_text_length_before_split: int = 1200 # 一个1200字符的段落如果语义连贯，可以考虑不切分。
    target_sentence_split_chunk_size: int = 600    # 略微增大子块的目标大小，使其包含更多上下文。
    sentence_split_chunk_overlap_sentences: int = 2  # 增加到2句重叠，以期在子块之间提供更好的语义连接。
    # --- 合并策略参数 ---
    min_chunk_length_to_avoid_merge: int = 250
    max_merged_chunk_size: int = 750


def split_text_into_sentences(text: str) -> List[str]:
    """
    Splits text into sentences using a regex-based approach.
    Handles common sentence terminators and aims to preserve meaningful units.
    """
    if not text:
        return []
    # 改进的句子分割正则表达式，考虑了中英文句号、问号、感叹号
    # 并尝试处理省略号和一些特殊情况。
    # (?<=[。？！\.!\?]) 会匹配这些标点符号后面的位置 (lookbehind)
    # \s* 匹配标点后的任意空格
    # (?!$) 确保不是字符串末尾 (避免在末尾标点后产生空句子)
    # 对于中文，句号、问号、感叹号通常直接结束句子。
    # 对于英文，. ! ? 后面通常有空格或换行。
    
    # 一个更简单的版本，直接按标点分割，然后清理
    sentences = re.split(r'([。？！\.!\?])', text)
    result = []
    current_sentence = ""
    for i in range(0, len(sentences), 2):
        part = sentences[i]
        terminator = sentences[i+1] if i+1 < len(sentences) else ""
        current_sentence = part + terminator
        if current_sentence.strip():
            result.append(current_sentence.strip())
    
    # 如果上面的分割不理想，可以尝试更复杂的，但这个简单版本通常够用
    # 例如：
    # sentences = re.split(r'(?<=[。？！\.!\?])\s*', text)
    # sentences = [s.strip() for s in sentences if s.strip()]
    return result if result else [text] # 如果无法分割，返回原文本作为一个句子


# --- START: 覆盖这个函数 ---
def split_markdown_table_by_rows(
    markdown_table_text: str,
    target_chunk_size: int,
    max_chunk_size: int,
    context: Optional[dg.AssetExecutionContext] = None
) -> List[Dict[str, Any]]:
    """
    Splits a Markdown table string by its data rows.
    It now tries to create smaller chunks, even for short tables, by grouping a few rows together.
    """
    sub_chunks_data: List[Dict[str, Any]] = []
    lines = markdown_table_text.strip().split('\n')
    
    if len(lines) < 2:
        if context: context.log.warning(f"Markdown table has less than 2 lines. Cannot process for row splitting.")
        return [{"text": markdown_table_text, "start_row_index": -1, "end_row_index": -1}]

    header_row = lines[0]
    separator_row = lines[1]
    data_rows = lines[2:]

    if not data_rows:
        if context: context.log.warning("Markdown table has no data rows. Returning header and separator as single chunk.")
        return [{"text": f"{header_row}\n{separator_row}", "start_row_index": -1, "end_row_index": -1}]

    # --- 新的、更激进的分割逻辑 ---
    current_sub_chunk_lines = []
    current_sub_chunk_start_row_idx = 0
    
    # 定义每个块的目标行数，例如 2-3 行，可以根据需要调整
    ROWS_PER_CHUNK = 2

    for i in range(0, len(data_rows), ROWS_PER_CHUNK):
        chunk_of_rows = data_rows[i:i + ROWS_PER_CHUNK]
        
        # 每个块都包含表头和分隔符，以保证上下文完整
        sub_chunk_text = "\n".join([header_row, separator_row] + chunk_of_rows)
        
        start_row_index = i
        end_row_index = i + len(chunk_of_rows) - 1

        sub_chunks_data.append({
            "text": sub_chunk_text,
            "start_row_index": start_row_index,
            "end_row_index": end_row_index
        })
        if context:
            context.log.debug(f"  Table sub-chunk created: data rows index {start_row_index}-{end_row_index}")
    
    return sub_chunks_data
# --- END: 覆盖结束 ---


def split_code_block_by_blank_lines(
    code_text: str,
    target_chunk_size: int, # 复用配置，但对于代码块，这个更像是一个上限指导
    max_chunk_size: int,    # 作为硬上限
    context: Optional[dg.AssetExecutionContext] = None
) -> List[str]:
    """
    Splits a code block string by blank lines (one or more empty lines).
    Tries to keep resulting chunks from exceeding max_chunk_size.
    """
    if not code_text.strip():
        return []

    # 使用正则表达式匹配一个或多个连续的空行作为分隔符
    # \n\s*\n 匹配一个换行符，后跟零或多个空白字符，再跟一个换行符
    potential_splits = re.split(r'(\n\s*\n)', code_text) # 保留分隔符以便后续处理
    
    sub_chunks = []
    current_chunk_lines = []
    current_chunk_char_count = 0

    # 第一个块总是从头开始
    if potential_splits:
        first_part = potential_splits.pop(0).strip()
        if first_part:
            current_chunk_lines.append(first_part)
            current_chunk_char_count += len(first_part)

    while potential_splits:
        delimiter = potential_splits.pop(0) # 这是分隔符 \n\s*\n
        if not potential_splits: # 没有更多内容了
            if delimiter.strip(): # 如果分隔符本身不是纯空白，也算内容
                 current_chunk_lines.append(delimiter.rstrip()) # 保留末尾的换行
                 current_chunk_char_count += len(delimiter.rstrip())
            break 
        
        next_part = potential_splits.pop(0).strip()
        if not next_part: # 如果下一个部分是空的，只处理分隔符
            if delimiter.strip():
                current_chunk_lines.append(delimiter.rstrip())
                current_chunk_char_count += len(delimiter.rstrip())
            continue

        # 检查加入 delimiter 和 next_part 是否会超长
        # 对于代码，我们通常希望在逻辑断点（空行）处分割，即使块较小
        # 但如果单个由空行分隔的块本身就超过 max_chunk_size，则需要进一步处理（目前简单截断或接受）
        
        # 简化逻辑：如果当前块非空，并且加入下一个部分（包括分隔的空行）会超过目标大小，
        # 或者严格超过最大大小，则结束当前块。
        # 这里的分隔符（空行）本身也应该被视为块的一部分，或者作为块的自然结束。

        # 更简单的策略：每个由 re.split 分割出来的非空部分（即代码段）自成一块
        # 如果代码段本身过长，则接受它，或者未来再细分
        if current_chunk_lines: # 如果当前块有内容
            # 检查如果加上 next_part 是否会超长（这里可以简化，因为空行分割通常意味着逻辑单元）
            # 我们先假设每个由空行分割的块都是一个独立的单元
            sub_chunks.append("\n".join(current_chunk_lines))
            if context: context.log.debug(f"  Code sub-chunk created (blank line split), len: {current_chunk_char_count}")
            current_chunk_lines = []
            current_chunk_char_count = 0
        
        if next_part: # 开始新的块
            current_chunk_lines.append(next_part)
            current_chunk_char_count += len(next_part)

    # 添加最后一个正在构建的子块
    if current_chunk_lines:
        sub_chunks.append("\n".join(current_chunk_lines))
        if context: context.log.debug(f"  Code sub-chunk created (blank line split, last), len: {current_chunk_char_count}")

    if not sub_chunks and code_text: # 如果完全没分割出任何东西（例如代码没有空行）
        if context: context.log.warning("Code block splitting by blank lines resulted in no sub-chunks. Returning original code block.")
        # 对于这种情况，我们可能需要一个字符分割器作为最终回退
        # 但为了简单起见，我们先返回原始代码块
        # 如果原始代码块 > max_chunk_size，它仍然会是一个大块
        if len(code_text) > max_chunk_size:
            if context: context.log.warning(f"  Original code block (len: {len(code_text)}) exceeds max_chunk_size ({max_chunk_size}) and was not split by blank lines. Consider character splitting as fallback.")
            # 这里可以插入 RecursiveCharacterTextSplitter 逻辑
            # from langchain_text_splitters import RecursiveCharacterTextSplitter
            # char_splitter = RecursiveCharacterTextSplitter(chunk_size=max_chunk_size, chunk_overlap=0, separators=["\n", " ", ""])
            # sub_chunks = char_splitter.split_text(code_text)
            # if context: context.log.info(f"    Fallback: Code block character split into {len(sub_chunks)} parts.")
            # return sub_chunks
        return [code_text] # 暂时返回原块

    # 过滤掉完全是空字符串的块 (re.split 可能产生)
    final_sub_chunks = [chunk for chunk in sub_chunks if chunk.strip()]
    return final_sub_chunks if final_sub_chunks else [code_text]


def _get_element_text(element: Any, context: dg.AssetExecutionContext) -> Optional[str]:
    """
    Extracts the text content from a DocumentElement, handling various types.
    """
    # 检查 element 是否有 text 属性
    if hasattr(element, 'text') and isinstance(element.text, str) and element.text.strip():
        return element.text.strip()
    
    # 对TableElement，它的markdown表示更有用
    if isinstance(element, TableElement) and hasattr(element, 'markdown_representation'):
        return element.markdown_representation
        
    # 对CodeBlockElement，它的code属性是内容
    if isinstance(element, CodeBlockElement) and hasattr(element, 'code'):
        return element.code

    # 最后的防线：尝试将元素转为字符串，但这通常表示有未处理的类型
    # context.log.warning(f"Element of type {type(element).__name__} has no direct text attribute. Falling back to str().")
    # return str(element)
    return None # 如果没有明确的文本内容，则返回None，避免注入描述性文字



@dg.asset(
    name="text_chunks",
    description="Cleans/chunks documents. Splits long elements, merges short ones, enriches with contextual metadata.",
    group_name="processing",
    deps=["parsed_documents"]
)
def clean_chunk_text_asset(
    context: dg.AssetExecutionContext,
    config: TextChunkerConfig,
    parsed_documents: List[ParsedDocumentOutput]
) -> List[ChunkOutput]:
    all_chunks: List[ChunkOutput] = []

    for doc_idx, parsed_doc in enumerate(parsed_documents):
        # --- START: 核心修正 - 统一提取并传递文档级元数据 ---
        doc_meta = parsed_doc.original_metadata
        # 兼容 "filename" 和 "file_name" 两种常见键名
        doc_filename = doc_meta.get("filename") or doc_meta.get("file_name") or f"doc_{doc_idx}"
        # 兼容多种可能的日期键名
        doc_creation_date = doc_meta.get("creation_date") or doc_meta.get("creation_datetime")
        doc_last_modified = doc_meta.get("last_modified") or doc_meta.get("last_modified_datetime")
        doc_author = doc_meta.get("author") or doc_meta.get("authors")

        # 将所有文档级别的元数据打包成一个字典
        document_level_metadata = {
            "filename": doc_filename,
            "creation_date": doc_creation_date,
            "last_modified": doc_last_modified,
            "author": doc_author,
        }
        # 清理掉值为None的键
        document_level_metadata = {k: v for k, v in document_level_metadata.items() if v is not None}
        
        context.log.info(f"Processing document for chunking: {doc_filename}")
        # --- END: 核心修正 ---

        current_title_hierarchy: Dict[int, str] = {}
        doc_internal_chunk_counter = 0

        for element_idx, element in enumerate(parsed_doc.elements):
            parent_id = str(uuid.uuid4())
            
            # 2. 准备基础元数据 - 现在从打包好的文档级元数据开始
            base_chunk_meta = document_level_metadata.copy() # 每个块都继承文档级元数据
            
            # 添加元素级元数据
            element_type_str = getattr(element, 'element_type', type(element).__name__)
            base_chunk_meta.update({
                "parent_id": parent_id,
                "paragraph_type": element_type_str,
                "source_element_index": element_idx,
            })

            if isinstance(element, TitleElement):
                 title_level = getattr(element, 'level', 1)
                 keys_to_remove = [lvl for lvl in current_title_hierarchy if lvl >= title_level]
                 for key in keys_to_remove:
                     del current_title_hierarchy[key]
                 current_title_hierarchy[title_level] = getattr(element, 'text', '').strip()
            
            for level, title in current_title_hierarchy.items():
                base_chunk_meta[f"title_hierarchy_{level}"] = title
            
            if hasattr(element, 'metadata') and element.metadata:
                page_num = getattr(element.metadata, 'page_number', None)
                if page_num is not None:
                    base_chunk_meta['page_number'] = page_num + 1

            # 3. 对不同类型的元素进行分块
            sub_chunks: List[Dict[str, Any]] = []
            
            text_content = _get_element_text(element, context)
            
            if not text_content:
                context.log.debug(f"Skipping element {element_idx} in {doc_filename} due to empty content.")
                continue

            # --- START: 对XLSX等文件解析出的Table文本进行特殊处理 ---
            # 假设非TableElement的表格内容会被解析为包含'|'和换行符的普通文本
            is_likely_table_text = '|' in text_content and '\n' in text_content

            if element_type_str == "TableElement" or is_likely_table_text:
                 # 无论元素类型是什么，只要内容像表格，就用表格分割器
                 base_chunk_meta["paragraph_type"] = "table" # 强制将类型标准化为'table'
                 sub_chunks = split_markdown_table_by_rows(text_content, config.target_sentence_split_chunk_size, config.max_merged_chunk_size, context)
            # --- END: 特殊处理 ---
            else:
                if len(text_content) > config.max_element_text_length_before_split:
                    sentences = split_text_into_sentences(text_content)
                    for sent in sentences:
                        if sent.strip():
                            sub_chunks.append({"text": sent.strip()})
                else:
                    sub_chunks.append({"text": text_content})

            # 4. 为所有生成的块创建 ChunkOutput 对象
            for sub_chunk_data in sub_chunks:
                doc_internal_chunk_counter += 1
                chunk_meta_final = base_chunk_meta.copy()
                chunk_meta_final["chunk_number_in_doc"] = doc_internal_chunk_counter
                
                if "start_row_index" in sub_chunk_data:
                    chunk_meta_final["table_original_start_row"] = sub_chunk_data["start_row_index"]
                    chunk_meta_final["table_original_end_row"] = sub_chunk_data["end_row_index"]

                all_chunks.append(ChunkOutput(
                    chunk_text=sub_chunk_data["text"],
                    source_document_id=doc_filename,
                    chunk_metadata=chunk_meta_final
                ))

    context.log.info(f"Chunking process finished. Total chunks generated: {len(all_chunks)}")
    if all_chunks:
        # 强制打印最后一个块的元数据，看filename是否存在
        context.log.info(f"Sample final chunk metadata: {all_chunks[-1].chunk_metadata}")
    
    context.add_output_metadata(metadata={"total_chunks_generated": len(all_chunks)})
    return all_chunks


@dg.asset(
    name="text_embeddings",
    description="Generates vector embeddings for text chunks.",
    group_name="processing",
    deps=["text_chunks"]
)
def generate_embeddings_asset( # <--- 保持同步，因为 GGUFEmbeddingResource.encode 是同步包装
    context: dg.AssetExecutionContext,
    text_chunks: List[ChunkOutput],
    embedder: GGUFEmbeddingResource
) -> List[EmbeddingOutput]:
    # +++ 新增打印语句 +++
    context.log.info(f"generate_embeddings_asset: Received {len(text_chunks)} text_chunks.")
    if text_chunks:
        context.log.info(f"generate_embeddings_asset: First chunk text (first 100 chars): '{text_chunks[0].chunk_text[:100]}'")
        context.log.info(f"generate_embeddings_asset: First chunk metadata: {text_chunks[0].chunk_metadata}")
    # +++ 结束新增打印语句 +++

    all_embeddings: List[EmbeddingOutput] = []
    if not text_chunks:
        context.log.warning("generate_embeddings_asset: No text chunks received, returning empty list.") # 添加一个明确的警告
        return all_embeddings
    
    # --- 确保 chunk_texts_to_encode 不为空才调用 embedder.encode ---
    chunk_texts_to_encode = [chunk.chunk_text for chunk in text_chunks if chunk.chunk_text and chunk.chunk_text.strip()]
    
    if not chunk_texts_to_encode:
        context.log.warning("generate_embeddings_asset: All received text chunks are empty or whitespace after filtering. Returning empty list.")
        # 即使原始 text_chunks 非空，但如果所有 chunk_text 都无效，也应该返回空 embedding 列表
        # 并且要确保下游知道期望的 EmbeddingOutput 数量可能是0
        return all_embeddings # 返回空列表是正确的

    vectors = embedder.encode(chunk_texts_to_encode)

    # --- 确保正确地将嵌入结果映射回原始的 text_chunks 列表（如果数量可能不一致）---
    # 当前的逻辑是假设 vectors 和 chunk_texts_to_encode 一一对应，并且 text_chunks 的顺序与 chunk_texts_to_encode 过滤前的顺序相关
    # 如果 chunk_texts_to_encode 进行了过滤，这里的循环需要更小心
    
    # 一个更安全的映射方式是，只为那些实际被编码的文本块创建 EmbeddingOutput
    # 但这要求下游能处理 EmbeddingOutput 列表长度可能小于 ChunkOutput 列表长度的情况，
    # 或者，我们应该为那些被过滤掉的 chunk 也创建一个带有零向量的 EmbeddingOutput。
    # 我们之前的 LocalModelHandler 修改是为了处理单个空文本，现在这里是资产层面的。

    # 保持与 LocalModelHandler 类似的健壮性：为所有传入的 text_chunks 生成 EmbeddingOutput，
    # 如果其文本为空或嵌入失败，则使用零向量。

    embedding_map = {text: vec for text, vec in zip(chunk_texts_to_encode, vectors)}

    for i, chunk_input in enumerate(text_chunks):
        model_name_for_log = os.getenv("EMBEDDING_MODEL_PATH", "API_Based_Embedder")
        embedding_vector_for_chunk = [0.0] * embedder.get_embedding_dimension() # 默认为零向量

        if chunk_input.chunk_text and chunk_input.chunk_text.strip() and chunk_input.chunk_text in embedding_map:
            embedding_vector_for_chunk = embedding_map[chunk_input.chunk_text]
        elif chunk_input.chunk_text and chunk_input.chunk_text.strip(): 
            # 文本有效但没有在 embedding_map 中找到 (可能因为 embedder.encode 内部的某些问题)
            context.log.warning(f"generate_embeddings_asset: Valid chunk text for chunk_id {chunk_input.chunk_id} was not found in embedding_map. Using zero vector.")
        else: # 文本本身就是空的
            context.log.info(f"generate_embeddings_asset: Chunk_id {chunk_input.chunk_id} has empty text. Using zero vector.")


        all_embeddings.append(EmbeddingOutput(
            chunk_id=chunk_input.chunk_id,
            chunk_text=chunk_input.chunk_text, # 存储原始文本，即使它是空的
            embedding_vector=embedding_vector_for_chunk,
            embedding_model_name=model_name_for_log,
            original_chunk_metadata=chunk_input.chunk_metadata
        ))
    
    context.add_output_metadata(metadata={"total_embeddings_generated": len(all_embeddings)})
    return all_embeddings


@dg.asset(
    name="vector_store_embeddings",
    description="Stores text embeddings into a ChromaDB vector store.",
    group_name="indexing",
    deps=["text_embeddings"]
)
def vector_storage_asset(
    context: dg.AssetExecutionContext,
    text_embeddings: List[EmbeddingOutput],
    chroma_db: ChromaDBResource
) -> None:
    if not text_embeddings:
        context.log.warning("vector_storage_asset: No embeddings received, nothing to store in ChromaDB.")
        context.add_output_metadata(metadata={"num_embeddings_stored": 0})
        return

    # --- START: 核心修复 ---
    # 筛选出那些拥有有效（非空）嵌入向量的条目
    valid_embeddings_to_store: List[EmbeddingOutput] = []
    for emb in text_embeddings:
        if emb.embedding_vector and len(emb.embedding_vector) > 0:
            valid_embeddings_to_store.append(emb)
        else:
            context.log.warning(f"Skipping storage for chunk_id {emb.chunk_id} due to empty embedding vector.")
    
    if not valid_embeddings_to_store:
        context.log.warning("vector_storage_asset: No valid embeddings found after filtering. Nothing to store.")
        context.add_output_metadata(metadata={"num_embeddings_stored": 0, "num_invalid_embeddings_skipped": len(text_embeddings)})
        return
        
    ids_to_store = [emb.chunk_id for emb in valid_embeddings_to_store]
    embeddings_to_store = [emb.embedding_vector for emb in valid_embeddings_to_store]
    documents_to_store = [emb.chunk_text for emb in valid_embeddings_to_store]
    cleaned_metadatas: List[Dict[str, Any]] = []

    for i, emb_output in enumerate(valid_embeddings_to_store):
    # --- END: 核心修复 ---
        original_meta = emb_output.original_chunk_metadata if isinstance(emb_output.original_chunk_metadata, dict) else {}
        meta = original_meta.copy()
        
        meta["chunk_text_in_meta"] = str(emb_output.chunk_text) if emb_output.chunk_text is not None else "[TEXT IS NULL]"

        cleaned_meta_item: Dict[str, Any] = {}
        for key, value in meta.items():
            if isinstance(value, dict):
                if key == "title_hierarchy" and not value: 
                    cleaned_meta_item[key] = "None"
                    context.log.debug(f"Metadata for chunk {emb_output.chunk_id}: Replaced empty title_hierarchy dict with 'None' string.")
                else:
                    try:
                        cleaned_meta_item[key] = json.dumps(value, ensure_ascii=False)
                    except TypeError:
                        cleaned_meta_item[key] = str(value)
                        context.log.warning(f"Metadata for chunk {emb_output.chunk_id}: Could not JSON serialize dict for key '{key}', used str(). Value: {str(value)[:100]}...")
            elif isinstance(value, list):
                try:
                    cleaned_meta_item[key] = json.dumps(value, ensure_ascii=False)
                except TypeError:
                    cleaned_meta_item[key] = str(value)
                    context.log.warning(f"Metadata for chunk {emb_output.chunk_id}: Could not JSON serialize list for key '{key}', used str(). Value: {str(value)[:100]}...")
            elif value is None:
                cleaned_meta_item[key] = "" 
            else: 
                cleaned_meta_item[key] = value
        cleaned_metadatas.append(cleaned_meta_item)

    # +++ 新增日志 +++
    if embeddings_to_store:
        context.log.info(f"vector_storage_asset: Sample embedding vector to be stored (first item, first 10 elements): {str(embeddings_to_store[0][:10]) if embeddings_to_store[0] else 'None'}")
        context.log.info(f"vector_storage_asset: Length of first embedding vector to be stored: {len(embeddings_to_store[0]) if embeddings_to_store[0] else 'N/A'}")
        is_first_all_zeros = all(v == 0.0 for v in embeddings_to_store[0]) if embeddings_to_store[0] else "N/A"
        context.log.info(f"vector_storage_asset: Is first sample embedding all zeros: {is_first_all_zeros}")
    # +++ 结束新增日志 +++

    context.log.info(f"vector_storage_asset: Preparing to add/update {len(ids_to_store)} items to ChromaDB collection '{chroma_db.collection_name}'.")
    if ids_to_store:
        context.log.info(f"vector_storage_asset: Sample ID to store: {ids_to_store[0]}")
        # 确保 documents_to_store 也有对应内容，并且不是 None
        sample_doc_text = "[EMPTY DOCUMENT]"
        if documents_to_store and documents_to_store[0] is not None:
            sample_doc_text = str(documents_to_store[0])[:100] # 显示前100字符
        elif documents_to_store and documents_to_store[0] is None:
            sample_doc_text = "[DOCUMENT IS NULL]"
        context.log.info(f"vector_storage_asset: Sample document to store (from documents_to_store, first 100 chars): '{sample_doc_text}'")
        
        sample_meta_text = "[NO METADATA]"
        if cleaned_metadatas:
            sample_meta_text = str(cleaned_metadatas[0])[:200] # 显示元数据摘要
        context.log.info(f"vector_storage_asset: Sample cleaned metadata for first item: {sample_meta_text}")

    try:
        chroma_db.add_embeddings(
            ids=ids_to_store, 
            embeddings=embeddings_to_store, 
            documents=documents_to_store, # 传递真实的文本内容给ChromaDB的documents字段
            metadatas=cleaned_metadatas
        )
        # 尝试获取并记录操作后的集合计数
        # 注意: chroma_db._collection 可能是私有属性，直接访问不推荐，但为了调试可以尝试
        # 更好的方式是 ChromaDBResource 提供一个 get_collection_count() 方法
        collection_count_after_add = -1 # 默认值
        try:
            if chroma_db._collection: # 确保 _collection 不是 None
                 collection_count_after_add = chroma_db._collection.count()
        except Exception as e_count:
            context.log.warning(f"vector_storage_asset: Could not get collection count after add: {e_count}")

        context.add_output_metadata(metadata={"num_embeddings_stored": len(ids_to_store), "collection_count_after_add": collection_count_after_add})
        context.log.info(f"vector_storage_asset: Successfully called add_embeddings. Stored {len(ids_to_store)} items. Collection count now: {collection_count_after_add}")
    except Exception as e_chroma_add:
        context.log.error(f"vector_storage_asset: Failed to add embeddings to ChromaDB: {e_chroma_add}", exc_info=True)
        raise

class BM25IndexConfig(dg.Config):
    index_file_path: str = "/home/zhz/zhz_agent/zhz_rag/stored_data/bm25_index/"


@dg.asset(
    name="keyword_index",
    description="Builds and persists a BM25 keyword index from text chunks.",
    group_name="indexing",
    deps=["text_chunks"]
)
def keyword_index_asset(
    context: dg.AssetExecutionContext,
    config: BM25IndexConfig, # 确保 BM25IndexConfig 在文件某处已定义
    text_chunks: List[ChunkOutput]
) -> None:
    if not text_chunks:
        context.log.warning("keyword_index_asset: No text chunks received, skipping BM25 index building.")
        context.add_output_metadata(metadata={"num_documents_indexed": 0, "index_directory_path": config.index_file_path})
        return

    # --- 新增：检查并记录空文本块 ---
    valid_chunks_for_indexing: List[ChunkOutput] = []
    for idx, chunk in enumerate(text_chunks):
        if chunk.chunk_text and chunk.chunk_text.strip():
            valid_chunks_for_indexing.append(chunk)
        else:
            context.log.warning(f"keyword_index_asset: Chunk {idx} (ID: {chunk.chunk_id}) has empty or whitespace-only text. Skipping for BM25 indexing.")
    
    if not valid_chunks_for_indexing:
        context.log.warning("keyword_index_asset: All received text chunks have empty or whitespace-only text after filtering. Skipping BM25 index building.")
        context.add_output_metadata(metadata={"num_documents_indexed": 0, "index_directory_path": config.index_file_path})
        return
    # --- 结束新增 ---

    # 使用过滤后的有效块
    corpus_texts = [chunk.chunk_text for chunk in valid_chunks_for_indexing]
    document_ids = [chunk.chunk_id for chunk in valid_chunks_for_indexing] # 确保ID与有效文本对应

    context.log.info(f"keyword_index_asset: Preparing to index {len(corpus_texts)} valid text chunks for BM25.")
    if corpus_texts: # 仅在有数据时打印样本
        context.log.info(f"keyword_index_asset: Sample document ID for BM25: {document_ids[0]}")
        context.log.info(f"keyword_index_asset: Sample document text for BM25 (first 50 chars): '{str(corpus_texts[0])[:50]}'")

    try:
        corpus_tokenized_jieba = [list(jieba.cut_for_search(text)) for text in corpus_texts]
        context.log.info(f"keyword_index_asset: Tokenized {len(corpus_tokenized_jieba)} texts for BM25.")
        
        bm25_model = bm25s.BM25() # 使用默认参数初始化
        context.log.info("keyword_index_asset: BM25 model initialized.")
        
        bm25_model.index(corpus_tokenized_jieba)
        indexed_doc_count = len(bm25_model.doc_freqs) if hasattr(bm25_model, 'doc_freqs') and bm25_model.doc_freqs is not None else len(corpus_tokenized_jieba)
        context.log.info(f"keyword_index_asset: BM25 model indexing complete for {indexed_doc_count} documents.")
        
        index_directory = config.index_file_path
        context.log.info(f"keyword_index_asset: BM25 index will be saved to directory: {index_directory}")
        os.makedirs(index_directory, exist_ok=True)
        
        bm25_model.save(index_directory) 
        context.log.info(f"keyword_index_asset: bm25_model.save('{index_directory}') called.")
        
        doc_ids_path = os.path.join(index_directory, "doc_ids.pkl")
        with open(doc_ids_path, 'wb') as f_out:
            pickle.dump(document_ids, f_out)
        context.log.info(f"keyword_index_asset: doc_ids.pkl saved to {doc_ids_path} with {len(document_ids)} IDs.")
        
        # 验证文件是否真的创建了
        expected_params_file = os.path.join(index_directory, "params.index.json") # bm25s 保存时会创建这个
        if os.path.exists(expected_params_file) and os.path.exists(doc_ids_path):
            context.log.info(f"keyword_index_asset: Verified that BM25 index files (e.g., params.index.json, doc_ids.pkl) exist in {index_directory}.")
        else:
            context.log.error(f"keyword_index_asset: BM25 index files (e.g., params.index.json or doc_ids.pkl) NOT FOUND in {index_directory} after save operations!")
            context.log.error(f"keyword_index_asset: Check - params.index.json exists: {os.path.exists(expected_params_file)}")
            context.log.error(f"keyword_index_asset: Check - doc_ids.pkl exists: {os.path.exists(doc_ids_path)}")
            # 如果文件未找到，可能需要抛出异常以使资产失败
            # raise FileNotFoundError(f"BM25 index files not found in {index_directory} after save.")

        context.add_output_metadata(
            metadata={
                "num_documents_indexed": len(corpus_texts), 
                "index_directory_path": index_directory,
                "bm25_corpus_size_actual": indexed_doc_count
            }
        )
        context.log.info("keyword_index_asset: BM25 indexing and saving completed successfully.")
    except Exception as e_bm25:
        context.log.error(f"keyword_index_asset: Error during BM25 indexing or saving: {e_bm25}", exc_info=True)
        raise

# --- KG Extraction 相关的配置和资产 ---


# class KGExtractionConfig(dg.Config):
#     extraction_prompt_template: str = KG_EXTRACTION_SINGLE_CHUNK_PROMPT_TEMPLATE_V1
#     local_llm_model_name: str = "Qwen3-1.7B-GGUF_via_llama.cpp"

# DEFAULT_KG_EXTRACTION_SCHEMA = {
#     "type": "object",
#     "properties": {
#         "entities": {
#             "type": "array",
#             "items": {
#                 "type": "object",
#                 "properties": {
#                     "text": {"type": "string", "description": "提取到的实体原文"},
#                     "label": {"type": "string", "description": "实体类型 (例如: PERSON, ORGANIZATION, TASK)"}
#                 },
#                 "required": ["text", "label"]
#             },
#             "description": "从文本中提取出的实体列表。"
#         },
#         "relations": {
#             "type": "array",
#             "items": {
#                 "type": "object",
#                 "properties": {
#                     "head_entity_text": {"type": "string", "description": "头实体的文本"},
#                     "head_entity_label": {"type": "string", "description": "头实体的类型 (例如: PERSON, TASK)"},
#                     "relation_type": {"type": "string", "description": "关系类型 (例如: WORKS_AT, ASSIGNED_TO)"},
#                     "tail_entity_text": {"type": "string", "description": "尾实体的文本"},
#                     "tail_entity_label": {"type": "string", "description": "尾实体的类型 (例如: ORGANIZATION, PERSON)"}
#                 },
#                 "required": ["head_entity_text", "head_entity_label", "relation_type", "tail_entity_text", "tail_entity_label"]
#             },
#             "description": "从文本中提取出的关系三元组列表。"
#         }
#     },
#     "required": ["entities", "relations"]
# }


# @dg.asset(
#     name="kg_extractions",
#     description="Extracts entities and relations from text chunks for knowledge graph construction.",
#     group_name="kg_building",
#     io_manager_key="pydantic_json_io_manager",
#     deps=["text_chunks"]
# )
# async def kg_extraction_asset(
#     context: dg.AssetExecutionContext, # Pylance 提示 dg.AssetExecutionContext 未定义 "SystemResource"
#     text_chunks: List[ChunkOutput],
#     config: KGExtractionConfig,
#     LocalLLM_api: LocalLLMAPIResource,
#     system_info: SystemResource  # <--- 我们添加了 system_info
# ) -> List[KGTripleSetOutput]:
#     all_kg_outputs: List[KGTripleSetOutput] = []
#     if not text_chunks:
#         context.log.info("No text chunks received for KG extraction, skipping.")
#         return all_kg_outputs

#     total_input_chunks = len(text_chunks)
#     total_entities_extracted_overall = 0
#     total_relations_extracted_overall = 0
#     successfully_processed_chunks_count = 0
    
#     # 并发控制参数
#     recommended_concurrency = system_info.get_recommended_concurrent_tasks(task_type="kg_extraction_llm")
#     CONCURRENT_REQUESTS_LIMIT = max(1, recommended_concurrency) # 直接使用HAL推荐，但至少为1
#     context.log.info(f"HAL recommended concurrency for 'kg_extraction_llm': {recommended_concurrency}. Effective limit set to: {CONCURRENT_REQUESTS_LIMIT}")
#     semaphore = asyncio.Semaphore(CONCURRENT_REQUESTS_LIMIT)


#     async def extract_kg_for_chunk(chunk: ChunkOutput) -> Optional[KGTripleSetOutput]:
#         async with semaphore:
#             # 使用单个chunk的prompt模板
#             prompt = config.extraction_prompt_template.format(text_to_extract=chunk.chunk_text)
#             try:
#                 context.log.debug(f"Starting KG extraction for chunk_id: {chunk.chunk_id}, Text (start): {chunk.chunk_text[:100]}...")
#                 structured_response = await LocalLLM_api.generate_structured_output(
#                     prompt=prompt, 
#                     json_schema=DEFAULT_KG_EXTRACTION_SCHEMA # 使用单个对象的schema
#                 )
                
#                 # 确保 structured_response 是字典类型
#                 if not isinstance(structured_response, dict):
#                     context.log.error(f"Failed KG extraction for chunk {chunk.chunk_id}: LLM response was not a dict. Got: {type(structured_response)}. Response: {str(structured_response)[:200]}")
#                     return None

#                 entities_data = structured_response.get("entities", [])
#                 extracted_entities_list = [
#                     ExtractedEntity(text=normalize_text_for_id(e.get("text","")), label=e.get("label","UNKNOWN").upper())
#                     for e in entities_data if isinstance(e, dict)
#                 ]
                
#                 relations_data = structured_response.get("relations", [])
#                 extracted_relations_list = [
#                     ExtractedRelation(
#                         head_entity_text=r.get('head_entity_text',""), 
#                         head_entity_label=r.get('head_entity_label',"UNKNOWN").upper(), 
#                         relation_type=r.get('relation_type',"UNKNOWN").upper(), 
#                         tail_entity_text=r.get('tail_entity_text',""), 
#                         tail_entity_label=r.get('tail_entity_label',"UNKNOWN").upper()
#                     ) 
#                     for r in relations_data if isinstance(r, dict) and 
#                                                r.get('head_entity_text') and r.get('head_entity_label') and
#                                                r.get('relation_type') and r.get('tail_entity_text') and
#                                                r.get('tail_entity_label')
#                 ]
                
#                 context.log.debug(f"Finished KG extraction for chunk_id: {chunk.chunk_id}. Entities: {len(extracted_entities_list)}, Relations: {len(extracted_relations_list)}")
#                 return KGTripleSetOutput(
#                     chunk_id=chunk.chunk_id,
#                     extracted_entities=extracted_entities_list,
#                     extracted_relations=extracted_relations_list,
#                     extraction_model_name=config.local_llm_model_name,
#                     original_chunk_metadata=chunk.chunk_metadata
#                 )
#             except Exception as e:
#                 context.log.error(f"Failed KG extraction for chunk {chunk.chunk_id}: {e}", exc_info=True)
#                 return None 

#     context.log.info(f"Starting KG extraction for {total_input_chunks} chunks with concurrency limit: {CONCURRENT_REQUESTS_LIMIT}.")
    
#     tasks = [extract_kg_for_chunk(chunk) for chunk in text_chunks]
    
#     results = await asyncio.gather(*tasks)
    
#     context.log.info(f"Finished all KG extraction tasks. Received {len(results)} results (including potential None for failures).")

#     for result_item in results:
#         if result_item and isinstance(result_item, KGTripleSetOutput):
#             all_kg_outputs.append(result_item)
#             total_entities_extracted_overall += len(result_item.extracted_entities)
#             total_relations_extracted_overall += len(result_item.extracted_relations)
#             successfully_processed_chunks_count +=1
#         elif result_item is None:
#             context.log.warning("A KG extraction task failed and returned None.")
            
#     context.log.info(f"KG extraction complete. Successfully processed {successfully_processed_chunks_count} out of {total_input_chunks} chunks.")
#     context.add_output_metadata(
#         metadata={
#             "total_chunks_input_to_kg": total_input_chunks, # 恢复为 total_input_chunks
#             "chunks_successfully_extracted_kg": successfully_processed_chunks_count,
#             "total_entities_extracted": total_entities_extracted_overall, 
#             "total_relations_extracted": total_relations_extracted_overall
#             # 移除了批处理相关的元数据 "total_batches_processed", "batch_size_configured"
#         }
#     )
#     return all_kg_outputs


# # --- KuzuDB 构建资产链 ---

# @dg.asset(
#     name="duckdb_schema", # <--- 修改资产名称
#     description="Creates the base schema (node and relation tables) in DuckDB.",
#     group_name="kg_building",
#     # deps=[kg_extraction_asset] # 保持依赖，确保在提取之后创建schema (逻辑上)
#                                  # 虽然schema创建本身不直接使用提取结果，但流水线顺序上合理
# )
# def duckdb_schema_asset(context: dg.AssetExecutionContext, duckdb_kg: DuckDBResource, embedder: GGUFEmbeddingResource): # <--- 修改函数名和资源参数
#     context.log.info("--- Starting DuckDB Schema Creation Asset ---")
    
#     # 获取嵌入维度，与KuzuDB时类似
#     EMBEDDING_DIM = embedder.get_embedding_dimension()
#     if not EMBEDDING_DIM:
#         raise ValueError("Could not determine embedding dimension from GGUFEmbeddingResource.")

#     node_table_ddl = f"""
#     CREATE TABLE IF NOT EXISTS ExtractedEntity (
#         id_prop VARCHAR PRIMARY KEY,
#         text VARCHAR,
#         label VARCHAR,
#         embedding FLOAT[{EMBEDDING_DIM}]
#     );
#     """

#     relation_table_ddl = f"""
#     CREATE TABLE IF NOT EXISTS KGExtractionRelation (
#         relation_id VARCHAR PRIMARY KEY,
#         source_node_id_prop VARCHAR,
#         target_node_id_prop VARCHAR,
#         relation_type VARCHAR
#         -- Optional: FOREIGN KEY (source_node_id_prop) REFERENCES ExtractedEntity(id_prop),
#         -- Optional: FOREIGN KEY (target_node_id_prop) REFERENCES ExtractedEntity(id_prop)
#     );
#     """
#     # 也可以为关系表的 (source, target, type) 创建复合唯一索引或普通索引以加速查询
#     relation_index_ddl = """
#     CREATE INDEX IF NOT EXISTS idx_relation_source_target_type 
#     ON KGExtractionRelation (source_node_id_prop, target_node_id_prop, relation_type);
#     """
    
#     ddl_commands = [node_table_ddl, relation_table_ddl, relation_index_ddl]

#     try:
#         with duckdb_kg.get_connection() as conn:
#             context.log.info("Executing DuckDB DDL commands...")
#             for command_idx, command in enumerate(ddl_commands):
#                 context.log.debug(f"Executing DDL {command_idx+1}:\n{command.strip()}")
#                 conn.execute(command)
#             context.log.info("DuckDB Schema DDL commands executed successfully.")
#     except Exception as e_ddl:
#         context.log.error(f"Error during DuckDB schema creation: {e_ddl}", exc_info=True)
#         raise
#     context.log.info("--- DuckDB Schema Creation Asset Finished ---")


# @dg.asset(
#     name="duckdb_nodes", # <--- 修改资产名称
#     description="Loads all unique extracted entities as nodes into DuckDB.",
#     group_name="kg_building",
#     deps=[duckdb_schema_asset, kg_extraction_asset] # <--- 修改依赖
# )
# def duckdb_nodes_asset(
#     context: dg.AssetExecutionContext,
#     kg_extractions: List[KGTripleSetOutput], # 来自 kg_extraction_asset 的输出
#     duckdb_kg: DuckDBResource,               # <--- 修改资源参数
#     embedder: GGUFEmbeddingResource          # 保持对 embedder 的依赖，用于生成嵌入
# ):
#         # --- START: 移动并强化初始日志 ---
#     print("<<<<< duckdb_nodes_asset FUNCTION ENTERED - PRINTING TO STDOUT >>>>>", flush=True) 
#     # 尝试使用 context.log，如果它此时可用
#     try:
#         context.log.info("<<<<< duckdb_nodes_asset FUNCTION CALLED - VIA CONTEXT.LOG - VERY BEGINNING >>>>>")
#     except Exception as e_log_init:
#         print(f"Context.log not available at the very beginning of duckdb_nodes_asset: {e_log_init}", flush=True)
#     # --- END: 移动并强化初始日志 ---

#     context.log.info("--- Starting DuckDB Node Loading Asset (Using INSERT ON CONFLICT) ---")
#     if not kg_extractions:
#         context.log.warning("No KG extractions received. Skipping node loading.")
#         return

#     # +++ 新增调试日志：检查表是否存在 +++
#     try:
#         with duckdb_kg.get_connection() as conn_debug:
#             context.log.info("Attempting to list tables in DuckDB from duckdb_nodes_asset:")
#             tables = conn_debug.execute("SHOW TABLES;").fetchall()
#             context.log.info(f"Tables found: {tables}")
#             if any('"ExtractedEntity"' in str(table_row).upper() for table_row in tables) or \
#                any('ExtractedEntity' in str(table_row) for table_row in tables) : # 检查大小写不敏感的匹配
#                 context.log.info("Table 'ExtractedEntity' (or similar) IS visible at the start of duckdb_nodes_asset.")
#             else:
#                 context.log.warning("Table 'ExtractedEntity' IS NOT visible at the start of duckdb_nodes_asset. Schema asset might not have run correctly or changes are not reflected.")
#     except Exception as e_debug_show:
#         context.log.error(f"Error trying to list tables in duckdb_nodes_asset: {e_debug_show}")
#     # +++ 结束新增调试日志 +++
    
#     unique_nodes_data_for_insert: List[Dict[str, Any]] = []
#     unique_nodes_keys = set() # 用于在Python层面去重，避免多次尝试插入相同实体

#     for kg_set in kg_extractions:
#         for entity in kg_set.extracted_entities:
#             # 规范化文本和标签，用于生成唯一键和存储
#             normalized_text = normalize_text_for_id(entity.text)
#             normalized_label = entity.label.upper() # 确保标签大写
            
#             # 为实体生成唯一ID (基于规范化文本和标签的哈希值)
#             # 注意：如果同一个实体（相同文本和标签）在不同chunk中被提取，它们的id_prop会一样
#             node_id_prop = hashlib.md5(f"{normalized_text}_{normalized_label}".encode('utf-8')).hexdigest()
            
#             node_unique_key_for_py_dedup = (node_id_prop) # 使用id_prop进行Python层面的去重

#             if node_unique_key_for_py_dedup not in unique_nodes_keys:
#                 unique_nodes_keys.add(node_unique_key_for_py_dedup)
                
#                 # 生成嵌入向量 (与KuzuDB时逻辑相同)
#                 embedding_vector_list = embedder.encode([normalized_text]) # embedder.encode期望一个列表
#                 final_embedding_for_db: List[float]

#                 if embedding_vector_list and embedding_vector_list[0] and \
#                    isinstance(embedding_vector_list[0], list) and \
#                    len(embedding_vector_list[0]) == embedder.get_embedding_dimension():
#                     final_embedding_for_db = embedding_vector_list[0]
#                 else:
#                     context.log.warning(f"Failed to generate valid embedding for node: {normalized_text} ({normalized_label}). Using zero vector. Embedding result: {embedding_vector_list}")
#                     final_embedding_for_db = [0.0] * embedder.get_embedding_dimension()
                    
#                 unique_nodes_data_for_insert.append({
#                     "id_prop": node_id_prop,
#                     "text": normalized_text,
#                     "label": normalized_label,
#                     "embedding": final_embedding_for_db # DuckDB的FLOAT[]可以直接接受Python的List[float]
#                 })

#     if not unique_nodes_data_for_insert:
#         context.log.warning("No unique nodes found in extractions to load into DuckDB.")
#         return

#     nodes_processed_count = 0
#     nodes_inserted_count = 0
#     nodes_updated_count = 0

#     upsert_sql = f"""
#     INSERT INTO "ExtractedEntity" (id_prop, text, label, embedding)
#     VALUES (?, ?, ?, ?)
#     ON CONFLICT (id_prop) DO UPDATE SET
#         text = excluded.text,
#         label = excluded.label,
#         embedding = excluded.embedding;
#     """
#     # excluded.column_name 用于引用试图插入但导致冲突的值

#     try:
#         with duckdb_kg.get_connection() as conn:
#             context.log.info(f"Attempting to UPSERT {len(unique_nodes_data_for_insert)} unique nodes into DuckDB ExtractedEntity table...")
            
#             # DuckDB 支持 executemany 用于批量操作，但对于 ON CONFLICT，逐条执行或构造大型 VALUES 列表可能更直接
#             # 或者使用 pandas DataFrame + duckdb.register + CREATE TABLE AS / INSERT INTO SELECT
#             # 这里为了清晰，我们先用循环执行，对于几千到几万个节点，性能尚可接受
#             # 如果节点数量非常大 (几十万以上)，应考虑更优化的批量upsert策略

#             for node_data_dict in unique_nodes_data_for_insert:
#                 params = (
#                     node_data_dict["id_prop"],
#                     node_data_dict["text"],
#                     node_data_dict["label"],
#                     node_data_dict["embedding"]
#                 )
#                 try:
#                     # conn.execute() 对于 DML (如 INSERT, UPDATE) 不直接返回受影响的行数
#                     # 但我们可以假设它成功了，除非抛出异常
#                     conn.execute(upsert_sql, params)
#                     # 无法直接判断是insert还是update，除非查询前后对比，这里简化处理
#                     nodes_processed_count += 1 
#                 except Exception as e_upsert_item:
#                     context.log.error(f"Error UPSERTING node with id_prop {node_data_dict.get('id_prop')} into DuckDB: {e_upsert_item}", exc_info=True)
            
#             # 我们可以查一下表中的总行数来间接了解情况
#             total_rows_after = conn.execute('SELECT COUNT(*) FROM "ExtractedEntity"').fetchone()[0]
#             context.log.info(f"Successfully processed {nodes_processed_count} node upsert operations into DuckDB.")
#             context.log.info(f"Total rows in ExtractedEntity table after upsert: {total_rows_after}")

#     except Exception as e_db_nodes:
#         context.log.error(f"Error during DuckDB node loading: {e_db_nodes}", exc_info=True)
#         raise
    
#     context.add_output_metadata({
#         "nodes_prepared_for_upsert": len(unique_nodes_data_for_insert),
#         "nodes_processed_by_upsert_statement": nodes_processed_count,
#     })
#     context.log.info("--- DuckDB Node Loading Asset Finished ---")


# @dg.asset(
#     name="duckdb_relations", # <--- 修改资产名称
#     description="Loads all extracted relationships into DuckDB.",
#     group_name="kg_building",
#     deps=[duckdb_nodes_asset] # <--- 修改依赖
# )
# def duckdb_relations_asset(
#     context: dg.AssetExecutionContext, 
#     kg_extractions: List[KGTripleSetOutput], # 来自 kg_extraction_asset
#     duckdb_kg: DuckDBResource                # <--- 修改资源参数
# ):
#     context.log.info("--- Starting DuckDB Relation Loading Asset ---")
#     if not kg_extractions:
#         context.log.warning("No KG extractions received. Skipping relation loading.")
#         return

#     relations_to_insert: List[Dict[str, str]] = []
#     unique_relation_keys = set() # 用于在Python层面去重

#     for kg_set in kg_extractions:
#         for rel in kg_set.extracted_relations:
#             # 从实体文本和标签生成源节点和目标节点的ID (与 duckdb_nodes_asset 中一致)
#             source_node_text_norm = normalize_text_for_id(rel.head_entity_text)
#             source_node_label_norm = rel.head_entity_label.upper()
#             source_node_id = hashlib.md5(f"{source_node_text_norm}_{source_node_label_norm}".encode('utf-8')).hexdigest()

#             target_node_text_norm = normalize_text_for_id(rel.tail_entity_text)
#             target_node_label_norm = rel.tail_entity_label.upper()
#             target_node_id = hashlib.md5(f"{target_node_text_norm}_{target_node_label_norm}".encode('utf-8')).hexdigest()
            
#             relation_type_norm = rel.relation_type.upper()

#             # 为关系本身生成一个唯一ID
#             relation_unique_str = f"{source_node_id}_{relation_type_norm}_{target_node_id}"
#             relation_id = hashlib.md5(relation_unique_str.encode('utf-8')).hexdigest()

#             if relation_id not in unique_relation_keys:
#                 unique_relation_keys.add(relation_id)
#                 relations_to_insert.append({
#                     "relation_id": relation_id,
#                     "source_node_id_prop": source_node_id,
#                     "target_node_id_prop": target_node_id,
#                     "relation_type": relation_type_norm
#                 })
    
#     if not relations_to_insert:
#         context.log.warning("No unique relations found in extractions to load into DuckDB.")
#         return

#     relations_processed_count = 0
    
#     # 使用 INSERT INTO ... ON CONFLICT DO NOTHING 来避免插入重复的关系 (基于 relation_id)
#     insert_sql = """
#     INSERT INTO KGExtractionRelation (relation_id, source_node_id_prop, target_node_id_prop, relation_type)
#     VALUES (?, ?, ?, ?)
#     ON CONFLICT (relation_id) DO NOTHING;
#     """

#     try:
#         with duckdb_kg.get_connection() as conn:
#             context.log.info(f"Attempting to INSERT {len(relations_to_insert)} unique relations into DuckDB KGExtractionRelation table...")
            
#             for rel_data_dict in relations_to_insert:
#                 params = (
#                     rel_data_dict["relation_id"],
#                     rel_data_dict["source_node_id_prop"],
#                     rel_data_dict["target_node_id_prop"],
#                     rel_data_dict["relation_type"]
#                 )
#                 try:
#                     conn.execute(insert_sql, params)
#                     # DuckDB的execute对于INSERT ON CONFLICT DO NOTHING不直接返回是否插入
#                     # 但我们可以假设它成功处理了（要么插入，要么忽略）
#                     relations_processed_count += 1
#                 except Exception as e_insert_item:
#                     context.log.error(f"Error INSERTING relation with id {rel_data_dict.get('relation_id')} into DuckDB: {e_insert_item}", exc_info=True)
            
#             total_rels_after = conn.execute("SELECT COUNT(*) FROM KGExtractionRelation").fetchone()[0]
#             context.log.info(f"Successfully processed {relations_processed_count} relation insert (ON CONFLICT DO NOTHING) operations.")
#             context.log.info(f"Total rows in KGExtractionRelation table after inserts: {total_rels_after}")

#     except Exception as e_db_rels:
#         context.log.error(f"Error during DuckDB relation loading: {e_db_rels}", exc_info=True)
#         raise
        
#     context.add_output_metadata({
#         "relations_prepared_for_insert": len(relations_to_insert),
#         "relations_processed_by_insert_statement": relations_processed_count,
#     })
#     context.log.info("--- DuckDB Relation Loading Asset Finished ---")



# @dg.asset(
#     name="duckdb_vector_index", # <--- 修改资产名称
#     description="Creates the HNSW vector index on the embedding column in DuckDB.",
#     group_name="kg_building",
#     deps=[duckdb_relations_asset]  # <--- 修改依赖
# )
# def duckdb_vector_index_asset(
#     context: dg.AssetExecutionContext, 
#     duckdb_kg: DuckDBResource # <--- 修改资源参数
# ):
#     context.log.info("--- Starting DuckDB Vector Index Creation Asset ---")
    
#     table_to_index = "ExtractedEntity"
#     column_to_index = "embedding"
#     # 索引名可以自定义，通常包含表名、列名和类型
#     index_name = f"{table_to_index}_{column_to_index}_hnsw_idx"
#     metric_type = "l2sq" # 欧氏距离的平方，与我们测试时一致

#     # DuckDB 的 CREATE INDEX ... USING HNSW 语句
#     # IF NOT EXISTS 确保了幂等性
#     index_creation_sql = f"""
#     CREATE INDEX IF NOT EXISTS {index_name} 
#     ON {table_to_index} USING HNSW ({column_to_index}) 
#     WITH (metric='{metric_type}');
#     """

#     try:
#         with duckdb_kg.get_connection() as conn:
#             # 在创建索引前，确保vss扩展已加载且持久化已开启 (虽然DuckDBResource的setup已做)
#             try:
#                 conn.execute("LOAD vss;")
#                 conn.execute("SET hnsw_enable_experimental_persistence=true;")
#                 context.log.info("DuckDB: VSS extension loaded and HNSW persistence re-confirmed for index creation asset.")
#             except Exception as e_vss_setup_idx:
#                 context.log.warning(f"DuckDB: Failed to re-confirm VSS setup for index asset: {e_vss_setup_idx}. "
#                                      "Proceeding, assuming it was set by DuckDBResource.")

#             context.log.info(f"Executing DuckDB vector index creation command:\n{index_creation_sql.strip()}")
#             conn.execute(index_creation_sql)
#             context.log.info(f"DuckDB vector index '{index_name}' creation command executed successfully (or index already existed).")

#     except Exception as e_index_asset:
#         context.log.error(f"Error during DuckDB vector index creation: {e_index_asset}", exc_info=True)
#         raise
    
#     context.log.info("--- DuckDB Vector Index Creation Asset Finished ---")


# --- 更新 all_processing_assets 列表 ---
all_processing_assets = [
    clean_chunk_text_asset,
    generate_embeddings_asset,
    vector_storage_asset,
    keyword_index_asset,
    # kg_extraction_asset,
    # duckdb_schema_asset,
    # duckdb_nodes_asset,
    # duckdb_relations_asset,
    # duckdb_vector_index_asset,
]
```

    |-- pydantic_models_dagster.py

``` py
# zhz_rag_pipeline/pydantic_models_dagster.py
from typing import List, Dict, Any, Union, Optional, Literal
# --- 修改：从 pydantic 导入 BaseModel 和 Field ---
from pydantic import BaseModel, Field
# --- 修改结束 ---
import uuid
# from typing import List # 这行是多余的，因为上面已经从 typing 导入了 List

class LoadedDocumentOutput(BaseModel):
    document_path: str
    file_type: str
    # --- VITAL FIX: Make raw_content optional and always a string ---
    raw_content: Optional[str] = None # raw_content is now optional and will only hold decoded text
    metadata: Dict[str, Any]

# --- 修改：在 ParsedDocumentOutput 定义之前定义其依赖的 Element 类型 ---
class DocumentElementMetadata(BaseModel):
    """通用元数据，可附加到任何文档元素上"""
    page_number: Optional[int] = None
    source_coordinates: Optional[Dict[str, float]] = None # 例如，PDF中的bbox
    custom_properties: Optional[Dict[str, Any]] = None # 其他特定于元素的属性

class TitleElement(BaseModel):
    element_type: Literal["title"] = "title"
    text: str
    level: int # 例如 1 代表 H1, 2 代表 H2
    metadata: Optional[DocumentElementMetadata] = None

class NarrativeTextElement(BaseModel): # 普通段落文本
    element_type: Literal["narrative_text"] = "narrative_text"
    text: str
    metadata: Optional[DocumentElementMetadata] = None

class ListItemElement(BaseModel):
    element_type: Literal["list_item"] = "list_item"
    text: str
    level: int = 0 # 列表嵌套层级，0代表顶层列表项
    ordered: bool = False # True代表有序列表项, False代表无序
    item_number: Optional[Union[int, str]] = None # 例如 "1", "a", "*"
    metadata: Optional[DocumentElementMetadata] = None

class TableElement(BaseModel):
    element_type: Literal["table"] = "table"
    text_representation: Optional[str] = None 
    markdown_representation: Optional[str] = None
    html_representation: Optional[str] = None
    caption: Optional[str] = None
    metadata: Optional[DocumentElementMetadata] = None

class CodeBlockElement(BaseModel):
    element_type: Literal["code_block"] = "code_block"
    code: str
    language: Optional[str] = None
    metadata: Optional[DocumentElementMetadata] = None

class ImageElement(BaseModel): 
    element_type: Literal["image"] = "image"
    alt_text: Optional[str] = None
    caption: Optional[str] = None
    metadata: Optional[DocumentElementMetadata] = None

class PageBreakElement(BaseModel):
    element_type: Literal["page_break"] = "page_break"
    metadata: Optional[DocumentElementMetadata] = None
    
class HeaderElement(BaseModel):
    element_type: Literal["header"] = "header"
    text: str
    metadata: Optional[DocumentElementMetadata] = None

class FooterElement(BaseModel):
    element_type: Literal["footer"] = "footer"
    text: str
    metadata: Optional[DocumentElementMetadata] = None

DocumentElementType = Union[
    TitleElement, 
    NarrativeTextElement, 
    ListItemElement, 
    TableElement, 
    CodeBlockElement,
    ImageElement,
    PageBreakElement,
    HeaderElement,
    FooterElement
]

class ParsedDocumentOutput(BaseModel):
    parsed_text: str = Field(description="文档内容的线性化纯文本表示，尽可能保留语义。") 
    elements: List[DocumentElementType] = Field(default_factory=list, description="从文档中解析出的结构化元素列表。")
    original_metadata: Dict[str, Any] = Field(description="关于原始文档的元数据，如文件名、路径、大小等。")
    summary: Optional[str] = None
# --- 已有模型 ---
class ChunkOutput(BaseModel):
    chunk_id: str = Field(default_factory=lambda: str(uuid.uuid4())) # 确保 Field 被导入
    chunk_text: str
    source_document_id: str 
    chunk_metadata: Dict[str, Any]

class EmbeddingOutput(BaseModel):
    chunk_id: str 
    chunk_text: str 
    embedding_vector: List[float]
    embedding_model_name: str 
    original_chunk_metadata: Dict[str, Any]

class ExtractedEntity(BaseModel):
    text: str 
    label: str 

class ExtractedRelation(BaseModel):
    head_entity_text: str
    head_entity_label: str
    relation_type: str
    tail_entity_text: str
    tail_entity_label: str

class KGTripleSetOutput(BaseModel):
    chunk_id: str 
    extracted_entities: List[ExtractedEntity] = Field(default_factory=list)
    extracted_relations: List[ExtractedRelation] = Field(default_factory=list) 
    extraction_model_name: str 
    original_chunk_metadata: Dict[str, Any]
```

    |-- resources.py

``` py
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/zhz_rag_pipeline/resources.py

import logging
import dagster as dg
import chromadb
from typing import List, Dict, Any, Optional, Iterator
import httpx
import json
import os
from contextlib import asynccontextmanager, contextmanager
from pydantic import Field as PydanticField, PrivateAttr
import asyncio
import time 
import duckdb
import sys
from queue import Empty
from pathlib import Path


# --- 日志和硬件管理器导入 ---
try:
    from zhz_rag.utils.interaction_logger import get_logger
except ImportError:
    import logging
    def get_logger(name: str) -> logging.Logger:
        logger = logging.getLogger(name)
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        return logger

try:
    from zhz_rag.utils.hardware_manager import HardwareManager, HardwareInfo
except ImportError as e_hal_import:
    print(f"ERROR: Failed to import HardwareManager/HardwareInfo: {e_hal_import}. HAL features will be disabled.")
    HardwareManager = None
    HardwareInfo = None


# --- GGUFEmbeddingResource: API客户端版本 ---

class GGUFEmbeddingResourceConfig(dg.Config):
    """
    GGUFEmbeddingResource 的配置类。
    """
    api_url: str = PydanticField(
        default="http://127.0.0.1:8089",
        description="URL of the standalone embedding API service."
    )

class GGUFEmbeddingResource(dg.ConfigurableResource):
    """
    用于与GGUF嵌入API服务交互的Dagster资源。
    负责初始化HTTP客户端、执行健康检查和文本编码。
    """
    api_url: str

    _client: httpx.AsyncClient = PrivateAttr()
    _logger: Optional[dg.DagsterLogManager] = PrivateAttr(default=None)
    _dimension: Optional[int] = PrivateAttr(default=None)
    _batch_size: int = PrivateAttr(default=128)

    def setup_for_execution(self, context: Optional[dg.InitResourceContext] = None) -> None:
        """
        初始化资源。现在可以接受一个可选的Dagster上下文。
        如果context为None（在FastAPI等非Dagster环境中使用），则使用默认配置。

        Args:
            context: Dagster的初始化资源上下文，可选。
        """
        self._logger = context.log if context else logging.getLogger("GGUFEmbeddingResource")
        self._client = httpx.AsyncClient(base_url=self.api_url, timeout=600.0)

        # 动态计算批处理大小
        try:
            if context and hasattr(context, 'resources_by_key') and "system_resource" in context.resources_by_key:
                system_resource = context.resources_by_key["system_resource"]
                physical_cores = system_resource._hw_info.cpu_physical_cores if system_resource._hw_info else 4
                self._batch_size = max(128, physical_cores * 64)
            else:
                self._batch_size = 128
        except Exception as e:
            self._logger.error(f"Failed to dynamically set batch size: {e}. Using default 128.", exc_info=True)
            self._batch_size = 128
        # 健康检查
        try:
            response = httpx.get(f"{self.api_url}/health")
            response.raise_for_status()
            health_data = response.json()
            if health_data.get("model_loaded"):
                self._dimension = health_data.get("dimension")
            else:
                raise RuntimeError(f"Embedding service at {self.api_url} is not healthy.")
        except Exception as e:
            self._logger.error(f"Failed to connect to embedding service: {e}")
            raise RuntimeError("Could not initialize GGUFEmbeddingResource.") from e

    def teardown_for_execution(self, context: dg.InitResourceContext) -> None:
        """
        在资源执行结束后关闭HTTP客户端。

        Args:
            context: Dagster的初始化资源上下文。
        """
        if hasattr(self, '_client') and not self._client.is_closed:
            async def _close():
                await self._client.aclose()
            try:
                loop = asyncio.get_running_loop()
                loop.create_task(_close())
            except RuntimeError:
                asyncio.run(_close())

    def get_embedding_dimension(self) -> int:
        """
        获取嵌入向量的维度。

        Returns:
            int: 嵌入向量的维度。

        Raises:
            ValueError: 如果嵌入维度不可用。
        """
        if self._dimension is None:
            raise ValueError("Embedding dimension not available.")
        return self._dimension

    def encode(self, texts: List[str], **kwargs: Any) -> List[List[float]]:
        """
        将文本列表编码为嵌入向量。

        Args:
            texts: 待编码的文本列表。
            **kwargs: 额外的关键字参数。

        Returns:
            List[List[float]]: 文本对应的嵌入向量列表。
        """
        if not texts:
            return []

        all_embeddings: List[List[float]] = []
        
        # 批处理循环
        for i in range(0, len(texts), self._batch_size):
            batch_texts = texts[i:i + self._batch_size]
            
            async def _async_encode_batch():
                try:
                    response = await self._client.post("/embed", json={"texts": batch_texts})
                    response.raise_for_status()
                    data = response.json()
                    return data.get("embeddings", [])
                except httpx.RequestError as e:
                    self._logger.error(f"Request to embedding service failed for a batch: {e}")
                    return [[] for _ in batch_texts]
                except Exception as e:
                    self._logger.error(f"An unexpected error occurred during embedding a batch: {e}")
                    return [[] for _ in batch_texts]

            # 在循环内部执行异步调用
            try:
                loop = asyncio.get_running_loop()
                if loop.is_running():
                    future = asyncio.run_coroutine_threadsafe(_async_encode_batch(), loop)
                    batch_embeddings = future.result(timeout=600)
                else:
                    batch_embeddings = asyncio.run(_async_encode_batch())
            except RuntimeError:
                batch_embeddings = asyncio.run(_async_encode_batch())

            all_embeddings.extend(batch_embeddings)
            time.sleep(0.1) # 在批次之间加入一个微小的延迟，避免瞬间打爆API

        return all_embeddings

class ChromaDBResourceConfig(dg.Config):
    """
    ChromaDBResource 的配置类。
    """
    collection_name: str = PydanticField(
        default="zhz_rag_collection",
        description="Name of the ChromaDB collection."
    )
    persist_directory: str = PydanticField(
        default=os.path.join(os.getenv("ZHZ_AGENT_PROJECT_ROOT", "/home/zhz/zhz_agent"), "zhz_rag", "stored_data", "chromadb_index"),
        description="Directory to persist ChromaDB data."
    )

class ChromaDBResource(dg.ConfigurableResource):
    """
    用于管理ChromaDB向量数据库的Dagster资源。
    支持初始化客户端、添加嵌入和查询嵌入。
    """
    collection_name: str
    persist_directory: str

    _client: Optional[chromadb.PersistentClient] = PrivateAttr(default=None)
    _collection: Optional[Any] = PrivateAttr(default=None)
    _logger: Optional[dg.DagsterLogManager] = PrivateAttr(default=None)
    _batch_size: int = PrivateAttr(default=4096)

    def setup_for_execution(self, context: dg.InitResourceContext) -> None:
        """
        初始化ChromaDB客户端和集合。

        Args:
            context: Dagster的初始化资源上下文。

        Raises:
            RuntimeError: 如果ChromaDB客户端或集合初始化失败。
        """
        self._logger = context.log
        os.makedirs(self.persist_directory, exist_ok=True)
        
        try:
            self._client = chromadb.PersistentClient(path=self.persist_directory)
            self._collection = self._client.get_or_create_collection(name=self.collection_name)
        except Exception as e:
            self._logger.error(f"Failed to initialize ChromaDB client or collection: {e}", exc_info=True)
            raise RuntimeError(f"Could not initialize ChromaDBResource due to: {e}") from e

    def teardown_for_execution(self, context: dg.InitResourceContext) -> None:
        """
        清理ChromaDB资源。

        Args:
            context: Dagster的初始化资源上下文。
        """
        if self._client:
            pass # PersistentClient doesn't need explicit close
        self._client = None
        self._collection = None

    def add_embeddings(
        self, 
        ids: List[str], 
        embeddings: List[List[float]], 
        documents: Optional[List[str]] = None, 
        metadatas: Optional[List[Dict[str, Any]]] = None
    ) -> None:
        """
        向ChromaDB集合添加嵌入向量。

        Args:
            ids: 文档ID列表。
            embeddings: 嵌入向量列表。
            documents: 原始文档文本列表，可选。
            metadatas: 文档元数据列表，可选。

        Raises:
            RuntimeError: 如果ChromaDB集合未初始化。
            Exception: 如果添加批次到ChromaDB失败。
        """
        if self._collection is None:
            msg = "ChromaDB collection is not initialized. Cannot add embeddings."
            self._logger.error(msg)
            raise RuntimeError(msg)
        
        if not ids:
            self._logger.warning("add_embeddings called with empty IDs list. Nothing to add.")
            return

        total_items = len(ids)
        for i in range(0, total_items, self._batch_size):
            batch_end = min(i + self._batch_size, total_items)
            batch_ids = ids[i:batch_end]
            batch_embeddings = embeddings[i:batch_end]
            batch_documents = documents[i:batch_end] if documents else None
            batch_metadatas = metadatas[i:batch_end] if metadatas else None

            try:
                self._collection.add(
                    ids=batch_ids,
                    embeddings=batch_embeddings,
                    documents=batch_documents,
                    metadatas=batch_metadatas
                )
            except Exception as e:
                self._logger.error(
                    f"Failed to add batch starting at index {i} to ChromaDB: {e}",
                    exc_info=True
                )
                raise
        
    def query_embeddings(
        self,
        query_embeddings: List[List[float]],
        n_results: int = 5,
        where_filter: Optional[Dict[str, Any]] = None,
        include: Optional[List[str]] = None
    ) -> Optional[Dict[str, Any]]:
        """
        从ChromaDB集合查询嵌入向量。

        Args:
            query_embeddings: 查询嵌入向量列表。
            n_results: 返回结果的数量，默认为5。
            where_filter: 用于过滤结果的条件字典，可选。
            include: 包含在结果中的字段，默认为["metadatas", "documents", "distances"]。

        Returns:
            Optional[Dict[str, Any]]: 查询结果字典，如果查询失败则为None。

        Raises:
            RuntimeError: 如果ChromaDB集合未初始化。
            Exception: 如果查询嵌入失败。
        """
        if self._collection is None:
            msg = "ChromaDB collection is not initialized. Cannot query embeddings."
            self._logger.error(msg)
            raise RuntimeError(msg)

        if include is None:
            include = ["metadatas", "documents", "distances"]

        try:
            results = self._collection.query(
                query_embeddings=query_embeddings,
                n_results=n_results,
                where=where_filter,
                include=include
            )
            return results
        except Exception as e:
            self._logger.error(f"Failed to query embeddings from ChromaDB: {e}", exc_info=True)
            raise
        

class LocalLLMAPIResourceConfig(dg.Config):
    """
    LocalLLMAPIResource 的配置类。
    """
    api_url: str = "http://127.0.0.1:8088/v1/chat/completions"
    default_temperature: float = 0.1
    default_max_new_tokens: int = 2048

class LocalLLMAPIResource(dg.ConfigurableResource):
    """
    用于与本地LLM API服务交互的Dagster资源。
    """
    api_url: str
    default_temperature: float
    default_max_new_tokens: int
    _logger: Optional[dg.DagsterLogManager] = PrivateAttr(default=None)

    def setup_for_execution(self, context: dg.InitResourceContext) -> None:
        """
        初始化本地LLM API资源。

        Args:
            context: Dagster的初始化资源上下文。
        """
        self._logger = context.log

    async def generate_structured_output(self, prompt: str, json_schema: Dict[str, Any], temperature: Optional[float] = None, max_new_tokens: Optional[int] = None) -> Dict[str, Any]:
        """
        从本地LLM生成结构化输出。

        Args:
            prompt: 发送给LLM的提示。
            json_schema: 期望的JSON输出结构。
            temperature: 生成温度，可选。
            max_new_tokens: 生成的最大新token数量，可选。

        Returns:
            Dict[str, Any]: LLM生成的结构化JSON输出。

        Raises:
            ValueError: 如果LLM响应格式不正确。
            Exception: 如果调用本地LLM失败。
        """
        logger_instance = self._logger if self._logger else dg.get_dagster_logger()
        temp_to_use = temperature if temperature is not None else self.default_temperature
        tokens_to_use = max_new_tokens if max_new_tokens is not None else self.default_max_new_tokens
        messages = [{"role": "user", "content": prompt}]
        payload = {"model": "local_kg_extraction_model", "messages": messages, "temperature": temp_to_use, "max_tokens": tokens_to_use, "response_format": {"type": "json_object", "schema": json_schema}}
        try:
            async with httpx.AsyncClient(timeout=httpx.Timeout(300.0)) as client:
                response = await client.post(self.api_url, json=payload)
                response.raise_for_status()
                response_json = response.json()
                if response_json.get("choices") and response_json["choices"][0].get("message"):
                    generated_text = response_json["choices"][0]["message"].get("content", "")
                    return json.loads(generated_text)
                raise ValueError(f"Local LLM response format is incorrect: {response_json}")
        except Exception as e:
            logger_instance.error(f"Error during Local LLM call: {e}", exc_info=True)
            raise

class GeminiAPIResourceConfig(dg.Config):
    """
    GeminiAPIResource 的配置类。
    """
    model_name: str = PydanticField(default="gemini/gemini-1.5-flash-latest", description="Name of the Gemini model.")
    proxy_url: Optional[str] = PydanticField(default_factory=lambda: os.getenv("LITELLM_PROXY_URL"), description="Optional proxy URL for LiteLLM.")
    default_temperature: float = 0.1
    default_max_tokens: int = 2048
    
class GeminiAPIResource(dg.ConfigurableResource):
    """
    用于通过LiteLLM调用Gemini API的Dagster资源。
    """
    model_name: str
    proxy_url: Optional[str]
    default_temperature: float
    default_max_tokens: int
    _api_key: Optional[str] = PrivateAttr(default=None)
    _logger: Optional[dg.DagsterLogManager] = PrivateAttr(default=None)

    def setup_for_execution(self, context: dg.InitResourceContext) -> None:
        """
        初始化Gemini API资源，加载API密钥。

        Args:
            context: Dagster的初始化资源上下文。
        """
        self._logger = context.log
        self._api_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
        if not self._api_key: self._logger.warning("Gemini API key not found.")

    async def call_completion(self, messages: List[Dict[str, str]], temperature: Optional[float] = None, max_tokens: Optional[int] = None) -> Optional[str]:
        """
        调用Gemini API生成文本补全。

        Args:
            messages: 对话消息列表。
            temperature: 生成温度，可选。
            max_tokens: 生成的最大token数量，可选。

        Returns:
            Optional[str]: 生成的文本内容，如果调用失败则为None。
        """
        import litellm
        logger_instance = self._logger if self._logger else dg.get_dagster_logger()
        if not self._api_key: return None
        litellm_params = {"model": self.model_name, "messages": messages, "api_key": self._api_key, "temperature": temperature or self.default_temperature, "max_tokens": max_tokens or self.default_max_tokens}
        if self.proxy_url: litellm_params["proxy"] = {"http": self.proxy_url, "https": self.proxy_url}
        try:
            response = await litellm.acompletion(**litellm_params)
            return response.choices[0].message.content if response and response.choices else None
        except Exception as e:
            logger_instance.error(f"Error calling Gemini via LiteLLM: {e}", exc_info=True)
            return None
        
class DuckDBResource(dg.ConfigurableResource):
    """
    用于管理DuckDB数据库连接的Dagster资源。
    支持连接数据库、加载VSS扩展和执行检查点。
    """
    db_file_path: str = PydanticField(
        default=os.path.join(os.getenv("ZHZ_AGENT_PROJECT_ROOT", "/home/zhz/zhz_agent"), "zhz_rag", "stored_data", "duckdb_knowledge_graph.db"),
        description="Path to the DuckDB database file."
    )
    _conn: Optional[duckdb.DuckDBPyConnection] = PrivateAttr(default=None)
    _logger: Optional[dg.DagsterLogManager] = PrivateAttr(default=None)

    def setup_for_execution(self, context: dg.InitResourceContext) -> None:
        """
        初始化DuckDB连接并加载VSS扩展。

        Args:
            context: Dagster的初始化资源上下文。

        Raises:
            RuntimeError: 如果DuckDB连接或VSS设置失败。
        """
        self._logger = context.log

        os.makedirs(os.path.dirname(self.db_file_path), exist_ok=True)
        
        try:
            self._conn = duckdb.connect(database=self.db_file_path, read_only=False)

            self._conn.execute("INSTALL vss;")
            self._conn.execute("LOAD vss;")
            self._conn.execute("SET hnsw_enable_experimental_persistence=true;")

        except Exception as e:
            self._logger.error(f"Error during DuckDB connection or VSS setup: {e}", exc_info=True)
            error_str = str(e).lower()
            if "already installed" in error_str or "already loaded" in error_str:
                self._logger.warning(f"VSS extension seems to be already installed/loaded, continuing...")
            else:
                raise RuntimeError(f"DuckDB connection/VSS setup failed: {e}") from e
        
    def teardown_for_execution(self, context: dg.InitResourceContext) -> None:
        """
        在资源执行结束后关闭DuckDB连接并执行检查点。

        Args:
            context: Dagster的初始化资源上下文。
        """
        if self._conn:
            try:
                self._conn.execute("CHECKPOINT;")
            except Exception as e_checkpoint:
                self._logger.error(f"Error executing CHECKPOINT for DuckDB: {e_checkpoint}", exc_info=True)
            finally:
                self._conn.close()
                self._conn = None 
        else:
            self._logger.info("No active DuckDB connection to teardown.")

    @contextmanager
    def get_connection(self) -> Iterator[duckdb.DuckDBPyConnection]:
        """
        获取DuckDB数据库连接的上下文管理器。

        Yields:
            duckdb.DuckDBPyConnection: DuckDB数据库连接对象。

        Raises:
            ConnectionError: 如果DuckDB连接未建立。
        """
        if not self._conn:
            raise ConnectionError("DuckDB connection not established. Ensure setup_for_execution was successful.")
        yield self._conn

class SystemResource(dg.ConfigurableResource):
    """
    用于获取系统硬件信息和推荐并发任务数的Dagster资源。
    """
    _hw_manager: Optional[Any] = PrivateAttr(default=None)
    _hw_info: Optional[Any] = PrivateAttr(default=None)
    _logger: Optional[dg.DagsterLogManager] = PrivateAttr(default=None)

    def setup_for_execution(self, context: dg.InitResourceContext) -> None:
        """
        初始化系统资源，检测硬件信息。

        Args:
            context: Dagster的初始化资源上下文。
        """
        self._logger = context.log
        if HardwareManager:
            self._hw_manager = HardwareManager()
            self._hw_info = self._hw_manager.get_hardware_info()
        else:
            self._logger.warning("HardwareManager not available.")

    def get_recommended_concurrent_tasks(self, task_type: str = "cpu_bound_llm") -> int:
        """
        获取推荐的并发任务数。

        Args:
            task_type: 任务类型，默认为"cpu_bound_llm"。

        Returns:
            int: 推荐的并发任务数。
        """
        if self._hw_manager: return self._hw_manager.recommend_concurrent_tasks(task_type=task_type)
        return 1

```

        |-- __init__.py

``` py
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/zhz_rag_pipeline/parsers/__init__.py
import logging # 添加 logging 导入
from typing import Callable, Dict, Any, Optional, Union

# 尝试导入 Pydantic 模型，如果失败，则类型别名使用 Any
try:
    from ..pydantic_models_dagster import ParsedDocumentOutput
    _ParserOutputType = Optional[ParsedDocumentOutput]
except ImportError:
    _ParserOutputType = Optional[Any] # Fallback

# 定义一个类型别名，表示解析函数的签名
# 输入可以是路径(str)或内容(str/bytes)，元数据字典，返回Pydantic模型或字典
ParserFunction = Callable[[Union[str, bytes], Dict[str, Any]], _ParserOutputType]

# 从各个解析器模块导入主解析函数
from .md_parser import parse_markdown_to_structured_output
from .docx_parser import parse_docx_to_structured_output
from .pdf_parser import parse_pdf_to_structured_output
from .xlsx_parser import parse_xlsx_to_structured_output
from .html_parser import parse_html_to_structured_output
from .txt_parser import parse_txt_to_structured_output

logger = logging.getLogger(__name__) # 添加 logger 实例

# 创建一个解析器注册表 (合并自 parser_dispatcher.py)
PARSER_REGISTRY: Dict[str, ParserFunction] = {
    ".md": parse_markdown_to_structured_output,
    ".docx": parse_docx_to_structured_output,
    ".pdf": parse_pdf_to_structured_output,
    ".xlsx": parse_xlsx_to_structured_output,
    ".html": parse_html_to_structured_output,
    ".htm": parse_html_to_structured_output,  # Alias for html
    ".txt": parse_txt_to_structured_output,
}

def dispatch_parsing( # 合并自 parser_dispatcher.py
    file_extension: str,
    content_or_path: Union[str, bytes], # 确保这里是 Union[str, bytes]
    original_metadata: Dict[str, Any]
) -> Optional[Any]: # 返回 Optional[Any] 以匹配下游期望
    parser_func = PARSER_REGISTRY.get(file_extension.lower())
    if parser_func:
        try:
            # 调用相应的解析函数
            # txt_parser 和 md_parser, html_parser 期望 content_str
            # docx_parser, pdf_parser, xlsx_parser 期望 file_path
            # content_or_path 变量在 ingestion_assets.py 中已经根据 file_ext 做了区分
            return parser_func(content_or_path, original_metadata)
        except Exception as e:
            logger.error(f"Error calling parser for '{file_extension}' on '{original_metadata.get('source_file_path', 'N/A')}': {e}", exc_info=True)
            return None # 解析失败返回 None
    else:
        logger.warning(f"No specific parser registered for file type '{file_extension}'.")
        # 尝试一个通用的纯文本提取作为最终回退（如果适用且有实现）
        # 或者直接返回None
        return None

def get_parser(file_extension: str) -> Optional[ParserFunction]: # 保留此函数以防其他地方用到
    return PARSER_REGISTRY.get(file_extension.lower())

__all__ = [
    "parse_markdown_to_structured_output",
    "parse_docx_to_structured_output",
    "parse_pdf_to_structured_output",
    "parse_xlsx_to_structured_output",
    "parse_html_to_structured_output",
    "parse_txt_to_structured_output",
    "get_parser", # 保留
    "dispatch_parsing", # 新增导出
    "PARSER_REGISTRY",
    "ParserFunction"
]
```

        |-- docx_parser.py

``` py
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/zhz_rag_pipeline/parsers/docx_parser.py

import os
from typing import List, Dict, Any, Optional, Union
import re

# --- 添加：为当前模块的 logger 进行基本配置 ---
import logging
logger = logging.getLogger(__name__)
if not logger.handlers: # 避免重复添加 handler (如果模块被多次导入)
    handler = logging.StreamHandler() # 输出到 stderr，通常会被 Dagster 捕获
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    logger.setLevel(logging.INFO) # 设置希望看到的最低日志级别 (INFO, DEBUG等)
    # logger.propagate = False # 可以考虑设置，防止日志向上传播到根logger导致重复打印，但通常 Dagster 会处理好
logger.info(f"Logger for {__name__} configured in docx_parser.py.") # 确认配置生效
# --- 结束添加 ---

# --- 依赖导入与可用性检查 ---
try:
    from unstructured.partition.docx import partition_docx
    from unstructured.documents.elements import (
        Element as UnstructuredElement,
        Text, 
        NarrativeText,
        Title,
        ListItem,
        Table,
        Image as UnstructuredImage, 
        Header as UnstructuredHeader, 
        Footer as UnstructuredFooter, 
        Address,
        EmailAddress,
        FigureCaption,
        PageBreak as UnstructuredPageBreak, 
        CodeSnippet
    )
    _UNSTRUCTURED_AVAILABLE_DOCX = True
    logging.info("Successfully imported Unstructured for DOCX parsing.")
except ImportError as e_unstructured:
    logging.error(f"Failed to import Unstructured for DOCX: {e_unstructured}. DOCX parsing will have limited functionality.")
    _UNSTRUCTURED_AVAILABLE_DOCX = False
    # 创建占位符类以避免后续 NameError
    class UnstructuredElement: pass
    class Text: pass                  # type: ignore
    class NarrativeText: pass          # type: ignore
    class Title: pass                  # type: ignore
    class ListItem: pass               # type: ignore
    class Table: pass                  # type: ignore
    class UnstructuredImage: pass      # type: ignore
    class UnstructuredHeader: pass     # type: ignore
    class UnstructuredFooter: pass     # type: ignore
    class Address: pass                # type: ignore
    class EmailAddress: pass           # type: ignore
    class FigureCaption: pass          # type: ignore
    class UnstructuredPageBreak: pass  # type: ignore
    class CodeSnippet: pass            # type: ignore

try:
    from markdownify import markdownify as md # type: ignore
    _MARKDOWNIFY_AVAILABLE = True
except ImportError:
    logging.warning("markdownify library not found. HTML table to Markdown conversion will be skipped.")
    _MARKDOWNIFY_AVAILABLE = False
    def md(html_content: str) -> str: # Fallback
        return f"[Markdownify not available. HTML content: {html_content[:100]}...]"

_PYDANTIC_MODELS_AVAILABLE_DOCX = False
try:
    from ..pydantic_models_dagster import (
        ParsedDocumentOutput, DocumentElementType, TitleElement, NarrativeTextElement,
        ListItemElement, TableElement, CodeBlockElement, PageBreakElement, ImageElement,
        HeaderElement, FooterElement, DocumentElementMetadata
    )
    _PYDANTIC_MODELS_AVAILABLE_DOCX = True
except ImportError:
    class BaseModel: pass
    class DocumentElementMetadata(BaseModel): page_number: Optional[int] = None
    
    class ParsedDocumentOutput(BaseModel): parsed_text: str; elements: list; original_metadata: dict; summary: Optional[str] = None
    class TitleElement(BaseModel): element_type:str="title"; text:str; level:int; metadata: Optional[DocumentElementMetadata] = None
    class NarrativeTextElement(BaseModel): element_type:str="narrative_text"; text:str; metadata: Optional[DocumentElementMetadata] = None
    class ListItemElement(BaseModel): element_type:str="list_item"; text:str; level:int=0; ordered:bool=False; item_number:Optional[Union[int, str]]=None; metadata: Optional[DocumentElementMetadata] = None
    class TableElement(BaseModel): # <--- 修改此行
        element_type:str="table"; 
        markdown_representation:Optional[str]=None; 
        html_representation:Optional[str]=None; 
        text_representation:Optional[str]=None; # <--- 添加此字段
        caption:Optional[str]=None; 
        metadata: Optional[DocumentElementMetadata] = None
    class CodeBlockElement(BaseModel): element_type:str="code_block"; code:str; language:Optional[str]=None; metadata: Optional[DocumentElementMetadata] = None
    class PageBreakElement(BaseModel): element_type:str="page_break"; metadata: Optional[DocumentElementMetadata] = None
    class ImageElement(BaseModel): element_type:str="image"; alt_text:Optional[str]=None; caption:Optional[str]=None; metadata: Optional[DocumentElementMetadata] = None
    class HeaderElement(BaseModel): element_type:str="header"; text:str; metadata: Optional[DocumentElementMetadata] = None
    class FooterElement(BaseModel): element_type:str="footer"; text:str; metadata: Optional[DocumentElementMetadata] = None
    DocumentElementType = Any

logger = logging.getLogger(__name__)

# --- 辅助函数 ---
def _create_doc_element_metadata(unstructured_element: UnstructuredElement) -> Optional[Union[DocumentElementMetadata, Dict[str, Any]]]:
    if not hasattr(unstructured_element, 'metadata'):
        return None
        
    meta_data_dict: Dict[str, Any] = {}
    if hasattr(unstructured_element.metadata, 'page_number') and unstructured_element.metadata.page_number is not None:
        meta_data_dict['page_number'] = unstructured_element.metadata.page_number
    
    if hasattr(unstructured_element.metadata, 'filename'):
        meta_data_dict['source_filename'] = unstructured_element.metadata.filename
    if hasattr(unstructured_element.metadata, 'filetype'):
        meta_data_dict['source_filetype'] = unstructured_element.metadata.filetype

    if not meta_data_dict:
        return None

    if _PYDANTIC_MODELS_AVAILABLE_DOCX:
        return DocumentElementMetadata(**meta_data_dict)
    else:
        return meta_data_dict

def _convert_unstructured_elements_to_custom(
    unstructured_elements: List[UnstructuredElement], 
    doc_path_for_log: str # 添加一个参数用于日志记录
) -> List[Any]:
    custom_elements: List[Any] = []
    
    file_basename_for_log = os.path.basename(doc_path_for_log)

    # --- 使用 print 进行强制调试 ---
    logger.info(f"DOCX Parser ({file_basename_for_log}): _convert_unstructured_elements_to_custom received {len(unstructured_elements)} elements from unstructured.")
    if not unstructured_elements:
        logger.warning(f"DOCX Parser ({file_basename_for_log}): Unstructured returned an empty list of elements. No custom elements will be generated by this function initially.")
    
    if not _UNSTRUCTURED_AVAILABLE_DOCX:
        logger.warning(f"DOCX Parser ({file_basename_for_log}): Unstructured library is not available (should have been caught earlier).")
        # 作为回退，我们可以尝试将每个元素的文本提取为 NarrativeTextElement (如果 unstructured_elements 非空但 _UNSTRUCTURED_AVAILABLE_DOCX 意外为 False)
        for el_idx, el_fallback in enumerate(unstructured_elements):
            fallback_text = getattr(el_fallback, 'text', f"[Unstructured not fully available - Element {el_idx+1} in {file_basename_for_log}]").strip()
            if fallback_text:
                if _PYDANTIC_MODELS_AVAILABLE_DOCX:
                    custom_elements.append(NarrativeTextElement(text=fallback_text))
                else:
                    custom_elements.append({"element_type": "narrative_text", "text": fallback_text})
        return custom_elements

    for el_idx, el in enumerate(unstructured_elements):
        el_type_name = type(el).__name__
        el_id_str = getattr(el, 'id', 'N/A')
        el_text_preview = getattr(el, 'text', '')[:50].strip().replace('\n', ' ') if getattr(el, 'text', '') else "[NO TEXT]"
        # --- 修改日志级别 ---
        logger.debug( # <--- 从 info 修改为 debug
            f"DOCX Parser ({file_basename_for_log}): Processing unstructured element index {el_idx}, "
            f"Type: {el_type_name}, ID: {el_id_str}, Text Preview: '{el_text_preview}'"
        )
        
        # 打印 el.metadata.text_as_html 的预览（如果存在）
        html_preview_from_meta = getattr(el.metadata, 'text_as_html', None) if hasattr(el, 'metadata') else None
        if html_preview_from_meta:
            logger.debug( # <--- 从 info 修改为 debug
                f"  └─ ({file_basename_for_log}) Unstructured Element (idx {el_idx}, type {el_type_name}) has text_as_html (len: {len(html_preview_from_meta)}). Preview: {html_preview_from_meta[:70]}"
            )
        # --- 结束修改 ---
        
        element_metadata = _create_doc_element_metadata(el)
        el_text = el.text.strip() if hasattr(el, 'text') and el.text else ""
        custom_el: Optional[Any] = None

        if isinstance(el, Title):
            level = el.metadata.category_depth if hasattr(el.metadata, 'category_depth') and el.metadata.category_depth is not None else 1
            if el_text:
                logger.debug(f"DOCX Parser ({file_basename_for_log}): Element index {el_idx} is Title (level {level}).")
                if _PYDANTIC_MODELS_AVAILABLE_DOCX: custom_el = TitleElement(text=el_text, level=level, metadata=element_metadata)
                else: custom_el = {"element_type": "title", "text": el_text, "level": level, "metadata": element_metadata}
        
        elif isinstance(el, ListItem):
            level = el.metadata.category_depth if hasattr(el.metadata, 'category_depth') and el.metadata.category_depth is not None else 0
            # 尝试从元数据获取更精确的列表信息 (unstructured 可能提供)
            # item_number 和 ordered 的逻辑可以根据 unstructured 的实际输出来完善
            if el_text:
                logger.debug(f"DOCX Parser ({file_basename_for_log}): Element index {el_idx} is ListItem (level {level}).")
                if _PYDANTIC_MODELS_AVAILABLE_DOCX: custom_el = ListItemElement(text=el_text, level=level, metadata=element_metadata)
                else: custom_el = {"element_type": "list_item", "text": el_text, "level": level, "metadata": element_metadata}

        elif isinstance(el, Table):
            logger.info(f"DOCX Parser ({file_basename_for_log}): Element index {el_idx} IS an unstructured.documents.elements.Table object.")
            html_table = el.metadata.text_as_html if hasattr(el.metadata, 'text_as_html') else None
            
            if html_table:
                logger.info(f"DOCX Parser ({file_basename_for_log}): Found HTML table content for Table element (idx {el_idx}). Length: {len(html_table)}. Preview: {html_table[:150]}")
            else:
                logger.warning(f"DOCX Parser ({file_basename_for_log}): No HTML table content (el.metadata.text_as_html) found for Table element (idx {el_idx}) from unstructured. Element ID: {el.id if hasattr(el, 'id') else 'N/A'}")

            md_table = None
            if html_table and _MARKDOWNIFY_AVAILABLE:
                try: 
                    md_table = md(html_table)
                    logger.info(f"DOCX Parser ({file_basename_for_log}): Successfully converted HTML table (idx {el_idx}) to Markdown. MD Length: {len(md_table) if md_table else 0}")
                except Exception as e_md: 
                    logger.warning(f"DOCX Parser ({file_basename_for_log}): Failed to convert HTML table (idx {el_idx}) to Markdown: {e_md}. HTML: {html_table[:100]}")
            
            raw_table_text_fallback = el.text.strip() if hasattr(el, 'text') and el.text else None
            caption_text = None
            if hasattr(el.metadata, 'table_captions') and el.metadata.table_captions:
                    caption_obj = el.metadata.table_captions[0]
                    if hasattr(caption_obj, 'text'):
                            caption_text = caption_obj.text
            
            if not caption_text and hasattr(el.metadata, 'filename'): # Redundant if filename is always doc_path_for_log
                    caption_text = f"Table from {file_basename_for_log}" # Use basename
            final_caption = caption_text if caption_text else "Table"

            final_md_table = md_table
            final_html_table = html_table
            final_text_representation = None

            if not final_md_table and not final_html_table and raw_table_text_fallback:
                logger.info(f"DOCX Parser ({file_basename_for_log}): Table (idx {el_idx}) has no HTML/MD rep, but has raw text from unstructured: '{raw_table_text_fallback[:100]}...' Using it as text_representation.")
                final_text_representation = raw_table_text_fallback
            elif not final_md_table and not final_html_table and not raw_table_text_fallback:
                    logger.warning(f"DOCX Parser ({file_basename_for_log}): Table (idx {el_idx}) has no HTML, Markdown, or raw text representation from unstructured.")


            if _PYDANTIC_MODELS_AVAILABLE_DOCX: 
                custom_el = TableElement(
                    markdown_representation=final_md_table, 
                    html_representation=final_html_table, 
                    text_representation=final_text_representation,
                    caption=final_caption, 
                    metadata=element_metadata
                )
            else: 
                custom_el = {
                    "element_type": "table", 
                    "markdown_representation": final_md_table, 
                    "html_representation": final_html_table, 
                    "text_representation": final_text_representation,
                    "caption": final_caption, 
                    "metadata": element_metadata
                }
        
        elif isinstance(el, (NarrativeText, Text, Address, EmailAddress, FigureCaption)):
            if el_text:
                logger.debug(f"DOCX Parser ({file_basename_for_log}): Element index {el_idx} is NarrativeText/Text like.")
                if _PYDANTIC_MODELS_AVAILABLE_DOCX: custom_el = NarrativeTextElement(text=el_text, metadata=element_metadata)
                else: custom_el = {"element_type": "narrative_text", "text": el_text, "metadata": element_metadata}
        
        elif isinstance(el, UnstructuredHeader):
            if el_text:
                logger.debug(f"DOCX Parser ({file_basename_for_log}): Element index {el_idx} is Header.")
                if _PYDANTIC_MODELS_AVAILABLE_DOCX: custom_el = HeaderElement(text=el_text, metadata=element_metadata)
                else: custom_el = {"element_type": "header", "text": el_text, "metadata": element_metadata}
        
        elif isinstance(el, UnstructuredFooter):
            if el_text:
                logger.debug(f"DOCX Parser ({file_basename_for_log}): Element index {el_idx} is Footer.")
                if _PYDANTIC_MODELS_AVAILABLE_DOCX: custom_el = FooterElement(text=el_text, metadata=element_metadata)
                else: custom_el = {"element_type": "footer", "text": el_text, "metadata": element_metadata}

        elif isinstance(el, UnstructuredImage):
            alt_text = el_text if el_text else (el.metadata.filename if hasattr(el.metadata, 'filename') else "Image")
            logger.debug(f"DOCX Parser ({file_basename_for_log}): Element index {el_idx} is Image. Alt text: {alt_text}")
            if _PYDANTIC_MODELS_AVAILABLE_DOCX: custom_el = ImageElement(alt_text=alt_text, metadata=element_metadata)
            else: custom_el = {"element_type": "image", "alt_text": alt_text, "metadata": element_metadata}

        elif isinstance(el, UnstructuredPageBreak):
            logger.debug(f"DOCX Parser ({file_basename_for_log}): Element index {el_idx} is PageBreak.")
            if _PYDANTIC_MODELS_AVAILABLE_DOCX: custom_el = PageBreakElement(metadata=element_metadata)
            else: custom_el = {"element_type": "page_break", "metadata": element_metadata}
        
        elif isinstance(el, CodeSnippet):
            if el_text:
                logger.debug(f"DOCX Parser ({file_basename_for_log}): Element index {el_idx} is CodeSnippet.")
                if _PYDANTIC_MODELS_AVAILABLE_DOCX: custom_el = CodeBlockElement(code=el_text, metadata=element_metadata) # language can be inferred later if needed
                else: custom_el = {"element_type": "code_block", "code": el_text, "metadata": element_metadata}

        else: 
            # This is the catch-all for any other Unstructured element type
            # or if an element doesn't have text but we still want to represent it (though usually skipped if no text)
            if el_text: # Only create an element if there's text
                logger.warning(f"DOCX Parser ({file_basename_for_log}): Unhandled Unstructured element type: {el_type_name} at index {el_idx}. Treating as NarrativeText. Text: {el_text[:50]}")
                if _PYDANTIC_MODELS_AVAILABLE_DOCX: custom_el = NarrativeTextElement(text=el_text, metadata=element_metadata)
                else: custom_el = {"element_type": "narrative_text", "text": el_text, "_unstructured_type": el_type_name, "metadata": element_metadata}
            elif el_type_name != "CompositeElement": # CompositeElement often has no direct text but contains other elements
                    logger.debug(f"DOCX Parser ({file_basename_for_log}): Skipping Unstructured element type: {el_type_name} at index {el_idx} due to no text content.")

        if custom_el:
            custom_elements.append(custom_el)
            
    return custom_elements


def _generate_linear_text_from_custom_elements(elements: List[Any]) -> str:
    text_parts = []
    for el_data_any in elements:
        el_data: Dict[str, Any] = {}
        if _PYDANTIC_MODELS_AVAILABLE_DOCX and hasattr(el_data_any, 'model_dump'):
            el_data = el_data_any.model_dump(exclude_none=True)
        elif isinstance(el_data_any, dict):
            el_data = el_data_any
        else:
            continue

        el_type = el_data.get("element_type")
        text_content = el_data.get("text", "")
        
        current_element_text = ""
        if el_type == "title":
            current_element_text = f"\n{'#' * el_data.get('level',1)} {text_content}\n"
        elif el_type == "narrative_text":
            current_element_text = text_content + "\n"
        elif el_type == "list_item":
            prefix = f"{el_data.get('item_number', '')}. " if el_data.get('ordered') and el_data.get('item_number') else "- "
            indent = "  " * el_data.get('level', 0)
            current_element_text = f"{indent}{prefix}{text_content}\n"
        elif el_type == "table":
            caption = el_data.get('caption', 'Unnamed Table')
            md_repr = el_data.get('markdown_representation')
            if md_repr: current_element_text = f"\n[Table: {caption}]\n{md_repr}\n"
            elif el_data.get('html_representation'): current_element_text = f"\n[Table (HTML): {caption}]\n{el_data.get('html_representation')[:200]}...\n"
        elif el_type == "code_block":
            lang = el_data.get('language', "") or ""
            code_content = el_data.get('code', "")
            current_element_text = f"\n```{lang}\n{code_content}\n```\n"
        elif el_type == "page_break":
            current_element_text = "\n---\n"
        elif el_type == "header" or el_type == "footer":
            current_element_text = f"\n[{el_type.capitalize()}]: {text_content}\n"
        elif el_type == "image":
            alt_text = el_data.get('alt_text', 'Image')
            current_element_text = f"\n[Image: {alt_text}]\n"
        
        if current_element_text:
            text_parts.append(current_element_text)

    full_text = "".join(text_parts)
    full_text = re.sub(r'\n{3,}', '\n\n', full_text).strip() # Clean up excessive newlines
    return full_text


# --- 主解析函数 ---
def parse_docx_to_structured_output(
    file_path: str, 
    original_metadata: Dict[str, Any]
) -> Optional[Union[ParsedDocumentOutput, Dict[str, Any]]]:
    file_basename_for_log = os.path.basename(file_path)
    
    # --- 使用 print 进行强制调试 ---
    logger.info(f"DOCX Parser: Attempting to parse DOCX file: {file_basename_for_log} using Unstructured")
    
    if not _UNSTRUCTURED_AVAILABLE_DOCX:
        logger.error(f"DOCX Parser ({file_basename_for_log}): Unstructured library is not available. DOCX parsing cannot proceed.")
        return None
    try:
        unstructured_elements = partition_docx(
            filename=file_path, 
            strategy="fast", 
            infer_table_structure=True,
        )
        logger.info(f"DOCX Parser ({file_basename_for_log}): Unstructured partitioned DOCX. Found {len(unstructured_elements)} raw elements from partition_docx.")

        custom_elements = _convert_unstructured_elements_to_custom(unstructured_elements, file_path)
        logger.info(f"DOCX Parser ({file_basename_for_log}): _convert_unstructured_elements_to_custom created {len(custom_elements)} custom elements.")
        
        linear_text = _generate_linear_text_from_custom_elements(custom_elements)
        logger.info(f"DOCX Parser ({file_basename_for_log}): Generated linear text (len: {len(linear_text)}). Preview: {linear_text[:100].replace(chr(10), ' ')}")

        if not custom_elements and not linear_text.strip() and unstructured_elements:
            logger.warning(f"DOCX Parser ({file_basename_for_log}): partition_docx returned {len(unstructured_elements)} elements, "
                            "but no custom elements or linear text were generated. This might indicate all elements were skipped "
                            "or had no text content suitable for conversion.")
        elif not custom_elements and not linear_text.strip() and not unstructured_elements:
                logger.warning(f"DOCX Parser ({file_basename_for_log}): partition_docx returned 0 elements, "
                                "and no custom elements or linear text were generated.")

        if _PYDANTIC_MODELS_AVAILABLE_DOCX:
            return ParsedDocumentOutput(
                parsed_text=linear_text,
                elements=custom_elements, 
                original_metadata=original_metadata
            )
        else:
            return {
                "parsed_text": linear_text,
                "elements": custom_elements,
                "original_metadata": original_metadata
            }
            
    except FileNotFoundError:
        logger.error(f"DOCX Parser ({file_basename_for_log}): File not found: {file_path}")
        return None
    except ImportError as ie:
        logger.error(f"DOCX Parser ({file_basename_for_log}): ImportError during DOCX parsing with Unstructured: {ie}.")
        return None
    except Exception as e:
        logger.error(f"DOCX Parser ({file_basename_for_log}): Critical error parsing DOCX file: {e}", exc_info=True)
        error_message = f"[ERROR PARSING DOCX: {file_basename_for_log} - {type(e).__name__}: {str(e)}]"
        if _PYDANTIC_MODELS_AVAILABLE_DOCX:
            return ParsedDocumentOutput(
                parsed_text=error_message,
                elements=[],
                original_metadata=original_metadata
            )
        else:
            return {
                "parsed_text": error_message,
                "elements": [],
                "original_metadata": original_metadata
            }
```

        |-- html_parser.py

``` py
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/zhz_rag_pipeline/parsers/html_parser.py
import os
import logging
from typing import List, Dict, Any, Optional, Union, Set
import re

try:
    from bs4 import BeautifulSoup, Tag, NavigableString
    _BS4_AVAILABLE = True
    logging.info("Successfully imported BeautifulSoup4 for HTML parsing.")
except ImportError:
    logging.error("BeautifulSoup4 (bs4) not found. HTML parsing will not be available.")
    _BS4_AVAILABLE = False
    class BeautifulSoup: pass # Placeholder
    class Tag: pass
    class NavigableString: pass


_PYDANTIC_MODELS_AVAILABLE_HTML = False
try:
    from ..pydantic_models_dagster import (
        ParsedDocumentOutput, DocumentElementType, TitleElement, NarrativeTextElement,
        ListItemElement, TableElement, CodeBlockElement, PageBreakElement,
        DocumentElementMetadata
    )
    _PYDANTIC_MODELS_AVAILABLE_HTML = True
except ImportError:
    class BaseModel: pass
    class DocumentElementMetadata(BaseModel): page_number: Optional[int] = None
    class ParsedDocumentOutput(BaseModel): parsed_text: str; elements: list; original_metadata: dict; summary: Optional[str] = None
    class TitleElement(BaseModel): element_type:str="title"; text:str; level:int; metadata: Optional[DocumentElementMetadata] = None
    class NarrativeTextElement(BaseModel): element_type:str="narrative_text"; text:str; metadata: Optional[DocumentElementMetadata] = None
    class ListItemElement(BaseModel): element_type:str="list_item"; text:str; level:int=0; ordered:bool=False; item_number:Optional[Union[int, str]]=None; metadata: Optional[DocumentElementMetadata] = None
    class TableElement(BaseModel): element_type:str="table"; markdown_representation:Optional[str]=None; html_representation:Optional[str]=None; caption:Optional[str]=None; metadata: Optional[DocumentElementMetadata] = None
    class CodeBlockElement(BaseModel): element_type:str="code_block"; code:str; language:Optional[str]=None; metadata: Optional[DocumentElementMetadata] = None
    class PageBreakElement(BaseModel): element_type:str="page_break"; metadata: Optional[DocumentElementMetadata] = None
    DocumentElementType = Any

logger = logging.getLogger(__name__)

# Tags to typically ignore for main content extraction
IGNORE_TAGS_HTML = ['script', 'style', 'nav', 'footer', 'header', 'aside', 'form', 'meta', 'link', 'button', 'input', 'noscript', 'iframe', 'canvas', 'svg', 'path']
# Tags that define a semantic block but we want to process their children
CONTAINER_TAGS_HTML = ['div', 'section', 'article', 'main', 'body', 'figure', 'figcaption', 'details', 'summary']


def _table_to_markdown(table_tag: Tag) -> str:
    """Converts a BeautifulSoup table Tag to a Markdown string."""
    md_rows = []
    header_processed = False
    
    # Process header (thead)
    thead = table_tag.find('thead')
    if thead:
        header_rows_tags = thead.find_all('tr')
        for hr_tag in header_rows_tags:
            header_cells = hr_tag.find_all(['th', 'td'])
            if header_cells:
                header_texts = [cell.get_text(separator=' ', strip=True) for cell in header_cells]
                md_rows.append("| " + " | ".join(header_texts) + " |")
                if not header_processed: # Add separator only after the first header row group
                    md_rows.append("| " + " | ".join(["---"] * len(header_texts)) + " |")
                    header_processed = True
    
    # Process body (tbody or direct tr in table)
    tbody = table_tag.find('tbody')
    if not tbody: # If no tbody, look for tr directly under table
        rows_to_process = table_tag.find_all('tr', recursive=False)
    else:
        rows_to_process = tbody.find_all('tr')
        
    for row_tag in rows_to_process:
        # Skip if this row was already processed as part of thead (if thead was missing)
        if not header_processed and row_tag.find('th'):
            header_cells = row_tag.find_all(['th', 'td'])
            header_texts = [cell.get_text(separator=' ', strip=True) for cell in header_cells]
            md_rows.append("| " + " | ".join(header_texts) + " |")
            md_rows.append("| " + " | ".join(["---"] * len(header_texts)) + " |")
            header_processed = True
            continue
        
        cell_texts = [cell.get_text(separator=' ', strip=True) for cell in row_tag.find_all('td')]
        if cell_texts: # Only add row if it has content
            md_rows.append("| " + " | ".join(cell_texts) + " |")
            
    return "\n".join(md_rows)

def _convert_html_tag_to_elements_recursive(tag: Tag, elements_list: List[Any], processed_tags: Set[Tag], current_list_level: int = 0):
    """
    Recursively processes a BeautifulSoup Tag and its children to extract structured elements.
    Modifies elements_list in place.
    """
    if tag in processed_tags or not isinstance(tag, Tag) or tag.name in IGNORE_TAGS_HTML:
        return

    tag_name = tag.name.lower()
    element_metadata = None # Placeholder for now, can be enhanced to include source line numbers etc.
    
    created_element = False

    if tag_name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
        level = int(tag_name[1:])
        text = tag.get_text(strip=True)
        if text:
            if _PYDANTIC_MODELS_AVAILABLE_HTML: elements_list.append(TitleElement(text=text, level=level, metadata=element_metadata))
            else: elements_list.append({"element_type": "title", "text": text, "level": level, "metadata": element_metadata})
            created_element = True
    
    elif tag_name == 'p':
        text = tag.get_text(strip=True)
        if text:
            if _PYDANTIC_MODELS_AVAILABLE_HTML: elements_list.append(NarrativeTextElement(text=text, metadata=element_metadata))
            else: elements_list.append({"element_type": "narrative_text", "text": text, "metadata": element_metadata})
            created_element = True

    elif tag_name in ['ul', 'ol']:
        ordered = tag_name == 'ol'
        start_num = int(tag.get('start', '1')) if ordered else 1
        
        # Iterate over direct children that are <li>
        direct_li_children = [child for child in tag.children if isinstance(child, Tag) and child.name == 'li']
        for i, li_tag in enumerate(direct_li_children):
            if li_tag in processed_tags: continue
            
            # Extract text directly under <li>, excluding text from nested lists
            li_text_parts = []
            for content_child in li_tag.contents:
                if isinstance(content_child, NavigableString):
                    stripped_text = content_child.strip()
                    if stripped_text: li_text_parts.append(stripped_text)
                elif isinstance(content_child, Tag) and content_child.name not in ['ul', 'ol']: # Get text from non-list children
                    li_text_parts.append(content_child.get_text(strip=True))
            
            final_li_text = " ".join(li_text_parts).strip()

            if final_li_text:
                item_num_str = str(start_num + i) if ordered else None
                if _PYDANTIC_MODELS_AVAILABLE_HTML: elements_list.append(ListItemElement(text=final_li_text, level=current_list_level, ordered=ordered, item_number=item_num_str, metadata=element_metadata))
                else: elements_list.append({"element_type": "list_item", "text": final_li_text, "level": current_list_level, "ordered": ordered, "item_number": item_num_str, "metadata": element_metadata})
            
            processed_tags.add(li_tag) # Mark <li> as processed for its direct text
            # Recursively process children of this <li> for nested lists or other elements
            for child_of_li in li_tag.children:
                if isinstance(child_of_li, Tag):
                     _convert_html_tag_to_elements_recursive(child_of_li, elements_list, processed_tags, current_list_level + 1)
        created_element = True # The list itself is an element boundary

    elif tag_name == 'table':
        md_table = _table_to_markdown(tag)
        caption_tag = tag.find('caption')
        caption_text = caption_tag.get_text(strip=True) if caption_tag else None
        if md_table or caption_text : # Only add if table has content or caption
            if _PYDANTIC_MODELS_AVAILABLE_HTML: elements_list.append(TableElement(markdown_representation=md_table, html_representation=str(tag), caption=caption_text, metadata=element_metadata))
            else: elements_list.append({"element_type": "table", "markdown_representation": md_table, "html_representation": str(tag), "caption": caption_text, "metadata": element_metadata})
        created_element = True

    elif tag_name == 'pre':
        code_tag = tag.find('code')
        code_text, lang = "", None
        if code_tag:
            code_text = code_tag.get_text() # Keep original spacing and newlines
            lang_class = code_tag.get('class', [])
            if lang_class: lang = next((cls.split('language-')[-1] for cls in lang_class if cls.startswith('language-')), None)
        else:
            code_text = tag.get_text()
        
        if code_text.strip(): # Check if there's actual code
            if _PYDANTIC_MODELS_AVAILABLE_HTML: elements_list.append(CodeBlockElement(code=code_text.strip('\n'), language=lang, metadata=element_metadata))
            else: elements_list.append({"element_type": "code_block", "code": code_text.strip('\n'), "language": lang, "metadata": element_metadata})
        created_element = True

    elif tag_name == 'blockquote':
        text = tag.get_text(strip=True)
        if text:
            if _PYDANTIC_MODELS_AVAILABLE_HTML: elements_list.append(NarrativeTextElement(text=text, metadata=element_metadata))
            else: elements_list.append({"element_type": "narrative_text", "text": text, "_is_blockquote": True, "metadata": element_metadata})
        created_element = True

    elif tag_name == 'hr':
        if _PYDANTIC_MODELS_AVAILABLE_HTML: elements_list.append(PageBreakElement(metadata=element_metadata))
        else: elements_list.append({"element_type": "page_break", "metadata": element_metadata})
        created_element = True
    
    processed_tags.add(tag)
    # If the tag itself wasn't a specific block element we handled, or it's a known container,
    # process its children.
    if not created_element or tag_name in CONTAINER_TAGS_HTML:
        for child in tag.children:
            if isinstance(child, Tag):
                _convert_html_tag_to_elements_recursive(child, elements_list, processed_tags, current_list_level)
            elif isinstance(child, NavigableString): # Handle loose text not in <p> etc.
                loose_text = child.strip()
                if loose_text and tag_name not in ['ul', 'ol']: # Avoid adding list item text twice
                    if _PYDANTIC_MODELS_AVAILABLE_HTML: elements_list.append(NarrativeTextElement(text=loose_text, metadata=element_metadata))
                    else: elements_list.append({"element_type": "narrative_text", "text": loose_text, "_is_loose_text": True, "metadata": element_metadata})

def _generate_linear_text_from_html_elements(elements: List[Any]) -> str:
    # This function is identical to the one in docx_parser.py, can be refactored to common_utils later.
    text_parts = []
    for el_data_any in elements:
        el_data: Dict[str, Any] = {}
        if _PYDANTIC_MODELS_AVAILABLE_HTML and hasattr(el_data_any, 'model_dump'):
            el_data = el_data_any.model_dump(exclude_none=True)
        elif isinstance(el_data_any, dict):
            el_data = el_data_any
        else: continue

        el_type = el_data.get("element_type")
        text_content = el_data.get("text", "")
        current_element_text = ""
        if el_type == "title": current_element_text = f"\n{'#' * el_data.get('level',1)} {text_content}\n"
        elif el_type == "narrative_text": current_element_text = text_content + "\n"
        elif el_type == "list_item":
            prefix = f"{el_data.get('item_number', '')}. " if el_data.get('ordered') and el_data.get('item_number') else "- "
            indent = "  " * el_data.get('level', 0)
            current_element_text = f"{indent}{prefix}{text_content}\n"
        elif el_type == "table":
            caption = el_data.get('caption','Unnamed Table')
            md_repr = el_data.get('markdown_representation')
            if md_repr: current_element_text = f"\n[Table: {caption}]\n{md_repr}\n"
        elif el_type == "code_block":
            lang = el_data.get('language', "") or ""
            code_content = el_data.get('code', "")
            current_element_text = f"\n```{lang}\n{code_content}\n```\n"
        elif el_type == "page_break": current_element_text = "\n---\n"
        if current_element_text: text_parts.append(current_element_text)
    full_text = "".join(text_parts)
    return re.sub(r'\n{3,}', '\n\n', full_text).strip()

def parse_html_to_structured_output(
    html_content_str: str, 
    original_metadata: Dict[str, Any]
) -> Optional[Union[ParsedDocumentOutput, Dict[str, Any]]]:
    logger.info(f"Attempting to parse HTML content (length: {len(html_content_str)} chars) using BeautifulSoup4")
    if not _BS4_AVAILABLE:
        logger.error("BeautifulSoup4 (bs4) is not available. HTML parsing cannot proceed.")
        return None

    elements: List[Any] = []
    try:
        # Try lxml first, then html.parser
        try:
            soup = BeautifulSoup(html_content_str, "lxml")
        except Exception: # Fallback if lxml is not installed or fails
            logger.warning("lxml parser not available or failed, falling back to html.parser for HTML.")
            soup = BeautifulSoup(html_content_str, "html.parser")

        # Attempt to find the main content area
        main_content_area = soup.find('article') or soup.find('main') or soup.body
        if not main_content_area:
            logger.warning("Could not find <article>, <main>, or <body> tag. Parsing entire document if possible.")
            main_content_area = soup # Fallback to entire soup object

        # Remove ignored tags before processing
        for ignore_tag_name in IGNORE_TAGS_HTML:
            for tag_to_remove in main_content_area.find_all(ignore_tag_name):
                tag_to_remove.decompose()
        
        processed_tags_set: Set[Tag] = set()
        _convert_html_tag_to_elements_recursive(main_content_area, elements, processed_tags_set)
        
        logger.info(f"Converted HTML to {len(elements)} custom elements.")
        
        linear_text = _generate_linear_text_from_html_elements(elements)
        logger.info(f"Generated linear text from HTML elements (length: {len(linear_text)}). Preview: {linear_text[:200]}")

        if _PYDANTIC_MODELS_AVAILABLE_HTML:
            return ParsedDocumentOutput(
                parsed_text=linear_text,
                elements=elements, # type: ignore
                original_metadata=original_metadata
            )
        else:
            return {
                "parsed_text": linear_text,
                "elements": elements,
                "original_metadata": original_metadata
            }
    except Exception as e:
        logger.error(f"Error parsing HTML content with BeautifulSoup4: {e}", exc_info=True)
        return None
```

        |-- md_parser.py

``` py
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/zhz_rag_pipeline/parsers/md_parser.py
import os
from markdown_it import MarkdownIt
from markdown_it.tree import SyntaxTreeNode
import logging
from typing import List, Dict, Any, Optional, Union
import re

# --- Pydantic 模型导入和占位符定义 ---
_PARSER_PYDANTIC_AVAILABLE = False
try:
    from ..pydantic_models_dagster import (
        ParsedDocumentOutput, DocumentElementType, TitleElement, NarrativeTextElement,
        ListItemElement, TableElement, CodeBlockElement, PageBreakElement,
        DocumentElementMetadata
    )
    _PARSER_PYDANTIC_AVAILABLE = True
except ImportError:
    class BaseModel: pass
    class DocumentElementMetadata(BaseModel): page_number: Optional[int] = None
    class ParsedDocumentOutput(BaseModel): parsed_text: str; elements: list; original_metadata: dict; summary: Optional[str] = None
    class TitleElement(BaseModel): element_type:str="title"; text:str; level:int; metadata: Optional[DocumentElementMetadata] = None
    class NarrativeTextElement(BaseModel): element_type:str="narrative_text"; text:str; metadata: Optional[DocumentElementMetadata] = None
    class ListItemElement(BaseModel): element_type:str="list_item"; text:str; level:int=0; ordered:bool=False; item_number:Optional[Union[int, str]]=None; metadata: Optional[DocumentElementMetadata] = None
    class TableElement(BaseModel): element_type:str="table"; markdown_representation:Optional[str]=None; html_representation:Optional[str]=None; caption:Optional[str]=None; metadata: Optional[DocumentElementMetadata] = None
    class CodeBlockElement(BaseModel): element_type:str="code_block"; code:str; language:Optional[str]=None; metadata: Optional[DocumentElementMetadata] = None
    class PageBreakElement(BaseModel): element_type:str="page_break"; metadata: Optional[DocumentElementMetadata] = None
    DocumentElementType = Any

logger = logging.getLogger(__name__)

# --- 辅助函数 ---
def _get_node_text_content(node: SyntaxTreeNode, exclude_lists_and_tables: bool = False) -> str:
    if node.type == "text":
        return node.content
    if node.type == "softbreak":
        return " "
    if node.type == "hardbreak":
        return "\n"
    if node.type == "code_inline":
        return f"`{node.content}`"
    
    if exclude_lists_and_tables and node.type in ["bullet_list", "ordered_list", "table"]:
        return ""

    content = ""
    if node.children:
        for child in node.children:
            content += _get_node_text_content(child, exclude_lists_and_tables)
    return content

def _convert_table_node_to_markdown(table_node: SyntaxTreeNode) -> str:
    md_rows = []
    
    thead_node = next((child for child in table_node.children if child.type == 'thead'), None)
    tbody_node = next((child for child in table_node.children if child.type == 'tbody'), None)

    header_texts = []
    if thead_node:
        tr_node_header = next((child for child in thead_node.children if child.type == 'tr'), None)
        if tr_node_header:
            header_texts = [_get_node_text_content(cell).strip() for cell in tr_node_header.children if cell.type == 'th']
    elif tbody_node: 
        first_row_in_tbody = next((child for child in tbody_node.children if child.type == 'tr'), None)
        if first_row_in_tbody and all(cell.type == 'th' for cell in first_row_in_tbody.children):
             header_texts = [_get_node_text_content(cell).strip() for cell in first_row_in_tbody.children]

    if header_texts:
        md_rows.append("| " + " | ".join(header_texts) + " |")
        md_rows.append("| " + " | ".join(["---"] * len(header_texts)) + " |")

    rows_container = tbody_node if tbody_node else table_node 
    
    first_row_in_container_is_header = False
    if not header_texts and rows_container: # 只有在没有thead且容器存在时，才检查第一行是否是表头
        first_tr = next((child for child in rows_container.children if child.type == 'tr'), None)
        if first_tr and all(cell.type == 'th' for cell in first_tr.children):
            # 如果第一行全是th，作为表头处理
            header_texts_from_body = [_get_node_text_content(cell).strip() for cell in first_tr.children]
            if header_texts_from_body:
                md_rows.append("| " + " | ".join(header_texts_from_body) + " |")
                md_rows.append("| " + " | ".join(["---"] * len(header_texts_from_body)) + " |")
                first_row_in_container_is_header = True

    if rows_container: # 确保 rows_container 存在
        for row_idx, tr_node in enumerate(child for child in rows_container.children if child.type == 'tr'):
            # 如果第一行已经被作为表头处理了，则跳过它
            if first_row_in_container_is_header and row_idx == 0:
                continue
            
            # 如果已经通过 thead 处理了表头，那么 tbody/table 下的所有 tr 都应视为数据行
            # 如果没有通过 thead 处理表头，并且当前行也不是被推断为表头的 tbody 第一行，那么它也是数据行
            cell_texts = [_get_node_text_content(cell).strip() for cell in tr_node.children if cell.type == 'td']
            if cell_texts or len(tr_node.children) > 0 : 
                md_rows.append("| " + " | ".join(cell_texts) + " |")
            
    return "\n".join(md_rows)

# --- 主转换函数 ---
def _convert_md_tree_to_elements(root_node: SyntaxTreeNode) -> List[Any]: 
    elements: List[Any] = []
    
    def _process_node_recursive(node: SyntaxTreeNode, current_semantic_level: int = 0, list_ctx: Optional[Dict] = None):
        nonlocal elements
        current_metadata = None 

        node_type = node.type
        
        if node_type == "heading":
            level = int(node.tag[1:])
            text = _get_node_text_content(node).strip()
            if text or node.children: 
                if _PARSER_PYDANTIC_AVAILABLE: elements.append(TitleElement(text=text, level=level, metadata=current_metadata))
                else: elements.append({"element_type": "title", "text": text, "level": level, "metadata": current_metadata})
        
        elif node_type == "paragraph":
            text = _get_node_text_content(node).strip()
            if text:
                if _PARSER_PYDANTIC_AVAILABLE: elements.append(NarrativeTextElement(text=text, metadata=current_metadata))
                else: elements.append({"element_type": "narrative_text", "text": text, "metadata": current_metadata})

        elif node_type == "bullet_list" or node_type == "ordered_list":
            is_ordered_list = (node_type == "ordered_list")
            child_list_ctx = {
                "ordered": is_ordered_list,
                "start_num": int(node.attrs.get("start", 1)) if node.attrs and is_ordered_list else 1,
                "item_idx_in_list": 0 
            }
            for child_node in node.children:
                if child_node.type == "list_item":
                    _process_node_recursive(child_node, current_semantic_level + 1, child_list_ctx)
        
        elif node_type == "list_item":
            item_text = _get_node_text_content(node, exclude_lists_and_tables=True).strip()
            
            if item_text and list_ctx: 
                display_level = current_semantic_level - 1 
                item_number_str = None
                if list_ctx["ordered"]:
                    item_number_str = str(list_ctx["start_num"] + list_ctx["item_idx_in_list"])
                    list_ctx["item_idx_in_list"] += 1
                else: 
                    item_number_str = node.markup if node.markup else "-" 

                if _PARSER_PYDANTIC_AVAILABLE:
                    elements.append(ListItemElement(
                        text=item_text, level=display_level, 
                        ordered=list_ctx["ordered"], 
                        item_number=item_number_str, metadata=current_metadata
                    ))
                else:
                    elements.append({
                        "element_type": "list_item", "text": item_text, 
                        "level": display_level, "ordered": list_ctx["ordered"], 
                        "item_number": item_number_str, "metadata": current_metadata
                    })
            
            for child_node in node.children:
                if child_node.type in ["bullet_list", "ordered_list"]:
                    _process_node_recursive(child_node, current_semantic_level, None) # Pass current_semantic_level for nested list

        elif node_type == "table":
            md_table_representation = _convert_table_node_to_markdown(node)
            if md_table_representation:
                if _PARSER_PYDANTIC_AVAILABLE: elements.append(TableElement(markdown_representation=md_table_representation, metadata=current_metadata))
                else: elements.append({"element_type": "table", "markdown_representation": md_table_representation, "metadata": current_metadata})

        elif node_type == "fence" or node_type == "code_block":
            code_content = node.content.strip('\n') 
            lang = node.info.strip() if node.info else None
            if _PARSER_PYDANTIC_AVAILABLE: elements.append(CodeBlockElement(code=code_content, language=lang, metadata=current_metadata))
            else: elements.append({"element_type": "code_block", "code": code_content, "language": lang, "metadata": current_metadata})

        elif node_type == "hr":
            if _PARSER_PYDANTIC_AVAILABLE: elements.append(PageBreakElement(metadata=current_metadata))
            else: elements.append({"element_type": "page_break", "metadata": current_metadata})

        elif node_type == "blockquote":
            text = _get_node_text_content(node).strip()
            if text:
                if _PARSER_PYDANTIC_AVAILABLE: elements.append(NarrativeTextElement(text=text, metadata=current_metadata))
                else: elements.append({"element_type": "narrative_text", "text": text, "_is_blockquote": True, "metadata": current_metadata})
        
        elif node.children and node_type not in ["list_item", "heading", "paragraph", "table", "fence", "code_block", "blockquote", "hr", "bullet_list", "ordered_list"]: # Avoid re-processing children of already handled types
             for child in node.children:
                _process_node_recursive(child, current_semantic_level, list_ctx) # Pass context along

    _process_node_recursive(root_node) 
    return elements

def _generate_parsed_text_from_elements_internal(elements: List[Any]) -> str:
    text_parts = []
    for el_data_any in elements:
        el_data = {}
        if _PARSER_PYDANTIC_AVAILABLE and hasattr(el_data_any, 'model_dump'): el_data = el_data_any.model_dump()
        elif isinstance(el_data_any, dict): el_data = el_data_any
        else: continue
        el_type = el_data.get("element_type")
        if el_type == "title": text_parts.append(f"\n{'#' * el_data.get('level',1)} {el_data.get('text','')}\n")
        elif el_type == "narrative_text": text_parts.append(el_data.get('text','') + "\n")
        elif el_type == "list_item":
            item_num_display = str(el_data.get('item_number','-')) 
            prefix = f"{item_num_display}. " if el_data.get('ordered') else f"{item_num_display} "
            indent = "  " * el_data.get('level',0)
            text_parts.append(f"{indent}{prefix}{el_data.get('text','')}\n")
        elif el_type == "table":
            caption_text = str(el_data.get('caption')) if el_data.get('caption') is not None else 'Unnamed Table'
            if el_data.get('markdown_representation'): 
                text_parts.append(f"\n[Table: {caption_text}]\n{el_data['markdown_representation']}\n")
            elif el_data.get('text_representation'): 
                text_parts.append(f"\n[Table: {caption_text}]\n{el_data['text_representation']}\n")
        elif el_type == "code_block":
            lang = el_data.get('language', "") or ""
            text_parts.append(f"\n```{lang}\n{el_data.get('code','')}\n```\n")
        elif el_type == "page_break": text_parts.append("\n---\n")
        if el_type not in ['list_item'] and text_parts and (not text_parts[-1].endswith("\n\n") and not text_parts[-1].endswith("\n---\n\n") ) : 
             text_parts.append("\n") 
             
    raw_text = "".join(text_parts)
    cleaned_text = raw_text.strip()
    cleaned_text = cleaned_text.replace('\r\n', '\n') 
    cleaned_text = re.sub(r'\n{3,}', '\n\n', cleaned_text) 
    return cleaned_text

def parse_markdown_to_structured_output(md_content_str: str, original_metadata: Dict[str, Any]) -> Optional[Union[ParsedDocumentOutput, Dict[str, Any]]]:
    logger.info(f"Parsing Markdown content (length: {len(md_content_str)} chars) using markdown-it-py with SyntaxTreeNode...")
    try:
        md_parser = MarkdownIt("commonmark", {'linkify': True}).enable("table")
        tokens = md_parser.parse(md_content_str)
        
        root_syntax_node = SyntaxTreeNode(tokens)
        structured_elements = _convert_md_tree_to_elements(root_syntax_node) 

        linear_text = _generate_parsed_text_from_elements_internal(structured_elements)

        if _PARSER_PYDANTIC_AVAILABLE:
            return ParsedDocumentOutput(
                parsed_text=linear_text,
                elements=structured_elements, 
                original_metadata=original_metadata
            )
        else:
            return {
                "parsed_text": linear_text,
                "elements": structured_elements,
                "original_metadata": original_metadata
            }
    except Exception as e:
        logger.error(f"Error in parse_markdown_to_structured_output: {e}", exc_info=True)
        return None

# --- Placeholder for other parsers (保持不变) ---
def parse_docx_to_structured_output(file_path: str, original_metadata: Dict[str, Any]) -> Optional[Union[ParsedDocumentOutput, Dict[str, Any]]]:
    logger.info(f"DOCX parser placeholder for: {file_path}") 
    text = f"[DOCX content of {os.path.basename(file_path)}]"
    if _PARSER_PYDANTIC_AVAILABLE: return ParsedDocumentOutput(parsed_text=text, elements=[NarrativeTextElement(text=text)], original_metadata=original_metadata) 
    return {"parsed_text": text, "elements": [{"element_type":"narrative_text", "text":text}], "original_metadata": original_metadata}

def parse_pdf_to_structured_output(file_path: str, original_metadata: Dict[str, Any]) -> Optional[Union[ParsedDocumentOutput, Dict[str, Any]]]:
    logger.info(f"PDF parser placeholder for: {file_path}") 
    text = f"[PDF content of {os.path.basename(file_path)}]"
    if _PARSER_PYDANTIC_AVAILABLE: return ParsedDocumentOutput(parsed_text=text, elements=[NarrativeTextElement(text=text)], original_metadata=original_metadata) 
    return {"parsed_text": text, "elements": [{"element_type":"narrative_text", "text":text}], "original_metadata": original_metadata}

def parse_xlsx_to_structured_output(file_path: str, original_metadata: Dict[str, Any]) -> Optional[Union[ParsedDocumentOutput, Dict[str, Any]]]:
    logger.info(f"XLSX parser placeholder for: {file_path}") 
    text = f"[XLSX content of {os.path.basename(file_path)}]"
    if _PARSER_PYDANTIC_AVAILABLE: return ParsedDocumentOutput(parsed_text=text, elements=[NarrativeTextElement(text=text)], original_metadata=original_metadata) 
    return {"parsed_text": text, "elements": [{"element_type":"narrative_text", "text":text}], "original_metadata": original_metadata}
        
def parse_html_to_structured_output(html_content_str: str, original_metadata: Dict[str, Any]) -> Optional[Union[ParsedDocumentOutput, Dict[str, Any]]]:
    logger.info(f"HTML parser placeholder for content length: {len(html_content_str)}") 
    text = f"[HTML content snippet: {html_content_str[:100]}]"
    if _PARSER_PYDANTIC_AVAILABLE: return ParsedDocumentOutput(parsed_text=text, elements=[NarrativeTextElement(text=text)], original_metadata=original_metadata) 
    return {"parsed_text": text, "elements": [{"element_type":"narrative_text", "text":text}], "original_metadata": original_metadata}

def parse_txt_to_structured_output(txt_content_str: str, original_metadata: Dict[str, Any]) -> Optional[Union[ParsedDocumentOutput, Dict[str, Any]]]:
    logger.info(f"TXT parser for content length: {len(txt_content_str)}") 
    if _PARSER_PYDANTIC_AVAILABLE:
        return ParsedDocumentOutput(
            parsed_text=txt_content_str, 
            elements=[NarrativeTextElement(text=txt_content_str)], 
            original_metadata=original_metadata
        ) 
    return {
        "parsed_text": txt_content_str, 
        "elements": [{"element_type":"narrative_text", "text":txt_content_str}], 
        "original_metadata": original_metadata
    }
```

        |-- pdf_parser.py

``` py
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/zhz_rag_pipeline/parsers/pdf_parser.py
import os
import logging
from typing import List, Dict, Any, Optional, Union
import re

try:
    import fitz  # PyMuPDF
    _PYMUPDF_AVAILABLE = True
    logging.info("Successfully imported PyMuPDF (fitz) for PDF parsing.")
except ImportError:
    logging.error("PyMuPDF (fitz) not found. PDF parsing will not be available.")
    _PYMUPDF_AVAILABLE = False
    # 占位符，以防 fitz 未安装时代码尝试引用它
    class fitz: 
        class Document: pass
        class Page: pass
        class Rect: pass 

_PYDANTIC_MODELS_AVAILABLE_PDF = False
try:
    from ..pydantic_models_dagster import (
        ParsedDocumentOutput, DocumentElementType, NarrativeTextElement,
        DocumentElementMetadata, PageBreakElement
    )
    _PYDANTIC_MODELS_AVAILABLE_PDF = True
except ImportError:
    class BaseModel: pass
    class DocumentElementMetadata(BaseModel): page_number: Optional[int] = None
    class ParsedDocumentOutput(BaseModel): parsed_text: str; elements: list; original_metadata: dict; summary: Optional[str] = None
    class NarrativeTextElement(BaseModel): element_type:str="narrative_text"; text:str; metadata: Optional[DocumentElementMetadata] = None
    class PageBreakElement(BaseModel): element_type:str="page_break"; metadata: Optional[DocumentElementMetadata] = None
    DocumentElementType = Any

logger = logging.getLogger(__name__)

def _create_pdf_element_metadata(page_number: Optional[int] = None) -> Optional[Union[DocumentElementMetadata, Dict[str, Any]]]:
    meta_data_dict: Dict[str, Any] = {}
    if page_number is not None:
        meta_data_dict['page_number'] = page_number
    
    if not meta_data_dict:
        return None

    if _PYDANTIC_MODELS_AVAILABLE_PDF:
        return DocumentElementMetadata(**meta_data_dict)
    else:
        return meta_data_dict

def parse_pdf_to_structured_output(
    file_path: str, 
    original_metadata: Dict[str, Any]
) -> Optional[Union[ParsedDocumentOutput, Dict[str, Any]]]:
    logger.info(f"Attempting to parse PDF file: {file_path} using PyMuPDF (fitz)")
    if not _PYMUPDF_AVAILABLE:
        logger.error("PyMuPDF (fitz) is not available. PDF parsing cannot proceed.")
        return None

    elements: List[Any] = []
    full_text_parts: List[str] = []

    try:
        doc = fitz.open(file_path)
        logger.info(f"PyMuPDF opened PDF. Pages: {doc.page_count}")

        for page_num in range(doc.page_count):
            page = doc.load_page(page_num)
            
            # 使用 "dict" 模式提取文本块，并按阅读顺序排序
            page_content_blocks = page.get_text("dict", sort=True).get("blocks", [])
            
            page_text_collected = []
            
            if page_content_blocks:
                for block in page_content_blocks:
                    if block['type'] == 0: # 0 表示文本块
                        block_text_lines = []
                        for line in block.get("lines", []):
                            line_content = "".join([span.get("text", "") for span in line.get("spans", [])])
                            block_text_lines.append(line_content)
                        block_text_content = "\n".join(block_text_lines).strip()
                        if block_text_content:
                            page_text_collected.append(block_text_content)
            
            if page_text_collected:
                # 将整页的文本作为一个 NarrativeTextElement
                page_full_text = "\n\n".join(page_text_collected) # 用双换行符分隔来自不同块的文本
                element_metadata = _create_pdf_element_metadata(page_number=page_num + 1)
                if _PYDANTIC_MODELS_AVAILABLE_PDF:
                    elements.append(NarrativeTextElement(text=page_full_text, metadata=element_metadata)) # type: ignore
                else:
                    elements.append({"element_type": "narrative_text", "text": page_full_text, "metadata": element_metadata})
                full_text_parts.append(page_full_text)
            
            # 在每页（除了最后一页）之后添加一个 PageBreakElement
            if page_num < doc.page_count - 1:
                if _PYDANTIC_MODELS_AVAILABLE_PDF:
                    elements.append(PageBreakElement(metadata=_create_pdf_element_metadata(page_number=page_num + 1))) # type: ignore
                else:
                    elements.append({"element_type": "page_break", "metadata": _create_pdf_element_metadata(page_number=page_num + 1)})


        doc.close()
        
        linear_text = "\n\n--- Page Break ---\n\n".join(full_text_parts) # 用特殊标记分隔页面文本
        linear_text = re.sub(r'\n{3,}', '\n\n', linear_text).strip()

        if _PYDANTIC_MODELS_AVAILABLE_PDF:
            return ParsedDocumentOutput(
                parsed_text=linear_text,
                elements=elements, # type: ignore
                original_metadata=original_metadata
            )
        else:
            return {
                "parsed_text": linear_text,
                "elements": elements,
                "original_metadata": original_metadata
            }

    except FileNotFoundError:
        logger.error(f"PDF file not found: {file_path}")
        return None
    except Exception as e:
        logger.error(f"Error parsing PDF file {file_path} with PyMuPDF: {e}", exc_info=True)
        return None
```

        |-- txt_parser.py

``` py
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/zhz_rag_pipeline/parsers/txt_parser.py
import os
import logging
from typing import Dict, Any, Optional, Union

_PYDANTIC_MODELS_AVAILABLE_TXT = False
try:
    from ..pydantic_models_dagster import (
        ParsedDocumentOutput, NarrativeTextElement,
        DocumentElementMetadata
    )
    _PYDANTIC_MODELS_AVAILABLE_TXT = True
except ImportError:
    class BaseModel: pass
    class DocumentElementMetadata(BaseModel): page_number: Optional[int] = None # Not really applicable for txt
    class ParsedDocumentOutput(BaseModel): parsed_text: str; elements: list; original_metadata: dict; summary: Optional[str] = None
    class NarrativeTextElement(BaseModel): element_type:str="narrative_text"; text:str; metadata: Optional[DocumentElementMetadata] = None
    # DocumentElementType is not strictly needed here as we only create NarrativeTextElement

logger = logging.getLogger(__name__)

def parse_txt_to_structured_output(
    txt_content_str: str, # For .txt, we expect the content string directly
    original_metadata: Dict[str, Any]
) -> Optional[Union[ParsedDocumentOutput, Dict[str, Any]]]:
    logger.info(f"Attempting to parse TXT content (length: {len(txt_content_str)} chars)")

    # For .txt files, the entire content is treated as a single narrative text block.
    # No complex structure is assumed or extracted.
    
    elements = []
    element_metadata = None # No specific sub-element metadata for a single block txt file

    if _PYDANTIC_MODELS_AVAILABLE_TXT:
        elements.append(NarrativeTextElement(text=txt_content_str, metadata=element_metadata)) # type: ignore
        doc_output = ParsedDocumentOutput(
            parsed_text=txt_content_str,
            elements=elements, # type: ignore
            original_metadata=original_metadata
        )
    else:
        elements.append({"element_type": "narrative_text", "text": txt_content_str, "metadata": element_metadata})
        doc_output = {
            "parsed_text": txt_content_str,
            "elements": elements,
            "original_metadata": original_metadata
        }
    
    logger.info(f"Successfully processed TXT content into a single element.")
    return doc_output
```

        |-- xlsx_parser.py

``` py
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/zhz_rag_pipeline/parsers/xlsx_parser.py
import os
import logging
from typing import List, Dict, Any, Optional, Union
import pandas as pd

# 确保安装了 'pandas', 'openpyxl', 'tabulate'
# pip install pandas openpyxl tabulate

# --- Pydantic 模型导入和占位符定义 (保持不变) ---
_PYDANTIC_MODELS_AVAILABLE_XLSX = False
try:
    from ..pydantic_models_dagster import (
        ParsedDocumentOutput, TableElement, DocumentElementMetadata
    )
    _PYDANTIC_MODELS_AVAILABLE_XLSX = True
except ImportError:
    class BaseModel: pass
    class DocumentElementMetadata(BaseModel): page_number: Optional[int] = None
    class ParsedDocumentOutput(BaseModel): parsed_text: str; elements: list; original_metadata: dict; summary: Optional[str] = None
    class TableElement(BaseModel): element_type: str = "table"; markdown_representation: Optional[str] = None; html_representation: Optional[str] = None; caption: Optional[str] = None; metadata: Optional[DocumentElementMetadata] = None

logger = logging.getLogger(__name__)

# --- 辅助函数 (保持不变) ---
def _create_xlsx_element_metadata(sheet_index: int, table_index_in_sheet: int) -> Optional[Union[DocumentElementMetadata, Dict[str, Any]]]:
    meta_data_dict: Dict[str, Any] = {
        'page_number': sheet_index, # 使用 page_number 表示工作表索引
        'custom_properties': {'table_index_in_sheet': table_index_in_sheet}
    }
    if _PYDANTIC_MODELS_AVAILABLE_XLSX:
        return DocumentElementMetadata(**meta_data_dict)
    else:
        return meta_data_dict

# --- 主解析函数 (全新版本) ---
def parse_xlsx_to_structured_output(
    file_path: str,
    original_metadata: Dict[str, Any]
) -> Optional[Union[ParsedDocumentOutput, Dict[str, Any]]]:
    """
    V2: 解析XLSX文件，智能识别并提取每个工作表内的多个独立表格。
    """
    logger.info(f"Attempting to parse XLSX file with multi-table support: {file_path}")
    
    try:
        xls = pd.ExcelFile(file_path)
    except Exception as e:
        logger.error(f"Failed to open Excel file {file_path}: {e}", exc_info=True)
        return None

    all_elements: List[Any] = []
    
    for sheet_idx, sheet_name in enumerate(xls.sheet_names):
        try:
            # 读取整个工作表，不指定表头，以便我们手动查找
            df_full = pd.read_excel(xls, sheet_name=sheet_name, header=None)
            
            if df_full.empty:
                logger.info(f"Sheet '{sheet_name}' is empty. Skipping.")
                continue

            # 通过查找空行来识别表格块
            is_row_empty = df_full.isnull().all(axis=1)
            # 获取空行的索引
            empty_row_indices = is_row_empty[is_row_empty].index.tolist()
            
            table_blocks: List[pd.DataFrame] = []
            last_split_index = -1
            
            for empty_idx in empty_row_indices:
                block_df = df_full.iloc[last_split_index + 1 : empty_idx].dropna(how='all')
                if not block_df.empty:
                    table_blocks.append(block_df)
                last_split_index = empty_idx
            
            # 添加最后一个块（从最后一个空行到末尾）
            final_block_df = df_full.iloc[last_split_index + 1 :].dropna(how='all')
            if not final_block_df.empty:
                table_blocks.append(final_block_df)

            logger.info(f"Sheet '{sheet_name}' was split into {len(table_blocks)} potential table blocks.")

            for table_idx, block_df in enumerate(table_blocks):
                # 将第一行作为表头
                header = block_df.iloc[0]
                table_data = block_df[1:]
                table_data.columns = header
                
                md_representation = table_data.to_markdown(index=False)
                table_caption = f"内容来自文件 '{os.path.basename(file_path)}' 的工作表 '{sheet_name}' (表格 {table_idx + 1})"
                
                element_metadata = _create_xlsx_element_metadata(
                    sheet_index=sheet_idx, 
                    table_index_in_sheet=table_idx
                )

                if _PYDANTIC_MODELS_AVAILABLE_XLSX:
                    table_el = TableElement(
                        markdown_representation=md_representation,
                        caption=table_caption,
                        metadata=element_metadata
                    )
                    all_elements.append(table_el)
                else:
                    all_elements.append({
                        "element_type": "table",
                        "markdown_representation": md_representation,
                        "caption": table_caption,
                        "metadata": element_metadata
                    })
                logger.info(f"  Successfully created TableElement for table {table_idx+1} in sheet '{sheet_name}'.")
                logger.debug(f"    - Table {table_idx+1} content preview: {md_representation[:200].replace(chr(10), ' ')}...")


        except Exception as e_sheet:
            logger.error(f"Failed to process sheet '{sheet_name}' in {file_path}: {e_sheet}", exc_info=True)
            continue
    
    if _PYDANTIC_MODELS_AVAILABLE_XLSX:
        return ParsedDocumentOutput(
            parsed_text="",
            elements=all_elements,
            original_metadata=original_metadata,
            summary=f"从文件 '{os.path.basename(file_path)}' 中解析了 {len(all_elements)} 个独立的表格。"
        )
    else:
        return {
            "parsed_text": "",
            "elements": all_elements,
            "original_metadata": original_metadata,
            "summary": f"从文件 '{os.path.basename(file_path)}' 中解析了 {len(all_elements)} 个独立的表格。"
        }
```

    |-- PKG-INFO
    |-- SOURCES.txt
    |-- dependency_links.txt
    |-- requires.txt
    |-- top_level.txt

==================================================

--- Structure and Code for: zhz_rag/ ---
|-- api/
|-- config/
|-- core_rag/
|-- crewai_integration/
|-- evaluation/
|-- finetuning/
|-- llm/
|-- stored_data/
|-- task_management/
|-- utils/
|-- zhz_rag_core.egg-info/
|-- __init__.py

``` py

```

|-- setup.py

``` py
# /home/zhz/zhz_agent/zhz_rag/setup.py
from setuptools import find_packages, setup

setup(
    name="zhz_rag_core",
    version="0.1.0",
    packages=find_packages(),
    install_requires=[
        # Pydantic 版本由主 requirements.txt 控制
        # LiteLLM 版本由主 requirements.txt 控制
        # ChromaDB 版本由主 requirements.txt 控制

        "protobuf>=4.25.0,<5.30.0", # 放宽 protobuf 上限，因为 pydantic 2.11.5 可能需要较新的
        "packaging>=23.2,<25.0",
        "rich>=13.7.0,<14.0.0",
        
        "fastapi>=0.110.0,<0.116.0", # 保持较新
        "starlette>=0.35.0,<0.47.0", # 保持较新

        "langchain-core>=0.1.50,<0.4.0", # 较新 langchain 可能更好兼容
        "langchain-text-splitters>=0.0.1,<0.3.0",

        "httpx>=0.27.0", # 使用较新 httpx
        "python-dotenv>=1.0.0",
        "neo4j>=5.0.0", # neo4j 驱动
        "sentence-transformers>=2.2.0", # sentence-transformers
        "transformers>=4.38.0,<4.39.0", # 固定您之前的版本或小幅更新
        "torch>=2.0.0",
        "numpy<2.0", # 保持 Numpy < 2.0
        "bm25s",
        "jieba",
        "uvicorn[standard]", # 添加 standard extras
        "pandas>=2.0.0",
        "sqlalchemy>=2.0.0",
        "databases[aiosqlite]>=0.9.0", # for async sqlite
        "apscheduler>=3.10.0",
        "pytz",
    ],
)
```

    |-- __init__.py

``` py

```

    |-- run_crew.py

``` py
# /home/zhz/zhz_agent/run_agent.py

import os
import json
import datetime

from crewai import Agent, Task, Crew, Process

# --- 导入我们自己的项目模块 (使用绝对导入) ---
from zhz_rag.crewai_integration.tools import HybridRAGTool, BaseMCPTool
from zhz_rag.config.pydantic_models import QueryRequest # 用于 RAG 工具的输入
from zhz_rag.utils.common_utils import call_mcpo_tool
from zhz_rag.llm.custom_crewai_llms import CustomGeminiLLM

# --- 环境配置 ---
from dotenv import load_dotenv
load_dotenv()

# --- CrewAI 基类和事件系统 ---
from crewai.tools import BaseTool
from pydantic import BaseModel, Field
from typing import Any, Dict, List, Optional, Type
from crewai.llms.base_llm import BaseLLM as CrewAIBaseLLM
try:
    from crewai.utilities.events.base_event_listener import BaseEventListener as CrewAIBaseCallbackHandler
    from crewai.utilities.events import LLMCallStartedEvent, LLMCallCompletedEvent
    print("Successfully imported BaseEventListener and Event Types")
except ImportError:
    print("Failed to import BaseEventListener or Event Types, using dummy classes.")
    class CrewAIBaseCallbackHandler: pass
    class LLMCallStartedEvent: pass
    class LLMCallCompletedEvent: pass

# --- LiteLLM ---
import litellm

# --- 定义简单工具以供测试 ---
class SimpleToolInput(BaseModel):
    message: str = Field(description="A simple message string for the tool.")

class MySimpleTestTool(BaseTool):
    name: str = "MySimpleTestTool"
    description: str = "A very simple test tool that takes a message and returns it."
    args_schema: Type[BaseModel] = SimpleToolInput

    def _run(self, message: str) -> str:
        print(f"MySimpleTestTool received: {message}")
        return f"MySimpleTestTool processed: {message}"

# --- 配置 Agent 使用的 LLM 实例 ---
GEMINI_MODEL_NAME = "gemini/gemini-1.5-flash-latest"
GEMINI_API_KEY = os.getenv("GOOGLE_API_KEY") or os.getenv("GEMINI_API_KEY")

if not GEMINI_API_KEY:
    print("CRITICAL ERROR: GOOGLE_API_KEY or GEMINI_API_KEY not set.")
    exit(1)

# --- 定义详细的事件监听器 ---
class MyDetailedLogger(CrewAIBaseCallbackHandler):
    def __init__(self):
        super().__init__()
        print(f"[{datetime.datetime.now()}] MyDetailedLogger: 已初始化。")

    def setup_listeners(self, crewai_event_bus):
        print(f"[{datetime.datetime.now()}] MyDetailedLogger: 正在设置监听器...")

        @crewai_event_bus.on(LLMCallStartedEvent)
        def handle_llm_start(source, event: LLMCallStartedEvent):
            self.on_llm_start_logic(source, event)

        @crewai_event_bus.on(LLMCallCompletedEvent)
        def handle_llm_completed(source, event: LLMCallCompletedEvent):
            self.on_llm_end_logic(source, event)

        print(f"[{datetime.datetime.now()}] MyDetailedLogger: 监听器设置完成。")

    def on_llm_start_logic(self, source, event: LLMCallStartedEvent):
        print(f"\n>>>> LLM 调用开始 (Event Logic) <<<<")
        llm_inputs = getattr(event, 'llm_inputs', {})
        messages = llm_inputs.get('messages')
        tools = llm_inputs.get('tools')
        print(f"来源 (Source): {source}")
        if messages:
            print("消息 (来自 event.llm_inputs):")
            if isinstance(messages, list) and len(messages) > 0:
                first_message = messages[0]
                if isinstance(first_message, dict) and 'content' in first_message:
                    content_snippet = str(first_message.get('content', ''))[:300]
                    print(f"   Role: {first_message.get('role')}, Content Snippet: {content_snippet}...")
                else:
                     print(f"  First message (raw): {first_message}")
            else:
                 print(f"  Messages (raw): {messages}")
        else:
            print("消息 (来自 event.llm_inputs): 无")
        if tools:
            print("工具 (来自 event.llm_inputs):")
            try:
                print(f"  {json.dumps(tools, indent=2, ensure_ascii=False)}")
            except Exception as e:
                print(f"  无法序列化工具为 JSON: {e}. 工具: {tools}")
        else:
            print("工具 (来自 event.llm_inputs): 无")
        print("----------------------------------")

    def on_llm_end_logic(self, source, event: LLMCallCompletedEvent):
        print(f"\n>>>> LLM 调用结束 (Event Logic) <<<<")
        response = getattr(event, 'llm_output', None)
        print(f"来源 (Source): {source}")
        if response:
            if hasattr(response, 'choices') and response.choices:
                choice = response.choices[0]
                if hasattr(choice, 'message') and choice.message:
                    print(f"  消息内容: {choice.message.content}")
                    if hasattr(choice.message, 'tool_calls') and choice.message.tool_calls:
                        print(f"  工具调用: {choice.message.tool_calls}")
                    else:
                        print(f"  工具调用: 无")
            elif hasattr(response, 'content'):
                print(f"  响应内容: {response.content}")
            else:
                print(f"  LLM 响应 (来自 event.llm_output): {str(response)[:500]}...")
        else:
            print("  在 event.llm_output 中未找到响应对象。")
        print("----------------------------------")

# --- 实例化 CustomGeminiLLM ---
custom_llm_tool_config = {"function_calling_config": {"mode": "AUTO"}}
zhz_agent_tool = HybridRAGTool()
researcher_tools = [zhz_agent_tool]

llm_for_agent = CustomGeminiLLM(
    model=GEMINI_MODEL_NAME,
    api_key=GEMINI_API_KEY,
    temperature=0.1,
    max_tokens=2048,
    tool_config=custom_llm_tool_config,
    agent_tools=researcher_tools # 传递工具列表以供缓存
)
print(f"Custom Agent LLM configured: {GEMINI_MODEL_NAME} with custom tool_config")

# --- 设置 BaseMCPTool 的调用器 ---
BaseMCPTool.set_mcpo_caller(call_mcpo_tool)

# --- 定义 Agents ---
researcher_agent = Agent(
    role='信息检索专家',
    goal='准确地回答用户查询，并且只使用提供的工具。',
    backstory=(
        "你是一位高级AI助手，专注于信息检索。"
        "你的专长在于高效地利用工具来查找最相关和最精确的答案来回应用户的查询。"
    ),
    llm=llm_for_agent,
    tools=researcher_tools,
    verbose=True,
    allow_delegation=False,
)

writer_agent = Agent(
    role='报告撰写专家',
    goal='根据提供的信息，撰写清晰、结构良好且富有洞察力的报告。',
    backstory=(
        "您是一位资深的报告撰写专家，拥有出色的分析和写作能力。"
        "您擅长将复杂的信息提炼成易于理解的报告，并能根据不同的输出状态（答案、澄清、错误）"
        "灵活调整报告内容和格式。"
    ),
    llm=llm_for_agent,
    verbose=True,
    allow_delegation=False,
)

# --- 定义 Tasks (包含上下文传递修复) ---
research_task_description = """你收到了来自用户的以下查询：

'{query}'

你应该使用提供的 `HybridRAGQueryTool` 工具来处理这个查询。
如果这个工具需要 `top_k_vector`, `top_k_kg`, 或 `top_k_bm25` 这些参数，请使用以下建议值：
top_k_vector: 5, top_k_kg: 3, top_k_bm25: 5。
在使用完必要的工具后，你的最终输出应该是（使用中文）：'我的最终答案是：' 
后跟你使用的工具返回的精确、完整的JSON字符串输出内容。"""

research_task_expected_output = "短语 '我的最终答案是：' 后跟你使用的工具返回的精确、完整的JSON字符串输出内容。"

research_task = Task(
    description=research_task_description,
    expected_output=research_task_expected_output,
    agent=researcher_agent,
)

report_writing_task = Task(
    description="""根据【前一个任务】（信息检索专家）提供的RAG工具输出（它是一个JSON字符串），生成一份报告或响应。
请仔细分析这个JSON字符串输出，它应该包含一个 'status' 字段。
1. 如果 'status' 是 'success'，则提取 'final_answer' 字段的内容，并基于此答案撰写一份简洁的报告。
2. 如果 'status' 是 'clarification_needed'，则提取 'clarification_question' 字段的内容，并向用户明确指出需要澄清的问题，例如：'系统需要澄清：[澄清问题]'。
3. 如果 'status' 是 'error'，则提取 'error_message' (或 'error') 字段的内容，并向用户报告错误，例如：'RAG服务发生错误：[错误信息]'。
你的最终输出必须是清晰、专业且符合上述情况的报告或响应。""",
    expected_output="一份清晰的报告，或者一个明确的澄清请求，或者一个错误报告。",
    agent=writer_agent,
    context=[research_task],
)

# --- 实例化监听器 ---
my_event_logger = MyDetailedLogger()

# --- 定义 Crew (添加 event_listeners) ---
office_brain_crew = Crew(
    agents=[researcher_agent, writer_agent],
    tasks=[research_task, report_writing_task],
    process=Process.sequential,
    verbose=True,
    event_listeners=[my_event_logger] # <<< --- 激活事件监听器 ---
)

# --- 启动 Crew ---
if __name__ == "__main__":
    print("--- 启动智能助手终端大脑 Crew (使用 CustomGeminiLLM 和事件监听器) ---")
    user_query_input = "公司2024年第一季度在华东和华北的总销售额一共是多少？"
    # --- 修复：kickoff inputs 只包含 query ---
    inputs = {'query': user_query_input}
    result = office_brain_crew.kickoff(inputs=inputs)
    print("\n\n=== 最终报告 ===\n")
    if hasattr(result, 'raw'):
        print(result.raw)
    else:
        print(result)
    print("\n--- Crew 任务完成 ---")
```

    |-- tools.py

``` py
# zhz_agent/custom_crewai_tools.py

import os
import json
import asyncio
import traceback
from typing import Type, List, Dict, Any, Optional, ClassVar
from pydantic import BaseModel, Field
from crewai.tools import BaseTool
import httpx

# 从 zhz_agent.pydantic_models 导入 QueryRequest
from zhz_rag.config.pydantic_models import QueryRequest # 用于 RAG 工具的输入

# MCPO 代理的基地址
MCPO_BASE_URL = os.getenv("MCPO_BASE_URL", "http://localhost:8006")

class BaseMCPTool(BaseTool):
    mcpo_base_url: str = MCPO_BASE_URL
    _call_mcpo_func: ClassVar[callable] = None

    @classmethod
    def set_mcpo_caller(cls, caller: callable):
        cls._call_mcpo_func = caller

    def __init__(self, **kwargs):
        super().__init__(**kwargs)

    async def _call_mcpo_endpoint(self, service_and_tool_path: str, payload: dict) -> dict | str:
        api_url = f"{self.mcpo_base_url}/{service_and_tool_path}"
        cleaned_payload = {k: v for k, v in payload.items() if v is not None}
        print(f"BaseMCPTool: Calling mcpo endpoint: POST {api_url} with payload: {json.dumps(cleaned_payload, ensure_ascii=False)}")
        
        # --- [修改] 移除 proxies=None 参数 ---
        async with httpx.AsyncClient(trust_env=False) as client:
            response = None
            try:
                headers = {"Content-Type": "application/json"}
                response = await client.post(api_url, json=cleaned_payload, headers=headers, timeout=300.0)
                print(f"BaseMCPTool: mcpo status code for {service_and_tool_path}: {response.status_code}")
                print(f"BaseMCPTool: mcpo response headers for {service_and_tool_path}: {response.headers}") # <--- 新增日志
                # 尝试分块读取响应或提前获取少量内容进行日志记录，以防响应过大卡住 .text 或 .json()
                try:
                    response_text_snippet = await response.aread(num_bytes=1024) # 读取前1KB
                    print(f"BaseMCPTool: mcpo response text snippet (first 1KB) for {service_and_tool_path}: {response_text_snippet.decode(errors='ignore')}")
                except Exception as e_read:
                    print(f"BaseMCPTool: Error reading response snippet: {e_read}")

                if response.status_code == 200:
                    try:
                        # print(f"BaseMCPTool: mcpo raw response text for {service_and_tool_path}: {response.text}") # 如果怀疑内容问题，可以取消注释，但小心大响应
                        return response.json()
                    except json.JSONDecodeError:
                        print(f"BaseMCPTool Warning: mcpo returned status 200 but response is not JSON for '{service_and_tool_path}'. Returning raw text.")
                        return response.text
                else:
                    error_text = f"mcpo call to '{service_and_tool_path}' failed with status {response.status_code}. Response: {response.text[:500]}..."
                    print(f"BaseMCPTool Error: {error_text}")
                    return {"error": error_text, "status_code": response.status_code}
            except httpx.RequestError as exc:
                error_msg = f"BaseMCPTool HTTP RequestError calling mcpo tool '{service_and_tool_path}': {type(exc).__name__} - {exc}"
                print(f"BaseMCPTool Error: {error_msg}"); traceback.print_exc()
                return {"error": error_msg, "exception_type": type(exc).__name__}
            except Exception as exc:
                error_msg = f"BaseMCPTool Unexpected error calling mcpo tool '{service_and_tool_path}': {type(exc).__name__} - {exc}"
                response_text_snippet = response.text[:500] if response and hasattr(response, 'text') else "Response object not available or no text."
                print(f"BaseMCPTool Error: {error_msg}. Response snippet: {response_text_snippet}"); traceback.print_exc()
                return {"error": error_msg, "exception_type": type(exc).__name__}

    def _handle_tool_result(self, result: dict | str, tool_name_for_log: str) -> str:
        print(f"BaseMCPTool DEBUG: {tool_name_for_log} result from mcpo: {str(result)[:500]}...")
        parsed_result = result
        if isinstance(result, str):
            try:
                parsed_result = json.loads(result)
            except json.JSONDecodeError:
                if "error" in result.lower() or "failed" in result.lower() or "traceback" in result.lower():
                    return f"调用 {tool_name_for_log} 失败，返回非JSON错误文本: {result}"
                print(f"BaseMCPTool Info: Result for {tool_name_for_log} is a non-JSON string, returning as is.")
                return result
        if isinstance(parsed_result, dict):
            if "error" in parsed_result and "status_code" in parsed_result:
                return f"调用 {tool_name_for_log} 时发生HTTP错误：{parsed_result.get('error')}"
            if parsed_result.get("status") == "error":
                error_msg = parsed_result.get("error_message", "未知错误")
                error_code = parsed_result.get("error_code", "NO_CODE")
                return f"工具 {tool_name_for_log} 执行失败 (错误码: {error_code})：{error_msg}"
            try:
                return json.dumps(parsed_result, ensure_ascii=False, indent=2)
            except Exception as e:
                print(f"BaseMCPTool Error formatting successful dict result for {tool_name_for_log}: {e}")
                return str(parsed_result)
        print(f"BaseMCPTool Warning: Unexpected result format from {tool_name_for_log} mcpo call: {type(result)}, content: {str(result)[:200]}")
        return f"从 {tool_name_for_log} 服务收到的结果格式不正确或无法处理: {str(result)[:500]}"

    def _run_default_sync_wrapper(self, **kwargs) -> str:
        tool_name = getattr(self, 'name', self.__class__.__name__)
        print(f"BaseMCPTool INFO: Synchronous _run called for {tool_name} with args: {kwargs}.")
        result_str = ""
        try:
            # --- 改进的 asyncio.run 处理 ---
            try:
                loop = asyncio.get_running_loop()
            except RuntimeError:
                loop = None

            async def async_runner():
                return await self._arun(**kwargs)

            if loop and loop.is_running():
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                    future = executor.submit(asyncio.run, async_runner())
                    result = future.result(timeout=120)
            else:
                result = asyncio.run(async_runner())
            result_str = str(result)
        except asyncio.TimeoutError:
            error_message = f"Tool {tool_name} execution timed out after 120 seconds."
            print(f"BaseMCPTool ERROR_TRACE: {error_message}"); result_str = error_message
        except RuntimeError as e:
            if "cannot run event loop while another loop is running" in str(e).lower() or "event loop is already running" in str(e).lower():
                error_message = (f"BaseMCPTool Error in {tool_name} _run: Nested asyncio event loop conflict. Original error: {e}")
            else:
                error_message = f"BaseMCPTool RuntimeError in {tool_name} _run: {type(e).__name__} - {e}"
            print(f"BaseMCPTool ERROR_TRACE: {error_message}"); traceback.print_exc();
            result_str = error_message
        except Exception as e:
            error_message = f"BaseMCPTool General Exception in {tool_name} _run: {type(e).__name__} - {e}"
            print(f"BaseMCPTool ERROR_TRACE: {error_message}"); traceback.print_exc(); result_str = error_message
        return result_str

class HybridRAGTool(BaseMCPTool):
    name: str = "HybridRAGQueryTool"
    description: str = (
        "【核心RAG工具】用于通过执行混合检索增强生成 (RAG) 搜索来回答用户问题。 "
        "该工具整合了向量检索、知识图谱检索和关键词检索，并进行智能融合和重排序。 "
        "当用户需要从知识库中获取信息、回答复杂问题或生成报告时，应调用此工具。"
    )
    args_schema: Type[BaseModel] = QueryRequest
    target_mcp_service_path: str = "zhz_agent_rag_service/query_rag_v2"

    async def _arun(self, query: str, top_k_vector: int, top_k_kg: int, top_k_bm25: int, **kwargs: Any) -> str: # --- 确认有 **kwargs ---
        tool_name_for_log = getattr(self, 'name', "HybridRAGTool")
        print(f"CrewAI Tool DEBUG: {tool_name_for_log}._arun called with query='{query}', top_k_vector={top_k_vector}, top_k_kg={top_k_kg}, top_k_bm25={top_k_bm25}, additional_kwargs={kwargs}")

        security_context = kwargs.get('security_context')
        if security_context:
            print(f"CrewAI Tool INFO: Received security_context (in HybridRAGTool): {str(security_context)[:200]}...")

        payload = {
            "query": query,
            "top_k_vector": top_k_vector,
            "top_k_kg": top_k_kg,
            "top_k_bm25": top_k_bm25
        }
        result = await self._call_mcpo_endpoint(self.target_mcp_service_path, payload)
        return self._handle_tool_result(result, self.name)

    def _run(self, query: str, top_k_vector: int, top_k_kg: int, top_k_bm25: int, **kwargs: Any) -> str: # --- 确认有 **kwargs ---
        return self._run_default_sync_wrapper(query=query, top_k_vector=top_k_vector, top_k_kg=top_k_kg, top_k_bm25=top_k_bm25, **kwargs)
```

    |-- constants.py

``` py
# zhz_agent/zhz_rag/config/constants.py

NEW_KG_SCHEMA_DESCRIPTION = """
# 知识图谱结构 (KuzuDB) 与 Cypher 查询生成规则

## 1. 节点定义:
- 节点标签: `:ExtractedEntity` (你必须且只能使用此节点标签)
- 节点属性:
    - `id_prop`: STRING (主键，实体的唯一标识)
    - `text`: STRING (实体的名称或文本内容)
    - `label`: STRING (实体类型。允许的值: "PERSON", "ORGANIZATION", "TASK")

## 2. 关系定义:
- 关系类型: `:WorksAt`
    - 结构: `(:ExtractedEntity {label:"PERSON"}) -[:WorksAt]-> (:ExtractedEntity {label:"ORGANIZATION"})`
    - 含义: 一个人 (PERSON) 在一个组织 (ORGANIZATION) 工作。
- 关系类型: `:AssignedTo`
    - 结构: `(:ExtractedEntity {label:"TASK"}) -[:AssignedTo]-> (:ExtractedEntity {label:"PERSON"})`
    - 含义: 一个任务 (TASK) 被分配给一个人 (PERSON)。

## 3. Cypher 查询生成 - 输出为 JSON 对象:

    你的【完整且唯一】的回答，必须是一个包含 "status" 和 "query" 字段的JSON对象。
    - 如果你能根据用户问题和Schema生成一个有效的Cypher查询：
        - "status" 字段应为 "success"。
        - "query" 字段应为该Cypher查询字符串。
    - 如果你无法生成有效的Cypher查询：
        - "status" 字段应为 "unable_to_generate"。
        - "query" 字段应为 "无法生成Cypher查询."。
    【不要在JSON之外或query字段内（当status为success时）包含任何解释或额外文本。】

## 4. JSON 输出格式示例:

### 示例 1 (能够生成查询):
用户问题: "任务'FixBug123'分配给了谁？"
你的【完整且唯一】的 JSON 回答:
```json
{
  "status": "success",
  "query": "MATCH (t:ExtractedEntity {text: 'FixBug123', label: 'TASK'})-[:AssignedTo]->(p:ExtractedEntity {label: 'PERSON'}) RETURN p.text AS Assignee"
}
示例 2 (无法根据Schema回答):
用户问题: "法国的首都是哪里？"
你的【完整且唯一】的 JSON 回答:
{
  "status": "unable_to_generate",
  "query": "无法生成Cypher查询."
}
"""
```

    |-- pydantic_models.py

``` py
# /home/zhz/zhz_agent/zhz_rag/config/pydantic_models.py
from pydantic import BaseModel, Field, root_validator
from typing import List, Dict, Any, Optional
from enum import Enum
from datetime import datetime
import uuid

# --- RAG Models ---
class QueryRequest(BaseModel):
    query: str
    # --- START: 新增 filters 字段 ---
    filters: Optional[Dict[str, Any]] = Field(default=None, description="Optional metadata filter to apply during retrieval.")
    # --- END: 新增 filters 字段 ---
    # --- 修改：为所有 top_k 参数提供默认值，使其变为可选 ---
    top_k_vector: int = Field(default=3, description="Number of results to retrieve from vector search.")
    top_k_bm25: int = Field(default=3, description="Number of results to retrieve from BM25 search.")
    top_k_kg: int = Field(default=2, description="Number of results to retrieve from Knowledge Graph search.")
    top_k_final: int = Field(default=3, description="Number of final results after fusion and reranking.")
    # --- 结束修改 ---

    class Config:
        json_schema_extra = {
            "example": {
                "query": "What are the main objectives of the project?",
                "filters": {"must": [{"key": "filename", "match": {"value": "report.docx"}}]}, # <-- 更新示例
                "top_k_vector": 3,
                "top_k_bm25": 3,
                "top_k_kg": 2,
                "top_k_final": 3
            }
        }
        # 移除了 root_validator 和 extra='forbid' 以简化并遵循新的实践
        # 如果您仍需要严格的字段检查，可以将 extra='forbid' 放回

class RetrievedDocument(BaseModel):
    source_type: str
    content: str
    score: Optional[float] = None
    metadata: Optional[Dict[str, Any]] = None

class HybridRAGResponse(BaseModel):
    original_query: str
    answer: str
    retrieved_sources: List[RetrievedDocument]
    debug_info: Optional[Dict[str, Any]] = None


# --- Task Management Models ---
class TaskStatus(str, Enum):
    PENDING = "pending"
    ACTIVE = "active"
    COMPLETED = "completed"
    CANCELLED = "cancelled"
    FAILED = "failed"
    REMINDING = "reminding"

class ReminderMethod(str, Enum):
    NOTIFICATION = "notification"

class TaskModel(BaseModel):
    id: str = Field(default_factory=lambda: str(uuid.uuid4()), description="任务的唯一ID (自动生成)")
    title: str = Field(description="任务标题")
    description: Optional[str] = Field(None, description="任务的详细描述")
    status: TaskStatus = Field(default=TaskStatus.PENDING, description="任务当前状态")
    created_at: datetime = Field(default_factory=datetime.utcnow, description="任务创建时间 (UTC)")
    updated_at: datetime = Field(default_factory=datetime.utcnow, description="任务最后更新时间 (UTC)")
    due_date: Optional[datetime] = Field(None, description="任务截止日期或计划执行时间 (UTC)")
    reminder_time: Optional[datetime] = Field(None, description="任务提醒时间 (UTC)")
    reminder_offset_minutes: Optional[int] = Field(None, description="提醒时间相对于due_date的提前分钟数 (例如10分钟前)")
    reminder_methods: List[ReminderMethod] = Field(default=[ReminderMethod.NOTIFICATION], description="提醒方式列表")
    priority: int = Field(default=0, description="任务优先级 (例如 0:普通, 1:重要, 2:紧急)")
    tags: List[str] = Field(default_factory=list, description="任务标签")
    action_type: Optional[str] = Field(None, description="任务到期时需要执行的动作类型 (例如 'navigate', 'send_message', 'run_report')")
    action_payload: Dict[str, Any] = Field(default_factory=dict, description="执行动作时需要的参数 (例如导航的目的地)")
    execution_result: Optional[str] = Field(None, description="任务执行后的结果或错误信息")
    last_executed_at: Optional[datetime] = Field(None, description="上次执行时间 (UTC)")

    class Config:
        use_enum_values = True
        # Pydantic v2 推荐使用 from_attributes 替代 orm_mode
        from_attributes = True


class CreateTaskRequest(BaseModel):
    title: str
    description: Optional[str] = None
    due_date: Optional[datetime] = None
    reminder_offset_minutes: Optional[int] = None
    reminder_methods: Optional[List[ReminderMethod]] = [ReminderMethod.NOTIFICATION]
    priority: Optional[int] = 0
    tags: Optional[List[str]] = None
    action_type: Optional[str] = None
    action_payload: Optional[Dict[str, Any]] = None
    
    class Config:
        extra = 'forbid'

class UpdateTaskRequest(BaseModel):
    title: Optional[str] = None
    description: Optional[str] = None
    status: Optional[TaskStatus] = None
    due_date: Optional[datetime] = None
    reminder_offset_minutes: Optional[int] = None
    reminder_methods: Optional[List[ReminderMethod]] = None
    priority: Optional[int] = None
    tags: Optional[List[str]] = None
    action_type: Optional[str] = None
    action_payload: Optional[Dict[str, Any]] = None

    class Config:
        extra = 'forbid'


class IdentifiedEntity(BaseModel):
    text: str = Field(description="识别出的实体文本。")
    label: Optional[str] = Field(None, description="推断的实体类型 (例如 PERSON, ORGANIZATION, TASK)。")

class ExtractedRelationItem(BaseModel): # 新建一个类名以避免与可能的其他同名类冲突
    head_entity_text: str
    head_entity_label: str
    relation_type: str
    tail_entity_text: str
    tail_entity_label: str

class ExtractedEntitiesAndRelationIntent(BaseModel):
    entities: List[IdentifiedEntity] = Field(default_factory=list, description="从用户查询中识别出的核心实体列表。")
    # --- 新增 "relations" 字段 ---
    relations: List[ExtractedRelationItem] = Field(default_factory=list, description="从用户查询中识别出的关系列表。")
    # --- "relation_hint" 字段可以保留，或者如果您觉得 "relations" 列表更全面，可以考虑移除或标记为废弃 ---
    relation_hint: Optional[str] = Field(None, description="[可选的旧字段] 如果用户查询暗示了实体间的特定关系，这里是关系的文本描述或关键词。新的 'relations' 列表更推荐。")


class QueryExpansionAndKGExtractionOutput(BaseModel):
    expanded_queries: List[str] = Field(
        default_factory=list,
        description="A list of expanded or related questions generated from the original query."
    )
    extracted_entities_for_kg: ExtractedEntitiesAndRelationIntent = Field(
        default_factory=ExtractedEntitiesAndRelationIntent,
        description="Structured entities and relations extracted for Knowledge Graph querying."
    )
    metadata_filter: Optional[Dict[str, Any]] = Field( # <--- 新增字段
        default=None,
        description="An optional metadata filter (e.g., {'filename': 'report.docx'}) to apply during retrieval, if a specific source is mentioned."
    )
    
class RagQueryPlan(BaseModel):
    """
    Represents the output of the V2 RAG query planner.
    It contains the core query string and a metadata filter for precise retrieval.
    """
    query: str = Field(description="The refined, core query string for semantic vector search.")
    metadata_filter: Dict[str, Any] = Field(
        default_factory=dict,
        description="A ChromaDB-compatible 'where' filter for metadata-based pre-filtering of document chunks."
    )


```

    |-- PKG-INFO
    |-- SOURCES.txt
    |-- dependency_links.txt
    |-- requires.txt
    |-- top_level.txt
    |-- ZHZ_AGENT_tasks.db
    |-- __init__.py

``` py

```

    |-- common_utils.py

``` py
# 文件: zhz_rag/utils/common_utils.py

import httpx
import json
import traceback
import os
import glob
from dotenv import load_dotenv
from datetime import datetime, timezone
import uuid
import logging
import asyncio
from typing import List, Dict, Any, Optional
import re
import unicodedata
import glob

load_dotenv()

# --- Logger Configuration ---
utils_logger = logging.getLogger("UtilsLogger")
utils_logger.setLevel(logging.INFO)
if not utils_logger.hasHandlers():
    _utils_console_handler = logging.StreamHandler()
    _utils_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
    _utils_console_handler.setFormatter(_utils_formatter)
    utils_logger.addHandler(_utils_console_handler)
    utils_logger.propagate = False
utils_logger.info("--- UtilsLogger configured ---")

# --- 【【【【【 核心修正点：统一路径计算逻辑到 zhz_rag 包内 】】】】】 ---
_CURRENT_FILE_DIR = os.path.dirname(os.path.abspath(__file__))
# __file__ 指向 .../zhz_rag/utils/common_utils.py
# os.path.dirname(_CURRENT_FILE_DIR) 将指向 .../zhz_rag/
_ZHZ_RAG_PACKAGE_DIR = os.path.dirname(_CURRENT_FILE_DIR)

STORED_DATA_ROOT_DIR = os.path.join(_ZHZ_RAG_PACKAGE_DIR, 'stored_data')

RAG_INTERACTION_LOGS_DIR = os.path.join(STORED_DATA_ROOT_DIR, 'rag_interaction_logs')
EVALUATION_RESULTS_LOGS_DIR = os.path.join(STORED_DATA_ROOT_DIR, 'evaluation_results_logs')
FINETUNING_GENERATED_DATA_DIR = os.path.join(_ZHZ_RAG_PACKAGE_DIR, 'finetuning', 'generated_data')
# --- 【【【【【 修正结束 】】】】】

# Ensure these directories exist
_DIRECTORIES_TO_CREATE = [
    STORED_DATA_ROOT_DIR,
    RAG_INTERACTION_LOGS_DIR,
    EVALUATION_RESULTS_LOGS_DIR,
    FINETUNING_GENERATED_DATA_DIR
]
for dir_path in _DIRECTORIES_TO_CREATE:
    if not os.path.exists(dir_path):
        try:
            os.makedirs(dir_path, exist_ok=True)
            utils_logger.info(f"Successfully created directory: {dir_path}")
        except Exception as e:
            utils_logger.error(f"Error creating directory {dir_path}: {e}. Consider creating it manually.")

# --- Log File Path Getters (无需修改，它们会使用上面新的常量) ---

def get_interaction_log_filepath() -> str:
    """Gets the full path for the current RAG interaction log file (daily rotation)."""
    today_str = datetime.now(timezone.utc).strftime("%Y%m%d")
    return os.path.join(RAG_INTERACTION_LOGS_DIR, f"rag_interactions_{today_str}.jsonl")

def get_evaluation_result_log_filepath(evaluation_name: str) -> str:
    """Gets the full path for an evaluation result log file (daily rotation, by evaluation name)."""
    today_str = datetime.now(timezone.utc).strftime("%Y%m%d")
    return os.path.join(EVALUATION_RESULTS_LOGS_DIR, f"eval_results_{evaluation_name}_{today_str}.jsonl")

def find_latest_rag_interaction_log(log_dir: str = RAG_INTERACTION_LOGS_DIR) -> Optional[str]:
    """
    Finds the latest RAG interaction log file (rag_interactions_*.jsonl) in the specified directory.
    Defaults to RAG_INTERACTION_LOGS_DIR.
    """
    utils_logger.debug(f"Searching for RAG interaction logs in: {log_dir}")
    rag_log_pattern = os.path.join(log_dir, "rag_interactions_*.jsonl")
    candidate_rag_logs = glob.glob(rag_log_pattern)

    if candidate_rag_logs:
        candidate_rag_logs.sort(key=os.path.getmtime, reverse=True)
        utils_logger.info(f"Automatically selected RAG interaction log: {candidate_rag_logs[0]}")
        return candidate_rag_logs[0]
    else:
        utils_logger.warning(f"No RAG interaction log files found matching pattern: {rag_log_pattern} in directory {log_dir}")
        return None

# --- 其他辅助函数 (保持不变) ---
# MCP Tool Calling Utility
MCPO_BASE_URL = os.getenv("MCPO_BASE_URL", "http://localhost:8006")

async def call_mcpo_tool(tool_name_with_prefix: str, payload: Dict[str, Any]) -> Dict[str, Any]:
    # (此函数内容保持不变)
    api_url = f"{MCPO_BASE_URL}/{tool_name_with_prefix}"
    cleaned_payload = {k: v for k, v in (payload or {}).items() if v is not None}
    utils_logger.info(f"CALL_MCPO_TOOL: Attempting to call {api_url}")
    timeout_config = httpx.Timeout(120.0, connect=10.0, read=120.0, write=10.0)
    async with httpx.AsyncClient(timeout=timeout_config) as client:
        try:
            headers = {"Content-Type": "application/json", "Accept": "application/json"}
            response = await client.post(api_url, json=cleaned_payload, headers=headers)
            response.raise_for_status()
            return {"success": True, "data": response.json()}
        except Exception as exc:
            # Simplified error handling for brevity
            return {"success": False, "error": str(exc)}


def load_jsonl_file(filepath: str, encoding: str = 'utf-8') -> List[Dict[str, Any]]:
    # (此函数内容保持不变)
    data_list: List[Dict[str, Any]] = []
    if not os.path.exists(filepath):
        utils_logger.error(f"File not found: {filepath}")
        return data_list
    try:
        with open(filepath, 'r', encoding=encoding) as f:
            for line in f:
                if line.strip(): data_list.append(json.loads(line.strip()))
    except Exception as e_file:
        utils_logger.error(f"Error reading or processing file {filepath}: {e_file}", exc_info=True)
        return []
    utils_logger.info(f"Successfully loaded {len(data_list)} entries from {filepath}")
    return data_list


def normalize_text_for_id(text: str) -> str:
    # (此函数内容保持不变)
    if not isinstance(text, str): return str(text)
    try:
        normalized_text = unicodedata.normalize('NFKD', text)
        normalized_text = normalized_text.lower().strip()
        return re.sub(r'\s+', ' ', normalized_text)
    except Exception:
        return text

def find_latest_rag_interaction_log(log_dir: str) -> Optional[str]:
    """
    Finds the latest RAG interaction log file in a given directory.
    This is useful for evaluation scripts.
    """
    # 确保我们使用的是此模块自己的 logger
    from .interaction_logger import interaction_logger_module_logger as logger
    logger.debug(f"Searching for RAG interaction logs in: {log_dir}")
    rag_log_pattern = os.path.join(log_dir, "rag_interactions_*.jsonl")
    candidate_rag_logs = glob.glob(rag_log_pattern)
    if candidate_rag_logs:
        candidate_rag_logs.sort(key=os.path.getmtime, reverse=True)
        logger.info(f"Automatically selected RAG interaction log: {candidate_rag_logs[0]}")
        return candidate_rag_logs[0]
    else:
        logger.warning(f"No RAG interaction log files found matching pattern: {rag_log_pattern} in directory {log_dir}")
        return None

def load_jsonl_file(filepath: str, encoding: str = 'utf-8') -> List[Dict[str, Any]]:
    """
    Loads a .jsonl file line by line into a list of dictionaries.
    """
    from .interaction_logger import interaction_logger_module_logger as logger
    data_list: List[Dict[str, Any]] = []
    if not os.path.exists(filepath):
        logger.error(f"File not found: {filepath}")
        return data_list
    try:
        with open(filepath, 'r', encoding=encoding) as f:
            for line_number, line in enumerate(f, 1):
                try:
                    if line.strip():
                        data_list.append(json.loads(line.strip()))
                except json.JSONDecodeError as e_json:
                    logger.warning(f"Skipping malformed JSON line {line_number} in {filepath}: {e_json}")
    except Exception as e_file:
        logger.error(f"Error reading or processing file {filepath}: {e_file}", exc_info=True)
        return []
    logger.info(f"Successfully loaded {len(data_list)} entries from {filepath}")
    return data_list
```

    |-- db_utils.py

``` py
# ZHZ_AGENT/database.py
import os
from databases import Database
from sqlalchemy import create_engine
from sqlalchemy.orm import declarative_base
from typing import Optional

# --- APScheduler 相关导入 ---
from apscheduler.schedulers.asyncio import AsyncIOScheduler
from apscheduler.jobstores.sqlalchemy import SQLAlchemyJobStore
# --- [修改] 明确导入并使用 pytz ---
import pytz #

# --- 数据库配置 ---
ZHZ_AGENT_DIR = os.path.dirname(os.path.abspath(__file__))
DB_NAME = "ZHZ_AGENT_tasks.db"
DATABASE_FILE_PATH = os.path.join(ZHZ_AGENT_DIR, DB_NAME)
DATABASE_URL = f"sqlite+aiosqlite:///{DATABASE_FILE_PATH}"

database = Database(DATABASE_URL)
sqlalchemy_engine = create_engine(DATABASE_URL.replace("+aiosqlite", ""))
Base = declarative_base() #

# --- 全局调度器实例定义 ---
scheduler: Optional[AsyncIOScheduler] = None

def get_scheduler() -> AsyncIOScheduler:
    """获取或创建调度器实例，并配置作业存储和 UTC 时区。"""
    global scheduler
    if scheduler is None:
        jobstore_url = f"sqlite:///{DATABASE_FILE_PATH}"
        jobstores = {
            'default': SQLAlchemyJobStore(url=jobstore_url, tablename='apscheduler_jobs_v2') #
        }
        # --- [修复] 明确使用 pytz.utc 设置时区 ---
        scheduler = AsyncIOScheduler(
            jobstores=jobstores,
            timezone=pytz.utc # <--- 强制使用 pytz.utc #
        )
        import logging
        logging.getLogger('apscheduler').setLevel(logging.DEBUG)
        print(f"APScheduler initialized with timezone: {pytz.utc}") # 确认使用 pytz.utc #
    return scheduler #
```

    |-- gemini_api_utils.py

``` py
# 文件: zhz_rag/utils/gemini_api_utils.py

import google.generativeai as genai
import os
from dotenv import load_dotenv
import logging
from typing import Optional

# --- 加载 .env ---
# 这样即使在非Dagster环境中也能获取环境变量
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
dotenv_path = os.path.join(project_root, '.env')
if os.path.exists(dotenv_path):
    load_dotenv(dotenv_path=dotenv_path)

# --- 配置日志 ---
gemini_util_logger = logging.getLogger(__name__)

class GeminiAPIClient:
    """
    一个独立的、不依赖于Dagster的Gemini API客户端。
    """
    def __init__(self, api_key: str, proxy_url: Optional[str] = None):
        self._api_key = api_key
        self._proxy_url = proxy_url
        self._configure_client()
        gemini_util_logger.info("GeminiAPIClient initialized.")

    def _configure_client(self):
        """配置google-generativeai客户端，包括代理。"""
        try:
            if self._proxy_url:
                os.environ['https_proxy'] = self._proxy_url
                os.environ['http_proxy'] = self._proxy_url
                gemini_util_logger.info(f"Using proxy for Gemini API: {self._proxy_url}")
            
            genai.configure(api_key=self._api_key)
        except Exception as e:
            gemini_util_logger.error(f"Failed to configure Gemini client: {e}", exc_info=True)
            raise

    @classmethod
    def from_env(cls):
        """通过环境变量方便地创建实例。"""
        api_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
        if not api_key:
            raise ValueError("GEMINI_API_KEY or GOOGLE_API_KEY not found in environment variables.")
        
        proxy_url = os.getenv("LITELLM_PROXY_URL") # 复用这个代理配置
        return cls(api_key=api_key, proxy_url=proxy_url)

    def get_model(self, model_name: str = "gemini-1.5-flash-latest"):
        """获取一个配置好的GenerativeModel实例。"""
        try:
            return genai.GenerativeModel(model_name)
        except Exception as e:
            gemini_util_logger.error(f"Failed to get Gemini model '{model_name}': {e}", exc_info=True)
            return None
```

    |-- hardware_manager.py

``` py
# zhz_agent/utils/hardware_manager.py
import os
import psutil
import torch
import logging
from typing import Optional, Dict, Any

# 配置日志记录器
logger = logging.getLogger(__name__)
if not logger.handlers:
    handler = logging.StreamHandler()
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    handler.setFormatter(formatter)
    logger.setLevel(logging.INFO)

class HardwareInfo:
    """用于存储检测到的硬件信息的简单数据类"""
    def __init__(self,
                 cpu_logical_cores: int,
                 cpu_physical_cores: int,
                 total_system_ram_gb: float,
                 gpu_available: bool = False,
                 gpu_name: Optional[str] = None,
                 gpu_vram_total_gb: float = 0.0):
        self.cpu_logical_cores = cpu_logical_cores
        self.cpu_physical_cores = cpu_physical_cores
        self.total_system_ram_gb = total_system_ram_gb
        self.gpu_available = gpu_available
        self.gpu_name = gpu_name
        self.gpu_vram_total_gb = gpu_vram_total_gb

    def __str__(self):
        gpu_str = (f"GPU: {self.gpu_name} with {self.gpu_vram_total_gb:.2f} GB VRAM"
                   if self.gpu_available else "GPU: Not available or not detected")
        return (f"HardwareInfo(CPU: {self.cpu_physical_cores} Physical/{self.cpu_logical_cores} Logical Cores, "
                f"RAM: {self.total_system_ram_gb:.2f} GB, {gpu_str})")

class HardwareManager:
    """
    硬件抽象层 (HAL)，用于检测系统当前的硬件特性，
    并为上层应用提供一个统一的接口来查询和利用这些资源。
    """
    def __init__(self):
        logger.info("Initializing HardwareManager...")
        self.hw_info: Optional[HardwareInfo] = None
        self._detect_hardware()
        if self.hw_info:
            logger.info(f"Hardware detection complete: {self.hw_info}")
        else:
            logger.error("Hardware detection failed to produce valid HardwareInfo.")

    def _detect_cpu_info(self) -> Dict[str, Any]:
        """检测CPU信息"""
        try:
            logical_cores = psutil.cpu_count(logical=True)
            physical_cores = psutil.cpu_count(logical=False)
            if physical_cores is None:
                logger.warning("Could not determine physical core count, using logical core count as fallback.")
                physical_cores = logical_cores
            logger.info(f"CPU Info: Logical cores={logical_cores}, Physical cores={physical_cores}")
            return {"cpu_logical_cores": logical_cores, "cpu_physical_cores": physical_cores}
        except Exception as e:
            logger.error(f"Error detecting CPU info: {e}", exc_info=True)
            return {"cpu_logical_cores": 1, "cpu_physical_cores": 1}

    def _detect_memory_info(self) -> Dict[str, Any]:
        """检测内存信息"""
        try:
            virtual_mem = psutil.virtual_memory()
            total_ram_gb = virtual_mem.total / (1024 ** 3)
            logger.info(f"Memory Info: Total RAM={total_ram_gb:.2f} GB")
            return {"total_system_ram_gb": total_ram_gb}
        except Exception as e:
            logger.error(f"Error detecting memory info: {e}", exc_info=True)
            return {"total_system_ram_gb": 0.0}

    def _detect_gpu_info(self) -> Dict[str, Any]:
        """检测GPU信息 (目前仅NVIDIA GPU通过torch.cuda)"""
        gpu_details = {"gpu_available": False, "gpu_name": None, "gpu_vram_total_gb": 0.0}
        try:
            if torch.cuda.is_available():
                gpu_details["gpu_available"] = True
                device_id = 0
                gpu_name = torch.cuda.get_device_name(device_id)
                props = torch.cuda.get_device_properties(device_id)
                vram_total_gb = props.total_memory / (1024 ** 3)
                gpu_details["gpu_name"] = gpu_name
                gpu_details["gpu_vram_total_gb"] = vram_total_gb
                logger.info(f"NVIDIA GPU Detected: Name='{gpu_name}', VRAM={vram_total_gb:.2f} GB")
            else:
                logger.info("torch.cuda.is_available() returned False. No compatible NVIDIA GPU detected.")
        except Exception as e:
            logger.error(f"Error detecting NVIDIA GPU info: {e}", exc_info=True)
        return gpu_details

    def _detect_hardware(self):
        """执行所有硬件检测并填充 HardwareInfo"""
        cpu_info = self._detect_cpu_info()
        mem_info = self._detect_memory_info()
        gpu_info = self._detect_gpu_info()
        
        self.hw_info = HardwareInfo(
            cpu_logical_cores=cpu_info["cpu_logical_cores"],
            cpu_physical_cores=cpu_info["cpu_physical_cores"],
            total_system_ram_gb=mem_info["total_system_ram_gb"],
            gpu_available=gpu_info["gpu_available"],
            gpu_name=gpu_info["gpu_name"],
            gpu_vram_total_gb=gpu_info["gpu_vram_total_gb"]
        )

    def get_hardware_info(self) -> Optional[HardwareInfo]:
        """返回检测到的硬件信息"""
        return self.hw_info

    def recommend_llm_gpu_layers(self, model_total_layers: int, model_size_on_disk_gb: float, kv_cache_gb_per_1k_ctx: float = 0.25, context_length_tokens: int = 4096, safety_buffer_vram_gb: float = 1.5) -> int:
        """根据可用VRAM推荐LLM应卸载到GPU的层数。"""
        if not self.hw_info or not self.hw_info.gpu_available or self.hw_info.gpu_vram_total_gb == 0:
            logger.info("GPU not available, recommending 0 GPU layers (CPU only).")
            return 0

        estimated_kv_cache_vram_gb = (context_length_tokens / 1000) * kv_cache_gb_per_1k_ctx
        available_vram_for_model_weights_gb = self.hw_info.gpu_vram_total_gb - estimated_kv_cache_vram_gb - safety_buffer_vram_gb
        logger.info(f"VRAM Details: Total={self.hw_info.gpu_vram_total_gb:.2f}GB, Est. KV Cache={estimated_kv_cache_vram_gb:.2f}GB, Safety Buffer={safety_buffer_vram_gb:.2f}GB. Available for weights={available_vram_for_model_weights_gb:.2f}GB.")

        if available_vram_for_model_weights_gb <= 0:
            return 0
        
        if available_vram_for_model_weights_gb >= model_size_on_disk_gb:
            logger.info(f"Sufficient VRAM to offload all {model_total_layers} layers.")
            return -1
        else:
            proportion_of_model_can_fit = available_vram_for_model_weights_gb / model_size_on_disk_gb
            recommended_layers = int(model_total_layers * proportion_of_model_can_fit)
            logger.info(f"VRAM can fit ~{proportion_of_model_can_fit:.0%}. Recommending {recommended_layers} GPU layers.")
            return min(max(0, recommended_layers), model_total_layers)

    def recommend_concurrent_tasks(self, task_type: str = "cpu_bound_llm") -> int:
        """根据CPU核心数和任务类型推荐并发任务数"""
        if not self.hw_info:
            return 1
        if task_type == "cpu_bound_llm":
            return max(1, self.hw_info.cpu_physical_cores // 2)
        elif task_type == "io_bound":
            return self.hw_info.cpu_logical_cores * 2
        else:
            return self.hw_info.cpu_physical_cores

# --- main 用于测试 HardwareManager ---
if __name__ == "__main__":
    import os # 需要导入os才能在_detect_cpu_info的fallback中使用os.cpu_count()
    print("--- Testing HardwareManager ---")
    hw_manager = HardwareManager()
    hw_info = hw_manager.get_hardware_info()
    
    if hw_info:
        print("\n--- Detected Hardware ---")
        print(f"   CPU Logical Cores: {hw_info.cpu_logical_cores}")
        print(f"   CPU Physical Cores: {hw_info.cpu_physical_cores}")
        print(f"   Total System RAM: {hw_info.total_system_ram_gb:.2f} GB")
        if hw_info.gpu_available:
            print(f"   GPU Name: {hw_info.gpu_name}")
            print(f"   GPU VRAM Total: {hw_info.gpu_vram_total_gb:.2f} GB")
        else:
            print("   GPU: Not available or not detected.")
        
        print("\n--- Recommendations ---")
        # 假设一个1.7B Q8模型 (约1.8GB磁盘大小)，模型总共32层 (Qwen1.7B是28层，这里用32示意)
        # 上下文长度4096，每1k上下文KV缓存占用0.25GB VRAM (fp16时约0.23GB/1k，这里取个近似值)
        # 安全余量1.5GB VRAM
        model_layers = 28 # Qwen3-1.7B
        model_disk_size = 1.8 # GB
        ctx_len = 4096
        kv_per_1k_ctx = 0.25 
        vram_buffer = 1.5

        recommended_gpu_layers = hw_manager.recommend_llm_gpu_layers(
            model_total_layers=model_layers,
            model_size_on_disk_gb=model_disk_size,
            kv_cache_gb_per_1k_ctx=kv_per_1k_ctx,
            context_length_tokens=ctx_len,
            safety_buffer_vram_gb=vram_buffer
        )
        print(f"   Recommended LLM GPU Layers (for {model_disk_size}GB model, {model_layers} layers, {ctx_len} ctx): {recommended_gpu_layers}")
        
        recommended_llm_workers = hw_manager.recommend_concurrent_tasks(task_type="cpu_bound_llm")
        print(f"   Recommended Concurrent LLM Tasks (cpu_bound_llm): {recommended_llm_workers}")
        
        recommended_io_workers = hw_manager.recommend_concurrent_tasks(task_type="io_bound")
        print(f"   Recommended Concurrent I/O Tasks: {recommended_io_workers}")

    else:
        print("Failed to get hardware information.")

```

    |-- interaction_logger.py

``` py
# 文件: zhz_rag/utils/interaction_logger.py
import os
import json
import asyncio
from typing import Dict, Any, Optional
from datetime import datetime, timezone
import logging
import aiofiles  # 使用 aiofiles 进行异步文件操作
import uuid
import traceback
import sys # <--- 添加此行

# --- 配置此模块的logger ---
# 使用一个特定的名字，以便在项目中其他地方可以按名获取，避免与根logger混淆
interaction_logger_module_logger = logging.getLogger("InteractionLoggerUtil")
# 建议从环境变量或配置文件读取日志级别，提供默认值
interaction_logger_module_logger.setLevel(os.getenv("INTERACTION_LOG_LEVEL", "INFO").upper())
# 设置propagate = False以防止日志消息被传递到父级logger（如root logger），避免重复输出
interaction_logger_module_logger.propagate = False

# 确保只添加一次处理器，防止重复配置
if not interaction_logger_module_logger.hasHandlers():
    _il_console_handler = logging.StreamHandler()
    _il_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
    _il_console_handler.setFormatter(_il_formatter)
    interaction_logger_module_logger.addHandler(_il_console_handler)
    interaction_logger_module_logger.info("--- InteractionLoggerUtil configured ---")


# --- 定义日志存储目录常量 ---
# __file__ 指向 .../zhz_rag/utils/interaction_logger.py
_CURRENT_FILE_DIR_IL = os.path.dirname(os.path.abspath(__file__))
# _ZHZ_RAG_PACKAGE_DIR_IL 指向 .../zhz_rag
_ZHZ_RAG_PACKAGE_DIR_IL = os.path.dirname(_CURRENT_FILE_DIR_IL)

# STORED_DATA_ROOT_DIR_IL 指向 .../zhz_rag/stored_data
_STORED_DATA_ROOT_DIR_IL = os.path.join(_ZHZ_RAG_PACKAGE_DIR_IL, 'stored_data')

RAG_INTERACTION_LOGS_DIR_DEFAULT = os.path.join(_STORED_DATA_ROOT_DIR_IL, 'rag_interaction_logs')
EVALUATION_RESULTS_LOGS_DIR_DEFAULT = os.path.join(_STORED_DATA_ROOT_DIR_IL, 'evaluation_results_logs')


async def _async_write_to_jsonl_robust(filepath: str, interaction_json_string: str):
    """
    一个健壮的异步函数，用于将字符串追加到文件。
    使用了 aiofiles 库来避免阻塞事件循环。
    """
    logger_to_use = interaction_logger_module_logger
    logger_to_use.debug(f"ASYNC_WRITE_ROBUST: Attempting to write to {filepath}")
    try:
        async with aiofiles.open(filepath, mode='a', encoding='utf-8') as f:
            await f.write(interaction_json_string)
            await f.flush() # aiofiles 的 flush 也是异步的
        logger_to_use.debug(f"ASYNC_WRITE_ROBUST: Successfully wrote and flushed to {filepath}")
    except Exception as e:
        logger_to_use.error(f"CRITICAL_LOG_FAILURE in _async_write_to_jsonl_robust: Failed to write to {filepath}. Error: {e}", exc_info=True)
        # 备用方案
        # print(f"CRITICAL_LOG_FAILURE: Could not write to {filepath}. Error: {e}")
        # traceback.print_exc()

def _sync_write_to_jsonl_robust(filepath: str, interaction_json_string: str):
    """
    一个健壮的同步函数，用于将字符串追加到文件，并确保数据刷入磁盘。
    """
    logger_to_use = interaction_logger_module_logger
    logger_to_use.debug(f"SYNC_WRITE_ROBUST: Attempting to write to {filepath}")
    try:
        # 'a' for append. '+' is not strictly needed for 'a' as it creates the file if it doesn't exist.
        with open(filepath, 'a', encoding='utf-8') as f:
            f.write(interaction_json_string)
            # 步骤1: 确保Python应用层缓冲区的内容写入操作系统缓冲区
            f.flush()
            # 步骤2: 请求操作系统将缓冲区内容实际写入磁盘，提供最强保证
            os.fsync(f.fileno())
        logger_to_use.debug(f"SYNC_WRITE_ROBUST: Successfully wrote and synced to {filepath}")
    except Exception as e:
        # 这种底层的关键日志如果失败，需要非常明确的错误提示
        logger_to_use.error(f"CRITICAL_LOG_FAILURE in _sync_write_to_jsonl_robust: Failed to write to {filepath}. Error: {e}", exc_info=True)


async def log_interaction_data(
    log_data: Dict[str, Any],
    is_evaluation_result: bool = False,
    evaluation_name_for_file: Optional[str] = None
):
    """
    Asynchronously logs interaction data to a JSONL file in the appropriate directory.

    Args:
        log_data (Dict[str, Any]): The dictionary containing the data to log.
        is_evaluation_result (bool): If True, logs to the evaluation results directory. 
                                     Otherwise, logs to the standard RAG interaction directory.
        evaluation_name_for_file (Optional[str]): A specific name for the evaluation file, e.g., 'answer_gemini'.
    """
    try:
        # --- 修正：根据 is_evaluation_result 选择正确的目录 ---
        if is_evaluation_result:
            target_dir = os.getenv("EVALUATION_RESULTS_LOGS_DIR", EVALUATION_RESULTS_LOGS_DIR_DEFAULT)
            if not evaluation_name_for_file:
                evaluation_name_for_file = "default_eval"
            # 文件名格式: eval_results_指定的名称_日期.jsonl
            log_filename = f"eval_results_{evaluation_name_for_file}_{datetime.now(timezone.utc).strftime('%Y%m%d')}.jsonl"
        else:
            target_dir = os.getenv("RAG_INTERACTION_LOGS_DIR", RAG_INTERACTION_LOGS_DIR_DEFAULT)
            # 文件名格式: rag_interactions_日期.jsonl
            log_filename = f"rag_interactions_{datetime.now(timezone.utc).strftime('%Y%m%d')}.jsonl"

        # 确保目标目录存在
        os.makedirs(target_dir, exist_ok=True)
        log_filepath = os.path.join(target_dir, log_filename)

        # 准备要写入的JSON字符串
        # 确保时间戳是字符串格式，避免JSON序列化问题
        if 'timestamp_utc' in log_data and isinstance(log_data['timestamp_utc'], datetime):
             log_data['timestamp_utc'] = log_data['timestamp_utc'].isoformat()
        
        log_entry_str = json.dumps(log_data, ensure_ascii=False)

        # 异步写入文件
        await _async_write_to_jsonl_robust(log_filepath, log_entry_str + '\n')
        
    except Exception as e:
        # 在独立的日志系统中记录日志本身的错误
        interaction_logger_module_logger.error(f"Failed to log interaction data. Error: {e}", exc_info=True)
        interaction_logger_module_logger.error(f"Original log data that failed: {str(log_data)[:500]}")

def get_logger(name: str) -> logging.Logger:
    """
    一个通用的函数，用于获取或创建具有标准配置的logger。
    这避免了在每个模块中重复配置logger。
    """
    logger = logging.getLogger(name)
    if not logger.hasHandlers():
        logger.setLevel(os.getenv(f"{name.upper()}_LOG_LEVEL", "INFO").upper())
        logger.propagate = False
        handler = logging.StreamHandler(sys.stdout) # 确保日志输出到标准输出
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - PID:%(process)d - %(levelname)s - %(message)s'
        )
        handler.setFormatter(formatter)
        logger.addHandler(handler)
    return logger
```

    |-- load_neo4j_data.py

``` py
# zhz_agent/load_neo4j_data.py
import json
import os
from neo4j import GraphDatabase, basic_auth
from dotenv import load_dotenv
import traceback

load_dotenv() # 确保加载 .env 文件中的NEO4J凭证

NEO4J_URI = os.getenv("NEO4J_URI", "bolt://localhost:7687")
NEO4J_USER = os.getenv("NEO4J_USER", "neo4j")
NEO4J_PASSWORD = os.getenv("NEO4J_PASSWORD") # 您需要确保这个密码是正确的

DATA_PATH = os.path.join(os.path.dirname(__file__), "data")
SAMPLE_KG_PATH = os.path.join(DATA_PATH, "sample_kg.json")

def clear_database(driver):
    """清除数据库中的所有节点和关系"""
    with driver.session(database="neo4j") as session:
        session.run("MATCH (n) DETACH DELETE n")
        print("Cleared all nodes and relationships from the database.")

def create_constraints(driver):
    """创建一些基本约束，确保节点属性的唯一性（如果适用）"""
    with driver.session(database="neo4j") as session:
        try:
            session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (p:Person) REQUIRE p.name IS UNIQUE")
            session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (pr:Project) REQUIRE pr.name IS UNIQUE")
            session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (r:Region) REQUIRE r.name IS UNIQUE")
            session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (prod:Product) REQUIRE prod.name IS UNIQUE")
            session.run("CREATE CONSTRAINT IF NOT EXISTS FOR (d:Document) REQUIRE d.id IS UNIQUE")
            # SalesAmount 通常不需要唯一约束，因为它可能重复（例如不同区域同一时期的销售）
            print("Ensured constraints are created (or already exist).")
        except Exception as e:
            print(f"Error creating constraints: {e}")


def load_data(driver, kg_data):
    """根据kg_data中的facts加载数据到Neo4j"""
    facts = kg_data.get("facts", [])
    
    with driver.session(database="neo4j") as session:
        entities_to_create = set()
        node_types_from_schema = { # 定义了主要实体的标签和它们的主要标识属性
            "Person": "name", "Project": "name", "Region": "name", 
            "Product": "name", "Document": "id", "Idea": "name" # 新增Idea类型
        }

        for fact in facts:
            subject_name = fact.get("subject")
            object_name = fact.get("object")
            fact_type = fact.get("type", "")

            subject_label = None
            # 基于fact_type或其他逻辑推断subject_label
            if "person_" in fact_type: subject_label = "Person"
            elif "region_" in fact_type: subject_label = "Region"
            elif "product_" in fact_type: subject_label = "Product"
            # ... 其他类型的映射 ...
            
            if subject_label and subject_name:
                prop_name = node_types_from_schema.get(subject_label, "name")
                entities_to_create.add((subject_label, prop_name, subject_name))

            object_label = None
            if not fact_type.endswith("_amount"): # 不是直接的销售额事实
                if "_project" in fact_type: object_label = "Project"
                elif "_product" in fact_type: object_label = "Product"
                elif "_document" in fact_type: object_label = "Document"
                elif "_idea" in fact_type: object_label = "Idea" # 新增对Idea类型的处理
                # ... 其他类型的映射 ...

                if object_label and object_name:
                    prop_name = node_types_from_schema.get(object_label, "name") # Document会用id, Idea会用name
                    entities_to_create.add((object_label, prop_name, object_name))
        
        for label, prop, value in entities_to_create:
            if value is not None:
                query = f"MERGE (n:{label} {{{prop}: $value}})"
                session.run(query, value=value)
                print(f"Merged node: ({label} {{{prop}: '{value}'}})")

        for fact in facts:
            s_name = fact.get("subject")
            rel = fact.get("relation")
            o_name = fact.get("object")
            fact_type = fact.get("type", "")
            period = fact.get("period")

            if fact_type == "region_sales_amount" and period:
                session.run("MERGE (r:Region {name: $s_name})", s_name=s_name)
                try:
                    # ... (销售额解析逻辑不变) ...
                    if isinstance(o_name, str) and '万元' in o_name:
                        numeric_val_str = o_name.replace('万元', '').strip()
                        numeric_val = float(numeric_val_str)
                        unit_val = '万元'
                    # ... (其他单位解析) ...
                    else:
                        numeric_val = float(o_name) # 尝试直接转换
                        unit_val = None 
                    
                    query = """
                    MATCH (r:Region {name: $s_name})
                    CREATE (sa:SalesAmount {numeric_amount: $num_val, period: $period, unit: $unit_val})
                    CREATE (r)-[:HAS_SALES_AMOUNT]->(sa)
                    """
                    session.run(query, s_name=s_name, num_val=numeric_val, period=period, unit_val=unit_val)
                    print(f"Created SalesAmount for {s_name}, {period}: {numeric_val} {unit_val or ''}")
                except ValueError:
                    print(f"Could not parse sales amount: {o_name} for {s_name}, {period}. Skipping this SalesAmount fact.")
                
            elif s_name and rel and o_name: 
                s_label, o_label = None, None
                s_prop, o_prop = "name", "name" 

                # --- 更精确的标签和属性推断 ---
                if fact_type == "person_project" and rel == "WORKS_ON":
                    s_label, o_label = "Person", "Project"
                elif fact_type == "person_idea" and rel == "PROPOSED_IDEA": # 新增
                    s_label, o_label = "Person", "Idea"
                elif fact_type == "region_product" and rel == "HAS_SALES_PRODUCT": # 假设type是 region_product
                    s_label, o_label = "Region", "Product"
                elif fact_type == "product_document" and rel == "RELATED_TO":
                    s_label, o_label = "Product", "Document"
                    o_prop = "id" # Document用id匹配
                # 您可以根据您的fact_type添加更多精确的映射规则

                if s_label and o_label:
                    query = f"""
                    MATCH (s:{s_label} {{{s_prop}: $s_name}})
                    MATCH (o:{o_label} {{{o_prop}: $o_name}})
                    MERGE (s)-[:{rel}]->(o)
                    """
                    session.run(query, s_name=s_name, o_name=o_name)
                    print(f"Merged relationship: ({s_label} {{{s_prop}:'{s_name}'}})-[:{rel}]->({o_label} {{{o_prop}:'{o_name}'}})")
                else:
                    print(f"Could not determine labels for fact: {fact} (s_label: {s_label}, o_label: {o_label}). Relationship not created.")
            else:
                print(f"Skipping incomplete fact: {fact}")


if __name__ == "__main__":
    driver = None
    try:
        driver = GraphDatabase.driver(NEO4J_URI, auth=basic_auth(NEO4J_USER, NEO4J_PASSWORD))
        driver.verify_connectivity()
        print("Successfully connected to Neo4j.")
        
        clear_database(driver) # 清空数据库
        create_constraints(driver) # 创建约束

        with open(SAMPLE_KG_PATH, 'r', encoding='utf-8') as f:
            kg_data_to_load = json.load(f)
        
        load_data(driver, kg_data_to_load)
        
        print("\nData loading process completed.")
        print("You can now verify the data in Neo4j Browser (http://localhost:7474).")
        print("Example query to check SalesAmount:")
        print("MATCH (r:Region)-[:HAS_SALES_AMOUNT]->(sa:SalesAmount) RETURN r.name, sa.numeric_amount, sa.unit, sa.period")
        print("Example query to check Person-Project:")
        print("MATCH (p:Person)-[:WORKS_ON]->(proj:Project) RETURN p.name, proj.name")

    except Exception as e:
        print(f"An error occurred: {e}")
        traceback.print_exc()
    finally:
        if driver:
            driver.close()
            print("Neo4j connection closed.")
```

    |-- retrievers/
    |-- __init__.py

``` py
# zhz_agent/core_rag/__init__.py
from .kg_retriever import KGRetriever
from .fusion_engine import FusionEngine
# 如果上面 retrievers/__init__.py 也做了导出，这里也可以考虑是否进一步导出
```

    |-- fusion_engine.py

``` py
import os
import hashlib
import jieba
from typing import List, Dict, Any, Optional
import logging
import asyncio
from sentence_transformers import CrossEncoder

from zhz_rag.config.pydantic_models import RetrievedDocument

class FusionEngine:
    def __init__(self, logger: Optional[logging.Logger] = None, rrf_k: int = 60, enable_reranker: bool = True):
        if logger:
            self.logger = logger
        else:
            self.logger = logging.getLogger("FusionEngineLogger")
            if not self.logger.hasHandlers():
                self.logger.setLevel(logging.INFO)
                handler = logging.StreamHandler()
                formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(module)s:%(lineno)d - %(message)s')
                handler.setFormatter(formatter)
                self.logger.addHandler(handler)
        
        self.rrf_k = rrf_k
        self.cross_encoder: Optional[CrossEncoder] = None # 明确类型
        
        # --- 根据标志决定是否加载再排序器 ---
        if enable_reranker:
            self._initialize_reranker()
        else:
            self.logger.warning("Reranker is explicitly disabled by configuration. Reranking will be skipped.")

    def _initialize_reranker(self):
        """
        严格从本地路径初始化Cross-Encoder模型。
        如果本地路径不存在，则禁用再排序功能。
        """
        # --- 使用您指定的本地模型路径 ---
        # 模型名称/ID
        model_name = "BAAI/bge-reranker-base"
        # 构建本地路径
        local_model_path = os.path.join(os.path.expanduser("~"), "models", model_name)

        self.logger.info(f"Attempting to load reranker model from local path: {local_model_path}")

        if not os.path.isdir(local_model_path):
            self.logger.error(f"Reranker model directory not found at: '{local_model_path}'. Reranking will be disabled.")
            self.cross_encoder = None
            return

        try:
            # 直接将本地路径传递给CrossEncoder
            self.cross_encoder = CrossEncoder(local_model_path, max_length=512)
            self.logger.info(f"Cross-Encoder model loaded successfully from '{local_model_path}'.")
        except Exception as e:
            self.logger.error(f"Failed to load Cross-Encoder model from '{local_model_path}': {e}", exc_info=True)
            self.cross_encoder = None
            
    async def rerank_documents(
        self,
        query: str,
        documents: List[RetrievedDocument],
        top_n: int = 5
    ) -> List[RetrievedDocument]:
        """
        使用Cross-Encoder模型对文档列表进行再排序，返回相关性最高的top_n个文档。
        """
        if self.cross_encoder is None:
            self.logger.warning("Reranker is not available, returning top_n documents without reranking.")
            return documents[:top_n]

        if not documents:
            return []

        if not query:
            self.logger.warning("Reranking query is empty. Returning original documents.")
            return documents
        
        self.logger.info(f"Reranking {len(documents)} documents for query: '{query[:50]}...'")

        model_input_pairs = [[query, doc.content] for doc in documents]
        
        def _predict():
            # 使用 try-except 包装 predict 调用，增加鲁棒性
            try:
                return self.cross_encoder.predict(model_input_pairs, show_progress_bar=False)
            except Exception as e:
                self.logger.error(f"Error during Cross-Encoder prediction: {e}", exc_info=True)
                return [] # 返回空列表表示预测失败
            
        scores = await asyncio.to_thread(_predict)

        if len(scores) != len(documents):
            self.logger.error("Reranking failed: number of scores does not match number of documents.")
            return documents[:top_n] # 失败时返回原始文档
        
        for doc, score in zip(documents, scores):
            doc.score = float(score)

        reranked_docs = sorted(documents, key=lambda d: d.score or -1.0, reverse=True)
        
        if reranked_docs:
            self.logger.info(f"Reranking complete. Top score: {reranked_docs[0].score:.4f}")
        
        return reranked_docs[:top_n]

    # ( _apply_rrf 和 fuse_results_with_rrf 方法保持不变 )
    def _apply_rrf(self, all_docs: List[RetrievedDocument]) -> List[RetrievedDocument]:
        if not all_docs:
            return []
        docs_by_source: Dict[str, List[RetrievedDocument]] = {}
        for doc in all_docs:
            source_type = doc.source_type or "unknown_source"
            if source_type not in docs_by_source:
                docs_by_source[source_type] = []
            docs_by_source[source_type].append(doc)

        doc_scores: Dict[str, float] = {}
        doc_objects: Dict[str, RetrievedDocument] = {}

        for source_type, docs_list in docs_by_source.items():
            sorted_docs = sorted(docs_list, key=lambda d: d.score if d.score is not None else -1, reverse=True)
            for rank, doc in enumerate(sorted_docs, 1):
                content_hash = hashlib.md5(doc.content.encode('utf-8')).hexdigest()
                if content_hash not in doc_scores:
                    doc_scores[content_hash] = 0.0
                    doc_objects[content_hash] = doc
                doc_scores[content_hash] += 1.0 / (self.rrf_k + rank)
        fused_results = []
        for content_hash, rrf_score in doc_scores.items():
            doc_obj = doc_objects[content_hash]
            doc_obj.score = rrf_score
            fused_results.append(doc_obj)
        fused_results.sort(key=lambda d: d.score or 0.0, reverse=True)
        return fused_results

    async def fuse_results_with_rrf(
        self,
        all_raw_retrievals: List[RetrievedDocument],
        top_n_final: int = 3
    ) -> List[RetrievedDocument]:
        self.logger.info(f"Fusing {len(all_raw_retrievals)} raw documents using RRF.")
        fused_and_ranked_results = self._apply_rrf(all_raw_retrievals)
        return fused_and_ranked_results[:top_n_final]
```

    |-- kg_retriever.py

``` py
# 文件: zhz_rag/core_rag/kg_retriever.py
import os
import json
import duckdb
from typing import List, Dict, Any, Optional, Iterator, TYPE_CHECKING
import logging
from contextlib import contextmanager
import asyncio
from cachetools import TTLCache # <--- 添加这一行
if TYPE_CHECKING:
    from zhz_rag.llm.local_model_handler import LocalModelHandler

from zhz_rag.config.pydantic_models import ExtractedEntitiesAndRelationIntent
from zhz_rag.utils.common_utils import normalize_text_for_id
from zhz_rag.utils.interaction_logger import log_interaction_data # <--- 确保这行存在
import uuid # <--- 添加导入
from datetime import datetime, timezone # <--- 添加导入

# 日志配置
kg_logger = logging.getLogger(__name__)
if not kg_logger.hasHandlers():
    kg_logger.setLevel(logging.DEBUG)
    ch = logging.StreamHandler()
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(filename)s:%(lineno)d - %(message)s')
    ch.setFormatter(formatter)
    kg_logger.addHandler(ch)
    kg_logger.propagate = False
kg_logger.info("KGRetriever (DuckDB) logger initialized/reconfirmed.")


class KGRetriever:
    # 使用 DuckDB 的环境变量或默认路径
    DUCKDB_KG_FILE_PATH_ENV = os.getenv(
        "DUCKDB_KG_FILE_PATH",
        os.path.join(os.getenv("ZHZ_AGENT_PROJECT_ROOT", "/home/zhz/zhz_agent"), "zhz_rag", "stored_data", "duckdb_knowledge_graph.db")
    )

    def __init__(self, db_file_path: Optional[str] = None, embedder: Optional['LocalModelHandler'] = None):
        self.db_file_path = db_file_path if db_file_path else self.DUCKDB_KG_FILE_PATH_ENV
        self._embedder = embedder
        self._retrieval_cache: TTLCache = TTLCache(maxsize=100, ttl=300) # <--- 修改这一行
        self._retrieval_cache_lock = asyncio.Lock()
        kg_logger.info(f"KGRetriever (DuckDB) initialized. DB file path set to: {self.db_file_path}")

        if not os.path.exists(self.db_file_path):
            kg_logger.warning(f"DuckDB database file not found at {self.db_file_path}. Retrieval operations will likely fail if the DB is not created by the Dagster pipeline first.")
        
        # 健康检查
        try:
            with self._get_duckdb_connection() as conn_test:
                result = conn_test.execute("SELECT 42;").fetchone()
                if result and result[0] == 42:
                    kg_logger.info("DuckDB connection test successful and VSS setup attempted in _get_duckdb_connection.")
                else:
                    kg_logger.warning("DuckDB connection test failed to return expected result.")
        except Exception as e_init_conn_test:
            kg_logger.error(f"Error during initial DuckDB connection test: {e_init_conn_test}", exc_info=True)
    
    @contextmanager
    def _get_duckdb_connection(self) -> Iterator[duckdb.DuckDBPyConnection]:
        """
        建立并返回一个DuckDB连接的上下文管理器。
        """
        conn: Optional[duckdb.DuckDBPyConnection] = None
        kg_logger.debug(f"_get_duckdb_connection: Attempting to connect to DB at {self.db_file_path}")
        try:
            if not os.path.exists(self.db_file_path):
                raise FileNotFoundError(f"DuckDB file '{self.db_file_path}' does not exist when trying to open connection.")

            conn = duckdb.connect(database=self.db_file_path, read_only=False)
            kg_logger.debug(f"DuckDB Connection object created for path: {self.db_file_path} (read_only=False)")
            
            try:
                conn.execute("LOAD vss;")
                kg_logger.debug("DuckDB: VSS extension loaded on connection.")
            except Exception as e_vss_setup:
                kg_logger.warning(f"DuckDB: Failed to setup VSS extension on connect: {e_vss_setup}. This might be okay if already set or not needed for this operation.")
            yield conn
        except Exception as e_outer:
            kg_logger.error(f"Error in _get_duckdb_connection: {e_outer}", exc_info=True)
            raise
        finally:
            kg_logger.debug("_get_duckdb_connection: Exiting context.")
            if conn:
                kg_logger.debug("Closing DuckDB connection.")
                conn.close()

    def _execute_duckdb_sql_query_sync(self, query: str, parameters: Optional[List[Any]] = None) -> List[Dict[str, Any]]:
        """
        执行DuckDB SQL查询并返回结果列表。
        （此版本不再包含通用日志记录逻辑）
        """
        kg_logger.info(f"--- Executing DuckDB SQL --- Query: {query.strip()}")
        if parameters:
            log_params = [
                str(p)[:100] + '...' if isinstance(p, list) and len(str(p)) > 100 else p
                for p in parameters
            ]
            kg_logger.info(f"Params: {log_params}")

        results_list: List[Dict[str, Any]] = []
        try:
            with self._get_duckdb_connection() as conn:
                prepared_statement = conn.execute(query, parameters)
                
                if prepared_statement.description:
                    column_names = [desc[0] for desc in prepared_statement.description]
                    raw_results = prepared_statement.fetchall()
                    
                    for row_tuple in raw_results:
                        results_list.append(dict(zip(column_names, row_tuple)))
                        
                kg_logger.info(f"DuckDB SQL executed. Records count: {len(results_list)}")
                if results_list: 
                    kg_logger.debug(f"First DuckDB record: {str(results_list[0])[:200]}")
                else:
                     kg_logger.debug("DuckDB SQL query returned 0 records.")
        except duckdb.Error as duckdb_err:
             kg_logger.error(f"DuckDB Error during SQL execution: '{query}'. Error: {duckdb_err}", exc_info=True)
        except Exception as e:
            kg_logger.error(f"Unexpected error executing DuckDB SQL query: '{query}'. Error: {e}", exc_info=True)
        return results_list
    
    def _format_duckdb_records_for_retrieval(
        self, 
        records: List[Dict[str, Any]], 
        query_context: str = "",
        source_type_prefix: str = "duckdb_kg"
    ) -> List[Dict[str, Any]]:
        formatted_docs = []
        if not records:
            return formatted_docs

        for record_data in records:
            content_parts = []
            entity_text = record_data.get("text") or record_data.get("target_text") or record_data.get("source_text")
            entity_label = record_data.get("label") or record_data.get("target_label") or record_data.get("source_label")
            relation_type = record_data.get("relation_type")
            
            if "source_text" in record_data and "target_text" in record_data and relation_type:
                content_parts = [
                    f"Source: {record_data['source_text']} ({record_data.get('source_label', 'Entity')})",
                    f"Relation: {relation_type}",
                    f"Target: {record_data['target_text']} ({record_data.get('target_label', 'Entity')})"
                ]
            elif entity_text:
                content_parts.append(f"Entity: {entity_text}")
                if entity_label:
                    content_parts.append(f"Label: {entity_label}")
            else:
                content_parts.append(f"Retrieved KG data: {json.dumps({k:v for k,v in record_data.items() if k != 'embedding'}, ensure_ascii=False, default=str)[:100]}")

            content_str = " | ".join(content_parts)
            
            doc_metadata = {
                "original_user_query_for_kg": query_context,
                "duckdb_retrieved_id_prop": record_data.get("id_prop") or record_data.get("source_id_prop"),
                "duckdb_retrieved_data": {k:v for k,v in record_data.items() if k != 'embedding'}
            }
            if record_data.get("_source_strategy"):
                doc_metadata["_source_strategy"] = record_data.get("_source_strategy")

            score_value = record_data.get("distance")
            if score_value is None:
                score_value = record_data.get("_score")

            if record_data.get("distance") is not None:
                similarity_score = 1.0 / (1.0 + float(score_value)) if score_value is not None else 0.5 
            elif isinstance(score_value, (int, float)):
                similarity_score = float(score_value)
            else:
                similarity_score = 0.5

            doc_data = {
                "source_type": source_type_prefix,
                "content": content_str,
                "score": similarity_score, 
                "metadata": {k: v for k, v in doc_metadata.items() if v is not None}
            }
            formatted_docs.append(doc_data)
        return formatted_docs

    async def retrieve(
        self, 
        user_query: str, 
        extracted_info: Optional[ExtractedEntitiesAndRelationIntent], # <--- 直接接收提取好的信息
        top_k: int = 3
    ) -> List[Dict[str, Any]]:
        kg_logger.info(f"Starting DuckDB KG retrieval for query: '{user_query}', top_k: {top_k}")

        cache_key = f"{user_query}_{top_k}"
        async with self._retrieval_cache_lock:
            cached_result = self._retrieval_cache.get(cache_key)
        
        if cached_result is not None:
            kg_logger.info(f"KG CACHE HIT for key: '{cache_key[:100]}...'")
            return cached_result
        
        kg_logger.info(f"KG CACHE MISS for key: '{cache_key[:100]}...'. Performing retrieval.")

        if not self._embedder:
            kg_logger.error("Embedder not configured for KGRetriever. Vector search will be skipped.")
        
        all_retrieved_records: List[Dict[str, Any]] = []
        processed_entity_ids = set()

        # 1. LLM提取步骤已被移除，直接使用传入的 extracted_info
        if extracted_info:
            entities_log = [e.model_dump() for e in extracted_info.entities]
            relations_log = [r.model_dump() for r in extracted_info.relations]
            kg_logger.info(f"Using pre-extracted KG info: Entities: {entities_log}, Relations: {relations_log}")
        else:
            kg_logger.info("No pre-extracted KG info provided.")

        # 2. 向量搜索
        if self._embedder:
            try:
                kg_logger.info(f"Generating embedding for vector search text: '{user_query}'")
                query_vector_list = await self._embedder.embed_query(user_query)
                if query_vector_list:
                    vector_search_sql = "SELECT id_prop, text, label, list_distance(embedding, ?) AS distance FROM ExtractedEntity ORDER BY distance ASC LIMIT ?;"
                    vector_results = self._execute_duckdb_sql_query_sync(vector_search_sql, [query_vector_list, top_k])
                    if vector_results:
                        for rec in vector_results: rec["_source_strategy"] = "vector_search"
                        all_retrieved_records.extend(vector_results)
                        processed_entity_ids.update(rec.get("id_prop") for rec in vector_results)
                        kg_logger.info(f"Retrieved {len(vector_results)} records via DuckDB vector search.")
                else:
                    kg_logger.warning("Failed to generate query embedding for vector search.")
            except Exception as e_vec_search:
                kg_logger.error(f"Error during DuckDB vector search: {e_vec_search}", exc_info=True)
        
        # 3. 基于LLM提取的实体进行精确查找
        if extracted_info and extracted_info.entities:
            for entity_info in extracted_info.entities:
                entity_text_norm = normalize_text_for_id(entity_info.text)
                entity_label_norm = entity_info.label.upper()
                exact_entity_sql = "SELECT id_prop, text, label FROM ExtractedEntity WHERE text = ? AND label = ? LIMIT 1;"
                entity_lookup_results = self._execute_duckdb_sql_query_sync(exact_entity_sql, [entity_text_norm, entity_label_norm])
                for rec in entity_lookup_results:
                    if rec.get("id_prop") not in processed_entity_ids:
                        rec["_source_strategy"] = "exact_entity_match"
                        all_retrieved_records.append(rec)
                        processed_entity_ids.add(rec.get("id_prop"))

        # 4. 基于LLM提取的结构化关系进行验证和邻居查找
        if extracted_info and extracted_info.relations:
            kg_logger.info(f"Found {len(extracted_info.relations)} structured relations to process.")
            for rel_item in extracted_info.relations:
                try:
                    head_text_norm = normalize_text_for_id(rel_item.head_entity_text)
                    head_label_norm = rel_item.head_entity_label.upper()
                    tail_text_norm = normalize_text_for_id(rel_item.tail_entity_text)
                    tail_label_norm = rel_item.tail_entity_label.upper()
                    relation_type_norm = rel_item.relation_type.upper()

                    kg_logger.info(f"Processing relation: ({head_text_norm}:{head_label_norm})-[{relation_type_norm}]->({tail_text_norm}:{tail_label_norm})")

                    relation_verification_sql = """
                    SELECT r.relation_type, s.id_prop AS source_id_prop, s.text AS source_text, s.label AS source_label, t.id_prop AS target_id_prop, t.text AS target_text, t.label AS target_label
                    FROM KGExtractionRelation r
                    JOIN ExtractedEntity s ON r.source_node_id_prop = s.id_prop
                    JOIN ExtractedEntity t ON r.target_node_id_prop = t.id_prop
                    WHERE s.text = ? AND s.label = ? AND t.text = ? AND t.label = ? AND r.relation_type = ? LIMIT 1;
                    """
                    relation_verification_params = [head_text_norm, head_label_norm, tail_text_norm, tail_label_norm, relation_type_norm]
                    
                    log_entry = {
                        "interaction_id": str(uuid.uuid4()), "timestamp_utc": datetime.now(timezone.utc).isoformat(),
                        "task_type": "kg_executed_query_for_eval", "user_query_for_task": user_query,
                        "generated_query_language": "SQL_DuckDB", "generated_query": relation_verification_sql.strip(),
                        "query_parameters": [str(p) for p in relation_verification_params],
                        "application_version": "kg_retriever_0.2_rel_verify"
                    }
                    try:
                        asyncio.create_task(log_interaction_data(log_entry))
                    except Exception as e_log:
                        kg_logger.error(f"Error queuing precise log for relation verification query: {e_log}", exc_info=True)

                    relation_verification_results = self._execute_duckdb_sql_query_sync(relation_verification_sql, relation_verification_params)
                    
                    if relation_verification_results:
                        kg_logger.info(f"    Successfully verified relation in KG: {relation_type_norm} between '{head_text_norm}' and '{tail_text_norm}'")
                        
                        for verified_rel_record in relation_verification_results:
                            head_entity_from_rel = {"id_prop": verified_rel_record.get("source_id_prop"), "text": verified_rel_record.get("source_text"), "label": verified_rel_record.get("source_label"), "_source_strategy": f"verified_relation_head_{relation_type_norm}"}
                            if head_entity_from_rel.get("id_prop") not in processed_entity_ids:
                                all_retrieved_records.append(head_entity_from_rel)
                                processed_entity_ids.add(head_entity_from_rel.get("id_prop"))
                                kg_logger.info(f"      Added head entity '{head_entity_from_rel.get('text')}' from verified relation to results.")
                            tail_entity_from_rel = {"id_prop": verified_rel_record.get("target_id_prop"), "text": verified_rel_record.get("target_text"), "label": verified_rel_record.get("target_label"), "_source_strategy": f"verified_relation_tail_{relation_type_norm}"}
                            if tail_entity_from_rel.get("id_prop") not in processed_entity_ids:
                                all_retrieved_records.append(tail_entity_from_rel)
                                processed_entity_ids.add(tail_entity_from_rel.get("id_prop"))
                                kg_logger.info(f"      Added tail entity '{tail_entity_from_rel.get('text')}' from verified relation to results.")
                        
                        if head_text_norm and head_label_norm and relation_type_norm:
                            find_other_tails_sql = """
                            SELECT t.id_prop, t.text, t.label, r.relation_type
                            FROM ExtractedEntity h
                            JOIN KGExtractionRelation r ON h.id_prop = r.source_node_id_prop
                            JOIN ExtractedEntity t ON r.target_node_id_prop = t.id_prop
                            WHERE h.text = ? AND h.label = ? AND r.relation_type = ? AND t.text != ?
                            LIMIT ?;
                            """
                            find_other_tails_params = [head_text_norm, head_label_norm, relation_type_norm, tail_text_norm, top_k]
                            other_tails_results = self._execute_duckdb_sql_query_sync(find_other_tails_sql, find_other_tails_params)
                            for rec in other_tails_results:
                                if rec.get("id_prop") not in processed_entity_ids:
                                    rec["_source_strategy"] = f"neighbor_tail_for_{relation_type_norm}"
                                    all_retrieved_records.append(rec)
                                    processed_entity_ids.add(rec.get("id_prop"))
                                    kg_logger.info(f"        Added neighbor tail entity '{rec.get('text')}' to results.")

                        if tail_text_norm and tail_label_norm and relation_type_norm:
                            find_other_heads_sql = """
                            SELECT h.id_prop, h.text, h.label, r.relation_type
                            FROM ExtractedEntity t
                            JOIN KGExtractionRelation r ON t.id_prop = r.target_node_id_prop
                            JOIN ExtractedEntity h ON r.source_node_id_prop = h.id_prop
                            WHERE t.text = ? AND t.label = ? AND r.relation_type = ? AND h.text != ?
                            LIMIT ?;
                            """
                            find_other_heads_params = [tail_text_norm, tail_label_norm, relation_type_norm, head_text_norm, top_k]
                            other_heads_results = self._execute_duckdb_sql_query_sync(find_other_heads_sql, find_other_heads_params)
                            for rec in other_heads_results:
                                if rec.get("id_prop") not in processed_entity_ids:
                                    rec["_source_strategy"] = f"neighbor_head_for_{relation_type_norm}"
                                    all_retrieved_records.append(rec)
                                    processed_entity_ids.add(rec.get("id_prop"))
                                    kg_logger.info(f"        Added neighbor head entity '{rec.get('text')}' to results.")
                    else:
                        kg_logger.info(f"    Relation {relation_type_norm} between '{head_text_norm}' and '{tail_text_norm}' not found in KG via exact match.")

                except Exception as e_rel_proc:
                    kg_logger.error(f"Error processing structured relation item {rel_item.model_dump_json()}: {e_rel_proc}", exc_info=True)

        if not all_retrieved_records:
            kg_logger.info(f"No records retrieved from DuckDB KG for query: '{user_query}' after all strategies.")
            return []

        unique_records = []
        seen_ids = set()
        for record in all_retrieved_records:
            record_id = record.get("id_prop") or record.get("source_id_prop")
            if record_id and record_id in seen_ids:
                continue
            unique_records.append(record)
            if record_id:
                seen_ids.add(record_id)
        
        formatted_docs = self._format_duckdb_records_for_retrieval(unique_records, user_query, "duckdb_kg")
        
        async with self._retrieval_cache_lock:
            self._retrieval_cache[cache_key] = formatted_docs
        kg_logger.info(f"KG CACHED {len(formatted_docs)} results for key: '{cache_key[:100]}...'")

        kg_logger.info(f"KGRetriever (DuckDB) retrieve method finished. Returning {len(formatted_docs)} formatted documents for fusion.")
        return formatted_docs



    def close(self):
        kg_logger.info(f"KGRetriever (DuckDB).close() called. (No persistent DB object to close in this version as connections are per-method).")
        pass

```

        |-- __init__.py

``` py
# zhz_agent/core_rag/retrievers/__init__.py
from .chromadb_retriever import ChromaDBRetriever
from .file_bm25_retriever import FileBM25Retriever
```

        |-- chromadb_retriever.py

``` py
# 文件: zhz_rag/core_rag/retrievers/chromadb_retriever.py

import asyncio
import json
from typing import List, Dict, Any, Optional
import chromadb
import logging
from .embedding_functions import LlamaCppEmbeddingFunction
from cachetools import TTLCache # <--- 添加这一行

# 配置ChromaDBRetriever的日志记录器
logger = logging.getLogger(__name__)
# 注意：在模块级别配置basicConfig可能会影响整个应用的日志行为。
# 通常建议在应用入口处统一配置。
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')

class ChromaDBRetriever:
    def __init__(
        self,
        collection_name: str,
        persist_directory: str,
        embedding_function: LlamaCppEmbeddingFunction
    ):
        """
        初始化ChromaDBRetriever。

        Args:
            collection_name (str): 要查询的ChromaDB集合名称。
            persist_directory (str): ChromaDB数据持久化的目录。
            embedding_function (LlamaCppEmbeddingFunction): 用于生成嵌入的函数实例。
        """
        self.collection_name = collection_name
        self.persist_directory = persist_directory
        self._embedding_function = embedding_function
        self._client: Optional[chromadb.Client] = None
        self._collection: Optional[chromadb.Collection] = None
        self._dimension: Optional[int] = None

    # --- 新增: 初始化TTLCache召回结果缓存和异步锁 ---
        # maxsize=100: 最多缓存100个查询结果
        # ttl=300: 缓存条目存活300秒（5分钟）
        self._retrieval_cache: TTLCache = TTLCache(maxsize=100, ttl=300)
        self._retrieval_cache_lock = asyncio.Lock()

        # 初始化依然是同步的，在服务启动时执行
        self._initialize_retriever()

    def _initialize_retriever(self):
        """
        初始化ChromaDB客户端和集合。
        """
        try:
            logger.info(f"Initializing ChromaDB client from path: {self.persist_directory}")
            self._client = chromadb.PersistentClient(path=self.persist_directory)
            
            logger.info(f"Getting or creating ChromaDB collection: {self.collection_name} using provided async embedding function.")
            self._collection = self._client.get_or_create_collection(
                name=self.collection_name,
                embedding_function=self._embedding_function 
            )

            if self._collection.count() == 0:
                logger.warning(f"ChromaDB collection '{self.collection_name}' is empty!")
            else:
                logger.info(f"ChromaDB collection '{self.collection_name}' loaded. Item count: {self._collection.count()}")
        except Exception as e:
            logger.error(f"Failed to initialize ChromaDB client or collection: {e}", exc_info=True)
            raise

    async def retrieve(
        self, 
        query_text: str, 
        n_results: int = 5, 
        include_fields: Optional[List[str]] = None,
        where_filter: Optional[Dict[str, Any]] = None  # <--- 新增参数
    ) -> List[Dict[str, Any]]:
        if self._collection is None or self._embedding_function is None:
            logger.error("Retriever is not properly initialized.")
            return []

        # --- 更新缓存键以包含过滤器 ---
        cache_key_parts = [query_text, str(n_results)]
        if where_filter:
            # 将过滤器字典转换为稳定的字符串表示
            filter_str = json.dumps(where_filter, sort_keys=True)
            cache_key_parts.append(filter_str)
        cache_key = "_".join(cache_key_parts)
        
        async with self._retrieval_cache_lock:
            cached_result = self._retrieval_cache.get(cache_key)
        
        if cached_result is not None:
            logger.info(f"ChromaDB CACHE HIT for key with filter: '{cache_key[:100]}...'")
            return cached_result
        
        logger.info(f"ChromaDB CACHE MISS for key: '{cache_key[:100]}...'. Performing retrieval.")
        if where_filter:
            logger.info(f"Applying metadata filter: {where_filter}")

        logger.info(f"Retrieving documents for query: '{query_text[:100]}...' with n_results={n_results}")
        
        try:
            query_embedding = await self._embedding_function.embed_query(query_text)
            
            if not query_embedding: 
                logger.error(f"Failed to generate embedding for query: {query_text[:100]}")
                return []
            
            # +++ 新增日志 +++
            logger.info(f"ChromaDBRetriever: Query embedding for '{query_text[:50]}' (first 10 elements): {str(query_embedding[:10])}")
            logger.info(f"ChromaDBRetriever: Length of query embedding: {len(query_embedding)}")
            is_query_emb_all_zeros = all(v == 0.0 for v in query_embedding)
            logger.info(f"ChromaDBRetriever: Is query embedding all zeros: {is_query_emb_all_zeros}")
            # +++ 结束新增日志 +++

            def _blocking_query():
                include_fields_query = include_fields if include_fields is not None else ["metadatas", "documents", "distances"]
                # --- 核心修改：在查询时应用 where_filter ---
                return self._collection.query(
                    query_embeddings=[query_embedding], 
                    n_results=n_results,
                    include=include_fields_query,
                    where=where_filter  # <--- 应用过滤器
                )

            results = await asyncio.to_thread(_blocking_query)
            
            retrieved_docs = []
            if results and results.get("ids") and results.get("ids")[0]:
                ids_list = results["ids"][0]
                documents_list = results.get("documents", [[]])[0]
                metadatas_list = results.get("metadatas", [[]])[0] 
                distances_list = results.get("distances", [[]])[0]

                for i in range(len(ids_list)):
                    chunk_id = ids_list[i]
                    metadata = metadatas_list[i] if metadatas_list and i < len(metadatas_list) else {}
                    distance = distances_list[i] if distances_list and i < len(distances_list) else float('inf')
                    content = documents_list[i] if documents_list and i < len(documents_list) else metadata.get("chunk_text", "[Content not found]")
                    score = (1 - distance / 2.0) if distance != float('inf') and distance <= 2.0 else 0.0 

                    retrieved_docs.append({
                        "id": chunk_id,
                        "content": content,
                        "score": score,
                        "distance": distance, 
                        "metadata": metadata,
                        "source_type": "vector_chromadb"
                    })
                
                logger.info(f"Retrieved {len(retrieved_docs)} documents from ChromaDB (with filter: {where_filter is not None}).")
            else:
                logger.info("No documents retrieved from ChromaDB for the query.")

            async with self._retrieval_cache_lock:
                self._retrieval_cache[cache_key] = retrieved_docs
            logger.info(f"ChromaDB CACHED {len(retrieved_docs)} results for key: '{cache_key[:100]}...'")
            return retrieved_docs

        except Exception as e:
            logger.error(f"Error during ChromaDB retrieval: {e}", exc_info=True)
            return []
        
        
    async def get_texts_by_ids(self, ids: List[str]) -> Dict[str, str]:
        """
        (异步) 根据提供的ID列表从ChromaDB集合中获取文档的文本内容。
        """
        if not self._collection:
            logger.error("ChromaDBRetriever: Collection is not initialized.")
            return {id_val: "[Error: Collection not initialized]" for id_val in ids}
        
        if not ids:
            return {}
            
        logger.info(f"ChromaDBRetriever: Getting texts for {len(ids)} IDs.")
        
        def _blocking_get():
            return self._collection.get(ids=ids, include=["documents"])

        try:
            results = await asyncio.to_thread(_blocking_get)
            retrieved_ids = results.get("ids", [])
            retrieved_docs = results.get("documents", [])
            
            texts_map = {doc_id: doc_text for doc_id, doc_text in zip(retrieved_ids, retrieved_docs)}

            for doc_id in ids:
                if doc_id not in texts_map:
                    texts_map[doc_id] = f"[Content for chunk_id {doc_id} not found in ChromaDB]"
                    logger.warning(f"ChromaDBRetriever: Content for ID '{doc_id}' not found in get() result.")

            logger.info(f"ChromaDBRetriever: Returning texts_map with {len(texts_map)} entries.")
            return texts_map

        except Exception as e:
            logger.error(f"ChromaDBRetriever: Error during get_texts_by_ids: {e}", exc_info=True)
            return {id_val: f"[Error retrieving content for ID {id_val}]" for id_val in ids}

```

        |-- embedding_functions.py

``` py
# 文件: zhz_rag/core_rag/retrievers/embedding_functions.py

import logging
from typing import List, Dict, TYPE_CHECKING, Optional, Sequence
import numpy as np
from chromadb import Documents, Embeddings
import asyncio # 确保 asyncio 已导入
from cachetools import TTLCache

if TYPE_CHECKING:
    from zhz_rag.llm.local_model_handler import LocalModelHandler

logger = logging.getLogger(__name__)

def l2_normalize_embeddings(embeddings: List[List[float]]) -> List[List[float]]:
    """对一批嵌入向量进行L2归一化。"""
    if not embeddings or not isinstance(embeddings, list):
        return []
    
    normalized_embeddings = []
    for emb_list in embeddings:
        if not emb_list or not isinstance(emb_list, list): 
            normalized_embeddings.append([])
            continue
        try:
            emb_array = np.array(emb_list, dtype=np.float32)
            norm = np.linalg.norm(emb_array)
            if norm == 0: 
                normalized_embeddings.append(emb_list) 
            else:
                normalized_embeddings.append((emb_array / norm).tolist())
        except Exception as e_norm:
            logger.error(f"Error during L2 normalization of an embedding: {e_norm}", exc_info=True)
            normalized_embeddings.append(emb_list) 
    return normalized_embeddings



class LlamaCppEmbeddingFunction:
    """
    一个与 LangChain 兼容的 ChromaDB 嵌入函数。
    V3: 恢复为原生异步，以解决同步/异步桥接导致的死锁问题。
    """
    def __init__(self, model_handler: 'LocalModelHandler'):
        if model_handler is None:
            raise ValueError("LocalModelHandler is required.")
        self.model_handler = model_handler
        self._dimension: Optional[int] = None
        self._query_cache: TTLCache = TTLCache(maxsize=200, ttl=3600)
        self._cache_lock = asyncio.Lock()
        
        try:
            self._dimension = self.model_handler.get_embedding_dimension()
            if self._dimension:
                 logger.info(f"LlamaCppEmbeddingFunction initialized. Dimension from handler: {self._dimension}")
        except Exception as e_dim_init:
            logger.warning(f"LlamaCppEmbeddingFunction: Error trying to get dimension during init: {e_dim_init}.")

    # --- 核心修改：恢复为 async def ---
    async def __call__(self, input: Documents) -> Embeddings:
        if not input:
            return []
        logger.info(f"LlamaCppEmbeddingFunction (ASYNC __call__): Generating embeddings for {len(input)} documents.")
        # 直接 await 异步方法
        return await self.model_handler.embed_documents(list(input))

    async def embed_documents(self, texts: List[str]) -> List[List[float]]:
        return await self.__call__(texts)

    async def embed_query(self, text: str) -> List[float]:
        async with self._cache_lock:
            cached_result = self._query_cache.get(text)
        
        if cached_result is not None:
            logger.info(f"Query Vector CACHE HIT for query: '{text[:50]}...'")
            return cached_result
        
        logger.info(f"Query Vector CACHE MISS for query: '{text[:50]}...'. Generating new embedding.")
        embedding_vector = await self.model_handler.embed_query(text)
        
        if embedding_vector:
            async with self._cache_lock:
                self._query_cache[text] = embedding_vector
        else:
            embedding_vector = [0.0] * (self._dimension or 1024)

        return embedding_vector
```

        |-- file_bm25_retriever.py

``` py
# /home/zhz/zhz_agent/zhz_rag/core_rag/retrievers/file_bm25_retriever.py

from typing import List, Dict, Any, Optional
import jieba
import bm25s
import pickle
import os
import logging
import sys
import numpy as np
from cachetools import TTLCache
import asyncio

# --- 日志配置 (标准化) ---
bm25_logger = logging.getLogger("BM25Retriever")
bm25_logger.setLevel(os.getenv("BM25_LOG_LEVEL", "INFO").upper())
bm25_logger.propagate = False

if not bm25_logger.hasHandlers():
    _handler = logging.StreamHandler(sys.stdout)
    _formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    _handler.setFormatter(_formatter)
    bm25_logger.addHandler(_handler)


class FileBM25Retriever:
    """
    一个从文件中加载BM25索引并执行关键词检索的检索器。
    它依赖于由Dagster流水线生成的索引文件和doc_ids文件。
    V2: 优化为异步接口并增加线程安全缓存。
    """
    def __init__(self, index_directory: str):
        """
        初始化 FileBM25Retriever。

        Args:
            index_directory (str): 存储 BM25 索引文件和 doc_ids.pkl 的目录路径。
        """
        self.index_directory_path = index_directory
        self._bm25_model: Optional[bm25s.BM25] = None
        self._doc_ids: Optional[List[str]] = None
        # 使用TTLCache缓存检索结果，缓存100个查询，每个存活300秒（5分钟）
        self._retrieval_cache: TTLCache = TTLCache(maxsize=100, ttl=300)
        self._retrieval_cache_lock = asyncio.Lock() # 异步锁保护缓存读写

        self._initialize_retriever()

    def _initialize_retriever(self):
        """
        同步加载 BM25 模型和文档ID。这个方法在服务启动时调用。
        """
        if not os.path.isdir(self.index_directory_path):
            bm25_logger.error(f"BM25 index directory not found at: {self.index_directory_path}")
            raise FileNotFoundError(f"BM25 index directory not found: {self.index_directory_path}")

        try:
            bm25_logger.info(f"Loading BM25 model from directory: {self.index_directory_path}")
            # 从磁盘加载模型，不加载语料库本身以节省内存
            self._bm25_model = bm25s.BM25.load(
                self.index_directory_path,
                load_corpus=False,
            )
            
            if self._bm25_model is None:
                bm25_logger.error("Failed to load BM25 model (bm25s.BM25.load returned None).")
                raise ValueError("Failed to load BM25 model.")
            
            bm25_logger.info("BM25 model loaded successfully.")

            doc_ids_path = os.path.join(self.index_directory_path, "doc_ids.pkl")
            if not os.path.exists(doc_ids_path):
                bm25_logger.error(f"doc_ids.pkl not found in {self.index_directory_path}")
                raise FileNotFoundError(f"doc_ids.pkl not found in {doc_ids_path}")
            
            with open(doc_ids_path, 'rb') as f_in:
                self._doc_ids = pickle.load(f_in)
            
            if self._doc_ids is None:
                bm25_logger.warning(f"doc_ids.pkl loaded, but it was empty or invalid.")
                self._doc_ids = []
            
            bm25_logger.info(f"Document IDs for BM25 loaded successfully. Total indexed documents: {len(self._doc_ids)}")

        except Exception as e:
            bm25_logger.error(f"Failed to load BM25 index or document IDs: {e}", exc_info=True)
            raise

    async def retrieve(self, query_text: str, n_results: int = 5) -> List[Dict[str, Any]]:
        """
        根据查询文本使用BM25检索相关的文档块ID和分数。
        这是一个异步包装的同步操作，并带有缓存。
        """
        if self._bm25_model is None or self._doc_ids is None:
            bm25_logger.error("BM25 Retriever is not properly initialized. Cannot retrieve.")
            return []

        cache_key = f"{query_text}_{n_results}"
        async with self._retrieval_cache_lock:
            cached_result = self._retrieval_cache.get(cache_key)
        
        if cached_result is not None:
            bm25_logger.info(f"BM25 CACHE HIT for key: '{cache_key[:100]}...'")
            return cached_result

        bm25_logger.info(f"BM25 CACHE MISS for key: '{cache_key[:100]}...'. Performing retrieval.")
        
        if not self._doc_ids:
            bm25_logger.warning("BM25 index is empty, no results to retrieve.")
            return []

        def _blocking_retrieve():
            try:
                query_tokenized = list(jieba.cut_for_search(query_text))
                bm25_logger.debug(f"Tokenized query for BM25: {query_tokenized}")

                all_scores = self._bm25_model.get_scores(query_tokenized)
                
                # 确保请求的结果数不超过索引中的文档总数
                actual_n_results = min(n_results, len(self._doc_ids))
                if actual_n_results <= 0:
                    return []
                
                # 获取分数最高的n个结果的索引
                top_n_indices = np.argsort(all_scores)[-actual_n_results:][::-1]

                retrieved_docs = []
                for index in top_n_indices:
                    # 再次检查索引有效性
                    if 0 <= index < len(self._doc_ids):
                        doc_id = self._doc_ids[index]
                        score = float(all_scores[index])
                        # 仅返回ID和分数，内容将在后续步骤中从docstore获取
                        retrieved_docs.append({
                            "id": doc_id,
                            "score": score,
                            "source_type": "keyword_bm25s"
                        })
                    else:
                        bm25_logger.warning(f"BM25 retrieval: Index {index} is out of bounds for doc_ids list (len: {len(self._doc_ids)}). Skipping.")
                
                return retrieved_docs

            except Exception as e_inner:
                bm25_logger.error(f"Error during blocking BM25 retrieval: {e_inner}", exc_info=True)
                return []

        retrieved_results = await asyncio.to_thread(_blocking_retrieve)
        
        async with self._retrieval_cache_lock:
            self._retrieval_cache[cache_key] = retrieved_results
        
        bm25_logger.info(f"Retrieved and cached {len(retrieved_results)} documents using BM25 for query: '{query_text[:50]}...'")
        return retrieved_results
```

    |-- rag_eval_data/
    |-- __init__.py

``` py
# /home/zhz/zhz_agent/zhz_rag/llm/__init__.py

from .llm_interface import call_llm_via_openai_api_local_only # 导入新函数

call_sglang_llm = call_llm_via_openai_api_local_only # 别名指向新函数

from .llm_interface import (
    generate_answer_from_context,
    # generate_cypher_query, # 已停用
    generate_expanded_queries,
    generate_intent_classification, # 这个现在直接用 litellm 调用 Gemini
    generate_clarification_question,
    generate_clarification_options,  # <--- 问题在这里
    NO_ANSWER_PHRASE_ANSWER_CLEAN,
    # NO_ANSWER_PHRASE_KG_CLEAN # 已停用
)
```

    |-- custom_crewai_llms.py

``` py
#/home/zhz/zhz_agent/custom_llm.py
import os
import json
import httpx
import asyncio
import traceback
from typing import List, Dict, Any, Optional, Union, Sequence, Type 
# --- CrewAI & LiteLLM Imports ---
from crewai.tools import BaseTool
from crewai.llms.base_llm import BaseLLM as CrewAIBaseLLM
import litellm

# --- [修改] Local Imports -> 改为绝对导入 ---
from zhz_rag.llm.llm_interface import call_sglang_llm # For SGLang LLM
from dotenv import load_dotenv

load_dotenv()

# --- SGLang Config ---
SGLANG_API_URL_FOR_LLM = os.getenv("SGLANG_API_URL", "http://localhost:30000/generate")

# --- CustomGeminiLLM (from ceshi/run_agent.py with fixes) ---
class CustomGeminiLLM(CrewAIBaseLLM):
    model_name: str
    api_key: str
    max_tokens: Optional[int] = 2048
    tool_config: Optional[Dict[str, Any]] = None
    stop: Optional[List[str]] = None
    _gemini_tools_cache: Optional[List[Dict[str, Any]]] = None

    def __init__(self, model: str, api_key: str, temperature: float = 0.1, max_tokens: Optional[int] = 2048, tool_config: Optional[Dict[str, Any]] = None, stop: Optional[List[str]] = None, agent_tools: Optional[List[BaseTool]] = None, **kwargs):
        super().__init__(model=model, temperature=temperature)
        self.model_name = model
        self.api_key = api_key
        self.max_tokens = max_tokens
        self.tool_config = tool_config or {"function_calling_config": {"mode": "AUTO"}}
        self.stop = stop
        if agent_tools:
            self._gemini_tools_cache = self._convert_crewai_tools_to_gemini_format(agent_tools)
            print(f"CustomGeminiLLM __init__: Cached {len(self._gemini_tools_cache)} tools.")
        else:
            print("CustomGeminiLLM __init__: No agent_tools provided for caching.")

    def _remove_unwanted_fields(self, schema: Dict[str, Any]) -> Dict[str, Any]:
        if not isinstance(schema, dict):
            return schema

        schema.pop('title', None)

        if "properties" in schema:
            if "type" not in schema:
                schema["type"] = "object"
            for prop_name, prop_def in list(schema["properties"].items()):
                if isinstance(prop_def, dict):
                    prop_def.pop('default', None)
                    prop_def.pop('title', None)
                    self._remove_unwanted_fields(prop_def)
        elif schema.get("type") == "object" and "properties" not in schema:
            schema["properties"] = {}

        keys_to_delete = [k for k, v in schema.items() if k == 'default']
        for k in keys_to_delete:
            del schema[k]

        for k, v in schema.items():
            if isinstance(v, dict):
                self._remove_unwanted_fields(v)
            elif isinstance(v, list):
                for i, item in enumerate(v):
                    if isinstance(item, dict):
                        v[i] = self._remove_unwanted_fields(item)
        return schema

    def _convert_crewai_tools_to_gemini_format(self, tools: Optional[List[BaseTool]]) -> Optional[List[Dict[str, Any]]]:
        if not tools:
            return None
        gemini_tool_declarations = []
        for tool_instance in tools:
            tool_name = tool_instance.name
            tool_description = tool_instance.description
            if not hasattr(tool_instance, 'args_schema') or not tool_instance.args_schema:
                parameters_schema = {"type": "object", "properties": {}}
            else:
                try:
                    if hasattr(tool_instance.args_schema, 'model_json_schema'):
                        pydantic_schema = tool_instance.args_schema.model_json_schema()
                    else:
                        pydantic_schema = tool_instance.args_schema.schema()
                    cleaned_schema = self._remove_unwanted_fields(pydantic_schema.copy())
                    parameters_schema = cleaned_schema
                except Exception as e:
                    print(f"Error processing schema for tool {tool_name}: {e}")
                    parameters_schema = {"type": "object", "properties": {}}
            gemini_tool_declarations.append({
                "name": tool_name,
                "description": tool_description,
                "parameters": parameters_schema
            })
        final_tools_for_litellm = []
        for declaration in gemini_tool_declarations:
            final_tools_for_litellm.append({
                "type": "function",
                "function": declaration
            })
        return final_tools_for_litellm

    def call(self, messages: Union[str, List[Dict[str, str]]], tools: Optional[List[dict]] = None, callbacks: Optional[List[Any]] = None, **kwargs: Any) -> Union[str, Any]:
        print(f"CustomGeminiLLM CALL method invoked.")
        print(f"  CALL - Tools received by CustomLLM.call: {'Yes' if tools else 'No'}")
        print(f"  CALL - Callbacks received by CustomLLM.call: {'Yes' if callbacks else 'No'}")

        if isinstance(messages, str):
            processed_messages = [{"role": "user", "content": messages}]
        else:
            processed_messages = messages

        litellm_params = {
            "model": self.model_name,
            "messages": processed_messages,
            "api_key": self.api_key,
            "temperature": self.temperature,
            "max_tokens": self.max_tokens,
            "stop": self.stop
        }

        # --- Proxy Addition ---
        proxy_url = os.getenv("LITELLM_PROXY_URL")
        if proxy_url:
            litellm_params["proxy"] = {
                "http": proxy_url,
                "https": proxy_url,
            }
            print(f"CustomGeminiLLM.call - Using proxy: {proxy_url}")
        else:
            print("CustomGeminiLLM.call - No proxy configured (LITELLM_PROXY_URL not set).")

        # --- Tool Handling (tools: null fix) ---
        final_tools_for_litellm = None
        received_tools_to_process = tools
        if not received_tools_to_process and self._gemini_tools_cache:
            print("  CALL - INFO: Tools argument was None, using cached tools.")
            received_tools_to_process = self._gemini_tools_cache

        if received_tools_to_process:
            cleaned_tools_for_litellm = []
            for tool_dict in received_tools_to_process:
                current_tool_def = tool_dict.copy()
                if current_tool_def.get("type") == "function" and "function" in current_tool_def:
                    func_def = current_tool_def["function"].copy()
                    if "parameters" in func_def:
                        func_def["parameters"] = self._remove_unwanted_fields(func_def["parameters"].copy())
                    current_tool_def["function"] = func_def
                    cleaned_tools_for_litellm.append(current_tool_def)
                else:
                    cleaned_tools_for_litellm.append(tool_dict)
            final_tools_for_litellm = cleaned_tools_for_litellm

        if final_tools_for_litellm:
            litellm_params["tools"] = final_tools_for_litellm
            fc_config = self.tool_config.get("function_calling_config", {})
            mode = fc_config.get("mode", "AUTO").upper()
            allowed_names = fc_config.get("allowed_function_names")

            if mode == "ANY" and allowed_names:
                litellm_params["tool_choice"] = {
                    "type": "function",
                    "function": {"name": allowed_names[0]}
                }
            elif mode in ["AUTO", "ANY", "NONE"]:
                litellm_params["tool_choice"] = mode.lower()
            else:
                litellm_params["tool_choice"] = "auto"
            print(f"CustomGeminiLLM DEBUG: Setting tool_choice to: {litellm_params['tool_choice']}")

        if callbacks:
            litellm_params["callbacks"] = callbacks

        try:
            print(f"CustomGeminiLLM.call - LiteLLM PARAMS (Preview): model={litellm_params['model']}, msgs_count={len(litellm_params['messages'])}, tools={'Yes' if 'tools' in litellm_params else 'No'}, tool_choice={litellm_params.get('tool_choice')}, proxy={'Yes' if 'proxy' in litellm_params else 'No'}")
            response = litellm.completion(**litellm_params)
        except Exception as e:
            print(f"CRITICAL ERROR: LiteLLM completion call failed: {e}")
            if callbacks:
                for handler in callbacks:
                    if hasattr(handler, 'on_llm_error'):
                        try:
                            handler.on_llm_error(error=e, llm=self, **kwargs)
                        except Exception as cb_err:
                            print(f"Error in callback on_llm_error: {cb_err}")
            raise

        llm_message_response = response.choices[0].message
        if hasattr(llm_message_response, 'tool_calls') and llm_message_response.tool_calls:
            print(f"CustomGeminiLLM.call - Detected tool_calls: {llm_message_response.tool_calls}")
            # --- ReAct Format Workaround (AttributeError fix) ---
            tool_call = llm_message_response.tool_calls[0]
            action = tool_call.function.name
            action_input = tool_call.function.arguments
            react_string = f"Action: {action}\nAction Input: {action_input}"
            print(f"CustomGeminiLLM.call - Returning ReAct string: {react_string}")
            return react_string
        else:
            print(f"CustomGeminiLLM.call - Returning text content.")
            return llm_message_response.content or ""

    def get_token_counter_instance(self):
        class GeminiTokenCounter:
            def __init__(self, model_name):
                self.model_name = model_name

            def count_tokens(self, text: Union[str, List[Dict[str,str]]]) -> int:
                try:
                    if isinstance(text, list):
                        return litellm.token_counter(model=self.model_name, messages=text)
                    return litellm.token_counter(model=self.model_name, text=str(text))
                except Exception as e:
                    print(f"Warning: Token counting failed ({e}), falling back to rough estimate.")
                    if isinstance(text, list):
                        return sum(len(str(m.get("content","")).split()) for m in text)
                    return len(str(text).split())
        return GeminiTokenCounter(model_name=self.model_name)


# --- CustomSGLangLLM (from hybrid_rag/custom_llm.py) ---
class CustomSGLangLLM(CrewAIBaseLLM):
    endpoint_url: str = SGLANG_API_URL_FOR_LLM
    model_name: str = "qwen2-3b-instruct"
    temperature: float = 0.1
    max_new_tokens_val: int = 1024

    def __init__(self, endpoint: Optional[str] = None, model: Optional[str] = None, temperature: Optional[float] = None, max_new_tokens: Optional[int] = None, **kwargs: Any):
        super().__init__(**kwargs)
        if endpoint: self.endpoint_url = endpoint
        if model: self.model_name = model
        if temperature is not None: self.temperature = temperature
        if max_new_tokens is not None: self.max_new_tokens_val = max_new_tokens
        print(f"CustomSGLangLLM initialized. Endpoint: {self.endpoint_url}, Model: {self.model_name}, Temp: {self.temperature}, MaxTokens: {self.max_new_tokens_val}")

    def _prepare_sglang_prompt(self, messages: Sequence[Dict[str, str]]) -> str:
        prompt_str = ""
        for message in messages:
            role = message.get("role")
            content = message.get("content")
            if role and content:
                prompt_str += f"<|im_start|>{role}\n{content}<|im_end|>\n"
        prompt_str += "<|im_start|>assistant\n"
        return prompt_str

    def call(self, messages: Sequence[Dict[str, str]], **kwargs: Any) -> str:
        print(f"CustomSGLangLLM.call received messages: {messages}")
        sglang_prompt = self._prepare_sglang_prompt(messages)
        print(f"CustomSGLangLLM.call prepared sglang_prompt (first 200 chars): {sglang_prompt[:200]}...")
        stop_sequences_for_sglang = kwargs.get("stop", ["<|im_end|>", "<|endoftext|>"])

        try:
            try:
                loop = asyncio.get_running_loop()
            except RuntimeError:
                loop = None

            async def async_runner():
                return await call_sglang_llm(
                    prompt=sglang_prompt,
                    temperature=self.temperature,
                    max_new_tokens=self.max_new_tokens_val,
                    stop_sequences=stop_sequences_for_sglang
                )

            if loop and loop.is_running():
                import concurrent.futures
                with concurrent.futures.ThreadPoolExecutor(max_workers=1) as executor:
                    future = executor.submit(asyncio.run, async_runner())
                    response_text = future.result(timeout=120)
            else:
                response_text = asyncio.run(async_runner())

        except Exception as e:
            print(f"CustomSGLangLLM.call: Error during SGLang call: {type(e).__name__} - {e}")
            traceback.print_exc()
            return f"LLM_CALL_ERROR: 调用SGLang服务失败 - {str(e)}"

        if response_text is None:
            print("CustomSGLangLLM.call: SGLang returned None.")
            return "LLM_CALL_ERROR: SGLang服务未返回任何文本。"

        print(f"CustomSGLangLLM.call: SGLang returned text (first 200 chars): {response_text[:200]}...")
        return response_text

    def get_token_ids(self, text: str) -> List[int]:
        print("CustomSGLangLLM.get_token_ids: Not implemented, returning empty list.")
        return []

    @property
    def support_function_calling(self) -> bool:
        return False

    @property
    def support_stop_words(self) -> bool:
        return True

    @property
    def available_models(self) -> List[str]:
        return [self.model_name]

    @property
    def context_window(self) -> int:
        return 32768

    @property
    def identifying_params(self) -> Dict[str, Any]:
        return {
            "model": self.model_name,
            "endpoint_url": self.endpoint_url,
            "temperature": self.temperature,
            "max_new_tokens": self.max_new_tokens_val,
        }
```

    |-- embedding_process_worker.py

``` py
# 文件: zhz_rag/llm/embedding_process_worker.py

import os
import logging
from typing import List, Dict, Any, Optional
from llama_cpp import Llama
import numpy as np

# --- 全局变量，用于在子进程中缓存模型实例 ---
# 注意: 每个进程池中的工作进程会有自己的这个变量副本
_process_local_model_cache: Dict[str, Llama] = {}
_process_local_model_dimension_cache: Dict[str, int] = {}

# --- 日志配置 (与 LocalModelHandler 类似，但确保独立) ---
worker_logger = logging.getLogger("EmbeddingProcessWorker")
# 避免重复添加处理器，如果此模块被多次导入或以某种方式重新加载
if not worker_logger.hasHandlers():
    worker_logger.setLevel(logging.INFO) # 或者 DEBUG
    # 注意：在多进程环境中，日志输出到控制台可能交错。
    # 对于生产环境，可能需要更复杂的日志策略（如QueueHandler）。
    # 但对于调试，StreamHandler 也可以。
    stream_handler = logging.StreamHandler()
    formatter = logging.Formatter('%(asctime)s - %(name)s - PID:%(process)d - %(levelname)s - %(message)s')
    stream_handler.setFormatter(formatter)
    worker_logger.addHandler(stream_handler)
    worker_logger.propagate = False


def l2_normalize_embeddings_worker(embeddings: List[List[float]]) -> List[List[float]]:
    if not embeddings or not isinstance(embeddings, list):
        return []
    normalized_embeddings = []
    for emb_list in embeddings:
        if not emb_list or not isinstance(emb_list, list) or not all(isinstance(x, (float, int)) for x in emb_list):
            worker_logger.warning(f"L2_NORM_WORKER: Skipping invalid or empty inner list: {emb_list}")
            normalized_embeddings.append([])
            continue
        try:
            emb_array = np.array(emb_list, dtype=np.float32)
            norm = np.linalg.norm(emb_array)
            if norm == 0:
                normalized_embeddings.append(emb_list)
            else:
                normalized_embeddings.append((emb_array / norm).tolist())
        except Exception as e_norm:
            worker_logger.error(f"Error during L2 normalization in EmbeddingProcessWorker: {e_norm}", exc_info=True)
            normalized_embeddings.append(emb_list)
    return normalized_embeddings


def _get_embedding_model_instance_in_worker(
    model_path: str,
    n_ctx: int,
    n_gpu_layers: int,
    pooling_type: int
) -> Optional[Llama]:
    """
    在当前工作进程中获取或创建并缓存 Llama 嵌入模型实例。
    使用 model_path 作为缓存键。
    """
    global _process_local_model_cache
    global _process_local_model_dimension_cache

    if model_path in _process_local_model_cache:
        worker_logger.info(f"WORKER: Reusing cached embedding model for path: {model_path}")
        return _process_local_model_cache[model_path]

    worker_logger.info(f"WORKER: Attempting to load embedding model in process: {model_path}")
    try:
        model = Llama(
            model_path=model_path,
            n_ctx=n_ctx,
            n_gpu_layers=n_gpu_layers,
            embedding=True,
            pooling_type=pooling_type,
            verbose=False # 在工作进程中减少冗余日志
        )
        dimension = model.n_embd()
        if not dimension or dimension <= 0:
            worker_logger.error(f"WORKER: Loaded model from {model_path} but got invalid dimension {dimension}.")
            return None
        
        _process_local_model_cache[model_path] = model
        _process_local_model_dimension_cache[model_path] = dimension
        worker_logger.info(f"WORKER: Embedding model loaded and cached for {model_path}. Dimension: {dimension}, Pooling: {pooling_type}")
        return model
    except Exception as e:
        worker_logger.error(f"WORKER: Failed to load embedding model in process for path {model_path}: {e}", exc_info=True)
        return None


def embed_texts_in_subprocess(
    texts: List[str],
    embedding_model_path: str,
    n_ctx_embed: int,
    n_gpu_layers_embed: int,
    pooling_type_embed: int
) -> List[List[float]]:
    """
    在子进程中执行批量文本嵌入。
    """
    worker_logger.info(f"WORKER: embed_texts_in_subprocess received {len(texts)} texts.")
    model = _get_embedding_model_instance_in_worker(
        embedding_model_path, n_ctx_embed, n_gpu_layers_embed, pooling_type_embed
    )
    if not model:
        worker_logger.error("WORKER: Failed to get model instance in subprocess. Returning empty embeddings.")
        return [[] for _ in texts]
    
    dimension = _process_local_model_dimension_cache.get(embedding_model_path)
    if not dimension: # 应该不会发生，因为 _get_embedding_model_instance_in_worker 会设置它
        worker_logger.error("WORKER: Model dimension not found in cache after model load. Critical error.")
        return [[] for _ in texts]

    default_zero_vector = [0.0] * dimension
    
    # 与 LocalModelHandler._blocking_embed_documents_internal 类似的处理逻辑
    valid_texts_with_indices: List[tuple[int, str]] = []
    for i, text in enumerate(texts):
        if text and text.strip():
            valid_texts_with_indices.append((i, text))
        else:
            worker_logger.warning(f"WORKER: Input text at original index {i} is empty or invalid. Will use zero vector. Text: '{text}'")

    if not valid_texts_with_indices:
        return [list(default_zero_vector) for _ in texts]

    valid_texts_to_embed = [text for _, text in valid_texts_with_indices]
    raw_embeddings_for_valid_texts: List[List[float]] = []

    try:
        response = model.create_embedding(input=valid_texts_to_embed)
        # ... (此处省略与 LocalModelHandler._blocking_embed_documents_internal 中类似的详细的响应解析和错误处理逻辑)
        # 为了简洁，我们先做一个简化版的解析，假设一切顺利
        # 在实际应用中，需要复制 LocalModelHandler 中对 response 的完整健壮性检查

        if response and "data" in response and isinstance(response["data"], list):
            embeddings_data = response["data"]
            if len(embeddings_data) == len(valid_texts_to_embed):
                for item_idx, item in enumerate(embeddings_data):
                    if isinstance(item, dict) and "embedding" in item and \
                       isinstance(item["embedding"], list) and len(item["embedding"]) == dimension:
                        raw_embeddings_for_valid_texts.append([float(x) for x in item["embedding"]])
                    else:
                        worker_logger.warning(f"WORKER: Valid text at valid_idx {item_idx} got invalid embedding. Using zero vector.")
                        raw_embeddings_for_valid_texts.append(list(default_zero_vector))
            else:
                worker_logger.error(f"WORKER: Mismatch in num embeddings received. Using zero vectors.")
                raw_embeddings_for_valid_texts = [list(default_zero_vector) for _ in valid_texts_to_embed]
        else:
            worker_logger.error(f"WORKER: Invalid response from create_embedding. Using zero vectors.")
            raw_embeddings_for_valid_texts = [list(default_zero_vector) for _ in valid_texts_to_embed]
            
    except Exception as e_batch_embed:
        worker_logger.error(f"WORKER: Error during batch embedding: {e_batch_embed}", exc_info=True)
        raw_embeddings_for_valid_texts = [list(default_zero_vector) for _ in valid_texts_to_embed]

    final_embeddings_ordered: List[List[float]] = [list(default_zero_vector) for _ in texts]
    valid_embedding_idx = 0
    for original_idx, _ in valid_texts_with_indices:
        if valid_embedding_idx < len(raw_embeddings_for_valid_texts):
            final_embeddings_ordered[original_idx] = raw_embeddings_for_valid_texts[valid_embedding_idx]
            valid_embedding_idx += 1
        else:
            final_embeddings_ordered[original_idx] = list(default_zero_vector)
            
    normalized_embeddings = l2_normalize_embeddings_worker(final_embeddings_ordered)
    worker_logger.info(f"WORKER: Successfully processed and normalized {len(normalized_embeddings)} document embeddings in subprocess.")
    return normalized_embeddings


def embed_query_in_subprocess(
    text: str,
    embedding_model_path: str,
    n_ctx_embed: int,
    n_gpu_layers_embed: int,
    pooling_type_embed: int
) -> List[float]:
    """
    在子进程中执行单个查询文本嵌入。
    """
    worker_logger.info(f"WORKER: embed_query_in_subprocess received query (first 100): '{text[:100]}'")
    model = _get_embedding_model_instance_in_worker(
        embedding_model_path, n_ctx_embed, n_gpu_layers_embed, pooling_type_embed
    )
    if not model:
        worker_logger.error("WORKER: Failed to get model instance for query. Returning empty embedding.")
        return []
        
    dimension = _process_local_model_dimension_cache.get(embedding_model_path)
    if not dimension:
        worker_logger.error("WORKER: Model dimension not found in cache for query. Critical error.")
        return []
    
    default_zero_vector = [0.0] * dimension

    if not text or not text.strip():
        worker_logger.warning("WORKER: Received empty or invalid text for query embedding. Returning zero vector.")
        return list(default_zero_vector)

    try:
        # 使用 create_embedding 来保持与批量接口的一致性，即使是单个查询
        # 因为我们观察到 Llama.embed() 可能不稳定
        response = model.create_embedding(input=[text])
        if response and "data" in response and isinstance(response["data"], list) and len(response["data"]) == 1:
            item = response["data"][0]
            if isinstance(item, dict) and "embedding" in item and \
               isinstance(item["embedding"], list) and len(item["embedding"]) == dimension:
                embedding_vector = [float(x) for x in item["embedding"]]
                normalized_list_of_list = l2_normalize_embeddings_worker([embedding_vector])
                final_embedding = normalized_list_of_list[0] if normalized_list_of_list and normalized_list_of_list[0] else list(default_zero_vector)
                worker_logger.info(f"WORKER: Successfully processed query embedding in subprocess. Dimension: {len(final_embedding)}")
                return final_embedding
            else:
                worker_logger.warning(f"WORKER: Query embedding response invalid format/dim. Using zero vector.")
                return list(default_zero_vector)
        else:
            worker_logger.error(f"WORKER: Invalid or empty response from create_embedding for query. Using zero vector.")
            return list(default_zero_vector)
    except Exception as e_query_embed:
        worker_logger.error(f"WORKER: Error during query embedding: {e_query_embed}", exc_info=True)
        return list(default_zero_vector)
```

    |-- llm_interface.py

``` py
# zhz_agent/llm.py (renamed to llm_interface.py as per typical module naming)

import pandas as pd
from cachetools import TTLCache
import os
import httpx  # 用于异步HTTP请求
import json  # 用于处理JSON数据
import asyncio  # 用于 asyncio.to_thread
from typing import List, Dict, Any, Optional, Union, Callable 
from dotenv import load_dotenv
import traceback  # Ensure traceback is imported
from pydantic import ValidationError
from zhz_rag.utils.interaction_logger import log_interaction_data # 导入修复后的健壮日志函数
from zhz_rag.config.constants import NEW_KG_SCHEMA_DESCRIPTION # <--- 确保导入这个常量

from zhz_rag.config.pydantic_models import ExtractedEntitiesAndRelationIntent, RagQueryPlan
# 提示词导入
from llama_cpp import Llama, LlamaGrammar 
from zhz_rag.llm.rag_prompts import (
    get_answer_generation_messages,
    get_clarification_question_messages,
    get_query_expansion_messages,
    get_suggestion_generation_messages,
    get_fusion_messages,
    get_document_summary_messages,
    get_task_extraction_messages,  # <--- 新增导入
    V2_PLANNING_PROMPT_TEMPLATE,
    V2_PLANNING_GBNF_SCHEMA      # <--- 新增导入
)
import logging
import re
import uuid  # 用于生成 interaction_id
from datetime import datetime, timezone  # 用于生成时间戳
import litellm # <--- 确保这个导入存在

# --- 全局 GBNF 模型实例管理 (性能优化) ---
_llm_gbnf_instance: Optional[Llama] = None
_llm_gbnf_instance_lock = asyncio.Lock()

def _get_gbnf_llm_instance() -> Llama:
    """
    获取一个单例的、线程安全的 Llama GBNF 模型实例。
    在第一次调用时初始化。
    """
    global _llm_gbnf_instance
    if _llm_gbnf_instance is None:
        model_path_from_env = os.getenv("LOCAL_LLM_GGUF_MODEL_PATH")
        if not model_path_from_env or not os.path.exists(model_path_from_env):
            llm_py_logger.critical(f"关键错误: GBNF 调用的 LLM 模型路径未设置或无效: {model_path_from_env}")
            raise ValueError("LOCAL_LLM_GGUF_MODEL_PATH 环境变量未配置或路径无效。")
        
        llm_py_logger.info(f"--- 正在加载 GBNF LLM 模型，请稍候... Path: {model_path_from_env} ---")
        _llm_gbnf_instance = Llama(
            model_path=model_path_from_env,
            n_gpu_layers=int(os.getenv("LLM_N_GPU_LAYERS", 0)),
            n_ctx=int(os.getenv("LLM_N_CTX", 4096)),
            verbose=False
        )
        llm_py_logger.info("--- GBNF LLM 模型加载成功 ---")
    return _llm_gbnf_instance
# --- 全局 GBNF 模型实例管理结束 ---

load_dotenv()  # 确保加载.env文件

_LLM_DIR = os.path.dirname(os.path.abspath(__file__))
RAG_INTERACTION_LOGS_DIR = os.path.join(_LLM_DIR, '..', '..', 'stored_data', 'rag_interaction_logs')


if not os.path.exists(RAG_INTERACTION_LOGS_DIR):
    try:
        os.makedirs(RAG_INTERACTION_LOGS_DIR)
    except Exception:
        pass

def get_llm_log_filepath() -> str:
    """获取当前LLM交互日志文件的完整路径，按天分割。"""
    today_str = datetime.now(timezone.utc).strftime("%Y%m%d")
    return os.path.join(RAG_INTERACTION_LOGS_DIR, f"rag_interactions_{today_str}.jsonl")

async def log_llm_interaction_to_jsonl(interaction_data: Dict[str, Any]):
    """
    将单条LLM交互数据异步追加到JSONL文件中。
    (This function might be part of what log_interaction_data uses, or an alternative logger. Keeping for completeness from original llm.py)
    """
    filepath = get_llm_log_filepath()
    try:
        def _write_sync():
            with open(filepath, 'a', encoding='utf-8') as f:
                f.write(json.dumps(interaction_data, ensure_ascii=False) + "\n")
        await asyncio.to_thread(_write_sync)
        llm_py_logger.debug(f"Successfully logged LLM interaction to {filepath}")
    except Exception as e:
        llm_py_logger.error(f"Failed to log LLM interaction to {filepath}: {e}", exc_info=True)

llm_py_logger = logging.getLogger("LLMUtilsLogger")
llm_py_logger.setLevel(os.getenv("LLM_LOG_LEVEL", "INFO").upper())

if not llm_py_logger.hasHandlers():
    _llm_console_handler = logging.StreamHandler()
    _llm_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
    _llm_console_handler.setFormatter(_llm_formatter)
    llm_py_logger.addHandler(_llm_console_handler)
    llm_py_logger.propagate = False

llm_py_logger.info("--- LLMUtilsLogger configured ---")

NO_ANSWER_PHRASE_ANSWER_CLEAN = "根据目前提供的资料，我无法找到关于您问题的明确信息。"
NO_ANSWER_PHRASE_KG_CLEAN = "从知识图谱中未找到直接相关信息。"
UNIQUE_STOP_TOKEN = "<|im_endofunable|>"
NO_ANSWER_PHRASE_ANSWER_WITH_STOP_TOKEN = f"{NO_ANSWER_PHRASE_ANSWER_CLEAN}{UNIQUE_STOP_TOKEN}"
# NO_ANSWER_PHRASE_KG_WITH_STOP_TOKEN = f"{NO_ANSWER_PHRASE_KG_CLEAN}{UNIQUE_STOP_TOKEN}"

# # Placeholder for the schema description. Replace with actual schema.
# NEW_KG_SCHEMA_DESCRIPTION = """
# {
#   "node_labels": ["Person", "Project", "Task", "Document", "Region", "SalesAmount", "Product"],
#   "relationship_types": ["WORKS_ON", "ASSIGNED_TO", "HAS_DOCUMENT", "HAS_SALES_AMOUNT", "RELATED_TO"],
#   "node_properties": {
#     "Person": [{"property": "name", "type": "STRING"}, {"property": "role", "type": "STRING"}],
#     "Project": [{"property": "name", "type": "STRING"}, {"property": "status", "type": "STRING"}],
#     "Task": [{"property": "name", "type": "STRING"}, {"property": "status", "type": "STRING"}, {"property": "priority", "type": "STRING"}],
#     "Document": [{"property": "id", "type": "STRING"}, {"property": "title", "type": "STRING"}, {"property": "type", "type": "STRING"}],
#     "Region": [{"property": "name", "type": "STRING"}],
#     "SalesAmount": [{"property": "period", "type": "STRING"}, {"property": "numeric_amount", "type": "FLOAT"}, {"property": "unit", "type": "STRING"}],
#     "Product": [{"property": "name", "type": "STRING"}, {"property": "category", "type": "STRING"}]
#   },
#   "relationship_properties": {},
#   "output_format_guidance": {
#     "description": "Your response MUST be a JSON object with two fields: 'status' and 'query'.",
#     "status_field": {
#       "description": "The 'status' field can be one of two values: 'success' or 'unable_to_generate'.",
#       "success": "If you can generate a Cypher query, status should be 'success'.",
#       "unable_to_generate": "If you cannot generate a Cypher query based on the question and schema, status should be 'unable_to_generate'."
#     },
#     "query_field": {
#       "description": "The 'query' field contains the Cypher query as a string if status is 'success'.",
#       "success_example": "MATCH (n) RETURN n LIMIT 1",
#       "unable_to_generate_example": "无法生成Cypher查询."
#     }
#   },
#   "examples": [
#     {
#       "User Question": "Who is task 'FixBug123' assigned to?",
#       "Your EXACT Response": {
#         "status": "success",
#         "query": "MATCH (t:Task {name: 'FixBug123'})<-[:ASSIGNED_TO]-(p:Person) RETURN p.name AS assignedTo"
#       }
#     },
#     {
#       "User Question": "What is the color of the sky?",
#       "Your EXACT Response": {
#         "status": "unable_to_generate",
#         "query": "无法生成Cypher查询."
#       }
#     }
#   ]
# }
# """

LLM_API_URL = os.getenv("SGLANG_API_URL", "http://localhost:8088/v1/chat/completions")

async def call_llm_via_openai_api_local_only( # 改个名字以示区分
    prompt: Union[str, List[Dict[str, str]]], # prompt 可以是字符串或消息列表
    temperature: float = 0.2,
    max_new_tokens: Optional[int] = 1024,
    stop_sequences: Optional[List[str]] = None,
    task_type: str = "unknown_local_llm_call",
    user_query_for_log: Optional[str] = None,
    model_name_for_log: str = "local_qwen_via_openai_api_compat",
    application_version_for_log: str = "0.1.0_local_compat"
) -> Optional[str]:
    llm_py_logger.info(f"Calling LOCAL LLM ({model_name_for_log}) for task: {task_type}, Target API: {LLM_API_URL}")

    current_messages: List[Dict[str, str]]
    original_prompt_for_log: str

    if isinstance(prompt, str): # 假设旧的SGLang风格的prompt字符串
        original_prompt_for_log = prompt
        # 尝试从SGLang格式转换为OpenAI messages格式
        # 这个转换逻辑需要根据您SGLang prompt的具体格式来定
        # 一个简化的例子，可能需要调整：
        current_messages = []
        # 简单的假设：如果prompt以<|im_start|>system开头，则提取system和user部分
        if prompt.startswith("<|im_start|>system"):
            parts = prompt.split("<|im_start|>")
            for part in parts:
                if not part.strip(): continue
                role_content = part.split("<|im_end|>")[0].strip()
                if "\n" in role_content:
                    role, content = role_content.split("\n", 1)
                    current_messages.append({"role": role.strip().lower(), "content": content.strip()})
        if not current_messages: # 如果转换失败或不是SGLang格式，则认为是单个user消息
            current_messages = [{"role": "user", "content": prompt}]
    elif isinstance(prompt, list):
        current_messages = prompt
        original_prompt_for_log = "Messages list provided directly."
    else:
        llm_py_logger.error(f"Invalid 'prompt' argument type: {type(prompt)}")
        return None

    payload = {
        "model": model_name_for_log, # 这个model名会被本地服务忽略，但符合OpenAI格式
        "messages": current_messages,
        "temperature": temperature,
        "max_tokens": max_new_tokens,
    }
    if stop_sequences:
        payload["stop"] = stop_sequences

    headers = {"Content-Type": "application/json"}
    llm_parameters_for_log = {k:v for k,v in payload.items() if k not in ['messages', 'model']} # model已在顶层记录
    raw_llm_output_text = None
    error_info = None

    try:
        async with httpx.AsyncClient(timeout=120.0) as client:
            response = await client.post(LLM_API_URL, json=payload, headers=headers) # LLM_API_URL 指向本地服务
            response.raise_for_status()
            response_json = response.json()
            if response_json.get("choices") and response_json["choices"][0].get("message"):
                raw_llm_output_text = response_json["choices"][0]["message"].get("content", "")
            else:
                raw_llm_output_text = "[[LLM_RESPONSE_MALFORMED_CHOICES_OR_MESSAGE_LOCAL]]"
            llm_py_logger.info(f"FULL Local LLM Raw Output for task '{task_type}': >>>{raw_llm_output_text}<<<")

    except Exception as e:
        llm_py_logger.error(f"Error calling local LLM service: {e}", exc_info=True)
        error_info = str(e)
        # 确保记录错误
        log_error_data = {
            "interaction_id": str(uuid.uuid4()), "timestamp_utc": datetime.now(timezone.utc).isoformat(),
            "task_type": task_type + "_local_error", "user_query_for_task": user_query_for_log,
            "llm_input_messages": current_messages,
            "llm_input_original_prompt_if_string": original_prompt_for_log if isinstance(prompt, str) else None,
            "llm_parameters": llm_parameters_for_log,
            "raw_llm_output": f"Error: {error_info}. Partial raw output: {str(raw_llm_output_text)[:200] if raw_llm_output_text else 'N/A'}",
            "error_details": traceback.format_exc(), "application_version": application_version_for_log
        }
        await log_interaction_data(log_error_data)
        return None # 出错时返回None

    # 记录成功的调用
    log_success_data = {
        "interaction_id": str(uuid.uuid4()), "timestamp_utc": datetime.now(timezone.utc).isoformat(),
        "task_type": task_type, "user_query_for_task": user_query_for_log,
        "llm_input_messages": current_messages,
        "llm_input_original_prompt_if_string": original_prompt_for_log if isinstance(prompt, str) else None,
        "llm_parameters": llm_parameters_for_log,
        "raw_llm_output": raw_llm_output_text, "application_version": application_version_for_log
    }
    await log_interaction_data(log_success_data)
    return raw_llm_output_text

# async def generate_cypher_query(user_question: str) -> Optional[str]: # kg_schema_description 参数可以移除了，因为它已包含在新的prompt函数中
#     llm_py_logger.info(f"Attempting to generate Cypher query (template-based) for: '{user_question}' via local service.")

#     messages_for_llm = get_cypher_generation_messages_with_templates(user_question)

#     cypher_stop_sequences = ['<|im_end|>', '```'] # 如果输出包含markdown的json块

#     llm_response_json_str = await call_llm_via_openai_api_local_only( 
#         prompt=messages_for_llm,
#         temperature=0.0, # 对于精确的JSON和Cypher生成，温度设为0
#         max_new_tokens=1024, # 允许足够的空间输出JSON和Cypher
#         stop_sequences=cypher_stop_sequences,
#         task_type="cypher_generation_template_based_local_service",
#         user_query_for_log=user_question,
#         model_name_for_log="qwen3_gguf_cypher_template_local"
#     )

#     if not llm_response_json_str:
#         llm_py_logger.warning(f"LLM call for Cypher (template-based) returned None or empty. User question: '{user_question}'")
#         return json.dumps({"status": "unable_to_generate", "query": "无法生成Cypher查询."}) # 始终返回JSON字符串

#     cleaned_json_str = llm_response_json_str.strip()
#     if cleaned_json_str.startswith("```json"):
#         cleaned_json_str = cleaned_json_str[len("```json"):].strip()
#     if cleaned_json_str.endswith("```"):
#         cleaned_json_str = cleaned_json_str[:-len("```")].strip()

#     try:

#         parsed_for_validation = json.loads(cleaned_json_str)
#         if isinstance(parsed_for_validation, dict) and \
#            "status" in parsed_for_validation and \
#            "query" in parsed_for_validation:
#             llm_py_logger.info(f"LLM returned valid JSON for Cypher (template-based): {cleaned_json_str}")
#             return cleaned_json_str
#         else:
#             llm_py_logger.warning(f"LLM output for Cypher (template-based) was JSON but not expected structure: {cleaned_json_str}")
#             return json.dumps({"status": "unable_to_generate", "query": "LLM输出JSON结构错误."})
#     except json.JSONDecodeError:
#         llm_py_logger.error(f"Failed to parse JSON response for Cypher (template-based): '{cleaned_json_str}'", exc_info=True)
#         # 如果不是有效的JSON，但包含"MATCH"，可能LLM直接输出了Cypher，尝试包装它
#         if "MATCH" in cleaned_json_str.upper() or "RETURN" in cleaned_json_str.upper():
#              llm_py_logger.warning("LLM output for Cypher (template-based) was not JSON but looks like Cypher, wrapping it.")
#              return json.dumps({"status": "success", "query": cleaned_json_str})
#         return json.dumps({"status": "unable_to_generate", "query": "LLM输出非JSON格式."})

async def generate_answer_from_context(
    user_query: str,
    context_str: str,
    prompt_builder: Optional[Callable[[str, str], List[Dict[str, str]]]] = None
) -> Optional[str]:
    """
    Generates an answer from context.
    V2: Can accept a dynamic prompt builder for specialized tasks like Table-QA.
    """
    from .rag_prompts import get_answer_generation_messages # 默认的 prompt builder

    # 如果没有提供特定的 prompt_builder，就使用默认的通用版本
    if prompt_builder is None:
        prompt_builder = get_answer_generation_messages
    
    llm_py_logger.info(f"Generating answer for query: '{user_query[:50]}...' using prompt builder: {prompt_builder.__name__}")
    
    # 使用 prompt_builder 来构建 messages
    messages = prompt_builder(user_query, context_str)
    
    final_answer = await call_llm_via_openai_api_local_only(
        prompt=messages, # <--- 将 messages 列表传递给 prompt 参数
        task_type=f"answer_generation_using_{prompt_builder.__name__}",
        model_name_for_log="qwen3_gguf_answer_gen_v2"
        # 其他参数如 temperature, max_tokens 会使用该函数的默认值
    )
    
    if final_answer and final_answer != NO_ANSWER_PHRASE_ANSWER_CLEAN:
        return final_answer
    elif final_answer: # 如果是 "无法找到信息"
        return final_answer
    else: # 如果返回None或空字符串
        llm_py_logger.warning("Answer generation returned None or empty string. Falling back to default no-answer phrase.")
        return NO_ANSWER_PHRASE_ANSWER_CLEAN

# async def generate_simulated_kg_query_response(user_query: str, kg_schema_description: str, kg_data_summary_for_prompt: str) -> Optional[str]:
#     prompt_str = f"""<|im_start|>system
# 你是一个知识图谱查询助手。你的任务是根据用户提出的问题、知识图谱Schema描述和图谱中的数据摘要，直接抽取出与问题最相关的1-2个事实片段作为答案。
# 只输出事实片段，不要解释，不要生成Cypher语句，不要包含任何额外对话或标记。
# 如果找不到直接相关的事实，请**直接且完整地**回答：“{NO_ANSWER_PHRASE_KG_WITH_STOP_TOKEN}”<|im_end|>
# <|im_start|>user
# 知识图谱Schema描述:
# {kg_schema_description}

# 知识图谱数据摘要: 
# {kg_data_summary_for_prompt}

# 用户问题: {user_query}<|im_end|>
# <|im_start|>assistant
# """
#     stop_sequences = ["<|im_end|>", UNIQUE_STOP_TOKEN]
#     return await call_llm_via_openai_api_local_only(
#         prompt=prompt_str,
#         temperature=0.5,
#         max_new_tokens=256,
#         stop_sequences=stop_sequences,
#         task_type="simulated_kg_query_response",
#         user_query_for_log=user_query
#     )


async def generate_clarification_question(original_query: str, uncertainty_reason: str) -> Optional[str]:
    llm_py_logger.info(f"调用LLM API生成澄清问题。原始查询: '{original_query}', 原因: '{uncertainty_reason}'")
    messages_for_llm = get_clarification_question_messages(original_query, uncertainty_reason)

    clarification_question_raw = await call_llm_via_openai_api_local_only(
        prompt=messages_for_llm,
        temperature=0.5,
        max_new_tokens=128,
        stop_sequences=['<|im_end|>'], # 对于Qwen系列，<|im_end|> 是一个常见的结束标记
        task_type="clarification_question_generation",
        user_query_for_log=original_query
    )
    
    if not clarification_question_raw or not clarification_question_raw.strip():
        llm_py_logger.warning("LLM未能生成澄清问题，返回默认提示。")
        return "抱歉，我不太理解您的意思，请您再具体说明一下。"  
    cleaned_question_from_llm = clarification_question_raw.strip()
    llm_py_logger.debug(f"LLM原始澄清输出 (清理后): '{cleaned_question_from_llm}'")
    potential_lines = cleaned_question_from_llm.splitlines()
    
    final_extracted_question = None

    for line in reversed(potential_lines):
        line_stripped = line.strip()
        if not line_stripped: # 跳过空行
            continue
        if line_stripped.endswith("？") or line_stripped.endswith("?"):
            if not (line_stripped.startswith("好的，") or \
                    line_stripped.startswith("首先，") or \
                    line_stripped.startswith("因此，") or \
                    line_stripped.startswith("所以，") or \
                    line_stripped.startswith("根据这个原因，") or \
                    "我需要生成一个" in line_stripped or \
                    "可能的澄清问题是" in line_stripped or \
                    "澄清问题应该是" in line_stripped or \
                    "接下来，" in line_stripped):
                final_extracted_question = line_stripped
                llm_py_logger.info(f"通过行分割和问号结尾提取到澄清问题: '{final_extracted_question}'")
                break 
        elif any(line_stripped.startswith(prefix) for prefix in ["请问您", "您对", "您具体指的是"]):
            final_extracted_question = line_stripped
            llm_py_logger.info(f"通过行分割和特定前缀提取到澄清问题: '{final_extracted_question}'")
            break

    if final_extracted_question:
        llm_py_logger.info(f"LLM成功生成并提取到最终澄清问题: {final_extracted_question}")
        return final_extracted_question
    else:
        potential_sentences = re.split(r'(?<=[。？！?])\s*', cleaned_question_from_llm)
        for sentence in reversed(potential_sentences):
            sentence_stripped = sentence.strip()
            if not sentence_stripped:
                continue
            if sentence_stripped.endswith("？") or sentence_stripped.endswith("?") or \
               any(sentence_stripped.startswith(prefix) for prefix in ["请问您", "您是想", "您具体指的是", "关于您提到的"]):
                if not (sentence_stripped.startswith("好的，") or \
                        sentence_stripped.startswith("首先，") or \
                        "我需要生成一个" in sentence_stripped or \
                        "可能的澄清问题是" in sentence_stripped): # 避免选择思考过程
                    final_extracted_question = sentence_stripped
                    llm_py_logger.info(f"通过句子分割和启发式规则提取到澄清问题: '{final_extracted_question}'")
                    break
        
        if final_extracted_question:
            llm_py_logger.info(f"LLM成功生成并提取到最终澄清问题 (后备逻辑): {final_extracted_question}")
            return final_extracted_question
        else:
            llm_py_logger.warning(f"未能通过所有启发式规则从LLM输出中提取明确的澄清问句。原始输出为: '{cleaned_question_from_llm}'。将返回默认澄清。")

            if len(cleaned_question_from_llm) < 70 and (cleaned_question_from_llm.endswith("？") or cleaned_question_from_llm.endswith("?")): # 70是个经验值
                 llm_py_logger.info(f"原始输出较短且以问号结尾，将其作为澄清问题返回: '{cleaned_question_from_llm}'")
                 return cleaned_question_from_llm
            return "抱歉，我不太理解您的意思，请您再具体说明一下。"


async def generate_clarification_options(original_query: str, uncertainty_reason: str) -> List[str]:
    prompt_str = f"""<|im_start|>system
你是一个智能助手，擅长根据用户查询的模糊性提供具体的澄清选项。
你的任务是根据用户原始查询和系统检测到的不确定性原因，生成3-5个具体的、可供用户选择的澄清选项。
每个选项都应该是一个简洁的短语或问题，帮助用户明确其意图。
你的回答必须是一个JSON数组（列表），其中每个元素是一个字符串（澄清选项）。
只输出JSON数组，不要包含任何其他解释、对话标记或代码块。

示例:
用户查询: "帮我预定机票。"
不确定性原因: "缺少出发城市、目的地、日期等信息。"
助手:
[
  "请问您想从哪个城市出发？",
  "请问您的目的地是哪里？",
  "请问您希望在哪一天出行？",
  "您有偏好的航空公司或舱位等级吗？"
]<|im_end|>
<|im_start|>user
用户原始查询: {original_query}
不确定性原因: {uncertainty_reason}

请生成澄清选项:<|im_end|>
<|im_start|>assistant
"""
    stop_sequences = ["<|im_end|>"]
    llm_py_logger.info(f"调用LLM API生成澄清选项 (Prompt长度: {len(prompt_str)} 字符)...")
    llm_output = await call_llm_via_openai_api_local_only(
        prompt=prompt_str,
        temperature=0.7,
        max_new_tokens=256,
        stop_sequences=stop_sequences,
        task_type="clarification_options_generation",
        user_query_for_log=original_query
    )

    options = []
    if llm_output:
        try:
            json_str = llm_output.strip()
            if json_str.startswith("```json"):
                json_str = json_str[len("```json"):].strip()
            if json_str.endswith("```"):
                json_str = json_str[:-len("```")].strip()
            
            parsed_options = json.loads(json_str)
            if isinstance(parsed_options, list) and all(isinstance(o, str) for o in parsed_options):
                options = parsed_options
                llm_py_logger.info(f"LLM成功生成 {len(options)} 个澄清选项。")
            else:
                llm_py_logger.warning(f"LLM生成的澄清选项JSON格式不符合预期 (不是字符串列表): {llm_output[:200]}...")
        except json.JSONDecodeError as e:
            llm_py_logger.error(f"解析LLM澄清选项JSON失败: {e}. 原始输出: {llm_output[:200]}...", exc_info=True)
        except Exception as e:
            llm_py_logger.error(f"处理LLM澄清选项时发生未知错误: {e}. 原始输出: {llm_output[:200]}...", exc_info=True)
    else:
        llm_py_logger.warning("LLM未能生成澄清选项。")
    
    if not options:
        options.append("请提供更多详细信息。")
    
    return options


INTENT_CLASSIFICATION_JSON_SCHEMA = {
    "type": "object",
    "properties": {
        "clarification_needed": {"type": "boolean"},
        "reason": {"type": "string"}
    },
    "required": ["clarification_needed", "reason"]
}

async def generate_intent_classification(user_query: str) -> Dict[str, Any]:
    llm_py_logger.info(f"Generating intent classification for query: '{user_query[:100]}...' using Gemini.")
    
    # 针对Gemini优化的Prompt，强调直接输出JSON
    system_prompt_for_intent = f"""你是一个智能意图分类器。你的任务是分析用户查询，判断该查询是否清晰明确，或者是否存在歧义、信息不足导致需要进一步澄清。
如果查询包含具体的命名实体（如人名“张三”、项目名“项目X”、产品名“新产品A”等），并且问题是关于这些实体的特定信息（例如“张三的职位是什么？”、“项目X的截止日期是哪天？”、“新产品A的功能有哪些？”），则通常认为查询是清晰的，不需要澄清。
只有当查询缺少定位关键信息所必需的核心实体，或者询问的范围过于宽泛无法直接操作时，才需要澄清。

如果查询需要澄清，请说明原因。
你的【唯一输出】必须是一个严格符合以下结构的JSON对象，不要包含任何其他文本、解释或markdown标记:
{{
  "clarification_needed": true/false,
  "reason": "如果需要澄清，请简要说明原因；如果不需要，则为空字符串。"
}}

示例1 (需要澄清 - 信息不足):
用户查询: "帮我预定明天去上海的机票。"
助手 JSON 输出:
{{
  "clarification_needed": true,
  "reason": "缺少出发城市、具体时间（上午/下午/晚上）、舱位等级等信息。"
}}

示例2 (不需要澄清 - 清晰):
用户查询: "公司最新的销售额报告在哪里可以找到？"
助手 JSON 输出:
{{
  "clarification_needed": false,
  "reason": ""
}}
"""
    
    messages_for_gemini = [
        {"role": "system", "content": system_prompt_for_intent},
        {"role": "user", "content": f"用户查询: {user_query}"}
    ]

    # 从环境变量获取Gemini配置
    gemini_model_name = os.getenv("CLOUD_LLM_MODEL_NAME_FOR_LITELLM", "gemini/gemini-1.5-flash-latest")
    gemini_api_key = os.getenv("GEMINI_API_KEY") # 或者 GOOGLE_API_KEY
    proxy_url = os.getenv("LITELLM_PROXY_URL")

    litellm_params: Dict[str, Any] = {
        "model": gemini_model_name,
        "messages": messages_for_gemini,
        "api_key": gemini_api_key,
        "temperature": 0.1, 
        "max_tokens": 256,  # 意图分类的JSON输出通常较短
        # "response_format": {"type": "json_object"} # LiteLLM的Gemini集成可能尚不支持此参数，暂时注释
    }
    if proxy_url:
        # LiteLLM 的 proxy 参数期望一个字典，或者直接是一个字符串URL (取决于LiteLLM版本和具体实现)
        # 为保险起见，我们按文档常见的字典格式提供
        litellm_params["proxy"] = {
            "http": proxy_url,
            "https": proxy_url,
        }
        # 或者，如果您的LiteLLM版本支持直接传递字符串URL作为代理：
        # litellm_params["api_base"] = proxy_url # 这会将代理用于所有请求，如果Gemini也通过此代理
        # litellm_params["base_url"] = proxy_url # 有些版本用 base_url
        # 更通用的方式是设置环境变量 HTTP_PROXY 和 HTTPS_PROXY，LiteLLM通常会读取它们
        # 但为了显式，我们这里尝试通过参数传递给litellm.acompletion

    llm_py_logger.info(f"Calling Gemini (via LiteLLM) for intent classification. Model: {gemini_model_name}")
    debug_params = {k:v for k,v in litellm_params.items() if k not in ['messages', 'api_key']}
    llm_py_logger.debug(f"LiteLLM params for intent (excluding messages & api_key): {debug_params}")
    
    raw_gemini_output_text = None
    error_info_intent = None
    parsed_result_dict: Optional[Dict[str, Any]] = None # 用于存储最终解析结果

    try:
        response = await litellm.acompletion(**litellm_params)
        if response and response.choices and response.choices[0].message and response.choices[0].message.content:
            raw_gemini_output_text = response.choices[0].message.content.strip()
            llm_py_logger.info(f"Gemini intent classification raw output: {raw_gemini_output_text[:300]}...")
            
            # 尝试解析JSON (与之前的提取逻辑类似)
            json_str_candidate = raw_gemini_output_text
            # 1. 尝试从 markdown block 中提取
            markdown_match = re.search(r"```json\s*(\{[\s\S]*?\})\s*```", json_str_candidate, re.DOTALL)
            if markdown_match:
                json_str_candidate = markdown_match.group(1)
                llm_py_logger.debug(f"Extracted JSON candidate from markdown: {json_str_candidate[:200]}...")
            
            # 2. 如果没有markdown，或者提取后仍然不是纯JSON，尝试直接解析或查找第一个 '{' 和最后一个 '}'
            try:
                parsed_result_dict = json.loads(json_str_candidate)
            except json.JSONDecodeError: # 如果直接解析失败
                first_brace = json_str_candidate.find('{')
                last_brace = json_str_candidate.rfind('}')
                if first_brace != -1 and last_brace != -1 and last_brace > first_brace:
                    json_str_candidate = json_str_candidate[first_brace : last_brace+1]
                    llm_py_logger.debug(f"Extracted JSON candidate by braces: {json_str_candidate[:200]}...")
                    try:
                        parsed_result_dict = json.loads(json_str_candidate)
                    except json.JSONDecodeError as e_json_brace:
                        error_info_intent = f"Failed to decode JSON from Gemini intent (braces): {e_json_brace}"
                        llm_py_logger.error(error_info_intent, exc_info=True)
                else: # 没有找到有效的花括号对
                    error_info_intent = "No valid JSON object found in Gemini intent output."
                    llm_py_logger.error(error_info_intent + f" Raw: {raw_gemini_output_text[:200]}")
            
            # 验证解析后的JSON结构
            if parsed_result_dict and isinstance(parsed_result_dict, dict) and \
               "clarification_needed" in parsed_result_dict and \
               "reason" in parsed_result_dict:
                llm_py_logger.info(f"Gemini successfully classified intent: {parsed_result_dict}")
                # 记录成功的调用
                log_data_intent = {
                    "interaction_id": str(uuid.uuid4()), "timestamp_utc": datetime.now(timezone.utc).isoformat(),
                    "task_type": "intent_classification_gemini", "user_query_for_task": user_query,
                    "llm_input_messages": messages_for_gemini, "llm_parameters": {k:v for k,v in litellm_params.items() if k not in ['messages', 'api_key', 'proxy']},
                    "raw_llm_output": raw_gemini_output_text, "application_version": "0.1.0_intent_gemini"
                }
                await log_interaction_data(log_data_intent)
                return parsed_result_dict
            else: # 解析成功但结构不对
                if parsed_result_dict: # 避免对None调用get
                    error_info_intent = f"Gemini intent output JSON structure mismatch. Parsed: {parsed_result_dict}"
                else: # parsed_result_dict 为 None (例如，花括号提取失败后)
                    error_info_intent = "Gemini intent output JSON structure mismatch (parsed_result_dict is None)."
                llm_py_logger.warning(error_info_intent)
        else: # response.choices[0].message.content 为空或不存在
            error_info_intent = "Gemini intent call returned empty or malformed response content."
            llm_py_logger.error(f"{error_info_intent} Full response object: {response}")

    except Exception as e_gemini_call:
        error_info_intent = f"Error calling Gemini for intent: {e_gemini_call}"
        llm_py_logger.error(error_info_intent, exc_info=True)

    # 如果执行到这里，说明出错了或者没有得到期望的JSON
    log_error_data_intent = {
        "interaction_id": str(uuid.uuid4()), "timestamp_utc": datetime.now(timezone.utc).isoformat(),
        "task_type": "intent_classification_gemini_error", "user_query_for_task": user_query,
        "llm_input_messages": messages_for_gemini, "llm_parameters": {k:v for k,v in litellm_params.items() if k not in ['messages', 'api_key', 'proxy']},
        "raw_llm_output": raw_gemini_output_text or "N/A", "error_details": error_info_intent,
        "application_version": "0.1.0_intent_gemini"
    }
    await log_interaction_data(log_error_data_intent)
    
    llm_py_logger.warning(f"Gemini failed to generate valid intent classification, defaulting to no clarification needed. Error: {error_info_intent or 'Unknown reason'}")
    return {"clarification_needed": False, "reason": f"Intent classification by Gemini failed: {error_info_intent or 'Unknown reason'}"}

# --- 新增：用于提取实体和关系意图的函数 ---
# async def extract_entities_for_kg_query(user_question: str) -> Optional[ExtractedEntitiesAndRelationIntent]:
#     llm_py_logger.info(f"Attempting to extract entities and relation intent for KG query (with GBNF) from: '{user_question}'")

#     # --- 使用您在 test_gbnf_extraction.py 中验证成功的 One-Shot Prompt 构建逻辑 ---
#     one_shot_example = """
# --- 示例 ---
# 输入文本: "Alice在ACME公司担任工程师。"
# 输出JSON:
# {
#   "entities": [
#     {"text": "Alice", "label": "PERSON"},
#     {"text": "ACME公司", "label": "ORGANIZATION"},
#     {"text": "工程师", "label": "TASK"}
#   ],
#   "relations": [
#     {"head_entity_text": "Alice", "head_entity_label": "PERSON", "relation_type": "WORKS_AT", "tail_entity_text": "ACME公司", "tail_entity_label": "ORGANIZATION"}
#   ]
# }
# --- 任务开始 ---"""
    
#     # system_content 部分与您的测试脚本保持一致
#     system_content_for_prompt = (
#         f"你是一个严格的JSON知识图谱提取器。请根据用户提供的文本，严格按照示例格式，生成一个包含'entities'和'relations'的JSON对象。\n"
#         f"{one_shot_example}"
#     )

#     # user_content 部分也与您的测试脚本保持一致
#     user_content_for_prompt = (
#         f"输入文本: \"{user_question}\"\n" # 注意：这里用的是 user_question，而不是固定的 sample_text_to_extract
#         f"输出JSON:\n"
#     )

#     full_prompt_for_extraction = (
#         f"<|im_start|>system\n{system_content_for_prompt}<|im_end|>\n"
#         f"<|im_start|>user\n{user_content_for_prompt}<|im_end|>\n"
#         f"<|im_start|>assistant\n"
#     )
#     # --- Prompt 构建结束 ---

#     llm_response_str = await call_local_llm_with_gbnf(
#         full_prompt=full_prompt_for_extraction,
#         grammar_str=KG_EXTRACTION_GBNF_STRING, # 使用我们定义的GBNF字符串
#         temperature=0.1,
#         max_tokens=1024, # 与您的测试脚本一致
#         repeat_penalty=1.2, # 与您的测试脚本一致
#         stop_sequences=["<|im_end|>"], # Qwen的停止标记
#         task_type="kg_entity_relation_extraction_gbnf",
#         user_query_for_log=user_question,
#         model_name_for_log="qwen3_gguf_kg_ext_gbnf"
#     )

#     if not llm_response_str:
#         llm_py_logger.warning(f"LLM call for KG entity/relation extraction (GBNF) returned None or empty. User question: '{user_question}'")
#         return None

    # GBNF应该确保输出是有效的JSON，所以我们可以直接尝试解析
    try:
        # .strip() 以防万一有额外的空白被GBNF的 space 规则匹配但未被移除
        parsed_data = json.loads(llm_response_str.strip())
        extracted_info = ExtractedEntitiesAndRelationIntent(**parsed_data)
        llm_py_logger.info(f"Successfully parsed Pydantic model from GBNF LLM output: {extracted_info.model_dump_json(indent=2)}")
        return extracted_info
    except json.JSONDecodeError as e_json:
        llm_py_logger.error(f"Failed to decode JSON from GBNF LLM output: '{llm_response_str}'. Error: {e_json}", exc_info=True)
        return None
    except Exception as e_pydantic: # Catch Pydantic validation errors
        llm_py_logger.error(f"Failed to validate Pydantic model from GBNF LLM JSON: '{llm_response_str}'. Error: {e_pydantic}", exc_info=True)
        return None
    

async def call_local_llm_with_gbnf(
    llm_instance: Llama, # <--- 接受一个已加载的实例作为参数
    full_prompt: str,
    grammar_str: str,
    temperature: float = 0.1,
    max_tokens: int = 1024,
    repeat_penalty: float = 1.2,
    stop_sequences: Optional[List[str]] = None,
    task_type: str = "gbnf_constrained_generation",
    user_query_for_log: Optional[str] = None,
    model_name_for_log: str = "local_qwen_gguf_gbnf",
    application_version_for_log: str = "0.1.0_gbnf"
) -> Optional[str]:
    llm_py_logger.info(f"Calling LOCAL LLM with GBNF for task: {task_type}. Prompt length: {len(full_prompt)}")

    raw_llm_output_text = None
    error_info = None
    
    try:
        compiled_grammar = LlamaGrammar.from_string(grammar_str)
        
        # 不再需要获取实例，直接使用传入的 llm_instance
        if llm_instance is None:
            raise ValueError("LLM GBNF instance is not available in the application context.")

        def _blocking_llm_call():
            response = llm_instance.create_completion(
                prompt=full_prompt,
                grammar=compiled_grammar,
                temperature=temperature,
                max_tokens=max_tokens,
                repeat_penalty=repeat_penalty,
                stop=stop_sequences or []
            )
            return response['choices'][0]['text']

        raw_llm_output_text = await asyncio.to_thread(_blocking_llm_call)
        llm_py_logger.info(f"GBNF Call: Raw LLM Output for task '{task_type}': >>>{raw_llm_output_text}<<<")

    except Exception as e:
        llm_py_logger.error(f"Error calling local LLM service with GBNF: {e}", exc_info=True)
        error_info = str(e)
        log_error_data = {
            "interaction_id": str(uuid.uuid4()), "timestamp_utc": datetime.now(timezone.utc).isoformat(),
            "task_type": task_type + "_error", "user_query_for_task": user_query_for_log,
            "llm_input_prompt": full_prompt[:500] + "...",
            "llm_parameters": {"temperature": temperature, "max_tokens": max_tokens, "repeat_penalty": repeat_penalty, "stop": stop_sequences},
            "raw_llm_output": f"Error: {error_info}. Partial raw output: {str(raw_llm_output_text)[:200] if raw_llm_output_text else 'N/A'}",
            "error_details": traceback.format_exc(), "application_version": application_version_for_log
        }
        await log_interaction_data(log_error_data)
        return None

    log_success_data = {
        "interaction_id": str(uuid.uuid4()), "timestamp_utc": datetime.now(timezone.utc).isoformat(),
        "task_type": task_type, "user_query_for_task": user_query_for_log,
        "llm_input_prompt": full_prompt[:500] + "...",
        "llm_parameters": {"temperature": temperature, "max_tokens": max_tokens, "repeat_penalty": repeat_penalty, "stop": stop_sequences, "grammar_used": True},
        "raw_llm_output": raw_llm_output_text, "application_version": application_version_for_log
    }
    await log_interaction_data(log_success_data)
    return raw_llm_output_text


async def generate_query_plan(llm_instance: Llama, user_query: str) -> Optional[RagQueryPlan]:
    """
    Analyzes the user query and generates a structured plan for retrieval,
    containing a core query string and a metadata filter.
    Uses GBNF for reliable JSON output.
    """
    # 导入我们新定义的 Prompt, GBNF Schema 和 Pydantic 模型
    from .rag_prompts import V2_PLANNING_PROMPT_TEMPLATE, V2_PLANNING_GBNF_SCHEMA
    from ..config.pydantic_models import RagQueryPlan

    llm_py_logger.info(f"Generating RAG query plan for: '{user_query[:100]}...'")

    # 1. 使用新的模板准备 Prompt
    full_prompt = V2_PLANNING_PROMPT_TEMPLATE.format(user_query=user_query)

    # 2. 调用带有 GBNF 约束的本地 LLM
    llm_response_str = await call_local_llm_with_gbnf(
        llm_instance=llm_instance,
        full_prompt=full_prompt,
        grammar_str=V2_PLANNING_GBNF_SCHEMA,
        temperature=0.0,  # 对于精确的JSON生成，使用零温度
        max_tokens=512,  # 为过滤器和查询提供足够的空间
        task_type="rag_query_planning", # 更新任务类型
        user_query_for_log=user_query,
        model_name_for_log="qwen3_gguf_rag_planner"
    )

    if not llm_response_str:
        llm_py_logger.warning("LLM query planner call returned no response. Falling back to simple query.")
        return RagQueryPlan(query=user_query, metadata_filter={})

    # 3. 解析结果并返回 Pydantic 对象
    try:
        # 清理LLM可能返回的 markdown 代码块标记
        cleaned_response = llm_response_str.strip()
        if cleaned_response.startswith("```json"):
            cleaned_response = cleaned_response[len("```json"):].strip()
        if cleaned_response.endswith("```"):
            cleaned_response = cleaned_response[:-3].strip()
            
        parsed_data = json.loads(cleaned_response)
        query_plan = RagQueryPlan(**parsed_data)
        
        llm_py_logger.info(f"Successfully generated query plan. Query: '{query_plan.query}', Filter: {query_plan.metadata_filter}")
        return query_plan
        
    except (json.JSONDecodeError, TypeError, ValidationError) as e:
        llm_py_logger.error(f"Failed to parse or validate LLM query plan. Error: {e}. Raw output: '{llm_response_str}'. Falling back to simple query.", exc_info=True)
        # 即使解析失败，也要保证RAG流程能继续，返回一个基础的计划
        return RagQueryPlan(query=user_query, metadata_filter={})
    

# --- V4 - Table QA Instruction Generation (Validated by PoC) ---

TABLE_QA_INSTRUCTION_PROMPT_TEMPLATE = """
# 指令
你是一个专门从用户问题中提取表格查询指令的AI。你的唯一任务是分析【用户问题】和【表格列名】，然后输出一个包含`row_identifier`和`column_identifier`的JSON对象。

## 表格信息
【表格列名】: {column_names}

## 用户问题
【用户问题】: "{user_query}"

## 输出要求
请严格按照以下格式输出一个JSON对象，不要包含任何其他文字或解释。
```json
{{
  "row_identifier": "string, 用户问题中提到的具体行名",
  "column_identifier": "string, 用户问题中提到的具体列名"
}}
```
你的JSON输出:
"""
# 使用主项目中已验证过的、最健壮的通用JSON GBNF Schema
from .rag_prompts import V2_PLANNING_GBNF_SCHEMA as TABLE_QA_INSTRUCTION_GBNF_SCHEMA


async def generate_table_lookup_instruction(llm_instance: Llama, user_query: str, table_column_names: List[str]) -> Optional[Dict[str, str]]:
    """
    Uses LLM to generate a structured instruction for table lookup, based on the user query and table columns.
    Returns a dictionary like {"row_identifier": "...", "column_identifier": "..."}.
    """
    llm_py_logger.info(f"Generating table lookup instruction for query: '{user_query}'")
    
    prompt = TABLE_QA_INSTRUCTION_PROMPT_TEMPLATE.format(
        column_names=", ".join(table_column_names),
        user_query=user_query
    )

    llm_response_str = await call_local_llm_with_gbnf(
        llm_instance=llm_instance,
        full_prompt=prompt,
        grammar_str=TABLE_QA_INSTRUCTION_GBNF_SCHEMA,
        temperature=0.0,
        max_tokens=256,
        task_type="table_qa_instruction_generation",
        user_query_for_log=user_query
    )
    if not llm_response_str:
        llm_py_logger.warning("LLM call for table instruction generation returned None.")
        return None
    try:
        instruction_json = json.loads(llm_response_str)
        if "row_identifier" in instruction_json and "column_identifier" in instruction_json:
            llm_py_logger.info(f"Successfully generated table lookup instruction: {instruction_json}")
            return instruction_json
        else:
            llm_py_logger.warning(f"Generated JSON is missing required keys: {instruction_json}")
            return None
    except json.JSONDecodeError:
        llm_py_logger.error(f"Failed to decode JSON from LLM for table instruction: {llm_response_str}")
        return None
    

async def generate_actionable_suggestion(llm_instance: Llama, user_query: str, failure_reason: str) -> Optional[str]:
    """
    Generates actionable suggestions for the user when the RAG system fails to find a direct answer.
    """
    llm_py_logger.info(f"Generating actionable suggestion for query: '{user_query}' due to: {failure_reason}")
    
    messages = get_suggestion_generation_messages(user_query, failure_reason)
    
    # 注意：这个函数生成的是自然语言，不需要 GBNF 约束，所以我们使用通用的 OpenAI API 格式调用。
    # 这里我们假设 `call_llm_via_openai_api_local_only` 内部最终会使用一个不需要 GBNF 的 Llama 实例，
    # 或者它是一个指向不同 LLM 服务的调用。为了保持当前架构，我们暂时不修改它。
    suggestion = await call_llm_via_openai_api_local_only(
        prompt=messages,
        temperature=0.7,
        max_new_tokens=512,
        task_type="suggestion_generation",
        user_query_for_log=user_query,
        model_name_for_log="qwen3_gguf_suggestion_gen"
    )

    if suggestion:
        cleaned_suggestion = re.sub(r"^(好的，|当然，|这里有一些建议：)\s*", "", suggestion.strip(), flags=re.IGNORECASE)
        return cleaned_suggestion
    
    return "您可以尝试换个问法，或检查相关文档是否已在知识库中。"


async def generate_expanded_queries(llm_instance: Llama, original_query: str) -> List[str]:
    """
    Expands a single user query into multiple related sub-queries to enhance retrieval coverage.
    Uses GBNF for reliable JSON output and caches the results.
    V2: Enhanced parsing to handle malformed JSON from the LLM.
    """
    llm_py_logger.info(f"Attempting to generate expanded queries for: '{original_query}'")
    
    from .rag_prompts import get_query_expansion_messages, V2_PLANNING_GBNF_SCHEMA 

    messages = get_query_expansion_messages(original_query)
    
    full_prompt = "".join([f"<|im_start|>{m['role']}\n{m['content']}<|im_end|>\n" for m in messages])
    full_prompt += "<|im_start|>assistant\n"

    llm_response_str = await call_local_llm_with_gbnf(
        llm_instance=llm_instance,
        full_prompt=full_prompt,
        grammar_str=V2_PLANNING_GBNF_SCHEMA,
        temperature=0.6,
        max_tokens=1024,
        task_type="query_expansion_gbnf",
        user_query_for_log=original_query
    )


    expanded_queries = []
    if llm_response_str:
        cleaned_response = llm_response_str.strip()
        try:
            parsed_data = json.loads(cleaned_response)
            
            if isinstance(parsed_data, list):
                expanded_queries = [q for q in parsed_data if isinstance(q, str)]
            elif isinstance(parsed_data, dict):
                # --- 新增的“特种手术”解析逻辑 ---
                # 尝试将字典的键作为JSON数组进行解析
                for key, value in parsed_data.items():
                    if isinstance(key, str) and key.strip().startswith('[') and key.strip().endswith(']'):
                        try:
                            # 找到了! 那个作为键的JSON数组字符串
                            potential_queries = json.loads(key)
                            if isinstance(potential_queries, list):
                                expanded_queries = [q for q in potential_queries if isinstance(q, str)]
                                llm_py_logger.info(f"Successfully extracted queries from a malformed JSON dictionary key.")
                                break # 找到就跳出循环
                        except json.JSONDecodeError:
                            continue # 这个键不是有效的JSON，继续检查下一个
                
                # 如果上述方法失败，尝试从常见的键中提取
                if not expanded_queries:
                    for key_option in ["queries", "sub_queries", "expanded_queries", "result"]:
                        if key_option in parsed_data and isinstance(parsed_data[key_option], list):
                            expanded_queries = parsed_data[key_option]
                            break
            
            if not expanded_queries:
                llm_py_logger.warning(f"Could not extract a list of strings from the parsed JSON: {parsed_data}")

        except (json.JSONDecodeError, TypeError) as e:
            llm_py_logger.error(f"Failed to decode JSON for query expansion: {cleaned_response}. Error: {e}")


    # 确保原始查询总是第一个
    final_queries = [original_query]
    for q in expanded_queries:
        # 确保不添加重复的查询
        if q.strip() and q.strip() not in final_queries:
            final_queries.append(q.strip())
    
    llm_py_logger.info(f"Final list of queries ({len(final_queries)} total, deduplicated): {final_queries}")
    return final_queries


async def generate_document_summary(llm_instance: Llama, user_query: str, document_content: str) -> str:
    """
    Generates a single, concise summary sentence for a document based on the user query.
    """
    llm_py_logger.info(f"Generating summary for document based on query: '{user_query[:50]}...'")
    
    messages = get_document_summary_messages(user_query, document_content)
    
    # 摘要任务是自然语言生成，不需要 GBNF
    summary = await call_llm_via_openai_api_local_only(
        prompt=messages,
        temperature=0.1, # 摘要需要精确，低温度
        max_new_tokens=150, # 限制输出长度
        stop_sequences=["\n", "。", "."], # 一句话结束就停止
        task_type="document_summary_generation",
        user_query_for_log=user_query
    )

    if summary and summary.strip().lower() != "irrelevant":
        llm_py_logger.info("Successfully generated a relevant summary.")
        return summary.strip()
    
    # --- 核心降级逻辑 ---
    llm_py_logger.warning("Summary generation failed or returned irrelevant. "
                        "Falling back to using a snippet of the original document content.")
    # 返回原始文档内容的前N个字符作为后备摘要
    return document_content[:500] + "..."


```

    |-- local_model_handler.py

``` py
# zhz_rag/llm/local_model_handler.py
# 版本: 简化版，不再处理多进程，仅用于定义 LlamaCppEmbeddingFunction

import os
import sys
import logging
from typing import List, Optional, Any, Sequence
import numpy as np
from cachetools import TTLCache
import asyncio

# --- 日志配置 ---
handler_logger = logging.getLogger("LocalModelHandler") # 日志名可以保持
if not handler_logger.hasHandlers():
    handler_logger.setLevel(logging.INFO)
    stream_handler = logging.StreamHandler(sys.stdout)
    formatter = logging.Formatter('%(asctime)s - %(name)s - PID:%(process)d - %(levelname)s - %(message)s')
    stream_handler.setFormatter(formatter)
    handler_logger.addHandler(stream_handler)
    handler_logger.propagate = False

# 为了避免与 core_rag 中的 LlamaCppEmbeddingFunction 混淆，
# 我们可以考虑重命名这个类，例如 MyModelHandlerForEmbeddings，
# 或者确保它的接口与 'LocalModelHandler' 类型提示兼容。
# 我们先保持类名，但确保它有 'embedding_model_path' 属性。
class LlamaCppEmbeddingFunction: # 这个类将作为 "model_handler"
    """
    一个与 Dagster 资源交互的嵌入函数包装器。
    它不直接创建模型，而是通过 GGUFEmbeddingResource 与工作进程通信。
    这个类将扮演 'LocalModelHandler' 的角色，被 core_rag 中的 LlamaCppEmbeddingFunction 使用。
    """
    def __init__(self, resource: Any, embedding_model_path_for_handler: Optional[str] = None): # resource 是 GGUFEmbeddingResource
        if resource is None:
            raise ValueError("GGUFEmbeddingResource is required.")
        self.resource = resource
        # CoreRetriever_LlamaCppEmbeddingFunction 期望 model_handler 有 embedding_model_path 属性
        self.embedding_model_path = embedding_model_path_for_handler if embedding_model_path_for_handler else os.getenv("EMBEDDING_MODEL_PATH")
        if not self.embedding_model_path:
             handler_logger.warning("embedding_model_path not provided to LocalModelHandler's LlamaCppEmbeddingFunction and not found in env. This might be an issue if downstream components expect it.")

        self._dimension: Optional[int] = None
        self._query_cache: TTLCache = TTLCache(maxsize=200, ttl=3600)
        self._cache_lock = asyncio.Lock()
        
        # 尝试在初始化时获取维度
        try:
            # 注意：resource.get_embedding_dimension() 是同步的，在异步 __init__ 中应小心
            # 但由于 __init__ 本身不是 async，这里直接调用是OK的
            # 如果 resource.get_embedding_dimension() 内部做了异步转同步，那也没问题
            dim = self.resource.get_embedding_dimension()
            if dim is not None:
                self._dimension = dim
                handler_logger.info(f"LocalModelHandler's LlamaCppEmbeddingFunction initialized. Dimension from resource: {self._dimension}")
            else:
                handler_logger.info("LocalModelHandler's LlamaCppEmbeddingFunction initialized. Dimension not immediately available from resource.")
        except Exception as e:
             handler_logger.warning(f"Error getting dimension from resource during init: {e}. Will fetch on first use.")


    async def __call__(self, input: Sequence[str]) -> List[List[float]]:
        if not input:
            return []
        handler_logger.info(f"LocalModelHandler's LlamaCppEmbeddingFunction: Generating embeddings for {len(input)} documents (via resource).")
        # 直接调用 Dagster 资源的 encode 方法
        # GGUFEmbeddingResource.encode 是同步包装的异步，所以 to_thread 是合适的
        embeddings = await asyncio.to_thread(self.resource.encode, list(input))
        return embeddings

    async def embed_documents(self, texts: Sequence[str]) -> List[List[float]]:
        return await self.__call__(list(texts))

    async def embed_query(self, text: str) -> List[float]:
        async with self._cache_lock:
            cached_result = self._query_cache.get(text)
        if cached_result is not None:
            handler_logger.info(f"Query Vector CACHE HIT for query (LocalModelHandler): '{text[:50]}...'")
            return cached_result
        
        handler_logger.info(f"Query Vector CACHE MISS for query (LocalModelHandler): '{text[:50]}...'. Generating new embedding.")
        embedding_list = await asyncio.to_thread(self.resource.encode, [text])
        
        if embedding_list and embedding_list[0]:
            embedding_vector = embedding_list[0]
            async with self._cache_lock:
                self._query_cache[text] = embedding_vector
            return embedding_vector
        
        dim = await self.get_dimension()
        return [0.0] * (dim or 1024) # Fallback to 1024 if dim is None

    def get_embedding_dimension(self) -> Optional[int]: # 改为同步，因为它在 __init__ 和 CoreRetriever 中被同步调用
        if self._dimension is None:
            try:
                dim_from_res = self.resource.get_embedding_dimension()
                if dim_from_res is not None:
                    self._dimension = dim_from_res
                    handler_logger.info(f"Dimension fetched from resource (sync): {self._dimension}")
            except Exception as e:
                handler_logger.error(f"Error fetching dimension from resource (sync): {e}")
        return self._dimension

    async def _get_embedding_dimension_from_worker_once(self) -> Optional[int]: # 辅助异步获取方法
         if self._dimension is None:
             try:
                 # GGUFEmbeddingResource.get_embedding_dimension() 是同步的
                 # 如果要在异步方法中调用，需要 to_thread
                 dim = await asyncio.to_thread(self.resource.get_embedding_dimension)
                 if dim is not None:
                     self._dimension = dim
                     handler_logger.info(f"Dimension fetched from resource (async helper): {self._dimension}")
             except Exception as e:
                 handler_logger.error(f"Error fetching dimension from resource (async helper): {e}")
         return self._dimension
```

    |-- rag_prompts.py

``` py
from typing import List, Dict, Any
# from zhz_rag.config.constants import NEW_KG_SCHEMA_DESCRIPTION


# 可以将 NO_ANSWER_PHRASE_ANSWER_CLEAN 也移到这里，或者从 constants.py 导入
NO_ANSWER_PHRASE_ANSWER_CLEAN = "根据目前提供的资料，我无法找到关于您问题的明确信息。" # 保持与 llm_interface.py 一致


def get_answer_generation_messages(user_query: str, context_str: str) -> List[Dict[str, str]]:
    """
    构建用于从上下文中生成答案的LLM输入messages。
    V4: 引入“引用优先”的思维链，强制模型优先引用原文，以解决幻觉和过度泛化问题。
    """
    system_prompt_for_answer = f"""
你是一个极其严谨、客观且专业的AI问答助手。你的核心任务是根据一份或多份【上下文信息】来回答【用户问题】。

**思维链 (Chain-of-Thought) 指导 - 【引用优先原则】:**

1.  **理解问题**: 首先，完全理解【用户问题】的核心意图。

2.  **扫描上下文寻找直接引文**:
    *   仔细阅读每一份【上下文信息】，寻找能够**直接、逐字回答**【用户问题】的句子或段落。
    *   **如果找到**: 优先使用这些原文来构建你的答案。你可以对原文进行少量删减或连接，但不能改变其核心意思和措辞。

3.  **基于多处信息进行综合**:
    *   **如果找不到单一的直接引文**: 尝试从上下文的不同部分提取相关事实和数据点。
    *   将这些事实**像拼图一样组合起来**，形成一个连贯的答案。在综合时，**必须**使用原文中的短语和术语。

4.  **元数据检查**:
    *   如果问题是关于文档的属性（如作者、文件名），请直接从 `Source Document Metadata` 中提取信息回答。

**核心指令与行为准则：**

*   **【绝对忠实于原文】**: 你的回答【必须且只能】是【上下文信息】中明确文字的直接引用或忠实转述。**严禁进行任何形式的总结、归纳、推断或引入外部知识。** 你是一个信息的搬运工，不是思想的创造者。
*   **【处理无法回答的情况】**: 如果在元数据和文本内容中都找不到足够的信息来回答问题，请只回答这一句话，不要添加任何其他内容：“{NO_ANSWER_PHRASE_ANSWER_CLEAN}”
*   **【答案风格：专业、客观】**: 直接针对用户问题进行回答，避免使用“根据我分析...”或“我认为...”等主观性词汇。

请严格遵循以上指令，以最高的准确性和忠实度来完成回答。
"""
    messages = [
        {"role": "system", "content": system_prompt_for_answer},
        {"role": "user", "content": f"用户问题: {user_query}\n\n上下文信息:\n{context_str}"}
    ]
    return messages


def get_clarification_question_messages(original_query: str, uncertainty_reason: str) -> List[Dict[str, str]]:
    """
    构建用于生成澄清问题的LLM输入messages。
    """
    system_prompt_for_clarification = f"""你的【唯一任务】是根据用户提供的【用户原始查询】和【不确定性原因】，生成一个【简洁、明确、友好且直接的澄清问句】。
    

**【严格的输出要求】**
* 你的【最终且唯一】的输出【必须】是这个澄清问句本身。
* 【绝对禁止】输出任何思考过程、解释、前缀、后缀或任何与澄清问句无关的文字。
* 澄清问句本身不应包含用户的原始查询或不确定性原因的复述。
/no_think


**示例：**

<example>
  <user_original_query>帮我查查天气</user_original_query>
  <uncertainty_reason>缺少地点信息</uncertainty_reason>
  <assistant_clarification_question>请问您想查询哪个城市的天气呢？</assistant_clarification_question>
</example>

<example>
  <user_original_query>分析一下销售数据</user_original_query>
  <uncertainty_reason>用户没有说明具体想对销售数据做什么操作，例如是汇总、筛选还是查找特定记录。</uncertainty_reason>
  <assistant_clarification_question>请问您希望对销售数据进行哪种具体操作，例如汇总统计、筛选特定条件，还是查找某些记录？</assistant_clarification_question>
</example>

<example>
  <user_original_query>给我推荐一些关于人工智能的书籍</user_original_query>
  <uncertainty_reason>用户没有说明偏好的人工智能子领域或书籍类型（入门/进阶/技术/哲学等）。</uncertainty_reason>
  <assistant_clarification_question>您对人工智能的哪个子领域或什么类型的书籍（如入门、技术实践、哲学探讨等）更感兴趣？</assistant_clarification_question>
</example>

<example>
  <user_original_query>我们公司的年假政策是怎么样的？</user_original_query>
  <uncertainty_reason>缺少公司名称，无法定位到具体的年假政策文档。</uncertainty_reason>
  <assistant_clarification_question>请问您的公司全称是什么？</assistant_clarification_question>
</example>

<example>
  <user_original_query>处理一下这个文件。</user_original_query>
  <uncertainty_reason>用户没有说明要对文件进行何种处理，也没有指明是哪个文件。</uncertainty_reason>
  <assistant_clarification_question>请问您希望对哪个文件进行什么具体操作呢？</assistant_clarification_question>
</example>
"""
    user_content = f"""用户原始查询: {original_query}
不确定性原因: {uncertainty_reason}

你应该输出的澄清问句:""" # 改为“澄清问句”

    messages = [
        {"role": "system", "content": system_prompt_for_clarification},
        {"role": "user", "content": user_content}
    ]
    return messages

# --- 精简的Cypher模板定义 (只保留一个核心模板) ---
# SIMPLIFIED_CYPHER_TEMPLATES = [
#     {
#         "id": "template_find_entity_attributes_by_text_label",
#         "description": "根据提供的实体文本和实体标签，查找该实体的所有基本属性。",
#         "template": "MATCH (n:ExtractedEntity {{text: $entity_text, label: $entity_label}}) RETURN n.text AS text, n.label AS label, n.id_prop AS id_prop LIMIT 1",
#         "params_needed": ["entity_text", "entity_label"]
#     }
# ]

# def get_cypher_generation_messages_with_templates(user_question: str) -> List[Dict[str, str]]: # 函数名保持一致
#     """
#     构建用于（基于【单个指定模板】）生成Cypher查询的LLM输入messages。
#     这个版本用于测试模型对单个模板的参数提取能力。
#     """
    
#     # 在这个测试版本中，我们假设总是使用第一个（也是唯一一个）模板
#     selected_template = SIMPLIFIED_CYPHER_TEMPLATES[0]
    
#     template_description_for_prompt = f"""你将使用以下Cypher查询模板：
# Template ID: {selected_template['id']}
# Description: {selected_template['description']}
# Cypher Structure: {selected_template['template']}
# Parameters Needed: {', '.join(selected_template['params_needed'])}
# """

#     system_prompt_for_cypher = f"""你是一个精确的参数提取助手。你的任务是根据用户问题，为下面提供的【唯一Cypher查询模板】提取参数，并构建一个Cypher查询。

# **【图谱Schema核心部分参考】**
# (你主要关注 `:ExtractedEntity` 节点及其属性: `text`, `label`, `id_prop`。其中 `label` 的常见值是 "PERSON", "ORGANIZATION", "TASK"。)
# {NEW_KG_SCHEMA_DESCRIPTION} 
# # ^^^ Schema描述已包含输出JSON格式 {{"status": "success/unable_to_generate", "query": "..."}} 的指导，请严格遵循该JSON输出格式。

# **【你的任务与输出要求】**
# 1.  仔细分析【用户问题】，理解其核心查询意图。
# 2.  判断该意图是否与提供的【当前需要填充的Cypher查询模板】描述相符。
# 3.  如果相符：
#     a.  从【用户问题】中提取填充该模板所需的所有【Parameters Needed】。确保参数值与Schema中的实体文本和标签格式相符（例如，标签应为大写 "PERSON", "ORGANIZATION", "TASK"）。
#     b.  将提取的参数值替换到模板的Cypher语句中（例如，`$entity_text` 替换为提取到的实体名）。
#     c.  最终输出一个JSON对象，格式为：`{{"status": "success", "query": "填充好参数的Cypher语句"}}`。
# 4.  如果不相符（例如，用户问题意图与模板描述不符，或无法从问题中提取到模板所需的所有关键参数）：
#     a.  最终输出一个JSON对象，格式为：`{{"status": "unable_to_generate", "query": "无法生成Cypher查询."}}`。
# 5.  【绝对禁止】输出任何除了上述指定JSON对象之外的文本、解释或思考过程。


# **【处理示例】**
# <example>
#   <user_question>我想知道张三的详细信息。</user_question>
#   <assistant_output_json>{{
#     "status": "success",
#     "query": "MATCH (n:ExtractedEntity {{text: '张三', label: 'PERSON'}}) RETURN n.text AS text, n.label AS label, n.id_prop AS id_prop LIMIT 1"
#   }}</assistant_output_json>
# </example>
# <example>
#   <user_question>项目Alpha的文档编写任务是什么？</user_question>
#   <assistant_output_json>{{
#     "status": "success",
#     "query": "MATCH (n:ExtractedEntity {{text: '项目alpha的文档编写任务', label: 'TASK'}}) RETURN n.text AS text, n.label AS label, n.id_prop AS id_prop LIMIT 1"
#   }}</assistant_output_json>
# </example>
# <example>
#   <user_question>法国的首都是哪里？</user_question>
#   <assistant_output_json>{{
#     "status": "unable_to_generate",
#     "query": "无法生成Cypher查询."
#   }}</assistant_output_json>
# </example>
# """
#     user_content = f"用户问题: {user_question}"

#     messages = [
#         {"role": "system", "content": system_prompt_for_cypher},
#         {"role": "user", "content": user_content}
#     ]
#     return messages

# --- 新增：实体与关系意图提取的提示词生成函数 ---
# def get_entity_relation_extraction_messages(user_question: str) -> List[Dict[str, str]]:
#     """
#     构建用于从用户查询中提取核心实体和关系意图的LLM输入messages。
#     目标是输出一个符合 ExtractedEntitiesAndRelationIntent Pydantic 模型结构的纯净JSON对象。
#     这个版本的Prompt极度强调JSON输出格式。
#     """
#     import re
#     match = re.search(r'label\s*:\s*STRING\s*\(实体类型。\s*允许的值\s*:\s*("([^"]+)"(?:,\s*"([^"]+)")*)\)', NEW_KG_SCHEMA_DESCRIPTION)
#     allowed_entity_labels_str = "PERSON, ORGANIZATION, TASK, DOCUMENT, PROJECT, REGION, PRODUCT, OTHER"
#     if match:
#         labels_group = match.group(1)
#         extracted_labels = re.findall(r'"([^"]+)"', labels_group)
#         if extracted_labels:
#             allowed_entity_labels_str = ", ".join(extracted_labels)
#             if "OTHER" not in extracted_labels:
#                 allowed_entity_labels_str += ", OTHER"

#     # --- V3 "最最严格" Prompt ---
#     system_prompt_for_entity_extraction = f"""<|im_start|>system
# USER_QUERY_TO_PROCESS:
# {user_question}

# TASK: Analyze USER_QUERY_TO_PROCESS. Output ONLY a valid JSON object.
# NO EXPLANATIONS. NO EXTRA TEXT. NO MARKDOWN. JUST JSON.

# JSON_OUTPUT_SCHEMA:
# {{
#   "entities": [
#     {{"text": "string, extracted entity text from USER_QUERY_TO_PROCESS", "label": "string, entity type from: [{allowed_entity_labels_str}], or OTHER"}}
#   ],
#   "relation_hint": "string, relation described in USER_QUERY_TO_PROCESS, or empty string"
# }}

# RULES:
# 1. Max 2 entities in "entities" array. If none, "entities" is `[]`.
# 2. "label" MUST be from the provided list or "OTHER".
# 3. If no relation_hint, value is `""`.
# 4. If USER_QUERY_TO_PROCESS yields no entities or relation, output: `{{"entities": [], "relation_hint": ""}}`

# YOUR_VALID_JSON_OUTPUT_ONLY:<|im_end|>""" # <--- 结尾引导更加直接

#     messages = [
#         {"role": "system", "content": system_prompt_for_entity_extraction}
#     ]
#     return messages


# =================================================================================================
# V2 - RAG Query Planner with Metadata Filtering Prompts
# =================================================================================================
V2_PLANNING_PROMPT_TEMPLATE = """
# 指令
你是一个专业的RAG查询规划专家。你的任务是分析用户的提问，并将其分解为一个结构化的JSON对象，该对象包含两个字段：`query` 和 `metadata_filter`。

## JSON结构说明
1.  `query` (字符串): 提炼出的核心搜索关键词。如果问题是关于文档的元数据（如作者、创建日期），这个字段可以是通用描述，如“文档元数据”。
2.  `metadata_filter` (JSON对象): 一个用于ChromaDB的`where`过滤器。
    - 可用字段: `filename`, `page_number`, `paragraph_type` ('text', 'table', 'title'), `author`。

## 示例
### 示例 1: 普通内容查询
用户提问: "RAG框架的核心优势是什么？"
AI输出:
```json
{{
    "query": "RAG框架的核心优势",
    "metadata_filter": {{}}
}}
### 示例 2: 带文件名和内容类型的复杂查询
用户提问: "给我看看'年度报告.pdf'第二章关于销售分析的表格"
AI输出:
{{
    "query": "销售分析 表格",
    "metadata_filter": {{
        "$and": [
            {{"filename": {{"$eq": "年度报告.pdf"}}}},
            {{"title_hierarchy_2": {{"$like": "%销售分析%"}}}},
            {{"paragraph_type": {{"$eq": "table"}}}}
        ]
    }}
}}
### 示例 3: 纯元数据查询 (新！)
用户提问: "complex_layout.docx的作者是谁？"
AI输出:
{{
    "query": "文档作者和贡献者信息",
    "metadata_filter": {{
        "filename": {{"$eq": "complex_layout.docx"}}
    }}
}}
### 用户问题
现在，请根据以下用户提问，生成对应的JSON对象。
用户提问: "{user_query}"
AI输出:
"""


# 用于约束规划器输出的GBNF Schema
V2_PLANNING_GBNF_SCHEMA = r'''
root   ::= object
value  ::= object | array | string | number | "true" | "false" | "null"
ws ::= ([ \t\n\r])*
object ::= "{" ws ( member ("," ws member)* )? ws "}"
member ::= string ws ":" ws value
array  ::= "[" ws ( value ("," ws value)* )? ws "]"
string ::= "\"" ( [^"\\\x7F\x00-\x1F] | "\\" ( ["\\/bfnrt] | "u" [0-9a-fA-F]{4} ) )* "\""
number ::= "-"? ([0-9] | [1-9] [0-9]*) ("." [0-9]+)? ([eE] [-+]? [0-9]+)?
'''

def get_table_qa_messages(user_query: str, context_str: str) -> List[Dict[str, str]]:
    """
    构建一个专门用于处理表格问答（Table-QA）的LLM输入messages。
    V4: 融合Qwen3官方文档的最佳实践，使用 /think 和 /no_think 标签精确控制模型行为。
    """
    system_prompt_for_table_qa = f"""
你是一个极其精确的Markdown表格数据提取专家。你的任务是严格遵循指令，为【用户问题】找到答案。

**指令:**

1.  **/think**
    *   **识别查询目标 (Identify Target):** 从【用户问题】“{user_query}”中，识别出**行标识**和**列名**。
    *   **扫描表格 (Scan Table):** 在【上下文信息】的表格中，找到**行标识**所在的那一行。
    *   **定位数值 (Locate Value):** 在该行中，找到**列名**对应的那一列，并提取其**单元格的值**。

2.  **/no_think**
    *   **如果成功找到值**: 使用模板 `"[行标识]的[列名]是[提取的值]。"` 来构建最终答案。
    *   **如果找不到**: 直接回答：“根据提供的表格，我无法找到关于'{user_query}'的信息。”

你的回答必须严格遵循先思考、后回答的格式，不要输出任何与最终答案无关的额外内容。
"""
    messages = [
        {"role": "system", "content": system_prompt_for_table_qa},
        {"role": "user", "content": f"用户问题: {user_query}\n\n上下文信息:\n{context_str}"}
    ]
    return messages

def get_suggestion_generation_messages(user_query: str, failure_reason: str) -> List[Dict[str, str]]:
    """
    构建用于在问答失败时生成智能建议的LLM输入messages。
    """
    system_prompt_for_suggestion = f"""
你是一个经验丰富、乐于助人且善于沟通的IT支持专家。你的任务是帮助一个因特定原因未能得到答案的用户。

**你的输入:**
1.  【用户原始问题】: 用户最初想问什么。
2.  【失败原因】: 系统为什么没能找到答案。

**你的任务:**
根据上述输入，生成一个简短、友好、包含**2-3个具体、可操作建议**的段落。这些建议应该能真正帮助“办公室电脑小白用户”解决问题。

**输出要求:**
*   **不要**复述“我找不到答案”这句话。你的输出将直接附加在这句话后面。
*   **不要**包含任何抱歉或客套话。直接给出建议。
*   建议必须具体且具有启发性。

**输出风格示例:**

*   **如果失败原因是“上下文信息不足”:**
    *   "您可以尝试换一种更宽泛的问法，或者检查一下您上传的《{'{document_name}'}》文件中是否确实包含了相关内容。"

*   **如果失败原因是“表格中找不到对应的行或列”:**
    *   "您可以核对一下问题中的实体名称（例如“产品B”）是否与表格中的完全一致，或者确认一下您想查询的列名（例如“价格”）是否存在于表格中。"

*   **如果失败原因是“检索结果为空”:**
    *   "这可能是因为知识库中还没有包含相关主题的文档。您可以尝试上传相关文件，或者调整一下问题的关键词，以便更好地匹配现有内容。"

请严格按照要求，生成有用的建议。
"""
    messages = [
        {"role": "system", "content": system_prompt_for_suggestion},
        {"role": "user", "content": f"【用户原始问题】: \"{user_query}\"\n\n【失败原因】: \"{failure_reason}\""}
    ]
    return messages


def get_query_expansion_messages(original_query: str) -> List[Dict[str, str]]:
    """
    构建用于将原始查询扩展为多个子问题的LLM输入messages。
    """
    system_prompt_for_expansion = """
你是一个专家级的查询分析师。你的任务是根据用户提供的【原始查询】，生成3个不同但相关的子问题，以探索原始查询的不同方面，从而帮助信息检索系统找到更全面、更深入的答案。

**输出要求:**
*   你的回答【必须】是一个JSON数组（列表），其中只包含字符串（子问题）。
*   【绝对禁止】输出任何除了这个JSON数组之外的文本、解释、对话标记或代码块。

**示例:**
【原始查询】: "介绍一下RAG技术及其在金融领域的应用"
【你的JSON输出】:
[
  "RAG技术的基本原理和核心组件是什么？",
  "RAG相比传统的模型微调有哪些优势和劣势？",
  "在金融领域，RAG技术有哪些具体的应用案例，例如风险评估或智能投顾？"
]
"""
    messages = [
        {"role": "system", "content": system_prompt_for_expansion},
        {"role": "user", "content": f"【原始查询】: \"{original_query}\""}
    ]
    return messages

def get_fusion_messages(original_query: str, fusion_context: str) -> List[Dict[str, str]]:
    """
    构建用于将多个子答案融合成一个最终报告的LLM输入messages。
    """
    system_prompt_for_fusion = """
你是一个顶级的【信息整合与报告撰写专家】。
你的任务是将一系列针对【原始问题】的【子问题与子答案】进行分析、整合、去重，并最终撰写成一份逻辑清晰、内容全面、专业且连贯的【最终报告】。

**核心指令:**

1.  **目标导向**: 你的【最终报告】必须直接、完整地回答【原始问题】。
2.  **信息来源**: 你【只能】使用【子问题与子答案】中提供的信息。严禁引入任何外部知识或进行不合理的推断。
3.  **整合与去重**: 将不同子答案中的相关信息进行逻辑上的连接和整合。如果多个子答案提到相同的事实，请在最终报告中只提及一次，避免重复。
4.  **结构化输出**: 如果内容复杂，请使用标题、列表（如 1., 2., ... 或 -）等方式来组织你的【最终报告】，使其易于阅读。
5.  **专业风格**: 保持客观、中立的语气。直接开始撰写报告内容，不要添加如“好的，这是您的报告”等多余的开场白。
6.  **处理矛盾/不足**: 如果提供的子答案信息不足以形成一份有意义的报告，或者信息之间存在明显矛盾，请直接回答“根据现有信息，无法就您的问题给出一个全面统一的答案。”

请现在基于以下信息，开始你的报告撰写工作。
"""
    
    user_content = f"""
【原始问题】:
{original_query}

【子问题与子答案】:
{fusion_context}

---
【你的最终报告】:
"""

    messages = [
        {"role": "system", "content": system_prompt_for_fusion},
        {"role": "user", "content": user_content}
    ]
    return messages


def get_document_summary_messages(user_query: str, document_content: str) -> List[Dict[str, str]]:
    """
    构建用于“为单个文档，针对用户问题，生成一句核心摘要”的LLM输入messages。
    """
    system_prompt_for_summary = f"""
你是一个高度专注的【信息摘要AI】。你的唯一任务是阅读一份【文档内容】，并根据【用户原始问题】，用一句话总结出该文档中与问题最相关的核心信息。

**核心指令:**

1.  **绝对相关**: 你的摘要【必须】直接回应【用户原始问题】。
2.  **绝对简洁**: 你的回答【只能】是一句话，不能超过50个字。
3.  **基于原文**: 你的摘要【必须】完全基于【文档内容】。
4.  **无相关信息处理**: 如果文档内容与用户问题完全不相关，请【直接且仅】输出字符串："irrelevant"

**示例 1:**
【用户原始问题】: "RAG的优势是什么？"
【文档内容】: "...RAG技术通过结合检索器和生成器，显著提升了答案的准确性和时效性，这是它相较于传统微调方法的核心优势..."
【你的输出】:
RAG技术的核心优势在于通过结合检索与生成，提升了答案的准确性和时效性。

**示例 2:**
【用户原始问题】: "介绍一下ACME公司的组织架构。"
【文档内容】: "...RAG技术通过结合检索器和生成器，显著提升了答案的准确性和时效性..."
【你的输出】:
irrelevant
"""
    
    user_content = f"""
【用户原始问题】:
{user_query}

【文档内容】:
{document_content}

---
【你的输出】:
"""

    messages = [
        {"role": "system", "content": system_prompt_for_summary},
        {"role": "user", "content": user_content}
    ]
    return messages

def get_task_extraction_messages(user_query: str, llm_answer: str) -> List[Dict[str, str]]:
    """
    构建用于从对话中提取待办任务信息的LLM输入消息。

    Args:
        user_query: 用户的原始查询。
        llm_answer: RAG系统生成的最终答案。

    Returns:
        一个符合OpenAI聊天格式的消息列表。
    """
    system_prompt = """
你是一个精准的任务信息提取AI。你的任务是分析【用户问题】和【AI的回答】，判断其中是否包含一个明确的、未来需要执行的待办事项或截止日期。

**提取规则:**
1.  **识别行动意图**: 只有当对话内容明确指向一个**未来**的、**具体**的行动时（如“提交报告”、“回电话”、“准备会议”），才视为一个任务。
2.  **提取任务标题 (title)**: 从对话中提炼出一个简洁、明确的任务标题。
3.  **提取截止日期 (due_date)**: 如果对话中提到了明确的日期或时间（如“明天下午3点”、“7月31日”），请将其提取出来，并转换为 `YYYY-MM-DD HH:MM:SS` 的标准格式。如果只提到了日期，时间部分可以用 `09:00:00`（上午9点）作为默认。
4.  **无任务情况**: 如果对话内容只是信息查询、事实陈述或已完成的事件，则不应提取任何任务。

**输出格式:**
你的回答**必须**是一个JSON对象。
- 如果识别出任务，JSON结构为: `{"task_found": true, "title": "任务标题", "due_date": "YYYY-MM-DD HH:MM:SS"}`
- 如果没有识别出任何任务，JSON结构为: `{"task_found": false, "title": null, "due_date": null}`

**示例:**

<example>
  <user_query>项目Alpha的报告什么时候要交？</user_query>
  <llm_answer>项目Alpha的报告需要在下周五（2025年6月28日）之前提交。</llm_answer>
  <assistant_json_output>
  {"task_found": true, "title": "提交项目Alpha的报告", "due_date": "2025-06-28 09:00:00"}
  </assistant_json_output>
</example>

<example>
  <user_query>去年我们公司的销售额是多少？</user_query>
  <llm_answer>去年我们公司的总销售额是500万元。</llm_answer>
  <assistant_json_output>
  {"task_found": false, "title": null, "due_date": null}
  </assistant_json_output>
</example>
"""
    
    user_content = f"""
【用户问题】: "{user_query}"
【AI的回答】: "{llm_answer}"

请根据以上对话，提取任务信息并输出JSON。
"""

    messages = [
        {"role": "system", "content": system_prompt},
        {"role": "user", "content": user_content}
    ]
    return messages


```

    |-- __init__.py

``` py

```

    |-- db_models.py

``` py
# zhz_agent/database_models.py
from sqlalchemy import Column, String, DateTime, Integer, Text, Enum as SQLAlchemyEnum, ForeignKey, Boolean, JSON
from sqlalchemy.sql import func
import uuid

# --- [修改] 从 pydantic_models 导入枚举 -> 改为绝对导入 ---
from zhz_rag.config.pydantic_models import TaskStatus, ReminderMethod

# --- [修改] 从新的 database.py 导入 Base -> 改为绝对导入 ---
from zhz_rag.utils.db_utils import Base # <--- 确保只从这里导入 Base #

class TaskDB(Base): # 命名为 TaskDB 以区分 Pydantic 的 TaskModel
    __tablename__ = "tasks"

    id = Column(String, primary_key=True, index=True, default=lambda: str(uuid.uuid4()))
    title = Column(String, index=True, nullable=False)
    description = Column(Text, nullable=True) #
    status = Column(SQLAlchemyEnum(TaskStatus), default=TaskStatus.PENDING, nullable=False) #
    created_at = Column(DateTime(timezone=True), server_default=func.now(), nullable=False) #
    updated_at = Column(DateTime(timezone=True), server_default=func.now(), onupdate=func.now(), nullable=False) #
    due_date = Column(DateTime(timezone=True), nullable=True) #
    reminder_time = Column(DateTime(timezone=True), nullable=True) #
    reminder_offset_minutes = Column(Integer, nullable=True) #
    reminder_methods = Column(JSON, default=[ReminderMethod.NOTIFICATION.value], nullable=False) #
    priority = Column(Integer, default=0, nullable=False) #
    tags = Column(JSON, default=[], nullable=False) #
    action_type = Column(String, nullable=True) #
    action_payload = Column(JSON, default={}, nullable=True) #
    execution_result = Column(Text, nullable=True) #
    last_executed_at = Column(DateTime(timezone=True), nullable=True) #

    def __repr__(self):
        return f"<TaskDB(id={self.id}, title='{self.title}', status='{self.status.value}')>"
```

    |-- jobs.py

``` py
# zhz_agent/task_jobs.py
from datetime import datetime
from typing import Dict, Any
import os
import traceback
import httpx # <--- 确保 httpx 已导入
import json # <--- 确保 json 已导入

# 从 .database 导入 database 对象以便查询任务详情
# 从 .pydantic_models 导入 TaskModel 以便类型转换
# 从 .main 导入 scheduler 以便在需要时重新调度（虽然通常作业函数不直接操作调度器）
# 更好的做法是通过参数传递必要的信息，而不是依赖全局导入
WINDOWS_HOST_IP = os.getenv("WINDOWS_HOST_IP_FOR_WSL", "192.168.3.11") # <--- 请务必替换为您真实的Windows IP
LOCAL_AGENT_PORT = os.getenv("LOCAL_AGENT_PORT", "8003") # 与 local_agent_app.py 中的端口一致

# 如果 WINDOWS_HOST_IP 仍然是占位符，给出提示
if WINDOWS_HOST_IP == "在此处填写您上一步找到的Windows主机IP":
    print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")
    print("REMINDER_JOB WARNING: WINDOWS_HOST_IP 未正确设置在 task_jobs.py 中!")
    print("请编辑 task_jobs.py 文件，将 '在此处填写您上一步找到的Windows主机IP' 替换为实际IP地址。")
    print("!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!")

LOCAL_AGENT_NOTIFY_URL = f"http://{WINDOWS_HOST_IP}:{LOCAL_AGENT_PORT}/notify"

async def send_task_reminder(task_id: str, task_title: str, reminder_methods: list):
    """
    实际发送任务提醒的函数。
    """
    print(f"REMINDER_JOB: [{datetime.utcnow()}] 正在为任务 '{task_id}' - '{task_title}' 发送提醒。")
    for method in reminder_methods:
        if method == "notification": # 假设 ReminderMethod.NOTIFICATION.value 是 "notification"
            print(f"  REMINDER_JOB: 尝试通过 Local Agent 发送桌面通知: '{task_title}'")
            try:
                async with httpx.AsyncClient(timeout=10.0) as client:
                    response = await client.post(
                        LOCAL_AGENT_NOTIFY_URL,
                        json={"title": f"任务提醒: {task_title}", "message": f"任务 '{task_title}' 即将到期或需要关注。"}
                    )
                    response.raise_for_status() # Raise an exception for bad status codes
                    print(f"  REMINDER_JOB: 本地代理通知请求发送成功. 状态: {response.status_code}")
            except httpx.RequestError as e:
                print(f"  REMINDER_JOB: 通过本地代理发送通知失败 (RequestError): {e}")
                traceback.print_exc()
            except Exception as e:
                print(f"  REMINDER_JOB: 通过本地代理发送通知失败 (General Error): {e}")
                traceback.print_exc()
        # elif method == "email": #
        #     print(f"  REMINDER_JOB: 模拟发送邮件提醒...")

async def execute_task_action(task_id: str, action_type: str, action_payload: Dict[str, Any]):
    """
    实际执行任务动作的函数。
    """
    print(f"EXECUTION_JOB: [{datetime.utcnow()}] 正在为任务 '{task_id}' 执行动作 '{action_type}'。")
    print(f"  EXECUTION_JOB: 动作参数: {action_payload}")

    final_result = f"动作 '{action_type}' 已模拟执行。"
    success = True

    if action_type == "navigate":
        destination = action_payload.get("destination")
        if destination:
            print(f"  EXECUTION_JOB: 模拟导航到 '{destination}'...")
            final_result = f"已模拟为导航到 '{destination}' 准备好路线。"
        else:
            final_result = "导航动作失败：缺少目的地。"
            success = False
    elif action_type == "log_event":
        event_details = action_payload.get("event_details", "无详情")
        print(f"  EXECUTION_JOB: 记录事件: '{event_details}'")
        final_result = f"事件 '{event_details}' 已记录。"
    else:
        final_result = f"未知的动作类型: {action_type}"
        success = False

    # 更新数据库中的任务状态和结果 (需要访问数据库)
    # 这部分逻辑最好通过API调用或服务层来完成，以避免循环导入和分散DB操作
    # 这里我们只打印信息，实际应用中需要实现DB更新
    print(f"  EXECUTION_JOB: 任务 '{task_id}' 执行完毕。结果: {final_result}, 状态: {'COMPLETED' if success else 'FAILED'}")

```

    |-- generated_data/
    |-- __init__.py

``` py

```

    |-- refine_answer_data.py

``` py
# zhz_agent/refine_answer_finetune_data.py
import json
import os
import pandas as pd
from typing import List, Dict, Any, Optional
import glob
from datetime import datetime

# 假设 utils.py 和 constants.py 在同一个 zhz_agent 包内
try:
    from zhz_rag.utils.common_utils import (
    RAG_INTERACTION_LOGS_DIR,
    EVALUATION_RESULTS_LOGS_DIR,
    FINETUNING_GENERATED_DATA_DIR,
    find_latest_rag_interaction_log # 确保这个也被导入
)
    from zhz_rag.config.pydantic_models import RetrievedDocument
    # NO_ANSWER_PHRASE_ANSWER_CLEAN 将从 llm.py 导入，或者在constants.py中定义
    # 我们需要与 llm.py -> generate_answer_from_context 一致的 "无法回答" 短语
    from zhz_rag.llm.llm_interface import NO_ANSWER_PHRASE_ANSWER_CLEAN 
except ImportError as e:
    print(f"ERROR: Could not import necessary modules for refine_answer_finetune_data: {e}")
    exit(1)

import logging

# 配置此脚本的logger
refine_answer_logger = logging.getLogger("RefineAnswerFinetuneDataLogger")
refine_answer_logger.setLevel(logging.INFO)
if not refine_answer_logger.hasHandlers():
    _console_handler = logging.StreamHandler()
    _formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
    _console_handler.setFormatter(_formatter)
    refine_answer_logger.addHandler(_console_handler)
    refine_answer_logger.info("--- RefineAnswerFinetuneDataLogger configured ---")

# --- 配置 ---
RAG_LOG_DIR = "zhz_rag/stored_data/evaluation_results_logs/"
EVAL_LOG_DIR = "zhz_rag/stored_data/evaluation_results_logs/"
FINETUNE_DATA_DIR = "zhz_rag/finetuning/generated_data/"
os.makedirs(FINETUNE_DATA_DIR, exist_ok=True)

# --- 与 run_batch_answer_evaluation.py 中类似的上下文格式化函数 ---
def format_contexts_for_prompt(context_docs_raw: List[Dict[str, Any]]) -> str:
    """
    将从日志中解析出的上下文文档列表格式化为单一字符串，用于构建LLM的输入Prompt。
    这个格式应该与 llm.py -> generate_answer_from_context 中构建上下文的方式一致。
    """
    context_strings_for_llm = []
    if not context_docs_raw:
        return "No context provided."
        
    for i, doc_data in enumerate(context_docs_raw):
        try:
            # 尝试使用RetrievedDocument模型解析，如果原始日志中已经是这个结构
            # 但通常日志中可能是字典列表
            doc_content = doc_data.get("content", "[Content not available]")
            doc_source = doc_data.get("source_type", "unknown_source")
            doc_score = doc_data.get("score")
            
            # 与 rag_service.py 中准备上下文给LLM的格式保持一致
            # 在 rag_service.py 中是:
            # f"Source Type: {doc.source_type}, Score: {doc.score:.4f}\nContent: {doc.content}"
            # 我们这里也尽量模拟，但日志中的score可能不存在或格式不同
            header = f"Source Type: {doc_source}"
            if doc_score is not None:
                try:
                    header += f", Score: {float(doc_score):.4f}"
                except ValueError:
                    header += f", Score: {doc_score}" # 如果分数不是数字，直接用原始值
            
            context_strings_for_llm.append(f"{header}\nContent: {doc_content}")

        except Exception as e:
            refine_answer_logger.warning(f"Could not parse a context document fully for prompt: {doc_data}. Error: {e}")
            content = doc_data.get("content", "[Content not available]")
            context_strings_for_llm.append(f"Content: {content}") # 简化版

    return "\n\n---\n\n".join(context_strings_for_llm) if context_strings_for_llm else "No context provided."


def construct_qwen_answer_input_prompt(user_question: str, formatted_context: str) -> str:
    """
    根据用户问题和格式化的上下文构建Qwen生成答案时的完整输入Prompt。
    这个函数必须与 llm.py 中 generate_answer_from_context 内部构建Prompt的逻辑完全一致。
    """
    # --- 从 llm.py 的 generate_answer_from_context 函数复制并粘贴完整的 prompt 模板 ---
    # 注意：这里需要确保模板与 llm.py 中的完全一致
    prompt = f"""
<|im_start|>system
你是一个AI问答助手。你的任务是根据【上下文信息】回答【用户问题】。

**核心指令：**

1.  **尝试直接回答：** 请首先仔细阅读【上下文信息】，如果其中包含能直接回答【用户问题】的内容，请用上下文中的信息直接、简洁地回答。
2.  **忠实原文：** 你的回答必须严格基于【上下文信息】，禁止加入任何外部知识或个人观点。
3.  **如果无法回答：** 如果你分析了【上下文信息】后，确认其中确实没有能回答【用户问题】的明确信息，那么请只回答以下这句话：
    "根据目前提供的资料，我无法找到关于您问题的明确信息。"
    **不要添加任何其他解释、建议或反问。**

**请直接给出答案，或者只给出上述那句固定的“无法找到信息”的回复。**
<|im_start|>user
用户问题: {user_question}

上下文信息:
{formatted_context}
<|im_end|>
<|im_start|>assistant
"""
    return prompt

def load_logs_to_dict(filepath: str, key_field: str = "interaction_id") -> Dict[str, Dict[str, Any]]:
    """将JSONL文件加载到一个以指定字段为键的字典中。"""
    data_dict = {}
    if not os.path.exists(filepath):
        refine_answer_logger.error(f"Log file not found: {filepath}")
        return data_dict
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                log_entry = json.loads(line.strip())
                if key_field in log_entry and log_entry[key_field]: # 确保key_field的值不是None或空
                    data_dict[log_entry[key_field]] = log_entry
                elif key_field == "original_interaction_id_ref" and log_entry.get("original_interaction_id_ref"):
                    data_dict[log_entry["original_interaction_id_ref"]] = log_entry
            except json.JSONDecodeError:
                refine_answer_logger.warning(f"Skipping malformed JSON line in {filepath}")
    return data_dict

def generate_finetune_samples_for_answer(
    rag_interaction_logs: Dict[str, Dict[str, Any]],
    answer_evaluation_logs: Dict[str, Dict[str, Any]]
) -> List[Dict[str, str]]:
    finetune_samples = []
    processed_ids = set()

    refine_answer_logger.info(f"Processing {len(rag_interaction_logs)} RAG interaction logs and {len(answer_evaluation_logs)} Answer evaluation logs.")

    for interaction_id, rag_log in rag_interaction_logs.items():
        if rag_log.get("task_type") != "rag_query_processing_success":
            continue

        if interaction_id in processed_ids:
            continue
        processed_ids.add(interaction_id)

        user_question = rag_log.get("user_query")
        qwen_generated_answer_raw = rag_log.get("processed_llm_output") # Qwen的原始答案
        # retrieved_context_docs 在 rag_log 中可能是 "retrieved_context_docs" 或 "retrieved_documents_summary"
        # 我们需要原始的、完整的上下文文档
        retrieved_context_docs_raw = rag_log.get("retrieved_context_docs") 
        
        if not retrieved_context_docs_raw and rag_log.get("debug_info"): # 尝试从debug_info获取
             retrieved_context_docs_raw = rag_log.get("debug_info",{}).get("retrieved_context_docs")


        if qwen_generated_answer_raw is None or not qwen_generated_answer_raw.strip():
            qwen_generated_answer = NO_ANSWER_PHRASE_ANSWER_CLEAN # 空答案视为无法回答
        else:
            qwen_generated_answer = qwen_generated_answer_raw.strip()

        if not user_question or not retrieved_context_docs_raw:
            refine_answer_logger.warning(f"Skipping RAG log {interaction_id} due to missing user_question or retrieved_context_docs.")
            continue
        
        # 构建Prompt
        formatted_contexts_for_prompt = format_contexts_for_prompt(retrieved_context_docs_raw)
        qwen_answer_input_prompt = construct_qwen_answer_input_prompt(user_question, formatted_contexts_for_prompt)

        ideal_answer_output = None
        source_of_ideal = "unknown"
        gemini_scores_for_log = {}

        eval_log = answer_evaluation_logs.get(interaction_id)

        if eval_log and eval_log.get("eval_llm_processed_output_json"):
            eval_json = eval_log["eval_llm_processed_output_json"]
            summary_eval = eval_json.get("evaluation_summary", {})
            dimensions_eval = eval_json.get("dimensions", {})
            
            overall_score_str = summary_eval.get("overall_answer_quality_score")
            faithfulness_score_str = dimensions_eval.get("faithfulness", {}).get("score")
            relevance_score_str = dimensions_eval.get("relevance", {}).get("score")
            completeness_score_str = dimensions_eval.get("completeness", {}).get("score")
            context_sufficiency = dimensions_eval.get("completeness", {}).get("context_sufficiency_assessment", "Unknown")
            gemini_suggestion_answer = eval_json.get("suggestion_for_answer_improvement", "").strip()

            try:
                overall_score = int(overall_score_str) if overall_score_str is not None else 0
                faithfulness_score = int(faithfulness_score_str) if faithfulness_score_str is not None else 0
                relevance_score = int(relevance_score_str) if relevance_score_str is not None else 0
                completeness_score = int(completeness_score_str) if completeness_score_str is not None else 0
                gemini_scores_for_log = {
                    "overall": overall_score,
                    "faithfulness": faithfulness_score,
                    "relevance": relevance_score,
                    "completeness": completeness_score,
                    "context_sufficiency": context_sufficiency
                }
            except (ValueError, TypeError) as e:
                refine_answer_logger.warning(f"Could not parse one or more scores for {interaction_id}: {e}")
                overall_score = faithfulness_score = relevance_score = completeness_score = 0
                gemini_scores_for_log = { # 记录解析失败
                    "overall": "parse_error", "faithfulness": "parse_error", 
                    "relevance": "parse_error", "completeness": "parse_error",
                    "context_sufficiency": context_sufficiency
                }
                
            # --- Completion选择逻辑 (改进版) ---
            ideal_answer_output = None # 重新初始化
            source_of_ideal = "unknown" # 重新初始化

            # 规则 0: Qwen的原始答案就是标准的“无法回答”短语
            is_qwen_standard_no_answer = (qwen_generated_answer == NO_ANSWER_PHRASE_ANSWER_CLEAN)

            # 规则 1: 上下文不足，且Qwen正确地给出了标准的“无法回答”
            if is_qwen_standard_no_answer and \
               context_sufficiency == "Insufficient" and \
               overall_score >= 4: # Gemini认为Qwen的这个“无法回答”是高质量的
                ideal_answer_output = NO_ANSWER_PHRASE_ANSWER_CLEAN
                source_of_ideal = "qwen_standard_no_answer_confirmed_by_gemini_context_insufficient"
            
            # 规则 2: 上下文不足，Qwen可能没有给出标准“无法回答”，但Gemini建议应指出信息不足
            elif not is_qwen_standard_no_answer and \
                 context_sufficiency == "Insufficient" and \
                 completeness_score <=2 and \
                 ("information is not available" in gemini_suggestion_answer.lower() or \
                  "context does not contain" in gemini_suggestion_answer.lower() or \
                  "cannot be answered from the context" in gemini_suggestion_answer.lower() or \
                  "should state that the information is not found" in gemini_suggestion_answer.lower()):
                ideal_answer_output = NO_ANSWER_PHRASE_ANSWER_CLEAN
                source_of_ideal = "gemini_suggests_no_answer_due_to_insufficient_context"

            # 规则 3: Gemini 整体评分很高 (例如 overall, faithfulness, relevance 都 >= 4)
            # 并且 Qwen 的答案不是标准的“无法回答”（如果已经是，则由规则1处理）
            elif not is_qwen_standard_no_answer and \
                 overall_score >= 4 and faithfulness_score >= 4 and relevance_score >= 4:
                ideal_answer_output = qwen_generated_answer
                source_of_ideal = "qwen_high_score_by_gemini"
                # 如果此时 Gemini 仍有改进建议，可以额外标记
                if gemini_suggestion_answer and \
                   gemini_suggestion_answer != "No improvement needed." and \
                   "suggestion" not in source_of_ideal: # 避免重复标记
                    source_of_ideal += "_with_minor_gemini_suggestion"


            # 规则 4: Qwen的答案评分不高，但Gemini给出了具体的改进建议
            # 我们将这类样本标记出来，completion暂时使用Qwen的答案，供人工审核和优化
            elif overall_score < 4 and \
                 gemini_suggestion_answer and \
                 gemini_suggestion_answer != "No improvement needed." and \
                 len(gemini_suggestion_answer) > 10: # 假设太短的建议可能不具体
                ideal_answer_output = qwen_generated_answer # 保留Qwen答案作为基础
                source_of_ideal = "qwen_low_score_with_gemini_suggestion_for_review"
                refine_answer_logger.info(f"Answer log {interaction_id} (Qwen: '{qwen_generated_answer[:100]}...') marked for review due to low score but has Gemini suggestion: '{gemini_suggestion_answer[:100]}...'")
            
            # 规则 5: 如果Qwen的答案是标准“无法回答”，但上下文其实是充分的，或者Gemini认为可以回答
            # 这通常意味着Qwen可能错误地判断无法回答，或者Gemini的评估与Qwen的判断不一致
            elif is_qwen_standard_no_answer and \
                 (context_sufficiency == "Sufficient" or (context_sufficiency == "Partially Sufficient" and completeness_score >=3)) and \
                 overall_score < 4 : # Gemini不认可这个“无法回答”
                ideal_answer_output = qwen_generated_answer # 保留Qwen的“无法回答”
                source_of_ideal = "qwen_no_answer_but_gemini_disagrees_or_context_sufficient_for_review"
                refine_answer_logger.info(f"Answer log {interaction_id}: Qwen said 'no answer', but Gemini scores/context sufficiency suggest it might be answerable. Marked for review. Gemini scores: {gemini_scores_for_log}, Suggestion: '{gemini_suggestion_answer[:100]}...'")

            # 规则 6: 其他所有情况，暂时跳过，等待更明确的规则或人工审核
            else:
                refine_answer_logger.info(f"Answer log {interaction_id} (Qwen: '{qwen_generated_answer[:100]}...') did not meet current finetune criteria. Needs manual review or rule adjustment. Gemini scores: {gemini_scores_for_log}, Suggestion: '{gemini_suggestion_answer[:100]}...'")
                continue

        else: # 没有有效的Gemini评估日志
            refine_answer_logger.warning(f"No valid Gemini evaluation found for Answer log {interaction_id}. Qwen's output: '{qwen_generated_answer[:100]}...'. Skipping for finetune data.")
            continue
            
        if ideal_answer_output is not None:
            finetune_samples.append({
                "prompt": qwen_answer_input_prompt,
                "completion": ideal_answer_output.strip(),
                "original_qwen_answer": qwen_generated_answer_raw.strip() if qwen_generated_answer_raw else NO_ANSWER_PHRASE_ANSWER_CLEAN, # 记录Qwen最原始的输出
                "gemini_scores": gemini_scores_for_log,
                "gemini_suggestion": gemini_suggestion_answer if eval_log and eval_log.get("eval_llm_processed_output_json") else None,
                "source_of_ideal": source_of_ideal,
                "interaction_id": interaction_id
            })

    refine_answer_logger.info(f"Generated {len(finetune_samples)} Answer finetuning samples.")
    return finetune_samples


if __name__ == "__main__":
    rag_log_file = find_latest_rag_interaction_log(RAG_INTERACTION_LOGS_DIR)
    
    eval_log_file = None
    if rag_log_file:
        rag_log_basename = os.path.basename(rag_log_file)
        date_str_match = "".join(filter(str.isdigit, rag_log_basename))
        if len(date_str_match) >= 8:
            date_str = date_str_match[:8]
            evaluation_name = "answer_gemini_flash" # 与 evaluation.py 中一致
            eval_file_name = f"eval_results_{evaluation_name}_{date_str}.jsonl"
            eval_log_file = os.path.join(EVAL_LOG_DIR, eval_file_name)
            refine_answer_logger.info(f"Attempting to load Answer evaluation results from: {eval_log_file}")
        else:
            refine_answer_logger.error(f"Could not reliably extract date from RAG log filename: {rag_log_basename}")

    if not rag_log_file or not eval_log_file or not os.path.exists(eval_log_file):
        refine_answer_logger.error("Required log files for answer finetune data generation not found. Exiting.")
    else:
        rag_interactions = load_logs_to_dict(rag_log_file, key_field="interaction_id")
        answer_evaluations = load_logs_to_dict(eval_log_file, key_field="original_interaction_id_ref")

        if rag_interactions and answer_evaluations:
            finetune_data = generate_finetune_samples_for_answer(rag_interactions, answer_evaluations)
            
            if finetune_data:
                today_for_filename = datetime.now().strftime("%Y%m%d")
                output_filepath = os.path.join(FINETUNING_GENERATED_DATA_DIR, f"answer_finetune_samples_{today_for_filename}.jsonl")
                
                with open(output_filepath, 'w', encoding='utf-8') as f_out:
                    for sample in finetune_data:
                        f_out.write(json.dumps(sample, ensure_ascii=False) + "\n")
                refine_answer_logger.info(f"Successfully saved {len(finetune_data)} Answer finetuning samples to: {output_filepath}")
                
                try:
                    df = pd.DataFrame(finetune_data)
                    csv_output_filepath = os.path.join(FINETUNING_GENERATED_DATA_DIR, f"answer_finetune_samples_review_{today_for_filename}.csv")
                    df.to_csv(csv_output_filepath, index=False, encoding='utf-8-sig')
                    refine_answer_logger.info(f"Reviewable CSV for answers saved to: {csv_output_filepath}")
                except Exception as e_csv:
                    refine_answer_logger.error(f"Failed to save answer review CSV: {e_csv}")
            else:
                refine_answer_logger.info("No answer finetuning samples were generated.")
        else:
            refine_answer_logger.error("Failed to load data from log files for answer finetuning.")
```

    |-- refine_cypher_data.py

``` py
# zhz_agent/refine_cypher_finetune_data.py
import json
import os
import pandas as pd
from typing import List, Dict, Any, Optional, Tuple
import glob
from datetime import datetime

# 假设 utils.py 和 constants.py 在同一个 zhz_agent 包内
try:
    from zhz_rag.utils.common_utils import get_interaction_log_filepath, get_evaluation_result_log_filepath, find_latest_rag_interaction_log # <--- 修改这里
    from zhz_rag.config.constants import NEW_KG_SCHEMA_DESCRIPTION
except ImportError as e:
    print(f"ERROR: Could not import necessary modules: {e}")
    # ... (错误处理)
    exit(1)

import logging

# 配置此脚本的logger
refine_logger = logging.getLogger("RefineFinetuneDataLogger")
refine_logger.setLevel(logging.INFO)
if not refine_logger.hasHandlers():
    _console_handler = logging.StreamHandler()
    _formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
    _console_handler.setFormatter(_formatter)
    refine_logger.addHandler(_console_handler)
    refine_logger.info("--- RefineFinetuneDataLogger configured ---")

# --- 配置 ---
# 原始RAG交互日志的目录 (包含cypher_generation类型)
RAG_LOG_DIR = "zhz_rag/stored_data/rag_interaction_logs/"
# Gemini评估结果日志的目录
EVAL_LOG_DIR = "zhz_rag/stored_data/evaluation_results_logs/"
# 输出微调数据文件的目录
FINETUNE_DATA_DIR = "zhz_rag/finetuning/generated_data/"
os.makedirs(FINETUNE_DATA_DIR, exist_ok=True)


def load_logs_to_dict(filepath: str, key_field: str = "interaction_id") -> Dict[str, Dict[str, Any]]:
    """将JSONL文件加载到一个以指定字段为键的字典中。"""
    data_dict = {}
    if not os.path.exists(filepath):
        refine_logger.error(f"Log file not found: {filepath}")
        return data_dict
    with open(filepath, 'r', encoding='utf-8') as f:
        for line in f:
            try:
                log_entry = json.loads(line.strip())
                if key_field in log_entry:
                    data_dict[log_entry[key_field]] = log_entry
                # 对于评估日志，我们可能需要用 original_interaction_id_ref 作为键
                elif key_field == "original_interaction_id_ref" and log_entry.get("original_interaction_id_ref"):
                    data_dict[log_entry["original_interaction_id_ref"]] = log_entry
            except json.JSONDecodeError:
                refine_logger.warning(f"Skipping malformed JSON line in {filepath}")
    return data_dict

def construct_qwen_input_prompt(user_question: str, schema_description: str) -> str:
    """
    根据用户问题和Schema描述构建Qwen生成Cypher时的完整输入Prompt。
    这个函数应该与 llm.py 中 generate_cypher_query 内部构建Prompt的逻辑一致。
    """
    # 这是我们在 llm.py -> generate_cypher_query 中使用的Prompt模板
    # 我们需要确保这里的模板与Qwen实际接收到的一致
    # 注意：这里使用了最新的V7版本（或您当前使用的版本）的Schema描述作为基础
    # 如果您的 generate_cypher_query 中的模板不同，请相应调整
    prompt = f"""<|im_start|>system
你是顶级Neo4j Cypher查询生成专家。你的任务是根据用户问题和严格提供的【知识图谱Schema】，生成一个【语法正确】、【逻辑合理】且【高效】的Cypher查询。

**【核心指令与约束 - 必须严格遵守！】**

1.  **【Schema绝对绑定 - 最高优先级】**:
    *   你生成的Cypher查询中所有用到的【节点标签】、【关系类型】、【属性名称】及其对应的【数据类型】，都**必须严格存在于**下面提供的 "知识图谱Schema描述" 中。
    *   在构建查询的每一步，都要反复与Schema核对。**严禁臆断、猜测或使用任何Schema中未明确定义的元素。**
    *   **属性名称的大小写和确切拼写必须与Schema完全一致。**
    *   **关系类型的名称和方向必须与Schema完全一致。** 例如，如果Schema定义为 `(Person)-[:WORKS_ON]->(Project)`，则查询中不能是 `(Project)-[:WORKS_ON]->(Person)`，除非Schema中也定义了反向关系。

2.  **【纯净输出格式 - 严格要求】**:
    *   如果能生成有效查询，你的回答**必须只包含纯粹的Cypher查询语句本身**。
    *   如果根据问题和Schema无法生成有效的Cypher查询（例如，问题超出了Schema表达能力，或问题本身逻辑不通），则**必须只输出固定的短语：“无法生成Cypher查询。”**
    *   **绝对禁止**在有效的Cypher语句前后添加任何前缀（如“Cypher查询: ”）、后缀、解释、注释、markdown标记（如 ```cypher ```）或任何其他多余的文本。

3.  **【属性与值的使用】**:
    *   当在`WHERE`子句中对属性进行匹配时，确保值的类型与Schema中定义的属性类型一致。例如，如果`name`是字符串，则匹配 `name: '张三'`；如果`year`是数字，则匹配 `year: 2023`。
    *   对于数值计算（如`SUM`, `AVG`），**必须**使用Schema中明确指定的数字类型属性（例如，`SalesAmount`节点的 `numeric_amount`）。

4.  **【查询构建逻辑指引】**:
    *   **实体识别**: 准确识别用户问题中的核心实体及其在Schema中对应的节点标签和属性。
    *   **关系路径**: 基于问题和Schema构建清晰的`MATCH`路径。
    *   **条件过滤**: 使用`WHERE`子句添加必要的过滤条件。
    *   **结果返回**: 使用`RETURN`子句指定需要返回的信息，并用`AS`为返回的列指定清晰、合法的别名（字母或下划线开头）。
    *   **多步查询**: 对于需要关联多个信息点的问题，合理使用`WITH`传递中间结果。
    *   **聚合**: 如需统计或汇总，正确使用`COUNT()`, `SUM()`, `COLLECT()`等聚合函数。

**【知识图谱Schema描述】**:
{schema_description}

**【查询示例 - 严格基于上述Schema】**:

*   用户问题: "张三参与了哪个项目？"
    Cypher查询: MATCH (p:Person {{name: '张三'}})-[:WORKS_ON]->(proj:Project) RETURN proj.name AS projectName

*   用户问题: "华东区域2024年第一季度的销售额是多少？"
    Cypher查询: MATCH (r:Region {{name: '华东'}})-[:HAS_SALES_AMOUNT]->(sa:SalesAmount {{period: '2024年第一季度'}}) RETURN sa.numeric_amount AS salesAmount, sa.unit AS salesUnit

*   用户问题: "查询所有产品的名称。"
    Cypher查询: MATCH (prod:Product) RETURN prod.name AS productName

*   用户问题: "项目X有哪些人参与？"
    Cypher查询: MATCH (p:Person)-[:WORKS_ON]->(proj:Project {{name: '项目X'}}) RETURN p.name AS participantName

*   用户问题: "2024年第一季度所有区域的总销售额是多少？"
    Cypher查询: MATCH (r:Region)-[:HAS_SALES_AMOUNT]->(sa:SalesAmount {{period: '2024年第一季度'}}) RETURN sum(sa.numeric_amount) AS totalSales, sa.unit AS commonUnit LIMIT 1 
    (此查询假设所有相关销售额的单位是相同的，并取第一个出现的单位作为代表)

*   用户问题: "与新产品A相关的文档ID是什么？"
    Cypher查询: MATCH (p:Product {{name: '新产品A'}})-[:RELATED_TO]->(d:Document) RETURN d.id AS documentId

*   用户问题: "公司CEO是谁？" (假设Schema中没有CEO信息)
    Cypher查询: 无法生成Cypher查询。

现在，请根据以下用户问题和上述Schema及规则生成Cypher查询。
<|im_end|>
<|im_start|>user
用户问题: {user_question}
<|im_end|>
<|im_start|>assistant
"""
    return prompt

def generate_finetune_samples_for_cypher(
    rag_interaction_logs: Dict[str, Dict[str, Any]],
    cypher_evaluation_logs: Dict[str, Dict[str, Any]]
) -> List[Dict[str, str]]:
    """
    根据原始交互日志和Gemini评估日志，生成用于Cypher微调的样本。
    返回一个列表，每个元素是 {"prompt": "...", "completion": "..."}
    """
    finetune_samples = []
    processed_ids = set()

    refine_logger.info(f"Processing {len(rag_interaction_logs)} RAG interaction logs and {len(cypher_evaluation_logs)} Cypher evaluation logs.")

    for interaction_id, rag_log in rag_interaction_logs.items():
        if rag_log.get("task_type") != "cypher_generation":
            continue

        if interaction_id in processed_ids:
            continue
        processed_ids.add(interaction_id)

        user_question = rag_log.get("user_query")
        qwen_generated_cypher_raw = rag_log.get("processed_llm_output") # 这是Qwen原始输出

        # --- 改进点: 处理Qwen输出为空或仅包含空白的情况 ---
        if qwen_generated_cypher_raw is None or not qwen_generated_cypher_raw.strip():
            qwen_generated_cypher = "无法生成Cypher查询." # 将空输出也视为无法生成
            refine_logger.info(f"Interaction {interaction_id}: Qwen output was empty/None, treating as '无法生成Cypher查询.'.")
        else:
            qwen_generated_cypher = qwen_generated_cypher_raw.strip()


        qwen_input_prompt = rag_log.get("llm_input_prompt")
        if not qwen_input_prompt:
            if user_question:
                qwen_input_prompt = construct_qwen_input_prompt(user_question, NEW_KG_SCHEMA_DESCRIPTION)
            else:
                refine_logger.warning(f"Skipping Cypher log {interaction_id} due to missing user_question for prompt reconstruction.")
                continue
        
        if not user_question: # qwen_generated_cypher 已确保非None
            refine_logger.warning(f"Skipping Cypher log {interaction_id} due to missing user_question.")
            continue

        ideal_cypher_output = None
        source_of_ideal = "unknown"
        gemini_score_for_log = None # 用于记录

        eval_log = cypher_evaluation_logs.get(interaction_id)

        if eval_log and eval_log.get("eval_llm_processed_output_json"):
            eval_json = eval_log["eval_llm_processed_output_json"]
            overall_score_str = eval_json.get("evaluation_summary", {}).get("overall_quality_score_cypher")
            gemini_suggestion_raw = eval_json.get("suggestion_for_improvement_cypher", "").strip()
            
            try:
                overall_score = int(overall_score_str)
                gemini_score_for_log = overall_score
            except (ValueError, TypeError):
                refine_logger.warning(f"Could not parse overall_quality_score_cypher for {interaction_id}: {overall_score_str}")
                overall_score = 0 # 默认给个低分
                gemini_score_for_log = 0

            # --- 规则1: Qwen自己就说无法生成 ---
            if qwen_generated_cypher == "无法生成Cypher查询.":
                # 如果Gemini也认为无法生成或评分低，那么采纳
                if "无法生成Cypher查询" in gemini_suggestion_raw or overall_score <= 2:
                    ideal_cypher_output = "无法生成Cypher查询."
                    source_of_ideal = "qwen_and_gemini_cannot_generate"
                # 如果Qwen说无法生成，但Gemini给出了高分建议，这很奇怪，需要人工看
                elif overall_score >=4 and "MATCH" in gemini_suggestion_raw.upper():
                     refine_logger.info(f"Cypher log {interaction_id}: Qwen said '无法生成', but Gemini suggested a high-score query '{gemini_suggestion_raw}'. Needs manual review.")
                     continue
                else: # Qwen说无法生成，Gemini建议不明确或中低分，也采纳Qwen的
                    ideal_cypher_output = "无法生成Cypher查询."
                    source_of_ideal = "qwen_cannot_generate_gemini_unclear"


            # --- 规则2: Qwen生成了查询，看Gemini评估 ---
            elif overall_score >= 4: # Gemini认为Qwen的输出质量高
                ideal_cypher_output = qwen_generated_cypher
                source_of_ideal = "qwen_high_score_by_gemini"

            
            # --- 规则3: Qwen的查询质量不高，但Gemini给出了具体的、看起来像Cypher的建议 ---
            elif gemini_suggestion_raw and \
                "无法生成Cypher查询" not in gemini_suggestion_raw and \
                "cannot be improved" not in gemini_suggestion_raw.lower() and \
                "needs to be extended" not in gemini_suggestion_raw.lower() and \
                ("MATCH " in gemini_suggestion_raw.upper() or \
                    "RETURN " in gemini_suggestion_raw.upper() or \
                    "CREATE " in gemini_suggestion_raw.upper() or \
                    "MERGE " in gemini_suggestion_raw.upper() or \
                    "WITH " in gemini_suggestion_raw.upper() or \
                    "OPTIONAL MATCH " in gemini_suggestion_raw.upper()
                ):

                # 简化处理：直接将 Gemini 的原始建议作为 completion 的候选
                # 清洗工作主要交给人工审核阶段
                # 我们仍然可以做非常基础的清理，比如首尾空格和常见的 markdown
                
                temp_completion = gemini_suggestion_raw.strip()
                if temp_completion.startswith("```") and temp_completion.endswith("```"):
                    temp_completion = temp_completion[3:-3].strip()
                    if temp_completion.lower().startswith("cypher"):
                        temp_completion = temp_completion[len("cypher"):].strip()
                elif temp_completion.startswith("`") and temp_completion.endswith("`"):
                    temp_completion = temp_completion[1:-1].strip()

                # 只要建议中包含核心Cypher关键字，我们就认为它有价值被审核
                core_cypher_keywords_check = ["MATCH", "RETURN", "CREATE", "MERGE", "WITH", "OPTIONAL MATCH"]
                suggestion_contains_cypher_keyword = False
                if temp_completion:
                    for core_keyword in core_cypher_keywords_check:
                        if core_keyword in temp_completion.upper():
                            suggestion_contains_cypher_keyword = True
                            break
                
                if suggestion_contains_cypher_keyword:
                    ideal_cypher_output = temp_completion # 使用初步清理后的建议
                    source_of_ideal = "gemini_suggestion_for_review" # 明确标记为需要审核
                    refine_logger.info(f"Interaction {interaction_id}: Gemini suggestion adopted for review. Raw: '{gemini_suggestion_raw[:150]}...', Processed for completion: '{ideal_cypher_output[:150]}...'")
                else:
                    refine_logger.warning(f"Interaction {interaction_id}: Gemini suggestion '{gemini_suggestion_raw[:150]}...' did not appear to contain core Cypher keywords after basic cleaning. Skipping.")
                    continue

            
            # --- 规则4: Gemini明确建议“无法生成” 或 Qwen的查询质量低且有严重问题 ---
            elif "无法生成Cypher查询" in gemini_suggestion_raw or \
                 (overall_score <= 2 and ("hallucinated" in eval_log.get("eval_llm_raw_output", "").lower() or \
                                         "schema violation" in eval_log.get("eval_llm_raw_output", "").lower() or \
                                         "syntax error" in eval_log.get("eval_llm_raw_output", "").lower())):
                ideal_cypher_output = "无法生成Cypher查询."
                source_of_ideal = "gemini_explicitly_cannot_generate_or_qwen_low_quality"
            
            # --- 规则5: 其他情况，需要人工审核 ---
            else:
                refine_logger.info(f"Cypher log {interaction_id} (Qwen: '{qwen_generated_cypher[:100]}...') needs manual review. Gemini score: {overall_score}, Suggestion: '{gemini_suggestion_raw[:100]}...'")
                continue 
        
        # --- 如果没有Gemini评估日志 ---
        else:
            refine_logger.warning(f"No valid Gemini evaluation found for Cypher log {interaction_id}. Qwen's output: '{qwen_generated_cypher[:100]}...'. Skipping for finetune data.")
            continue
            
        if ideal_cypher_output is not None:
            finetune_samples.append({
                "prompt": qwen_input_prompt,
                "completion": ideal_cypher_output.strip(), # 确保completion也strip
                "original_qwen_cypher": qwen_generated_cypher,
                "gemini_score": gemini_score_for_log,
                "source_of_ideal": source_of_ideal,
                "interaction_id": interaction_id
            })

    refine_logger.info(f"Generated {len(finetune_samples)} Cypher finetuning samples.")
    return finetune_samples


if __name__ == "__main__":
    # 1. 确定要处理的原始RAG交互日志文件 (包含cypher_generation)
    #    和对应的Gemini评估结果日志文件 (包含cypher_evaluation_result)
    
    # 自动查找最新的原始RAG交互日志
    rag_log_file = find_latest_rag_interaction_log(RAG_LOG_DIR) # utils.py中的函数
    
    # 构造对应的Gemini Cypher评估结果文件名
    # 假设评估文件名与原始日志文件名日期部分相同，且评估类型固定
    eval_log_file = None
    if rag_log_file:
        rag_log_basename = os.path.basename(rag_log_file)
        date_str_match = "".join(filter(str.isdigit, rag_log_basename)) # 提取文件名中的日期部分
        if len(date_str_match) >= 8: # 确保提取到至少YYYYMMDD
            date_str = date_str_match[:8]

            # 根据 evaluation.py 中 log_interaction_data 的 evaluation_name_for_file 参数构造
            evaluation_name = "cypher_gemini_flash" 
            eval_file_name = f"eval_results_{evaluation_name}_{date_str}.jsonl"
            
            eval_log_file = os.path.join(EVAL_LOG_DIR, eval_file_name)
            refine_logger.info(f"Attempting to load Cypher evaluation results from: {eval_log_file}")
        else:
            refine_logger.error(f"Could not reliably extract date from RAG log filename: {rag_log_basename}")
    
    if not rag_log_file or not eval_log_file or not os.path.exists(eval_log_file):
        refine_logger.error("Required log files not found. Exiting.")
        if not rag_log_file: refine_logger.error(f"RAG interaction log missing (expected pattern rag_interactions_*.jsonl in {RAG_LOG_DIR})")
        if rag_log_file and (not eval_log_file or not os.path.exists(eval_log_file)): refine_logger.error(f"Cypher evaluation result log missing (expected: {eval_log_file})")
    else:
        rag_interactions = load_logs_to_dict(rag_log_file, key_field="interaction_id")
        cypher_evaluations = load_logs_to_dict(eval_log_file, key_field="original_interaction_id_ref")

        if rag_interactions and cypher_evaluations:
            finetune_data = generate_finetune_samples_for_cypher(rag_interactions, cypher_evaluations)
            
            if finetune_data:
                # 获取当前日期用于文件名
                today_for_filename = datetime.now().strftime("%Y%m%d")
                output_filepath = os.path.join(FINETUNE_DATA_DIR, f"cypher_finetune_samples_{today_for_filename}.jsonl")
                
                with open(output_filepath, 'w', encoding='utf-8') as f_out:
                    for sample in finetune_data:
                        f_out.write(json.dumps(sample, ensure_ascii=False) + "\n")
                refine_logger.info(f"Successfully saved {len(finetune_data)} Cypher finetuning samples to: {output_filepath}")
                
                # 也可以同时保存一个CSV版本供人工审查
                try:
                    df = pd.DataFrame(finetune_data)
                    csv_output_filepath = os.path.join(FINETUNE_DATA_DIR, f"cypher_finetune_samples_review_{today_for_filename}.csv")
                    df.to_csv(csv_output_filepath, index=False, encoding='utf-8-sig')
                    refine_logger.info(f"Reviewable CSV saved to: {csv_output_filepath}")
                except Exception as e_csv:
                    refine_logger.error(f"Failed to save review CSV: {e_csv}")
            else:
                refine_logger.info("No finetuning samples were generated.")
        else:
            refine_logger.error("Failed to load data from log files.")
```

        |-- answer_finetune_samples_20250531.jsonl
        |-- answer_finetune_samples_review_20250531.csv
        |-- cypher_finetune_samples_20250531.jsonl
        |-- cypher_finetune_samples_review_20250531.csv
    |-- bm25_index/
    |-- chromadb_index/
    |-- evaluation_results_logs/
    |-- rag_interaction_logs/
    |-- __init__.py

``` py

```

        |-- rag_interactions_20250619.jsonl
        |-- rag_interactions_20250620.jsonl
        |-- rag_interactions_20250621.jsonl
        |-- eval_results_answer_gemini_flash_20250621.jsonl
        |-- abef57e9-85f2-45a6-88f2-96578bb2b80f/
        |-- chroma.sqlite3
            |-- data_level0.bin
            |-- header.bin
            |-- index_metadata.pickle
            |-- length.bin
            |-- link_lists.bin
        |-- data.csc.index.npy
        |-- doc_ids.pkl
        |-- indices.csc.index.npy
        |-- indptr.csc.index.npy
        |-- params.index.json

``` json
{
    "k1": 1.5,
    "b": 0.75,
    "delta": 0.5,
    "method": "lucene",
    "idf_method": "lucene",
    "dtype": "float32",
    "int_dtype": "int32",
    "num_docs": 46417,
    "version": "0.2.13",
    "backend": "numpy"
}
```

        |-- vocab.index.json

``` json
{"介绍性":0,"2526":1,"205652":2,"work":3,"conducts":4,"grandma":5,"PositionalEncoding":6,"303":7,"tual":8,"Low":9,"repaid":10,"reuse":11,"Fidler":12,"do":13,"Ohta":14,"showing":15,"permutations":16,"regulators":17,"realism":18,"113":19,"classification81":20,"F3":21,"destined":22,"JavaScript":23,"Revisited":24,"lead":25,"07630":26,"authoritative":27,"Vishwesh":28,"1081":29,"42069":30,"1404":31,"Toy":32,"974":33,"443":34,"ours":35,"proceeds":36,"HybridSequential":37,"Garipov":38,"cost":39,"graph":40,"€":41,"系统":42,"drink":43,"Employee":44,"K520":45,"commitment":46,"advertising":47,"gradually":48,"541":49,"me":50,"Moritz":51,"gpu2":52,"contrary":53,"relational":54,"Decaf":55,"Recursive":56,"RETGEN":57,"irrespective":58,"488018":59,"shipped":60,"Ghazvininejad":61,"69e":62,"REVEAL":63,"rewarding":64,"remind":65,"digging":66,"sports":67,"Grefenstette":68,"Discussions128":69,"Letting":70,"thermodynamic":71,"it":72,"IJCNLP":73,"ponder":74,"uncontested":75,"constructor":76,"x2":77,"optimiza":78,"Mechanical":79,"speaker":80,"0.850":81,"6482":82,"Kadavath":83,"disclosures":84,"Huang":85,"uwsd":86,"550000":87,"Master":88,"diag":89,"alxnorden":90,"xix":91,"Density":92,"AdaptiveRAG":93,"Miglani":94,"11206":95,"080567":96,"插入":97,"页码":98,"VOCdevkit":99,"usage":100,"Abstract":101,"image":102,"Polynomial":103,"1052":104,"av":105,"Save":106,"\u0012":107,"Allergan":108,"understand":109,"none":110,"383":111,"Khvedchenya":112,"cool":113,"expressed":114,"deduction":115,"plateaus":116,"Cucerzan":117,"Rare":118,"thing":119,"Planning":120,"conflicts":121,"stan":122,"team":123,"deferrals":124,"Megatron":125,"practices":126,"6741":127,"5998":128,"Then":129,"Segmentation":130,"picturing":131,"reintegrate":132,"brief":133,"ACHIEVEMENT":134,"unimodal":135,"Approximately":136,"Ethernet":137,"Seoul":138,"Professional":139,"weak":140,"nicely":141,"cognition":142,"rotation":143,"decreases":144,"parabola":145,"hours":146,"styles":147,"DiskANN":148,"laude":149,"Zisserman":150,"1.0974":151,"Frankle":152,"Fashion":153,"0430":154,"conceive":155,"ulation":156,"forcement":157,"vulnerabilities":158,"bad":159,"termination":160,"nels":161,"extrac":162,"enterprises":163,"parity":164,"oddities":165,"recalculating":166,"make":167,"emulation":168,"Generating":169,"idles":170,"strikes":171,"#":172,"ition":173,"Decoupling":174,"Beach":175,"Reddi":176,"130":177,"JAX":178,"13971":179,"letting":180,"proba":181,"GRG":182,"Vandermersch":183,"photorealistic":184,"Operational":185,"doing":186,"noir":187,"Ko":188,"von":189,"Former":190,"depths":191,"Twelve":192,"1314":193,"Biel":194,"turned":195,"CONFLICTINGQA":196,"delving":197,"smooths":198,"equates":199,"ural":200,"Gains":201,"edgarro":202,"Kasparov":203,"career":204,"bottou":205,"MediaTek":206,"Inventory":207,"diningtable":208,"statistically":209,"hedging":210,"Schedulers":211,"Russia":212,"Stockholder":213,"jitter":214,"material":215,"542":216,"ily":217,"marvelous":218,"RNG":219,"desideratum":220,"A.1":221,"217289":222,"inventing":223,"Alberti":224,"wrapper":225,"wake":226,"stocking":227,"where":228,"interconnect":229,"anonymous":230,"SVG":231,"Bochkovskiy":232,"§":233,"Typically":234,"statis":235,"Skeleton":236,"372":237,"8047":238,"266":239,"remarkably":240,"360":241,"analo":242,"Clear":243,"range":244,"reaching":245,"bo":246,"repo":247,"measured":248,"preserves":249,"outline":250,"Stakeholder":251,"Type":252,"Alexander":253,"Centered":254,"bounds":255,"Environment":256,"610689":257,"block2":258,"Outputs":259,"profilers":260,"6551":261,"ingenious":262,"rolling":263,"transparency":264,"accompanying":265,"1267":266,"provoke":267,"236":268,"SoC":269,"method":270,"Learning":271,"judgement":272,"sessions":273,"included":274,"961":275,"progressively":276,"Parinov":277,"realizing":278,"Mo":279,"interrupts":280,"highlight":281,"users":282,"seeking":283,"ensure":284,"825":285,"analytic":286,"Brand":287,"3623":288,"Tanida":289,"irrevocable":290,"instrumental":291,"4.5":292,"Matrix":293,"mplot3d":294,"differentiability":295,"representa":296,"30th":297,"Classifiers":298,"typo":299,"Dover":300,"dinner":301,"ory":302,"Wellesley":303,"posterior":304,"tiply":305,"py":306,"Loader":307,"hun":308,"BlackRock":309,"investigate":310,"Ahmed":311,"pullover":312,"medical":313,"TextCNN":314,"0803":315,"employer":316,"Considering":317,"rise":318,"0.054":319,"parameterization":320,"Lehtinen":321,"Ancient":322,"excit":323,"monies":324,"approximating":325,"initializa":326,"inserts":327,"Covariances":328,"Forty":329,"schnauzer":330,"blurry":331,"ller":332,"Football":333,"resignation":334,"Duties":335,"Statistic":336,"Goals":337,"Baratov":338,"fewest":339,"：":340,"billing":341,"Fisher":342,"6114":343,"第一":344,"VonMises":345,"Probably":346,"client314":347,"xxi":348,"changes":349,"HGX":350,"Countries":351,"dip":352,"keqing":353,"Jacob":354,"strike":355,"Pacific":356,"Asia":357,"Libratus":358,"LaTeX":359,"Flexible":360,"05676":361,"JONES":362,"TensorDataset":363,"accept":364,"GFLOPs":365,"Virtual":366,"Bogin":367,"Ollivier":368,"Shlens":369,"Karl":370,"CSCO":371,"Constant":372,"try":373,"infin":374,"characters":375,"advisable":376,"Reported":377,"auxiliary":378,"residuals":379,"文本":380,"email":381,"Taxes":382,"collecting":383,"contributor":384,"Discussions118":385,"Pasupat":386,"Yum":387,"sharey":388,"Vol":389,"detaches":390,"apps":391,"Perry":392,"PoET":393,"duty":394,"turning":395,"percent":396,"justification":397,"recruit":398,"80000":399,"oneself":400,"887":401,"roles":402,"Harrison":403,"Evaluate":404,"Hai":405,"6756":406,"rameters":407,"lastly":408,"Registers":409,"7A":410,"translated":411,"Kosaraju":412,"1057":413,"numerator":414,"Profit":415,"isometry":416,"goods":417,"Cortes":418,"retrieval":419,"intensity":420,"relatedness":421,"horribly":422,"updatable":423,"jjangga0214":424,"abstractive":425,"764":426,"debias":427,"describes":428,"Gehring":429,"usual":430,"Library157":431,"A.45":432,"cosines":433,"vanish":434,"exams":435,"RELATIONS":436,"Stochastic":437,"transactional":438,"Gamma":439,"exploit":440,"Stranger":441,"847":442,"0.96":443,"KRESS":444,"President":445,"lineup":446,"919":447,"Companion":448,"General":449,"tokeniza":450,"Unims":451,"Dagar":452,"Bunke":453,"5265":454,"anticipate":455,"regime":456,"Tomas":457,"repetition":458,"ELMo":459,"703":460,"OECD":461,"backpropgation":462,"26868380288030347":463,"contacted":464,"2015":465,"quadratic":466,"verwandte":467,"blackbox":468,"tity":469,"configura":470,"0.108":471,"Proc":472,"Nominee":473,"peculiar":474,"calibrate":475,"Restricted":476,"adam":477,"dw":478,"Beneficially":479,"regards":480,"lin1":481,"def":482,"254":483,"resent":484,"socket":485,"727":486,"Country":487,"fails":488,"Flight":489,"BENCHMARK":490,"materials":491,"instan":492,"vision":493,"XW":494,"98%":495,"restructuring":496,"demonstrations":497,"Recommender":498,"dicted":499,"⟩":500,"rlrc":501,"GAN":502,"armed":503,"Strides":504,"SUPERCOMPUTING":505,"printed":506,"negative":507,"116":508,"ditions":509,"continuum":510,"关键":511,"tweak":512,"TB":513,"Completed":514,"bespoke":515,"Barham":516,"restatements":517,"Video":518,"itory":519,"dump":520,"GENREAD":521,"anomalous":522,"keypoints":523,"nearby":524,"clamp":525,"foodservice":526,"Discussions271":527,"93":528,"Discussions228":529,"3232":530,"Logit":531,"desire":532,"rejection":533,"practitioners":534,"circumstance":535,"Discussions112":536,"gio":537,"Flammarion":538,"correspon":539,"retrieved":540,"override":541,"Hi":542,"cluster":543,"366":544,"checkpoints":545,"IPC":546,"120th":547,"538":548,"FSE":549,"branding":550,"problem":551,"valuing":552,"exorbitant":553,"SquareRootScheduler":554,"snip":555,"facets":556,"CBR":557,"QA":558,"indemnification":559,"Predicted":560,"j":561,"tensorflow":562,"answering":563,"engineers":564,"social":565,"Setting":566,"867":567,"dients":568,"welfare":569,"ongo":570,"embedded":571,"16568":572,"eralization":573,"downloading":574,"Evans":575,"Parthe":576,"symbolically":577,"apoints":578,"Extreme":579,"survey":580,"Versus":581,"visited":582,"hallucinator":583,"Grid":584,"serving":585,"Unknown":586,"imately":587,"Weiss":588,"evaluative":589,"heytitle":590,"MobileNetV3":591,"GAAP":592,"mirrors":593,"大型":594,"assesses":595,"potentially":596,"21776":597,"Tan":598,"1.8788":599,"Broyde":600,"ToTensor":601,"Strictly":602,"LGBTQ":603,"LayerNorm":604,"LM":605,"CODM":606,"1993":607,"098":608,"Local":609,"addition":610,"Clara":611,"1208":612,"Bottle":613,"ReshapeTransform":614,"Programs":615,"fering":616,"Elementwise":617,"precisely":618,"cloud":619,"utils":620,"presence":621,"glance":622,"Family":623,"Earth":624,"lobbyists":625,"League":626,"Nonethe":627,"investigating":628,"SECURITIES":629,"gross":630,"curl":631,"already":632,"":633,"403000":634,"2150":635,"Considered":636,"Hinton":637,"Changing":638,"MA":639,"Activerag":640,"dispensing":641,"439":642,"enacted":643,"mind":644,"equilibrium":645,"syn":646,"saddled":647,"artistic":648,"scarcity":649,"Wear":650,"minus":651,"begins":652,"sentation":653,"CONCLUSION":654,"Gholampoor":655,"Sally":656,"534":657,"Hongshen":658,"defaultdict":659,"reverses":660,"9158":661,"underpinnings":662,"precon":663,"spirit":664,"suggesting":665,"fail":666,"86":667,"maximizes":668,"diversion":669,"watch":670,"interfering":671,"referenced":672,"7818":673,"reviewing":674,"hist":675,"trillions":676,"Chase":677,"sends":678,"193776":679,"building":680,"India":681,"Governance":682,"Texas":683,"4253":684,"christabella":685,"discovers":686,"hamper":687,"resiliency":688,"marked":689,"expensed":690,"ABC":691,"programmatically":692,"When":693,"ResNeXt":694,"Mitigate":695,"pour":696,"surro":697,"enterprise":698,"configu":699,"hor":700,"gurations":701,"cycle":702,"g":703,"Dahl":704,"contractors":705,"Reduced":706,"covariances":707,"imations":708,"<":709,"occasionally":710,"troduces":711,"tokyo":712,"spurred":713,"initiate":714,"0.0674":715,"Automatica":716,"Leviathan":717,"timizers":718,"interface":719,"11205":720,"evaluated":721,"329":722,"adequacy":723,"liqingnz":724,"Wen":725,"Alexa":726,"Tuning":727,"truera":728,"Which":729,"CUDA117":730,"vulnerable":731,"knowledge":732,"Exploratory":733,"sword":734,"cTBLS":735,"rapid":736,"CleanTechnica":737,"vided":738,"Figure":739,"owned":740,"inverse":741,"Cover":742,"501":743,"zon":744,"n0":745,"dental":746,"mse":747,"ó":748,"Millican":749,"xf":750,"Regulation":751,"story":752,"footwear":753,"Q1K1":754,"lo":755,"deep":756,"pressure":757,"响应":758,"Consent":759,"15.1":760,"transferred":761,"01846559300690354":762,"DAWN":763,"atter":764,"L4":765,"WeightDecay":766,"Stanford":767,"ch6":768,"1020":769,"0.869":770,"Assuming":771,"parallelograms":772,"Load":773,"architects":774,"open":775,"257":776,"chair":777,"extend":778,"bulk":779,"pushes":780,"Therefore":781,"backed":782,"Mark":783,"continuous":784,"volution":785,"step":786,"LSTMScratch":787,"interpretation":788,"charged":789,"497":790,"Robustness":791,"Varatharajan":792,"extrapolating":793,"Aspects":794,"pow":795,"bearing":796,"𝚺":797,"Paris":798,"Jaitly":799,"CC":800,"infringement":801,"278":802,"一个":803,"confirming":804,"unprac":805,"computed":806,"Perceptron":807,"moved":808,"cropping":809,"attorney":810,"evade":811,"Processes":812,"spinning":813,"config":814,"biolaysumm":815,"deductible":816,"hv1":817,"nowadays":818,"pen":819,"invest":820,"bachelor":821,"wire":822,"October":823,"such":824,"Belonging":825,"reject":826,"c72329e68a732bef0452e4b96a1c341c8910f81f":827,"Skills":828,"175":829,"5010":830,"curate":831,"conceptually":832,"thorough":833,"Corrupt":834,"MaxPooling":835,"vocabulary":836,"prepare":837,"measures":838,"Sandler":839,"Liked":840,"hypers":841,"Proceeds":842,"q2":843,"plagued":844,"wind":845,"monkey":846,"094":847,"whe":848,"Lester":849,"Wilson":850,"pable":851,"11%":852,"DeepSpeed":853,"separate":854,"unsqueeze":855,"737":856,"themes":857,"feeds":858,"VAE":859,"Types":860,"balancing":861,"Principle":862,"ics":863,"constructs":864,"1903":865,"spective":866,"refrains":867,"colors":868,"distinguishes":869,"rewards":870,"ACE":871,"naturalness":872,"118":873,"A.10":874,"peripherals":875,"border":876,"18000":877,"Imputation":878,"bn1":879,"disconnected":880,"phases":881,"106000":882,"warranty":883,"4881":884,"484":885,"mostly":886,"neurones":887,"Reconstruction":888,"—":889,"resolution":890,"funk":891,"KBQA":892,"16":893,"437":894,"desir":895,"二季度":896,"rings":897,"mistakes":898,"pora":899,"prior":900,"downweighting":901,"nadaraya":902,"Revisiting":903,"facilitated":904,"ARKS":905,"Kundu":906,"Integrated":907,"straightforwardly":908,"correctly":909,"2028":910,"Bain":911,"Hashimoto":912,"01991":913,"1958":914,"Paradoxically":915,"18.1":916,"dummies":917,"4728":918,"Ideas":919,"CodeS":920,"2557":921,"lenging":922,"contours":923,"localize":924,"racy":925,"augmented":926,"reshaping":927,"commence":928,"Utilities":929,"teristic":930,"\"":931,"EfficientQA":932,"Paper":933,"Identification":934,"sprung":935,"内部":936,"parame":937,"400001":938,"regulariza":939,"weight":940,"contraction":941,"fines":942,"audios":943,"po":944,"1000W":945,"Dynamo":946,"lag":947,"5602":948,"refining":949,"Exact":950,"regarded":951,"A.51":952,"584%":953,"cryptonaut":954,"However":955,"reserved":956,"cultural":957,"Applications":958,"descriptor":959,"\t":960,"delightfully":961,"emanating":962,"Achilles":963,"3118":964,"Deadlines":965,"FFN":966,"modifier":967,"EU":968,"Inclusion":969,"develops":970,"haves":971,"→":972,"mance":973,"preferred":974,"refer":975,"882":976,"2608.4":977,"unlabeled":978,"Ilya":979,"sovereign":980,"situation":981,"Modeling":982,"RoI":983,"except":984,"9580":985,"repertoire":986,"D4":987,"normally":988,"prospectively":989,"compile":990,"ten":991,"darker":992,"Interpreting":993,"Discussions185":994,"0007":995,"2209":996,"trustee":997,"NNP":998,"decomposi":999,"Fortune":1000,"market":1001,"Zadeh":1002,"Jouppi":1003,"9023":1004,"Rules":1005,"song":1006,"Zero":1007,"save":1008,"一张":1009,"111982":1010,"compulsory":1011,"Ghahramani":1012,"avinashingit":1013,"bothered":1014,"buyers":1015,"hashlib":1016,"1104":1017,"Through":1018,"erful":1019,"whether":1020,"Transposed":1021,"05941":1022,"templates":1023,"School":1024,"Duan":1025,"\b":1026,"0253":1027,"bitrary":1028,"Nes":1029,"𝑚":1030,"5891":1031,"935":1032,"latest":1033,"minimalistic":1034,"impairment":1035,"support":1036,"0.9":1037,"leaking":1038,"vermicelli":1039,"improved":1040,"recurring":1041,"credits":1042,"model88":1043,"incorrect":1044,"annual":1045,"cours":1046,"以":1047,"9.3":1048,"yes":1049,"ordering":1050,"excess":1051,"Reset":1052,"realms":1053,"ln1":1054,"标准文件":1055,"synchronous":1056,"Akiba":1057,"785":1058,"cussed":1059,"AGX":1060,"310455":1061,"∝":1062,"Technical":1063,"count":1064,"Essence":1065,"Hyperplanes":1066,"Inability":1067,"iterator":1068,"03167":1069,"Convening":1070,"]":1071,"svg":1072,"16009":1073,"RTX":1074,"designers":1075,"belongs":1076,"Should":1077,"Marvell":1078,"backpropaga":1079,"RBF":1080,"275":1081,"teed":1082,"tend":1083,"43%":1084,"neatly":1085,"diagnosing":1086,"Other":1087,"cisions":1088,"evaluates":1089,"inquiries":1090,"induce":1091,"MM":1092,"gender":1093,"distrust":1094,"informed":1095,"Motor":1096,"connecting":1097,"liefs":1098,"169738":1099,"respected":1100,"tioners":1101,"unary":1102,"scenario":1103,"Weerasekera":1104,"dart":1105,"automatic":1106,"solicits":1107,"Morozov":1108,"confusing":1109,"Repocoder":1110,"cockpit":1111,"2385":1112,"JEN":1113,"568":1114,"TokenEm":1115,"doodlers":1116,"electronic":1117,"Steiner":1118,"859":1119,"ERM":1120,"Hu":1121,"tiating":1122,"w":1123,"retail":1124,"tool":1125,"387814":1126,"confusion":1127,"factorized":1128,"1702844732454753":1129,"mated":1130,"demands":1131,"2337":1132,"Aboobakuru":1133,"𝐷":1134,"CSV":1135,"lightly":1136,"Chunked":1137,"142":1138,"whopping":1139,"260":1140,"PythonBackend":1141,"halved":1142,"surfboard":1143,"Antonoglou":1144,"theo":1145,"demonstrates":1146,"Administers":1147,"deny":1148,"COT":1149,"06940":1150,"marker":1151,"Moczulski":1152,"when":1153,"searched":1154,"feet":1155,"nvda":1156,"Sankhy":1157,"Smirnov":1158,"lacks":1159,"Mensch":1160,"Associates":1161,"nobody":1162,"easier":1163,"indicated":1164,"AvgPool2d":1165,"pleasing":1166,"11487":1167,"Cosine":1168,"Deployment":1169,"1545":1170,"Before":1171,"unissued":1172,"hedges":1173,"inforcement":1174,"cy":1175,"characterizes":1176,"Floating":1177,"3915e":1178,"430":1179,"咖啡":1180,"2.8991":1181,"interdisciplinary":1182,"fortune":1183,"channels":1184,"counterparts":1185,"3333":1186,"Focusing":1187,"etry":1188,"Little":1189,"suppliers":1190,"336":1191,"ubiquity":1192,"considers":1193,"生成":1194,"Later":1195,"sure":1196,"067":1197,"plete":1198,"bombs":1199,"VOC":1200,"wd":1201,"remain":1202,"Retrievers":1203,"constraint":1204,"soliciting":1205,"1GB":1206,"cleverness":1207,"Î":1208,"state":1209,"dense2":1210,"Parikh":1211,"gru":1212,"conclusions":1213,"sharex":1214,"RRGCode":1215,"revolutionized":1216,"codet5":1217,"季度":1218,"conveniences":1219,"absurdly":1220,"779":1221,"Shareholder":1222,"Mechan":1223,"sq":1224,"match":1225,"SKP":1226,"convs":1227,"20.2":1228,"SN":1229,"Methodist":1230,"Freitas":1231,"5395":1232,"04085":1233,"Binomial":1234,"Digitalization":1235,"investigates":1236,"marketplaces":1237,"12.1":1238,"4e443f8a2eca6b1dac8a6c57641b67dd40621a49":1239,"Concurrently":1240,"790":1241,"decompression":1242,"Searching":1243,"5552":1244,"constrained":1245,"vs":1246,"formulated":1247,"caveats":1248,"Discussions212":1249,"continue":1250,"OneHotCategorical":1251,"specialize":1252,"cartoons":1253,"Grave":1254,"07892":1255,"activation":1256,"01798":1257,"Estimate":1258,"283":1259,"indx":1260,"Dzreyev":1261,"insufficient":1262,"taking":1263,"TokenEmbedding":1264,"Cuong":1265,"thereof":1266,"enforces":1267,"protective":1268,"tv":1269,"metrics":1270,"invariant":1271,"Rates":1272,"Implicit":1273,"scratch":1274,"Recommendation":1275,"focused":1276,"ement":1277,"Athletic":1278,"widespread":1279,"Detaching":1280,"simple":1281,"Calc":1282,"Ital":1283,"fo":1284,"previ":1285,"trained":1286,"cocktail":1287,"therein":1288,"consultant":1289,"899878":1290,"10214259921521483":1291,"meta":1292,"varying":1293,"absent":1294,"countersuits":1295,"kernel":1296,"partnering":1297,"from":1298,"admit":1299,"investors":1300,"wrong":1301,"amendments":1302,"LogSumExp":1303,"instantaneously":1304,"CAD":1305,"·":1306,"website235":1307,"Governments":1308,"chassis":1309,"008":1310,"transcription":1311,"disk":1312,"ffn":1313,"semi":1314,"expres":1315,"PSU":1316,"unauthorized":1317,"Williamson":1318,"919740":1319,"competitively":1320,"Section":1321,"Pulfer":1322,"Pedersen":1323,"011":1324,"triv":1325,"方案":1326,"guaran":1327,"dy":1328,"intuitions":1329,"assessing":1330,"purple":1331,"Dropping":1332,"Discussions249":1333,"dynamic":1334,"recognize":1335,"Significant":1336,"development":1337,"554":1338,"subjective":1339,"proxyvote":1340,"Jassy":1341,"644":1342,"aeronautics":1343,"hpo":1344,"stragglers":1345,"messy":1346,"environments":1347,"Alobeidli":1348,"NICE":1349,"vgg19":1350,"584605":1351,"compu":1352,"hopes":1353,"factory":1354,"Yorkshire":1355,"water":1356,"187":1357,"RPM":1358,"Deduct":1359,"prohibits":1360,"EMEA":1361,"subclass":1362,"logistics":1363,"publish":1364,"keepdim":1365,"declared":1366,"本":1367,"Frazier":1368,"Abdallah":1369,"Caches":1370,"Christmas":1371,"Collaborative":1372,"candor":1373,"enqueued":1374,"fitting":1375,"057":1376,"gives":1377,"tracking":1378,"exert":1379,"volatility":1380,"distributor":1381,"intents":1382,"Lian":1383,"ns":1384,"unable":1385,"Iris":1386,"3.91":1387,"delivery":1388,"pokemon":1389,"prediction":1390,"maturities":1391,"revolution":1392,"ance":1393,"0956":1394,"consent":1395,"representative":1396,"返回":1397,"Krueger":1398,"bilinear":1399,"financial":1400,"leveraged":1401,"Carlo":1402,"yields":1403,"jobs":1404,"Matching":1405,"detailed":1406,"Akabane":1407,"4%":1408,"1083":1409,"vulnerability":1410,"Learing":1411,"horsepower":1412,"Chao":1413,"2959":1414,"throughs":1415,"overcome":1416,"New":1417,"gated":1418,"Aircraft":1419,"285":1420,"NestMLP":1421,"pros":1422,"Transformers":1423,"2188":1424,"fairness":1425,"thread":1426,"Ding":1427,"tanks":1428,"shed":1429,"lating":1430,"mass":1431,"purchasing":1432,"troubling":1433,"dimensionality":1434,"4190":1435,"Grace":1436,"ciples":1437,"reconciliation":1438,"borrow":1439,"AC":1440,"worrying":1441,"IT":1442,"proactively":1443,"12.3":1444,"adjourn":1445,"1460":1446,"___":1447,"Plan":1448,"automobiles":1449,"registers":1450,"June":1451,"undergone":1452,"doi":1453,"synthesises":1454,"sheep":1455,"Oversees":1456,"description":1457,"additionally":1458,"9038":1459,"hierarchy":1460,"300":1461,"55th":1462,"sparing":1463,"05934":1464,"1905":1465,"convergence":1466,"presented":1467,"Naturally":1468,"dramati":1469,"Va":1470,"myriad":1471,"15000":1472,"Zaheer":1473,"RandomVerticalFlip":1474,"sheet":1475,"tree":1476,"Parent":1477,"closer":1478,"GmbH":1479,"Dirichlet":1480,"Karlen":1481,"informally":1482,"evangelize":1483,"1085":1484,"G":1485,"adheres":1486,"MUST":1487,"BPPT":1488,"lowrank":1489,"Queries":1490,"block":1491,"ai11":1492,"destination":1493,"Launching":1494,"287":1495,"574":1496,"elaborating":1497,"mathematically":1498,"FTIR":1499,"CMU":1500,"spent":1501,"thereafter":1502,"resume":1503,"Mask":1504,"certified":1505,"catdoor":1506,"Jiang":1507,"rearrange":1508,"wavy":1509,"cient":1510,"Preprocessing":1511,"Lazy":1512,"src":1513,"B.5":1514,"conduct":1515,"12":1516,"268684":1517,"displace":1518,"beijing":1519,"Symmetry":1520,"evolving":1521,"square":1522,"pxrds":1523,"Gauss":1524,"1.4":1525,"therewith":1526,"ity":1527,"capture":1528,"Xeon":1529,"release":1530,"Truong":1531,"Fi":1532,"Pillar":1533,"二段":1534,"spectrum":1535,"Santurkar":1536,"Monarch":1537,"assertion":1538,"x3":1539,"integrates":1540,"Informally":1541,"68":1542,"geometry":1543,"Semiconductor":1544,"225":1545,"RMS":1546,"start":1547,"consecutive":1548,"synchronously":1549,"Miniconda":1550,"355":1551,"Reingold":1552,"Tvqa":1553,"domains":1554,"Loshchilov":1555,"induces":1556,"51":1557,"attribution":1558,"transformed":1559,"HARVEY":1560,"Dividing":1561,"9%":1562,"downward":1563,"Aggregate":1564,"Multilingual":1565,"propagations":1566,"convert":1567,"efficacy":1568,"Matrices":1569,"hoped":1570,"c":1571,"retrospect":1572,"drawing":1573,"扫描":1574,"distance":1575,"closing":1576,"GUI":1577,"3219":1578,"hot":1579,"elsewhere":1580,"1645":1581,"Dassault":1582,"3.7":1583,"快速":1584,"Deviation":1585,"14831":1586,"compares":1587,"putative":1588,"Concretely":1589,"219":1590,"utilized":1591,"racially":1592,"capitalized":1593,"scrape":1594,"Nick":1595,"":1596,"unjust":1597,"1s":1598,"Musthafa":1599,"set":1600,"864":1601,"Success":1602,"Jaguar":1603,"Joshua":1604,"ports":1605,"eases":1606,"Ama":1607,"Sharing":1608,"BNLeNetScratch":1609,"deadline":1610,"backported":1611,"Rouder":1612,"Diluted":1613,"wrangle":1614,"birdsong":1615,"3598":1616,"1703":1617,"9500":1618,"Densely":1619,"tensions":1620,"155":1621,"GoogleNet":1622,"ABD":1623,"look":1624,"Computer":1625,"Hardt":1626,"statistical":1627,"0840":1628,"hspace":1629,"deceptive":1630,"Breitkopf":1631,"𝐴":1632,"platform":1633,"1800":1634,"enhanced":1635,"SUPER":1636,"swer":1637,"PCAST":1638,"CREA":1639,"ognizing":1640,"Rose":1641,"Discussions111":1642,"earlier":1643,"neurophysiologists":1644,"burgled":1645,"redesign":1646,"Searcher":1647,"1024":1648,"Late":1649,"be":1650,"KAPING":1651,"fer":1652,"jacket":1653,"sent":1654,"compiled":1655,"Bud":1656,"strictly":1657,"repurpose":1658,"Uszkoreit":1659,"effortlessly":1660,"seasoned":1661,"unfolding":1662,"estimator":1663,"937":1664,"Practitioners":1665,"utilizing":1666,"ilar":1667,"intensities":1668,"released":1669,"⇐":1670,"mathematical":1671,"Averaging":1672,"encoun":1673,"16137":1674,"ridge":1675,"soup":1676,"fulfillment":1677,"EfficientNet":1678,"Veroyatnostei":1679,"Threadripper":1680,"VGG":1681,"analogous":1682,"algorithms":1683,"focus":1684,"533":1685,"stuck":1686,"Algorithm":1687,"Estimates":1688,"experimentally":1689,"Itoh":1690,"interdependency":1691,"MulBackward0":1692,"lenses":1693,"fact":1694,"prototype":1695,"xmax":1696,"ulously":1697,"fishersnedecor":1698,"vibration":1699,"Jeff":1700,"Wiener":1701,"Xiong":1702,"10436":1703,"marginalization":1704,"adhere":1705,"twice":1706,"karolszk":1707,"cents":1708,"product":1709,"fied":1710,"\u000b":1711,"Estimators":1712,"Theorem":1713,"boxes2":1714,"lowering":1715,"opportunity":1716,"int32":1717,"overfits":1718,"FactorScheduler":1719,"1W":1720,"holistic":1721,"bash":1722,"Keeping":1723,"167":1724,"Dense":1725,"chiplets":1726,"existence":1727,"uncertainties":1728,"businesses":1729,"reductions":1730,"dictable":1731,"synonyms":1732,"coined":1733,"modulates":1734,"incorporates":1735,"SOC":1736,"Cultural":1737,"Netflix":1738,"nonlinear":1739,"puts":1740,"Discussions219":1741,"relu":1742,"Required":1743,"ymin":1744,"note":1745,"Discussions103":1746,"Bang":1747,"food":1748,"Jiehang":1749,"740":1750,"representational":1751,"Vi":1752,"textures":1753,"saving":1754,"0.000022":1755,"NotAnotherSystem":1756,"Subject":1757,"Hayashi":1758,"nicest":1759,"wholly":1760,"1973":1761,"Retrieved":1762,"intra":1763,"cons":1764,"54%":1765,"Interpolation":1766,"transla":1767,"optimization":1768,"thinking":1769,"hypercomplex":1770,"boards":1771,"Tokens":1772,"xticks":1773,"naively":1774,"NVIDIA":1775,"hunting":1776,"txt":1777,"Comment":1778,"FashionMNIST":1779,"1860":1780,"Fakoor":1781,"breakthrough":1782,"Articles":1783,"mutual":1784,"neg":1785,"11271":1786,"Arabic":1787,"Caseres":1788,"Zipf":1789,"Working":1790,"road":1791,"automatically192":1792,"misappropriation":1793,"trumps":1794,"nected":1795,"Discussions105":1796,"nms":1797,"Software":1798,"epsilion":1799,"attend":1800,"Sutter":1801,"Maximizing":1802,"74":1803,"46.18621024399691":1804,"Expanding":1805,"propelling":1806,"6ms":1807,"Resources":1808,"SENets":1809,"shape":1810,"picks":1811,"lapse":1812,"fronts":1813,"plant":1814,"uration":1815,"proposals":1816,"Publications":1817,"Alongside":1818,"resists":1819,"Fidelity":1820,"369":1821,"Approves":1822,"fluency":1823,"constitutes":1824,"mnist":1825,"powered":1826,"CrowdStrike":1827,"iterations":1828,"𝛼":1829,"899":1830,"these":1831,"severely":1832,"chatbots":1833,"Katsouros":1834,"sin2":1835,"28001":1836,"menu":1837,"ZipFile":1838,"bring":1839,"logi":1840,"tipping":1841,"equivalents":1842,"ingredients":1843,"Biophysics":1844,"pretrained":1845,"-----":1846,"derivative":1847,"cartoonishly":1848,"magazine":1849,"Safety":1850,"IBM":1851,"spline":1852,"0605":1853,"Symmetric":1854,"tau":1855,"dient":1856,"EFSUM":1857,"annotating":1858,"attrition":1859,"918":1860,"substituted":1861,"win":1862,"loss":1863,"meanvec":1864,"此处":1865,"enrichment":1866,"Bakshy":1867,"panel":1868,"loping":1869,"descriptors":1870,"truncates":1871,"Business":1872,"cepts":1873,"enriched":1874,"Champaign":1875,"settings":1876,"Sano":1877,"He":1878,"gaps":1879,"Ave":1880,"initializes":1881,"Understudy":1882,"CAP":1883,"Create":1884,"c1816da3821ae9f43899be655002f6c723e91b88":1885,"upgrade":1886,"Wesley":1887,"801392782910287192":1888,"KG":1889,"tripling":1890,"symbolic":1891,"suffered":1892,"36":1893,"consequential":1894,"420":1895,"Forrester":1896,"Q1":1897,"sorting":1898,"products":1899,"Jose":1900,"¬":1901,"mean":1902,"calc":1903,"Liberty":1904,"neuroscience":1905,"augmenta":1906,"invoked":1907,"50000":1908,"Of":1909,"Become":1910,"Background":1911,"Iterating":1912,"plotted":1913,"squarely":1914,"configurable":1915,"Computations":1916,"elusive":1917,"exciting":1918,"Extension":1919,"ISO":1920,"library72":1921,"acteristics":1922,"policies":1923,"deviates":1924,"stacked":1925,"ch13":1926,"HomunculusK":1927,"mispredict":1928,"entered":1929,"Professor":1930,"shouldn":1931,"elucidate":1932,"swering":1933,"learners":1934,"cat":1935,"410":1936,"indicators":1937,"Discussions119":1938,"denoising":1939,"COLING":1940,"biopharmaceutical":1941,"PROVIDE":1942,"Britton":1943,"tense":1944,"12000":1945,"features":1946,"2347":1947,"smell":1948,"dard":1949,"1007":1950,"Devices":1951,"gold":1952,"construc":1953,"traction":1954,"4208":1955,"residual":1956,"Balances":1957,"0819":1958,"alent":1959,"simplest":1960,"transmit":1961,"𝑔":1962,"bia":1963,"games":1964,"⪰":1965,"conservative":1966,"Verify":1967,"Hilbert":1968,"valid":1969,"agation":1970,"Recursion":1971,"577":1972,"appropriate":1973,"ptb":1974,"5234904.66":1975,"densenets":1976,"molecular":1977,"_":1978,"J":1979,"consult":1980,"boggling":1981,"recalling":1982,"insights":1983,"protection":1984,"Brent":1985,"freezes":1986,"0001":1987,"gamer":1988,"Davis":1989,"universe":1990,"secret":1991,"Tensilica":1992,"repay":1993,"549":1994,"Wxf":1995,"MuJoCo":1996,"digit":1997,"23":1998,"registrations":1999,"Tao":2000,"administers":2001,"Relative":2002,"Accounts":2003,"915":2004,"poor":2005,"distinguishing":2006,"1235":2007,"itive":2008,"550MB":2009,"writes":2010,"396":2011,"Entity":2012,"3125":2013,"summa":2014,"06490":2015,"JPEGImages":2016,"headquarter":2017,"0240":2018,"probably":2019,"magnetic":2020,"peel":2021,"391":2022,"cancelable":2023,"hiddens":2024,"Jeon":2025,"plex":2026,"TransformerEncoderBlock":2027,"templated":2028,"13707981127012328":2029,"worthy":2030,"Preliminary":2031,"exemplar":2032,"Perplexity":2033,"textbooks":2034,"Bhunia":2035,"sending":2036,"request":2037,"possibly":2038,"ensembles":2039,"generalizes":2040,"506":2041,"593":2042,"Increased":2043,"sped":2044,"tives":2045,"Bayesian":2046,"xxiv":2047,"synchronize":2048,"Currently":2049,"risky":2050,"44639554136672527":2051,"6347":2052,"ized":2053,"Victorri":2054,"decreased":2055,"2035":2056,"overestimation":2057,"500":2058,"18264":2059,"semiconductor":2060,"Salakhutdinov":2061,"Parti":2062,"challenge":2063,"dispensable":2064,"80186":2065,"slides":2066,"convenience":2067,"370":2068,"temporary":2069,"A.18":2070,"suf":2071,"acquired":2072,"Data":2073,"removes":2074,"fairly":2075,"615":2076,"Davidson":2077,"theory":2078,"Dumoulin":2079,"nominations":2080,"00341":2081,"prematurely":2082,"Chernovenkis":2083,"competing":2084,"evaluator":2085,"films":2086,"github":2087,"supreme":2088,"cor":2089,"444":2090,"years":2091,"Schein":2092,"Weissenborn":2093,"Founders":2094,"explain":2095,"planes":2096,"1023":2097,"Nocedal":2098,"2951":2099,"empirical":2100,"3GPP":2101,"claimed":2102,"Falcon":2103,"sliced":2104,"archived":2105,"出":2106,"function91":2107,"1039":2108,"gadgets":2109,"precursor":2110,"protecting":2111,"0.085":2112,"tics":2113,"explode":2114,"891":2115,"occupying":2116,"elections":2117,"H2":2118,"FreeBase":2119,"Backpropagation":2120,"plexity":2121,"servation":2122,"INVESTOR":2123,"Lacroix":2124,"countable":2125,"funding":2126,"Discussions285":2127,"213":2128,"430578":2129,"transposes":2130,"matrix":2131,"1479":2132,"nearest":2133,"hack":2134,"institutions":2135,"264475":2136,"reinvest":2137,"机制":2138,"subsidiaries":2139,"malizing":2140,"protections":2141,"sneak":2142,"122":2143,"Earnings":2144,"Thor":2145,"663":2146,"thereunto":2147,"Rong":2148,"Della":2149,"N95":2150,"finite":2151,"957":2152,"indemnify":2153,"imply":2154,"powering":2155,"Error":2156,"mises":2157,"safely":2158,"Execution":2159,"903":2160,"characterized":2161,"3860":2162,"hospitalized":2163,"5513":2164,"1543":2165,"2303":2166,"Kavukcuoglu":2167,"establish":2168,"Boltzmann":2169,"internal":2170,"grab":2171,"survival":2172,"polysemy":2173,"endre":2174,"9649":2175,"runtimes":2176,"0033":2177,"cd":2178,"NVMe":2179,"litigating":2180,"Sort":2181,"899840":2182,"concentration":2183,"387":2184,"ICML":2185,"453":2186,"1855":2187,"Re2G":2188,"currencies":2189,"Sennrich":2190,"763":2191,"phoneme":2192,"3313":2193,"Filed":2194,"0.4651":2195,"reasons":2196,"07339":2197,"segments":2198,"events":2199,"accelerators":2200,"lucky":2201,"tenured":2202,"Turning":2203,"𝜉":2204,"Narang":2205,"can":2206,"Determine":2207,"Compilers":2208,"improve":2209,"hap":2210,"Offset":2211,"𝜋":2212,"Francisco":2213,"RAP":2214,"881":2215,"Sawarkar":2216,"Cartesian":2217,"ubuntu2204":2218,"workstations":2219,"bound":2220,"Complex":2221,"receptive":2222,"BROOKE":2223,"Sandoz":2224,"Although":2225,"schon":2226,"Night":2227,"telecommunications":2228,"Advisory":2229,"targets":2230,"expressive":2231,"EC2":2232,"LazyConv2d":2233,"Piperag":2234,"JJ":2235,"033362897489792855":2236,"Sequence":2237,"WRITTEN":2238,"5.4":2239,"Nowadays":2240,"0.01":2241,"20.1":2242,"eling":2243,"CSSC":2244,"SNLIDataset":2245,"thriving":2246,"quantified":2247,"Skip":2248,"constituted":2249,"calendar":2250,"Investigating":2251,"按计划":2252,"patch":2253,"𝜃":2254,"集成":2255,"filings":2256,"Concept":2257,"1.1950":2258,"switched":2259,"satisfying":2260,"boilerplate":2261,"02558173674804846":2262,"harnessing":2263,"2869":2264,"holds":2265,"UK":2266,"效果":2267,"con":2268,"966186":2269,"3111":2270,"AdaptiveAvgPool2d":2271,"view":2272,"557":2273,"Star":2274,"fueling":2275,"bases":2276,"unseen":2277,"rightly":2278,"plished":2279,"declaration":2280,"socially":2281,"starting":2282,"1037":2283,"后":2284,"𝜅":2285,"Managing":2286,"refusal":2287,"0.0992":2288,"frequent":2289,"classic":2290,"Blitzstein":2291,"create":2292,"username":2293,"Debut":2294,"ideally":2295,"concerns":2296,"Following":2297,"cial":2298,"sleight":2299,"738":2300,"Ohm":2301,"Gool":2302,"ingests":2303,"189":2304,"06800":2305,"Insights":2306,"Samsung":2307,"notationally":2308,"Better":2309,"compelling":2310,"intense":2311,"quarterly":2312,"d1":2313,"rl":2314,"Pavlick":2315,"717":2316,"stable":2317,"checkpointing":2318,"degrees":2319,"lived":2320,"relegated":2321,"204":2322,"Bistro":2323,"DEMAND":2324,"diagnostics":2325,"converged":2326,"misses":2327,"axon":2328,"correlation":2329,"prerequisites":2330,"months":2331,"voice":2332,"447":2333,"uninstall":2334,"Vesting":2335,"Diffused":2336,"opportunities":2337,"2256":2338,"596":2339,"structural":2340,"Revolution":2341,"Reliance":2342,"sively":2343,"Heavy":2344,"readings":2345,"date":2346,"intermediary":2347,"Kiros":2348,"ads":2349,"ducing":2350,"logistical":2351,"checking":2352,"Stores":2353,"𝑖":2354,"Thankfully":2355,"reporter":2356,"direction":2357,"puter":2358,"experiencing":2359,"2215":2360,"trip":2361,"Nominating":2362,"Surprisal":2363,"Anguelov":2364,"elementwise":2365,"Schema":2366,"Scott":2367,"elaborate":2368,"+":2369,"1570":2370,"Zeming":2371,"faced":2372,"zxydi1992":2373,"1066":2374,"incur":2375,"1042":2376,"ker":2377,"Guez":2378,"optimality":2379,"Structure":2380,"connect":2381,"1964":2382,"Army":2383,"Elman":2384,"encoding":2385,"066":2386,"propagated":2387,"accelerated":2388,"170284":2389,"707":2390,"recorded":2391,"arrangement":2392,"0.8218":2393,"047":2394,"transmitter":2395,"Aji":2396,"references":2397,"faces":2398,"propagating":2399,"decommitment":2400,"sup":2401,"Discussions259":2402,"importantly":2403,"本节":2404,"coauthors":2405,"caption":2406,"1525.4":2407,"ability":2408,"intensive":2409,"RegNetX32":2410,"filed":2411,"owning":2412,"Acquisitions":2413,"nxby":2414,"tremendous":2415,"conve":2416,"mouse":2417,"precondi":2418,"TSRs":2419,"correctable":2420,"Using":2421,"Equity":2422,"topic":2423,"1Whr":2424,"normalizes":2425,"Ichioka":2426,"navigable":2427,"00e":2428,"900579":2429,"Intelligent":2430,"fulfill":2431,"Find":2432,"Jensen":2433,"0704":2434,"pretty":2435,"345":2436,"affecting":2437,"willing":2438,"prices":2439,"outlines":2440,"alities":2441,"sian":2442,"GPU":2443,"Rumelhart":2444,"963":2445,"fields":2446,"Dependence":2447,"Space":2448,"circumstances":2449,"scratched":2450,"Saha":2451,"Bandits":2452,"Conv2D":2453,"Presetting":2454,"922":2455,"rgb":2456,"Wightman":2457,"manipulation":2458,"ℎ":2459,"dominates":2460,"1D":2461,"087":2462,"mainstream":2463,"keep":2464,"blows":2465,"Marcel":2466,"converges":2467,"sockets":2468,"specifically":2469,"vanishing":2470,"lently":2471,"talent":2472,"ScaleKernel":2473,"diagnose":2474,"Convince":2475,"Train":2476,"convince":2477,"complete":2478,"surprise":2479,"ticular":2480,"Solving":2481,"essence":2482,"99th":2483,"structured":2484,"10001":2485,"2758":2486,"charters":2487,"LLMLingua":2488,"expedience":2489,"simplifies":2490,"engine":2491,"lexical":2492,"effi":2493,"Buchatskaya":2494,"hosted":2495,"Founda":2496,"applicability":2497,"parison":2498,"my":2499,"maintain":2500,"convNet":2501,"ferent":2502,"disposition":2503,"NeMo":2504,"Again":2505,"chunked":2506,"971":2507,"gram":2508,"304":2509,"idling":2510,"defaulted":2511,"thirty":2512,"Vectorizing":2513,"tively":2514,"corners":2515,"played":2516,"1.2517":2517,"escalation":2518,"q":2519,"Allowance":2520,"Mobile":2521,"overridden":2522,"cise":2523,"INFO":2524,"Review":2525,"L2":2526,"09018":2527,"high":2528,"Spectrum":2529,"云端":2530,"AV":2531,"Entailment":2532,"doubling":2533,"initialized":2534,"18e":2535,"Riedl":2536,"6":2537,"2023":2538,"dissenting":2539,"indexes":2540,"presses":2541,"Outstanding":2542,"Lori":2543,"shutter":2544,"previous":2545,"turns":2546,"people":2547,"sparsification":2548,"331":2549,"CoWoS":2550,"Likelihood":2551,"⇒":2552,"stakeholders":2553,"purchased":2554,"closeness":2555,"APPS":2556,"943467":2557,"776":2558,"deviations":2559,"Bioinformatics":2560,"Backward":2561,"prepopulated":2562,"StudentT":2563,"maximization":2564,"posted":2565,"Baek":2566,"tar":2567,"tends":2568,"explod":2569,"0.017":2570,"rpm":2571,"nally":2572,"quantized":2573,"Liaw":2574,"mask":2575,"Cuts":2576,"1.2266":2577,"enjoyed":2578,"181500":2579,"158591":2580,"projects":2581,"Discussions287":2582,"beta1":2583,"Ng":2584,"mult":2585,"configspace":2586,"mechanization":2587,"Shortly":2588,"Reds":2589,"admits":2590,"Gaus":2591,"Both":2592,"Constraints":2593,"reimplementing":2594,"remaster":2595,"ExactGP":2596,"ComposeTransform":2597,"sufficiently":2598,"Le":2599,"efficient":2600,"overflowing":2601,"smarter":2602,"sits":2603,"旨在":2604,"45th":2605,"no":2606,"inch":2607,"waves":2608,"Tech":2609,"unneeded":2610,"06983":2611,"momentum":2612,"wiggily":2613,"Boston":2614,"Analogy":2615,"partition":2616,"货币":2617,"life":2618,"Flat":2619,"Luc":2620,"retention":2621,"exercisable":2622,"managed":2623,"Schuster":2624,"distin":2625,"decay":2626,"Multitask":2627,"Discrepancies":2628,"L1":2629,"zeus":2630,"notification":2631,"Measuring":2632,"MeSH":2633,"548":2634,"loyalty":2635,"三段":2636,"strengthened":2637,"biographical":2638,"hides":2639,"JUNE":2640,"movie":2641,"39":2642,"Take":2643,"Development":2644,"186":2645,"grew":2646,"vonenkis":2647,"owe":2648,"rated":2649,"interaction":2650,"sys":2651,"2":2652,"speci":2653,"exponential":2654,"come":2655,"VLSI":2656,"inventors":2657,"040":2658,"parametrize":2659,"jective":2660,"originate":2661,"0604":2662,"path":2663,"retains":2664,"Lakshya":2665,"outputting":2666,"way":2667,"attractive":2668,"AdaptiveMaxPool2d":2669,"downsample":2670,"885":2671,"Su":2672,"effects":2673,"034437":2674,"verification":2675,"INSTALLED":2676,"Combining":2677,"Career":2678,"可行":2679,"bars":2680,"Dropout":2681,"Discussions44":2682,"Do":2683,"dd":2684,"ample":2685,"Wallace":2686,"__.":2687,"subspace":2688,"Gard":2689,"Akad":2690,"courses6":2691,"Role":2692,"Reward":2693,"dataset108":2694,"bounding":2695,"recycling":2696,"05100":2697,"effectiveness":2698,"Module":2699,"Collobert":2700,"beyond":2701,"eigensystem":2702,"468":2703,"Changes":2704,"A.26":2705,"Parvez":2706,"808":2707,"model90":2708,"imposition":2709,"uploading":2710,"impen":2711,"yet":2712,"Woolley":2713,"Pair":2714,"middle":2715,"summarization":2716,"feeding":2717,"Life":2718,"guar":2719,"lighter":2720,"strangely":2721,"Jauvin":2722,"appreciate":2723,"1929":2724,"ci":2725,"rejecting":2726,"hitting":2727,"compositions":2728,"THESE":2729,"Col":2730,"complying":2731,"agreed":2732,"advisors":2733,"Josiah":2734,"probabilty":2735,"directing":2736,"duration":2737,"124":2738,"CPUs":2739,"due":2740,"conditions":2741,"5744":2742,"Occam":2743,"Automatically":2744,"natu":2745,"CDF":2746,"undemanding":2747,"judgment":2748,"Moving":2749,"negated":2750,"fortunes":2751,"predated":2752,"sgd":2753,"harming":2754,"Stewart":2755,"chronous":2756,"002":2757,"913":2758,"transposed":2759,"tuple":2760,"also":2761,"taxes":2762,"chasing":2763,"182":2764,"897":2765,"Reacc":2766,"backward":2767,"Work":2768,"nvidia":2769,"fuse":2770,"proxyholders":2771,"CBOW":2772,"pronounced":2773,"1074":2774,"431":2775,"edits":2776,"8500":2777,"2.8356":2778,"clusters":2779,"lack":2780,"hang":2781,"grossly":2782,"perceptions":2783,"spired":2784,"Recruitment":2785,")":2786,"𝑑":2787,"neu":2788,"It":2789,"0800":2790,"fication":2791,"remembers":2792,"553":2793,"und":2794,"conventionally":2795,"0.248":2796,"9127":2797,"provisional":2798,"trademarks":2799,"Luong":2800,"pivotal":2801,"multilayer":2802,"values":2803,"modularize":2804,"Simpler":2805,"809":2806,"ChatGPT164":2807,"RetMol":2808,"That":2809,"Disk":2810,"Deductions":2811,"\u0011":2812,"Blattmann":2813,"Inspections":2814,"grapple":2815,"subramanian":2816,"Remainder":2817,"intermediaries":2818,"formulating":2819,"kiplus1":2820,"5032":2821,"internationally":2822,"3038":2823,"before":2824,"See":2825,"Dog":2826,"ligand":2827,"GRAPE":2828,"Doctoral":2829,"depreciable":2830,"streets":2831,"suggestion":2832,"Acl":2833,"borrowing":2834,"facto":2835,"ValueError":2836,"worried":2837,"市场":2838,"Remeasurement":2839,"Amount":2840,"interview":2841,"Baby":2842,"headline":2843,"totic":2844,"correlates":2845,"Poole":2846,"Hoping":2847,"d":2848,"stead":2849,"Murray":2850,"identification":2851,"Nelson":2852,"HyperParameters":2853,"him":2854,"gates":2855,"resis":2856,"Gigaflops":2857,"trousers":2858,"BookCorpus":2859,"PAM":2860,"showed":2861,"owes":2862,"Apply":2863,"Courez":2864,"ting":2865,"You":2866,"Beyer":2867,"eventually":2868,"x5":2869,"intersection":2870,"Microarchitecture":2871,"4693":2872,"Parallelization":2873,"nipulating":2874,"653495":2875,"831":2876,"IEE":2877,"artists":2878,"975":2879,"8582.0341796875":2880,"bones":2881,"Albert":2882,"blessing":2883,"port":2884,"tallest":2885,"accuracy":2886,"folders":2887,"resolve":2888,"Levskaya":2889,"action":2890,"Casper":2891,"cated":2892,"0.160":2893,"Settlement":2894,"Carbin":2895,"convey":2896,"CodeICL":2897,"interleaved":2898,"KnowledgeNavigator":2899,"iters":2900,"LAION":2901,"2110":2902,"presenting":2903,"summable":2904,"line":2905,":":2906,"miniconda3":2907,"resnet":2908,"Programming":2909,"ç":2910,"equiv":2911,"PDF":2912,"Discussions243":2913,"Europe":2914,"filevich":2915,"|":2916,"cyclic":2917,"2.6461":2918,"determinants":2919,"grouped":2920,"scipy":2921,"Poet":2922,"dry":2923,"Limkonchotiwat":2924,"Stack":2925,"mend":2926,"rele":2927,"Authority":2928,"Gmail":2929,"1951":2930,"limit":2931,"triple":2932,"SUM":2933,"hparams":2934,"986":2935,"Circle":2936,"43006":2937,"verified":2938,"IJCAI":2939,"lgov":2940,"Transformations":2941,"quantize":2942,"comparative":2943,"circular":2944,"comparisons":2945,"rigtorp":2946,"prejudices":2947,"Compress":2948,"Shrivastava":2949,"20549":2950,"16000":2951,"designer":2952,"Payouts":2953,"TVQA":2954,"mentioned":2955,"1.4914":2956,"229":2957,"951":2958,"eigenvectors":2959,"bins":2960,"Shortcuts":2961,"crucial":2962,"Min":2963,"decline":2964,"Pavlov":2965,"928":2966,"Observations":2967,"teen":2968,"gradi":2969,"throttle":2970,"Cheng":2971,"Lyu":2972,"Sadly":2973,"Imagen":2974,"44.7":2975,"procuring":2976,"871":2977,"amounted":2978,"Architecture":2979,"reimplement":2980,"hyperpa":2981,"casual":2982,"all":2983,"ral":2984,"aerial":2985,"partnered":2986,"Charter":2987,"1027":2988,"MobileNet":2989,"Sarthi":2990,"safeguard":2991,"A.15":2992,"bottlenecks":2993,"clouds":2994,"claims":2995,"observability":2996,"convexity":2997,"ATI":2998,"9979e":2999,"footage":3000,"Sande":3001,"conveys":3002,"solutions":3003,"1950s":3004,"reward":3005,"losing":3006,"56.3":3007,"xxxv":3008,"1962":3009,"conv2d":3010,"StickBreakingTransform":3011,"Ammar":3012,"3430e":3013,"falls":3014,"57th":3015,"cell":3016,"inherent":3017,"dim":3018,"25%":3019,"select":3020,"interesting":3021,"Kocetkov":3022,"ences":3023,"confidences":3024,"2448":3025,"offline":3026,"Berkeley":3027,"DRIVE":3028,"𝐾":3029,"decode":3030,"accrued":3031,"Caution":3032,"pub":3033,"disproportionately":3034,"2007":3035,"Input":3036,"replacements":3037,"penalties":3038,"tall":3039,"utilize":3040,"mantic":3041,"wavelengths":3042,"Hubbard":3043,"Uijlings":3044,"demographic":3045,"0.012":3046,"proxy":3047,"Restructuring":3048,"draft":3049,"rotate":3050,"geogunow":3051,"listing":3052,"781":3053,"appending":3054,"Defines":3055,"itate":3056,"REALM":3057,"SHA":3058,"年":3059,"datasets":3060,"v3":3061,"Run":3062,"curatorial":3063,"Discussions216":3064,"Whhh":3065,"最小":3066,"chosen":3067,"2483":3068,"Directions":3069,"64%":3070,"complicates":3071,"formula":3072,"immaterial":3073,"Address":3074,"attained":3075,"cheaply":3076,"taxonomy":3077,"webpages":3078,"hf":3079,"P5":3080,"Alteration":3081,"prompted":3082,"𝜖":3083,"stand":3084,"estimates":3085,"interspersed":3086,"announced":3087,"limelight":3088,"badly":3089,"breathing":3090,"225517":3091,"quential":3092,"Swap":3093,"Firm":3094,"Compare":3095,"principle":3096,"destroying":3097,"Exploiting":3098,"deciding":3099,"var":3100,"proceedings":3101,"consistently":3102,"dig":3103,"mandatory":3104,"454":3105,"Unions":3106,"Truncate":3107,"585e9cc93e70b39160e7921475f9bcd7d31219ce":3108,"49th":3109,"cumulant":3110,"perturbations":3111,"已":3112,"Eliot":3113,"overlook":3114,"0.7722":3115,"4354":3116,"Grants":3117,"Christmann":3118,"Discussions208":3119,"latory":3120,"Item":3121,"oversaw":3122,"Months":3123,"ccchengff":3124,"SumBackward0":3125,"barber":3126,"Polozov":3127,"engaged":3128,"thoroughly":3129,"computationally":3130,"70%":3131,"034":3132,"150%":3133,"Pre":3134,"Roles":3135,"852":3136,"Replace":3137,"proof":3138,"tation":3139,"seeks":3140,"delays":3141,"ques":3142,"Goodman":3143,"Round":3144,"caffeinated":3145,"bookkeeping":3146,"91":3147,"advancing":3148,"Pointer":3149,"ementwise":3150,"KaVD":3151,"uninformative":3152,"Discussions106":3153,"305":3154,"SigmoidTransform":3155,"2010s":3156,"pytorch":3157,"hotel":3158,"church":3159,"sourcing":3160,"Included":3161,"Markov":3162,"recog":3163,"environment":3164,"slope":3165,"IEEE":3166,"cedures":3167,"calme":3168,"TRIME":3169,"likely":3170,"Klein":3171,"accomplishing":3172,"Kwak":3173,"versions":3174,"Kaplun":3175,"seamless":3176,"manufacturer":3177,"diameter":3178,"imagine":3179,"thorny":3180,"14858":3181,"allreduce":3182,"challenges":3183,"toss":3184,"7969":3185,"2020s":3186,"anticlimactic":3187,"第二节":3188,"DPR":3189,"Navigating":3190,"OmniTab":3191,"One":3192,"Hanover":3193,"W":3194,"2s":3195,"shards":3196,"Add":3197,"substrates":3198,"honor":3199,"Stride":3200,"Potential":3201,"Berkov":3202,"509":3203,"breed":3204,"Trade":3205,"334":3206,"Proxy":3207,"served":3208,"layer":3209,"Rogers":3210,"Discussions176":3211,"hidden":3212,"4966":3213,"Significance":3214,"microservices":3215,"daisy":3216,"Sun":3217,"DISC":3218,"conventions":3219,"flight":3220,"house":3221,"01108":3222,"736":3223,"Act":3224,"alternating":3225,"Why":3226,"Friedrich":3227,"至":3228,"CA":3229,"Israeli":3230,"vicinity":3231,"dependently":3232,"syntactic":3233,"spots":3234,"Reproduction":3235,"pd":3236,"replicate":3237,"chan":3238,"actu":3239,"serman":3240,"Teye":3241,"Simulate":3242,"contain":3243,"gaining":3244,"starve":3245,"Shuai":3246,"permute":3247,"990":3248,"409":3249,"elegant":3250,"notes":3251,"519127":3252,"⟨":3253,"ServiceNow":3254,"Shao":3255,"Bastien":3256,"Regardless":3257,"486":3258,"icant":3259,"recon":3260,"Benton":3261,"justifying":3262,"Mathemat":3263,"tures":3264,"Discussions282":3265,"Forfeitures":3266,"discipline":3267,"Han":3268,"ubuntu":3269,"Plankalk":3270,"framing":3271,"registry":3272,"affects":3273,"coincident":3274,"transferring":3275,"2210":3276,"Days":3277,"Left":3278,"Scialom":3279,"10.6":3280,"Compiling":3281,"Choose":3282,"12363":3283,"Contributions":3284,"Imperative":3285,"ceded":3286,"14403":3287,"369000":3288,"padded":3289,"Sciences":3290,"Depending":3291,"motion":3292,"1095":3293,"expressiveness":3294,"ducts":3295,"Discussions110":3296,"peak":3297,"PRE":3298,"invert":3299,"paragraphs":3300,"267":3301,"Alterna":3302,"reflect":3303,"ac":3304,"60mm":3305,"Cognitive":3306,"categorical":3307,"Admission":3308,"0.927":3309,"Block":3310,"SelectBackward0":3311,"vanishes":3312,"Compact":3313,"arrows":3314,"Increase":3315,"Gevers":3316,"greenhouse":3317,"correlations":3318,"Scratch":3319,"Subsample":3320,"inductive":3321,"1032":3322,"Thesis":3323,"Hamid":3324,"revolutionize":3325,"adjacent":3326,"Chihuahuas":3327,"chmod":3328,"formal":3329,"Ryzen":3330,"etrable":3331,"imgs":3332,"minimization":3333,"ethnicity":3334,"2762":3335,"ticket":3336,"11019":3337,"thirds":3338,"fourth":3339,"alphas":3340,"causally":3341,"disasters":3342,"hybridize":3343,"Gopalakrishna":3344,"copilots":3345,"interactions":3346,"Staff":3347,"Loan":3348,"certification":3349,"9.4":3350,"400MB":3351,"detected":3352,"Frozen":3353,"corruption":3354,"norms":3355,"suppression":3356,"下方":3357,"dependency":3358,"Prompting":3359,"lkevinzc":3360,"Sainath":3361,"Buslaev":3362,"Supply":3363,"Further":3364,"trigonometric":3365,"185":3366,"Obtaining":3367,"layerwise":3368,"Discussions69":3369,"L3":3370,"466":3371,"dilution":3372,"system":3373,"factoid":3374,"Decoding":3375,"brianhendee":3376,"affairs":3377,"SKCODER":3378,"Masius":3379,"0":3380,"bills":3381,"0.5253":3382,"default":3383,"Without":3384,"revises":3385,"Fine":3386,"indications":3387,"Soft":3388,"Memory":3389,"Backgammon":3390,"misconception":3391,"52":3392,"Additional":3393,"5000":3394,"cauchy":3395,"unzip":3396,"异步":3397,"280":3398,"5313":3399,"valuable":3400,"TAG":3401,"863":3402,"delimited":3403,"1e6":3404,"000":3405,"Machine":3406,"unsur":3407,"DPUs":3408,"Including":3409,"hundred":3410,"Audio":3411,"Sergeev":3412,"generator":3413,"Activation":3414,"design":3415,"核心":3416,"scales":3417,"capabilities":3418,"notable":3419,"Lavin":3420,"Jaeger":3421,"Attend":3422,"Omni":3423,"识别":3424,"Boulevard":3425,"tell":3426,"Revisions":3427,"equivalence":3428,"dur":3429,"165":3430,"agendas":3431,"ming":3432,"confidential":3433,"transition":3434,"LSI":3435,"generality":3436,"free":3437,"Huawei":3438,"QPI":3439,"whatsoever":3440,"transported":3441,"administrative":3442,"compact":3443,"novel":3444,"opponent":3445,"apples":3446,"GOALS":3447,"reverse":3448,"outfitted":3449,"disribution":3450,"q2p":3451,"LlamaIndex":3452,"Feng":3453,"workflow":3454,"oak":3455,"„":3456,"外传":3457,"0.0975":3458,"diminish":3459,"stat":3460,"accommodates":3461,"flash":3462,"0.229":3463,"recurrently":3464,"RAC":3465,"Prepares":3466,"collaborates":3467,"adtygan":3468,"Rackoff":3469,"DRELL":3470,"2567":3471,"shirts":3472,"identical":3473,"13.6":3474,"sexuality":3475,"hotdog":3476,"recognizing":3477,"Exchange":3478,"152":3479,"Vgnmn":3480,"intertwined":3481,"121":3482,"421":3483,"Conventions":3484,"excitation":3485,"disaggregation":3486,"sink":3487,"Helper":3488,"Bonn":3489,"Binary":3490,"serviceability":3491,"530%":3492,"disturbing":3493,"6.6":3494,"905":3495,"Surpassed":3496,"situated":3497,"错误":3498,"plates":3499,"judgments":3500,"Breaches":3501,"Wong":3502,"AnyNetA":3503,"instruction":3504,"interchangeable":3505,"Per":3506,"客户端":3507,"indifferent":3508,"vector":3509,"Similar":3510,"contractor":3511,"Kindi":3512,"lot":3513,"4090":3514,"Recht":3515,"风景图":3516,"sep":3517,"relations":3518,"Percentages":3519,"Diffusion":3520,"CONCRETE":3521,"3f":3522,"𝑌":3523,"Equivalents":3524,"Weld":3525,"Reinforcement":3526,"sify":3527,"weaknesses":3528,"USSR":3529,"Smeulders":3530,"catastroph":3531,"heavy":3532,"Service":3533,"rewritten":3534,"tems":3535,"ancient":3536,"signing":3537,"Economy":3538,"arms":3539,"GTX":3540,"rig":3541,"2017":3542,"Discussions226":3543,"058":3544,"plentiful":3545,"03":3546,"8.8":3547,"Hybrid":3548,"Peking":3549,"atten":3550,"Penalty":3551,"perception":3552,"EKF":3553,"KNN":3554,"Codification":3555,"Bebchuk":3556,"sell":3557,"righly":3558,"upperlefts":3559,"inv":3560,"technique":3561,"ICLR":3562,"GPU1":3563,"earmarked":3564,"berghe":3565,"owners":3566,"fades":3567,"performant":3568,"animation":3569,"Brunot":3570,"AA":3571,"unknowns":3572,"2394":3573,"denied":3574,"Asai":3575,"McMaster":3576,"explaining":3577,"Millions":3578,"Subsequently":3579,"enter":3580,"branches":3581,"combine":3582,"Git":3583,"dia":3584,"amateur":3585,"fare":3586,"Rerank":3587,"producing":3588,"daily":3589,"𝜙":3590,"Unlimiformer":3591,"pat":3592,"Rodol":3593,"readiness":3594,"238":3595,"digits":3596,"rithms":3597,"pilot":3598,"trees":3599,"210":3600,"redefine":3601,"supercomputer":3602,"implements":3603,"waiver":3604,"Central":3605,"steinsag":3606,"Journal":3607,"varieties":3608,"explosion":3609,"operated":3610,"686":3611,"Deferred":3612,"traversing":3613,"beddings":3614,"Transaction":3615,"fill":3616,"Sourcing":3617,"trols":3618,"accrual":3619,"Aside":3620,"engineered":3621,"valu":3622,"Inventories":3623,"crisply":3624,"xlarge":3625,"complexities":3626,"Vap":3627,"4021":3628,"destroy":3629,"corpora":3630,"6980":3631,"scientific":3632,"":3633,"issuance":3634,"noising":3635,"Bessel":3636,"Finite":3637,"library":3638,"autocom":3639,"author":3640,"100002":3641,"threaded":3642,"representer":3643,"interpreters":3644,"thermodynamics":3645,"SE":3646,"embeddings":3647,"272":3648,"nominees":3649,"1062":3650,"unrest":3651,"translating":3652,"senorcinco":3653,"Demographics":3654,"GPG":3655,"08477":3656,"stubbornly":3657,"Pad":3658,"616":3659,"1649":3660,"filtration":3661,"skill":3662,"overhang":3663,"399":3664,"Sec":3665,"backpropagation":3666,"manufacturers":3667,"satisfies":3668,"unchanged":3669,"Chi2":3670,"037703019195187606":3671,"underscores":3672,"Automata":3673,"Sonnet":3674,"anisms":3675,"X800":3676,"compared":3677,"beach":3678,"3085":3679,"224":3680,"Eugene":3681,"Launch":3682,"rescale":3683,"872":3684,"malware":3685,"periodicity":3686,"HEREWITH":3687,"reloaded":3688,"employers":3689,"MacOSX":3690,"4641":3691,"ering":3692,"theorem":3693,"ConceptFlow":3694,"jurisdictions":3695,"Record":3696,"Tensor":3697,"Yards":3698,"4656":3699,"TOMCCAP":3700,"breakdown":3701,"Approaching":3702,"orates":3703,"func":3704,"MaxPool2d":3705,"shuffling":3706,"Ensure":3707,"02972":3708,"clearing":3709,"Asanovic190":3710,"repatriate":3711,"reconsider":3712,"discrep":3713,"belong":3714,"feed":3715,"Informa":3716,"textbook":3717,"float64":3718,"drove":3719,"hvN":3720,"legge":3721,"631":3722,"contained":3723,"impair":3724,"724":3725,"PositiveDefiniteTransform":3726,"1959":3727,"106730":3728,"nondisclosure":3729,"2304":3730,"prerequisite":3731,"Discussions67":3732,"commute":3733,"07747":3734,"issuances":3735,"injecting":3736,"Fang":3737,"lengthy":3738,"modest":3739,"hurdles":3740,"Vectorization":3741,"implement":3742,"445":3743,"Living":3744,"Presidential":3745,"2026":3746,"cross":3747,"as":3748,"Contingencies":3749,"borders":3750,"liner":3751,"Ample":3752,"convnets":3753,"achievement":3754,"3TG":3755,"Xv":3756,"hampers":3757,"FALCON":3758,"analy":3759,"xxiii":3760,"sentences":3761,"tighter":3762,"RandomHorizontalFlip":3763,"1x1":3764,"Adjust":3765,"unlock":3766,"217%":3767,"transpose":3768,"multiprocessors":3769,"95%":3770,"Observed":3771,"ensures":3772,"446":3773,"555":3774,"URL":3775,"cats":3776,"efficientqa":3777,"Denton":3778,"Soricut":3779,"fundamentals":3780,"Discussions147":3781,"loses":3782,"667":3783,"compounds":3784,"TensorFlow":3785,"stunning":3786,"decimal":3787,"Excessive":3788,"58%":3789,"&":3790,"hierarchically":3791,"apple":3792,"Sustainability":3793,"contextualized":3794,"downloadable":3795,"GPs":3796,"=":3797,"CAL":3798,"tempted":3799,"Memberships":3800,"Quanta":3801,"rectangular":3802,"names":3803,"biases":3804,"unranked":3805,"Discussions115":3806,"560":3807,"5929":3808,"𝑜":3809,"assistance":3810,"176":3811,"rnns":3812,"companies":3813,"surveys":3814,"RBA":3815,"unroll":3816,"seaborn47":3817,"otherwise":3818,"Sherrington":3819,"247":3820,"wards":3821,"begun":3822,"gumbel":3823,"安全性":3824,"Corporate":3825,"82":3826,"reinvestment":3827,"4060":3828,"accrue":3829,"IMAGE":3830,"vious":3831,"transpired":3832,"They":3833,"Parzen":3834,"Optimality":3835,"Category":3836,"Like":3837,"365":3838,"els":3839,"1x1conv":3840,"Radial":3841,"labs":3842,"wider":3843,"Quinlan":3844,"A.19":3845,"passed":3846,"Used":3847,"异常":3848,"amounts":3849,"00846":3850,"AudioMAE":3851,"3922":3852,"08056711961080017":3853,"2012":3854,"timators":3855,"squaring":3856,"Ultimate":3857,"Genius":3858,"tantalum":3859,"Weyerhaeuser":3860,"Biological":3861,"Discussions181":3862,"probabilis":3863,"boundaries":3864,"toward":3865,"net2":3866,"integrate":3867,"finds":3868,"anal":3869,"Scalable":3870,"eigs":3871,"gradient":3872,"837":3873,"ECOSYSTEM":3874,"penalty":3875,"Podoprikhin":3876,"trimmed":3877,"Products":3878,"Everything":3879,"denial":3880,"eyed":3881,"t":3882,"occurring":3883,"344":3884,"gets":3885,"VOCSegDataset":3886,"orange":3887,"1735":3888,"promoted":3889,"Lectures":3890,"Requirement":3891,"样式":3892,"augmen":3893,"trace":3894,"obfuscate":3895,"LI":3896,"575":3897,"Mathematically":3898,"Games":3899,"714":3900,"057646":3901,"shifted":3902,"sponsors":3903,"liable":3904,"tant":3905,"gravity":3906,"training":3907,"Rhyu":3908,"156":3909,"BCB":3910,"world":3911,"wh":3912,"boosting":3913,"group":3914,"14":3915,"Cooling":3916,"ReF":3917,"Discussions279":3918,"P4":3919,"prohibit":3920,"16.2":3921,"Machines":3922,"FactSet":3923,"ViT":3924,"mensions":3925,"8427":3926,"en":3927,"965637":3928,"integrity":3929,"Florian":3930,"GENOMICS":3931,"comparably":3932,"calculates":3933,"differently":3934,"Kardas":3935,"sandal":3936,"webentwicklung":3937,"biguation":3938,"components":3939,"556":3940,"cv":3941,"sam":3942,"firsthand":3943,"blocks":3944,"accepted":3945,"durable":3946,"proven":3947,"illustrates":3948,"pursuit":3949,"2227":3950,"Foreign":3951,"experimen":3952,"Some":3953,"period":3954,"skis":3955,"ingredient":3956,"Theory":3957,"tweaks":3958,"9213":3959,"forming":3960,"specialized":3961,"fusion":3962,"workings":3963,"prop":3964,"limita":3965,"parametrized":3966,"Focuses":3967,"downstream":3968,"sibility":3969,"ammunition":3970,"questions":3971,"Daly":3972,"aligning":3973,"protein":3974,"1000000":3975,"𝛿":3976,"Kong":3977,"1e4":3978,"Artif":3979,"~":3980,"simplified":3981,"13547":3982,"Exponential":3983,"2.6":3984,"692":3985,"approved":3986,"IC":3987,"Contracts":3988,"teams":3989,"Keskar":3990,"HIV":3991,"866":3992,"Continued":3993,"al":3994,"besides":3995,"é":3996,"exercises":3997,"3305":3998,"105599":3999,"distracted":4000,"dissertation":4001,"cohorts":4002,"misclassify":4003,"harvesting":4004,"10588":4005,"CSPs":4006,"daughter":4007,"true":4008,"47986":4009,"invites":4010,"automatically":4011,"0.000017":4012,"Whereas":4013,"1212":4014,"chained":4015,"decoded":4016,"presently":4017,"sooner":4018,"DAIR":4019,"azimjonn":4020,"cesses":4021,"why":4022,"zh":4023,"cels":4024,"ecosystem":4025,"exposed":4026,"𝑐":4027,"portrait":4028,"HPO":4029,"nonpara":4030,"False":4031,"conducted":4032,"cap":4033,"how":4034,"2009":4035,"USENIX":4036,"install":4037,"MaskLM":4038,"settlements":4039,"helps":4040,"versatility":4041,"Boxes":4042,"425":4043,"answer":4044,"website16":4045,"Leadership":4046,"Swope":4047,"406":4048,"ATM":4049,"skincare":4050,"城市":4051,"Jupyter":4052,"reproduction":4053,"No":4054,"lens":4055,"resulting":4056,"Barth":4057,"multiplexer209":4058,"Con":4059,"gl":4060,"variants":4061,"White":4062,"inconvenient":4063,"365109":4064,"063172":4065,"Hs":4066,"233":4067,"1898":4068,"2237":4069,"InputBlaster":4070,"Complicating":4071,"Ajay":4072,"superhuman":4073,"VOCtrainval":4074,"1985":4075,"v0":4076,"Upsampling":4077,"educational":4078,"subgraph":4079,"bananas":4080,"6907":4081,"card":4082,"dealing":4083,"246889":4084,"16x":4085,"sparse":4086,"disaster":4087,"ought":4088,"publications":4089,"exploration":4090,"Adagrad":4091,"最终":4092,"follows":4093,"rectness":4094,"Fork":4095,"Announced":4096,"tica":4097,"mainboard":4098,"reflection":4099,"LEFT":4100,"munication":4101,"television":4102,"resurgence":4103,"RegNetX":4104,"accelerator":4105,"present":4106,"16.3":4107,"blockchain":4108,"2.7281":4109,"Task":4110,"10b5":4111,"overloaded":4112,"exper":4113,"healthy":4114,"500px":4115,"maximally":4116,"Selected":4117,"notified":4118,"Named":4119,"dataset":4120,"assumptions":4121,"393":4122,"Terms":4123,"equipped":4124,"ice":4125,"speeding":4126,"apart":4127,"former":4128,"Downsampling":4129,"07164":4130,"𝑈":4131,"finishing":4132,"HSUN":4133,"fluctuated":4134,"Uppercase":4135,"8103":4136,"Call":4137,"blurring":4138,"Rescuing":4139,"distortion":4140,"Discussions156":4141,"provider":4142,"distorts":4143,"40":4144,"multimedia":4145,"635":4146,"zoom":4147,"Kernels":4148,"00019":4149,"37":4150,"Quantenna":4151,"Wow":4152,"Tung":4153,"Accounting":4154,"Emer":4155,"PolyScheduler":4156,"While":4157,"Factory":4158,"Balaji":4159,"treating":4160,"put":4161,"c2":4162,"083":4163,"synthesis":4164,"Fund":4165,"trievers":4166,"662":4167,"Pan":4168,"Academies":4169,"15604":4170,"Seq2SeqDecoder":4171,"Yu":4172,"accelerating":4173,"Okroshiashvili":4174,"adapters":4175,"misnomer":4176,"Fix":4177,"ch11":4178,"collects":4179,"energy":4180,"Incorrectly":4181,"Hence":4182,"spreadsheet":4183,"Gbps":4184,"golden":4185,"cians":4186,"overflow":4187,"Van":4188,"pretend":4189,"biologists":4190,"Discussions253":4191,"Active":4192,"gap":4193,"xvi":4194,"After":4195,"64":4196,"captioning":4197,"0.8944":4198,"partnerships":4199,"surrogate":4200,"Base":4201,"TalNeoran":4202,"abbreviate":4203,"underfitting":4204,"A.21":4205,"00005":4206,"campus":4207,"0.048":4208,"dedicated":4209,"Realistic":4210,"exit":4211,"ar":4212,"Spatio":4213,"cumulating":4214,"Syne":4215,"consecu":4216,"Unterthiner":4217,"uncorrelated":4218,"666":4219,"paving":4220,"Allahyar":4221,"Bell":4222,"journey":4223,"terize":4224,"correction":4225,"Compressed":4226,"T5":4227,"pioneering":4228,"flag":4229,"grass":4230,"unk":4231,"Sets":4232,"08345":4233,"Discussions214":4234,"int64":4235,"spades":4236,"encoded":4237,"Germany":4238,"capped":4239,"如下":4240,"Op":4241,"second":4242,"dent":4243,"_.":4244,"Donahue":4245,"confused":4246,"AnyNet":4247,"constraints":4248,"Weakly":4249,"incremental":4250,"iPad":4251,"Intellectual":4252,"constants":4253,"cepting":4254,"Arora":4255,"after":4256,"exhibiting":4257,"Penalties":4258,"Steve":4259,"adaptivity":4260,"Goodwill":4261,"Cocke":4262,"nonequilibrium":4263,"shortcuts":4264,"because":4265,"party":4266,"partnership":4267,"Leibler":4268,"monolingual":4269,"Fiscal":4270,"Emirates":4271,"unlike":4272,"coordinate":4273,"lenges":4274,"Needs":4275,"Quadrana":4276,"示例":4277,"academics":4278,"Shukla":4279,"projections":4280,"Canada":4281,"Luo":4282,"Optane":4283,"⊈":4284,"FORM":4285,"Ili":4286,"categorizing":4287,"preconfig":4288,"gle":4289,"iterating":4290,"abstractions":4291,"terminated":4292,"plans":4293,"primary":4294,"Deng":4295,"Denote":4296,"unigrams":4297,"Expenses":4298,"target":4299,"Schmidhuber":4300,"autonomous":4301,"light":4302,"amends":4303,"keyring":4304,"ignited":4305,"2031":4306,"bel":4307,"research":4308,"ResNets":4309,"3140976097":4310,"Returning":4311,"technological":4312,"Discussions167":4313,"peers":4314,"remained":4315,"ys":4316,"hinted":4317,"4463":4318,"pathbreaking":4319,"authority":4320,"impractical":4321,"vfdev":4322,"squashing":4323,"initializers":4324,"Repeated":4325,"iterates":4326,"NumPy":4327,"alize":4328,"A.44":4329,"Features":4330,"flaws":4331,"themselves":4332,"env":4333,"strengthening":4334,"023":4335,"omelette":4336,"pinecone":4337,"signs":4338,"上海":4339,"depreciated":4340,"数量":4341,"ODMs":4342,"12.9":4343,"paired":4344,"sketches":4345,"1965":4346,"Prize":4347,"Indices":4348,"theorists":4349,"uninhibited":4350,"preds":4351,"source":4352,"hurt":4353,"alphabetically":4354,"mented":4355,"pursuing":4356,"uncontrollably":4357,"1777":4358,"Computing":4359,"cinematic":4360,"cards":4361,"usefully":4362,"Recog":4363,"Bluetooth":4364,"Match":4365,"Previously":4366,"investor":4367,"Momentum":4368,"adapts":4369,"LD":4370,"Bertolami":4371,"Definition":4372,"Gad":4373,"Physiology":4374,"Posterior":4375,"Instructions":4376,"graphs":4377,"contracts":4378,"Bardenet":4379,"1080":4380,"Bellemare":4381,"assists":4382,"Point":4383,"equitable":4384,"margin":4385,"kens":4386,"Individual":4387,"532":4388,"hashtag":4389,"disseminating":4390,"29442952580755816":4391,"Musto":4392,"bottlenecked":4393,"Directors":4394,"exocentric":4395,"opens":4396,"Bowen":4397,"eponymously":4398,"Demand":4399,"fitted":4400,"1556":4401,"金额":4402,"Winston":4403,"Holt":4404,"sequently":4405,"Roy":4406,"prefact":4407,"utterances":4408,"tradition":4409,"BioASQ":4410,"criterion":4411,"diver":4412,"chaining":4413,"RAM":4414,"11692":4415,"361":4416,"fluctuates":4417,"Srivastava":4418,"TACL":4419,"4th":4420,"eagle":4421,"sound":4422,"TOTAL":4423,"Nsight186":4424,"Hochreiter":4425,"cally":4426,"Classes":4427,"Continually":4428,"loose":4429,"L40":4430,"A.37":4431,"Forum":4432,"Appeals":4433,"certiorari":4434,"1231e":4435,"026035979388614055":4436,"pdf":4437,"senses":4438,"ODQA":4439,"conv3":4440,"带有":4441,"spreads":4442,"ned":4443,"pessimistic":4444,"ton":4445,"lumped":4446,"corporum":4447,"oscillate":4448,"877":4449,"compiler":4450,"12.2":4451,"deepen":4452,"conceptualized":4453,"11446":4454,"incurs":4455,"AlphaGo":4456,"posited":4457,"Kirkpatrick":4458,"Conversion":4459,"tutorials":4460,"diminished":4461,"constrain":4462,"injunctive":4463,"His":4464,"homelessness":4465,"touch":4466,"5.7":4467,"Discussions134":4468,"Muhyun":4469,"Oki":4470,"286":4471,"avoiding":4472,"dict":4473,"xlabel":4474,"Originally":4475,"dispositive":4476,"2001":4477,"084":4478,"slippery":4479,"Smooth":4480,"odds":4481,"3.486784":4482,"enhancement":4483,"then":4484,"3028":4485,"questionnaires":4486,"Cappelli":4487,"Page":4488,"encourage":4489,"SIAM":4490,"stant":4491,"RKHS":4492,"49346":4493,"DenseNet":4494,"302":4495,"regularization":4496,"sands":4497,"2010":4498,"riddled":4499,"8675309":4500,"nu":4501,"1.5":4502,"Hessians":4503,"acceptance":4504,"u202f":4505,"trips":4506,"conflict":4507,"contributed":4508,"swapaxes":4509,"shortcomings":4510,"hour":4511,"labor":4512,"infrequently":4513,"9352":4514,"Partial":4515,"Accountability":4516,"stitching":4517,"222":4518,"813":4519,"Maaten":4520,"methodological":4521,"classi":4522,"maximum":4523,"extractall":4524,"appoint":4525,"\u0014":4526,"possessive":4527,"shaping":4528,"Sphinx":4529,"Uc":4530,"maturing":4531,"936":4532,"sanctions":4533,"apt":4534,"loader":4535,"confer":4536,"Refer":4537,"unfolded":4538,"Michael":4539,"1Who":4540,"R100":4541,"fra":4542,"1.2890":4543,"Ingredient":4544,"PERSONS":4545,"volunteers":4546,"Excluding":4547,"holders":4548,"价格":4549,"appeal":4550,"Mikolov":4551,"Sponsoring":4552,"disciplines":4553,"Ima":4554,"Steinhardt":4555,"9185e":4556,"heritage":4557,"EOS":4558,"supermajority":4559,"Touvron":4560,"sequen":4561,"14%":4562,"flying":4563,"∉":4564,"piracy":4565,"78":4566,"imshow":4567,"Asset":4568,"1Whc":4569,"Generate":4570,"3788":4571,"ternary":4572,"Certificate":4573,"Kasai":4574,"derivation":4575,"fallout":4576,"AliPay":4577,"scale":4578,"underperforming":4579,"Period":4580,"Shu":4581,"Intell":4582,"Devlin":4583,"compatibility":4584,"10612":4585,"triever":4586,"Returns":4587,"percentile":4588,"Interactions":4589,"subspaces":4590,"heuristic":4591,"Composition":4592,"popular":4593,"4S":4594,"ta":4595,"store":4596,"satisfactory":4597,"subjected":4598,"tabular":4599,"indiscriminately":4600,"hansent":4601,"hp":4602,"assistant":4603,"reduced":4604,"Power":4605,"Gong":4606,"quad":4607,"incorporat":4608,"guarantees":4609,"4013":4610,"abstentions":4611,"Qu":4612,"455":4613,"eyes":4614,"Polysemy":4615,"9321":4616,"imports":4617,"11.7":4618,"h2":4619,"166":4620,"Z":4621,"325":4622,"4931":4623,"1038":4624,"5B":4625,"civil":4626,"cumsum":4627,"LakshKD":4628,"reality":4629,"Kung":4630,"closures":4631,"docu":4632,"Beneficial":4633,"overview":4634,"Discussions272":4635,"densities":4636,"Yuan":4637,"Reference":4638,"stitutional":4639,"par":4640,"Coxe":4641,"Eigenvectors":4642,"calculating":4643,"Drive":4644,"Linux":4645,"assigned":4646,"pooling":4647,"compensatory":4648,"aimed":4649,"04805":4650,"simultaneous":4651,"Merity":4652,"Maxwell":4653,"birds":4654,"randomness":4655,"corporate":4656,"threat":4657,"query":4658,"machine":4659,"plau":4660,"Black":4661,"Once":4662,"3c914d17d80b1459be871a5039ac23e752a53cbe":4663,"very":4664,"Strauss":4665,"927":4666,"71e":4667,"solely":4668,"registration":4669,"Construct":4670,"pandas":4671,"underly":4672,"took":4673,"3332":4674,"entiation":4675,"precursors":4676,"meeting":4677,"cors":4678,"mycologist":4679,"taller":4680,"ARES":4681,"scenarios":4682,"agnostic":4683,"610":4684,"white":4685,"Attention":4686,"yuntai":4687,"Jetson":4688,"marvelously":4689,"si":4690,"1559":4691,"perturbation":4692,"appearing":4693,"Louis":4694,"adopt":4695,"Owens":4696,"alization":4697,"adjective":4698,"71":4699,"010":4700,"probabilistic":4701,"reporters":4702,"Hypothesis":4703,"joint":4704,"back":4705,"arithmetic":4706,"refinance":4707,"Signature":4708,"ignores":4709,"treat":4710,"Duchi":4711,"Vinyals":4712,"ended":4713,"harmless":4714,"tells":4715,"05148":4716,"Designated":4717,"Levi":4718,"args":4719,"breaking":4720,"F1":4721,"Ended":4722,"Andrew":4723,"chicken":4724,"unfair":4725,"background":4726,"troller":4727,"Suppression":4728,"patient":4729,"Computershare":4730,"Computation":4731,"billions":4732,"bf":4733,"bidirectionally":4734,"determiner":4735,"":4736,"positional":4737,"ipynb":4738,"00003":4739,"0277":4740,"lets":4741,"treats":4742,"guage":4743,"Orr":4744,"434":4745,"Certain":4746,"Male":4747,"01392":4748,"NVL72":4749,"Yoshua":4750,"pends":4751,"ProgressBoard":4752,"Determining":4753,"1026":4754,"straight":4755,"death":4756,"Requests":4757,"860":4758,"webcasts":4759,"百分":4760,"61.917364":4761,"appointed":4762,"R2A":4763,"名称":4764,"fa":4765,"Information":4766,"driving":4767,"LinkedIn":4768,"Liujun":4769,"physics":4770,"multiplicities":4771,"1f77b4":4772,"grammatically":4773,"orientation":4774,"lenet":4775,"Continuum":4776,"Mature":4777,"Corthorn":4778,"came":4779,"Informatics":4780,"diagnosed":4781,"agenda":4782,"Oregon":4783,"Regulations":4784,"adadelta":4785,"899983":4786,"penghao":4787,"TWINS":4788,"Hongler":4789,"1500000":4790,"exceed":4791,"ical":4792,"insofar":4793,"220":4794,"Drell":4795,"𝑓":4796,"disappears":4797,"Discussions146":4798,"titions":4799,"driven":4800,"427400":4801,"Any":4802,"rstrip":4803,"Stu":4804,"Elements":4805,"promotion":4806,"08402":4807,"Urtasun":4808,"ReTraCk":4809,"tradiational":4810,"2.7":4811,"regularize":4812,"Recent":4813,"320":4814,"Usually":4815,"861":4816,"Split":4817,"nats":4818,"photo":4819,"grained":4820,"negotiated":4821,"OPT":4822,"incapable":4823,"o1":4824,"Royal":4825,"Haddow":4826,"Jannach":4827,"longtime":4828,"Envy":4829,"ular":4830,"09476":4831,"Santos":4832,"activa":4833,"SAP":4834,"df":4835,"Also":4836,"curve":4837,"0.35":4838,"059":4839,"influx":4840,"consumers":4841,"Introduced":4842,"CALIFORNIA":4843,"Miniconda3":4844,"←":4845,"discriminative":4846,"Owned":4847,"Sensitive":4848,"Multiply":4849,"H800":4850,"Those":4851,"Signatures":4852,"深圳":4853,"bar":4854,"Signing":4855,"boosts":4856,"Dimensional":4857,"900570":4858,"62":4859,"simultaneously":4860,"level":4861,"couple":4862,"20B":4863,"in":4864,"shirt":4865,"aforemen":4866,"Goodfellow":4867,"Pascal":4868,"Aarush":4869,"eryone":4870,"expansion":4871,"filer":4872,"withheld":4873,"Selipsky":4874,"Fairness":4875,"initiative":4876,"beneath":4877,"630":4878,"threatened":4879,"338":4880,"loads":4881,"Convenience":4882,"perturb":4883,"CosineScheduler":4884,"Colloquially":4885,"hz":4886,"dirty":4887,"68e":4888,"LG":4889,"fifth":4890,"exposes":4891,"iter":4892,"Word":4893,"Cremonesi":4894,"0.002295":4895,"probable":4896,"meaning":4897,"hyperkernels":4898,"worker":4899,"Cap":4900,"expansive":4901,"Discussions123":4902,"balances":4903,"remotely":4904,"shuffle":4905,"hit":4906,"Added":4907,"simulate":4908,"060":4909,"ri":4910,"infrequent":4911,"infrastructures":4912,"624":4913,"Equations":4914,"2042040":4915,"Goldman":4916,"00923":4917,"Machine137":4918,"timeliness":4919,"disregard":4920,"forces":4921,"75%":4922,"ear":4923,"6008":4924,"alignment":4925,"Designing":4926,"arose":4927,"computable":4928,"0.0":4929,"hierarchical":4930,"accom":4931,"Recommendations":4932,"Korea":4933,"1078":4934,"Strong":4935,"Sequential":4936,"1705":4937,"totaling":4938,"marketable":4939,"distillation":4940,"Suddenly":4941,"ill":4942,"devoted":4943,"governs":4944,"0.246":4945,"22026":4946,"206":4947,"Discussions101":4948,"Discussions257":4949,"intention":4950,"committee":4951,"incrementally":4952,"KIF":4953,"--------":4954,"Advances":4955,"eligible":4956,"weighs":4957,"trickier":4958,"Metrics":4959,"Madaan":4960,"wow":4961,"TPU":4962,"based":4963,"ambiguity":4964,"manager":4965,"disentangle":4966,"SQuAD":4967,"tenant":4968,"torch":4969,"preferably":4970,"Presiding":4971,"518":4972,"nominated":4973,"theories":4974,"configuring":4975,"mistaking":4976,"Year":4977,"perquisites":4978,"4127":4979,"uous":4980,"Retention":4981,"alcohol":4982,"Fubini":4983,"DALL":4984,"Kirstain":4985,"wikitext":4986,"necessity":4987,"jointly":4988,"0.608":4989,"breaks":4990,"COMMISSION":4991,"Desay":4992,"muscle":4993,"recently":4994,"841":4995,"23rd":4996,"reused":4997,"xd":4998,"Rainier":4999,"executes":5000,"LoRA":5001,"Waste":5002,"stochasticity":5003,"5040":5004,"WeightDecayScratch":5005,"1925":5006,"50d":5007,"dawn":5008,"segmented":5009,"JDS":5010,"billion":5011,"1048576":5012,"qualifies":5013,"modicum":5014,"26e":5015,"conda":5016,"laptop":5017,"Renesas":5018,"comparing":5019,"link":5020,"iment":5021,"appropriateness":5022,"Carrying":5023,"AudioLDM":5024,"improvements":5025,"phrase":5026,"Notified":5027,"clearer":5028,"compo":5029,"Done":5030,"内容":5031,"microscope":5032,"couldn":5033,"microinstructions":5034,"Boxcar":5035,"letters":5036,"Long":5037,"semester":5038,"invariance":5039,"cohesive":5040,"Siva":5041,"neighbor":5042,"12.5":5043,"renormalization":5044,"Vanden":5045,"News":5046,"independencies":5047,"4000":5048,"2011":5049,"458":5050,"becomes":5051,"asynchronous":5052,"Score":5053,"646":5054,"they":5055,"Calculate":5056,"Efficiency":5057,"085":5058,"lieu":5059,"rough":5060,"983":5061,"NP":5062,"attempts":5063,"invariances":5064,"Hawaiian":5065,"511":5066,"408":5067,"leased":5068,"Lv":5069,"Taco":5070,"visual":5071,"Severance":5072,"Lu":5073,"wearing":5074,"sport":5075,"R3":5076,"Please":5077,"734384":5078,"terawatt":5079,"careful":5080,"introductory":5081,"Ilyas":5082,"vest":5083,"demonstrate":5084,"wafer":5085,"corpus":5086,"dressing":5087,"identities":5088,"Los":5089,"529":5090,"0.00176955":5091,"employing":5092,"modalities":5093,"tariffs":5094,"1894":5095,"mailing":5096,"citations":5097,"communi":5098,"bench":5099,"Over":5100,"diner":5101,"instantaneous":5102,"contradiction":5103,"Environmental":5104,"Answer":5105,"𝑝":5106,"CIO":5107,"fan":5108,"partite":5109,"Iterators":5110,"advancements":5111,"copying":5112,"remembering":5113,"Operations":5114,"Wistron":5115,"indeed":5116,"include":5117,"3032613031191755":5118,"tutorial269":5119,"Brands":5120,"BOS":5121,"Photorealistic":5122,"tion59":5123,"vastava":5124,"iStock":5125,"overshoots":5126,"024":5127,"domly":5128,"agriculture":5129,"reigned":5130,"𝐹":5131,"Rajchandra":5132,"enable":5133,"autumn":5134,"did":5135,"initializing":5136,"assessment":5137,"imagination":5138,"overload":5139,"1122":5140,"transporting":5141,"mine":5142,"bold":5143,"g2":5144,"url":5145,"Mnih":5146,"payable":5147,"Device":5148,"PLUG":5149,"discussions":5150,"°":5151,"methods":5152,"defects":5153,"84%":5154,"autonomously":5155,"Condition":5156,"Tesla":5157,"Puri":5158,"utilizes":5159,"converging":5160,"optimizers":5161,"overshoot":5162,"operating":5163,"trade":5164,"1034":5165,"Ab":5166,"Compose":5167,"Lipchitz":5168,"Dump":5169,"Source":5170,"ICASSP":5171,"rag":5172,"mismatching":5173,"imputed":5174,"Timer":5175,"leakage":5176,"Nos":5177,"monomials":5178,"hub":5179,"almost":5180,"Fellowship":5181,"bathrooms":5182,"Fibre":5183,"dro":5184,"spread":5185,"Marketing":5186,"interpreter":5187,"01701":5188,"stocks":5189,"Still":5190,"xxvii":5191,"degrade":5192,"Gunrock198":5193,"much":5194,"recognized":5195,"February":5196,"knack":5197,"1051":5198,"serialized":5199,"Abdel":5200,"Structured":5201,"topk":5202,"1281":5203,"subgradient":5204,"1648877":5205,"Fosler":5206,"character":5207,"dently":5208,"Combined":5209,"arrive":5210,"pens":5211,"multiplied":5212,"beating":5213,"NOW":5214,"08415":5215,"restaurant":5216,"163":5217,"cells":5218,"aircraft":5219,"Home":5220,"shjustinbaek":5221,"exten":5222,"delegates":5223,"morning":5224,"dot":5225,"至关重要":5226,"chunk":5227,"Few":5228,"decodes":5229,"shaded":5230,"illness":5231,"Dropbox":5232,"153846":5233,"标准":5234,"CFO":5235,"evolve":5236,"n":5237,"GPT":5238,"declare":5239,"Logical":5240,"126":5241,"technical":5242,"resolutions":5243,"0.75":5244,"Extend":5245,"completely":5246,"launched":5247,"Dawn":5248,"Id":5249,"seek":5250,"horses":5251,"Permit":5252,"60":5253,"maybe":5254,"invalidation":5255,"scholars":5256,"sate":5257,"tailed":5258,"logic":5259,"𝜇":5260,"sharper":5261,"14.10":5262,"BUSINESS":5263,"sults":5264,"failing":5265,"..":5266,"Files":5267,"full":5268,"2771":5269,"inextricably":5270,"Rome":5271,"military":5272,"Galactica":5273,"xaxis":5274,"Bougares":5275,"assert":5276,"Amortizable":5277,"ambiguous":5278,"Prolificdreamer":5279,"trend":5280,"A.23":5281,"Posts":5282,"Discussions145":5283,"reimburse":5284,"departments":5285,"表格":5286,"function":5287,"7262":5288,"indirect":5289,"Geopolitical":5290,"reputational":5291,"BM25":5292,"𝜔":5293,"Thai":5294,"demic":5295,"123":5296,"Vaswani":5297,"Dif":5298,"videos":5299,"controversial":5300,"第二季":5301,"Team":5302,"smorgasbord":5303,"automation":5304,"DG":5305,"Kumar":5306,"Reverse":5307,"stationary":5308,"02311":5309,"0.4472":5310,"112":5311,"Schuhmann":5312,"pieds":5313,"forum2":5314,"LeGresley":5315,"Souza":5316,"interrelations":5317,"idx":5318,"notify":5319,"timestamps":5320,"StepLR":5321,"SPARCStation":5322,"6b50d":5323,"Slicing":5324,"Soares":5325,"configure":5326,"Monterey":5327,"enforceable":5328,"monomial":5329,"Lerer":5330,"reference":5331,"Q":5332,"phenom":5333,"Pouget":5334,"Peer":5335,"stop":5336,"1296":5337,"touched":5338,"NEO":5339,"Bulletin":5340,"unwanted":5341,"Beta":5342,"Tokenize":5343,"Horovod":5344,"0.4":5345,"Russakovsky":5346,"Corrado":5347,"immanent":5348,"handy":5349,"Almost":5350,"evaluating":5351,"Counsel":5352,"controls":5353,"defendants":5354,"reduce":5355,"inevitably":5356,"laws":5357,"Distill1":5358,"824":5359,"rack":5360,"intelli":5361,"propaga":5362,"interpretability":5363,"Discussions180":5364,"differences":5365,"Years":5366,"Were":5367,"highlights":5368,"parallel":5369,"asynchrony":5370,"nuanced":5371,"sur":5372,"AIGC":5373,"ca":5374,"332":5375,"694":5376,"Output":5377,"brought":5378,"comp":5379,"DigitalVision":5380,"optimize":5381,"Einstein":5382,"pulling":5383,"0.0987":5384,"Utilizing":5385,"managing":5386,"survive":5387,"PatchEmbedding":5388,"experts":5389,"15131":5390,"Init":5391,"cording":5392,"contests":5393,"RoBERTa":5394,"Notice":5395,"tanh":5396,"Roukos":5397,"Declaration":5398,"multiscale":5399,"1506":5400,"proving":5401,"Marsel":5402,"qbaza":5403,"searchers":5404,"possible":5405,"revealed":5406,"super":5407,"Spotify":5408,"16063":5409,"1068":5410,"ASIF":5411,"speculative":5412,"补充":5413,"Sigmoid":5414,"importance":5415,"tremendously":5416,"773617148399353":5417,"bright":5418,"cifar":5419,"developed":5420,"Chaitanya":5421,"Santa":5422,"300d":5423,"564":5424,"dif":5425,"A.3":5426,"outstanding":5427,"mixer":5428,"classifies":5429,"dependencies":5430,"1865e":5431,"Luan":5432,"erally":5433,"209":5434,"DropoutMLPScratch":5435,"Tench":5436,"Ansys":5437,"Denoting":5438,"dormant":5439,"accurate":5440,"313":5441,"domestication":5442,"communications":5443,"evil":5444,"ordinary":5445,"revoke":5446,"Index":5447,"4.0":5448,"stepsize":5449,"AIBs":5450,"Individuals":5451,"AMIs":5452,"mixed":5453,"%":5454,"1984":5455,"787":5456,"Hoane":5457,"UPON":5458,"orderly":5459,"response":5460,"Curious":5461,"associate":5462,"Omnivore":5463,"03635":5464,"encapsulates":5465,"appointment":5466,"Management":5467,"unnecessarily":5468,"Percentile":5469,"sweep":5470,"potheses":5471,"标题":5472,"registrant":5473,"pins":5474,"Discussions224":5475,"excep":5476,"gen":5477,"cide":5478,"thresholds":5479,"Ron":5480,"proliferation":5481,"scheme":5482,"scientist":5483,"Studio":5484,"200MB":5485,"eliminated":5486,"initiates":5487,"adapter":5488,"6G":5489,"03172":5490,"partial":5491,"377":5492,"ize":5493,"finished":5494,"accident":5495,"08207":5496,"East":5497,"Italy":5498,"Revocable":5499,"𝐻":5500,"amplified":5501,"A.11":5502,"ber":5503,"0.119":5504,"Kojima":5505,"Say":5506,"14.5":5507,"entity":5508,"interpolating":5509,"compression":5510,"Datastore":5511,"OSDI":5512,"Lease":5513,"100000":5514,"Rhode":5515,"1008":5516,"Macherey":5517,"144887":5518,"sourced":5519,"iloc":5520,"092":5521,"recovering":5522,"796":5523,"Singer":5524,"Statement":5525,"Remember":5526,"1700s":5527,"Moon":5528,"Rombach":5529,"stars":5530,"00100":5531,"pose":5532,"commit":5533,"Actually":5534,"disadvantage":5535,"oscillates":5536,"SageMakerBackend":5537,"图片说明":5538,"Abdullah":5539,"growth":5540,"mercial":5541,"0.000018":5542,"nanasDataset":5543,"body":5544,"disc":5545,"linearity":5546,"enumerate":5547,"FTSE":5548,"Boyd":5549,"xxviii":5550,"tested":5551,"1049":5552,"language":5553,"werner":5554,"invali":5555,"toronto":5556,"Prasanth":5557,"autonomy":5558,"ratings":5559,"descrip":5560,"5167":5561,"711":5562,"offending":5563,"Discussions303":5564,"17.2":5565,"cardinality":5566,"742":5567,"Novikoff":5568,"carded":5569,"Field":5570,"champagne":5571,"se":5572,"Goldberg":5573,"comparison":5574,"dev":5575,"7890":5576,"War":5577,"de":5578,"attracts":5579,"Kiureghian":5580,"learning":5581,"crashing":5582,"Fundamentally":5583,"Quadratic":5584,"series":5585,"yticks":5586,"X":5587,"∞":5588,"inspecting":5589,"1968":5590,"0.162":5591,"‘":5592,"subs":5593,"01116":5594,"vertices":5595,"ACT":5596,"Infomatique":5597,"Mdiag":5598,"accordance":5599,"Jr":5600,"typical":5601,"3544903":5602,"阅读":5603,"SFI":5604,"Rank":5605,"Unsettled":5606,"Wahba":5607,"Repair":5608,"Discussions163":5609,"checks":5610,"Neumann":5611,"butterfly":5612,"Qualcomm":5613,"criminal":5614,"Petton":5615,"Arsenin":5616,"Whz":5617,"proxyholder":5618,"Discussions148":5619,"TruLens":5620,"rectional":5621,"directionality":5622,"covariance":5623,"Discussions130":5624,"𝜂":5625,"curated":5626,"blog":5627,"session":5628,"Schwartz":5629,"perceptrons":5630,"WILL":5631,"bigger":5632,"Amari":5633,"sort":5634,"434195":5635,"campaign":5636,"culprit":5637,"lives":5638,"Rationale":5639,"cx":5640,"finer":5641,"PLEASE":5642,"structions":5643,"INS":5644,"empiricism":5645,"Specific":5646,"2068874e4b9a9f0fb07ebe0ad2b29754449ccacd":5647,"Seq2SeqEncoder":5648,"Randomly":5649,"智能手机":5650,"KSJM":5651,"comprehensively":5652,"pratikhack":5653,"legions":5654,"969":5655,"Fundamentals":5656,"20cm":5657,"quotas":5658,"RIAG":5659,"Corresponding":5660,"Syst":5661,"retaliatory":5662,"SWITCH":5663,"395153":5664,"measurement":5665,"drift":5666,"A1":5667,"aka":5668,"Farley":5669,"088":5670,"99":5671,"PricewaterhouseCoopers":5672,"computes":5673,"sites":5674,"Discussions124":5675,"uptime":5676,"VGA":5677,"distinct":5678,"895":5679,"EDITSUM":5680,"mined":5681,"guages":5682,"𝝅":5683,"11B":5684,"consider":5685,"resumed":5686,"onerous":5687,"001636":5688,"pokemondb276":5689,"Estimation":5690,"bill":5691,"eralize":5692,"g3":5693,"secant":5694,"Explicit":5695,"Driver":5696,"collaboration":5697,"comprehensive":5698,"Yavuz":5699,"decrees":5700,"forfeiture":5701,"Minima":5702,"HHHTHTTHHHHHT":5703,"0.153":5704,"adverse":5705,"06318":5706,"Bommasani":5707,"proficient":5708,"being":5709,"sity":5710,"850":5711,"harvested":5712,"responses":5713,"241193":5714,"torchvision":5715,"noses":5716,"Bookeeping":5717,"502":5718,"ADDITIONAL":5719,"misstatement":5720,"0013":5721,"598":5722,"QK":5723,"genetic":5724,"Goodreads":5725,"ruling":5726,"every":5727,"helpful":5728,"watershed":5729,"05118":5730,"accumulating":5731,"NPM":5732,"Japanese":5733,"Tufano":5734,"099":5735,"601":5736,"gpt":5737,"feel":5738,"第三":5739,"SIMULATION":5740,"upcoming":5741,"Concerns":5742,"undermine":5743,"1989":5744,"Andrei":5745,"laser":5746,"broadcasts":5747,"nmt":5748,"MDPs":5749,"storms":5750,"Giel":5751,"Advisors":5752,"generate":5753,"distinction":5754,"Social":5755,"ceedings":5756,"cursory":5757,"introduction":5758,"great":5759,"reusable":5760,"milestone":5761,"1000000000":5762,"Recurrent":5763,"RINK":5764,"1410":5765,"amplifiers":5766,"____________________________________________________________________________________________":5767,"Identify":5768,"translates":5769,"centralized":5770,"unlocked":5771,"TM":5772,"boycotts":5773,"raining":5774,"superintelligence":5775,"tech":5776,"Indemnity":5777,"exhausted":5778,"streamlines":5779,"ERP":5780,"systems":5781,"Fitting":5782,"experimenter":5783,"R25":5784,"parking":5785,"forms":5786,"296042":5787,"793":5788,"Grounded":5789,"fiscal":5790,"deadly":5791,"Bi":5792,"Broader":5793,"git":5794,"deposits":5795,"2403":5796,"depicted":5797,"grey":5798,"grayscale":5799,"compiling":5800,"Submit":5801,"HSG":5802,"fully":5803,"deliveries":5804,"prominently":5805,"optima":5806,"protected":5807,"ichi":5808,"idation":5809,"reprint":5810,"IPython":5811,"au":5812,"Neutral":5813,"Tasks":5814,"它":5815,"order":5816,"...":5817,"householding":5818,"DenseBlock":5819,"bol":5820,"eng":5821,"conv":5822,"697":5823,"differentiated":5824,"broader":5825,"Nesterov":5826,"Ils":5827,"toy":5828,"50":5829,"Already":5830,"Consultant":5831,"Agirre":5832,"Ayed":5833,"standards":5834,"1988":5835,"fairy":5836,"293":5837,"scores":5838,"debiases":5839,"Optionally":5840,"13474":5841,"standard":5842,"England":5843,"ond":5844,"Buildings":5845,"dial":5846,"provision":5847,"50th":5848,"misaligned":5849,"desires":5850,"fills":5851,"Papamarkou":5852,"Warstadt":5853,"16420":5854,"dive":5855,"Today":5856,"Joseph":5857,"Axon":5858,"describe":5859,"requir":5860,"close":5861,"fee":5862,"menial":5863,"风景":5864,"bleu":5865,"Gormley":5866,"Sie":5867,"undiagnosed":5868,"forum":5869,"consumed":5870,"prohibitively":5871,"Ö":5872,"993":5873,"rain":5874,"xavier":5875,"strongly":5876,"famous":5877,"l2":5878,"loud":5879,"imported":5880,"07258":5881,"leadership":5882,"Nucleus":5883,"Regularization":5884,"tained":5885,"exceeds":5886,"0.1456":5887,"rather":5888,"Whc":5889,"ProjX":5890,"Target":5891,"kNN":5892,"Female":5893,"aggregation":5894,"bird":5895,"objective":5896,"Moreover":5897,"3316":5898,"AdditiveAttention":5899,"gures":5900,"designated":5901,"increasing":5902,"FDII":5903,"Tolstoy":5904,"4039":5905,"delayed":5906,"upfront":5907,"sensor":5908,"Discussions79":5909,"跨越":5910,"Sessions":5911,"neces":5912,"493":5913,"Normal":5914,"08774":5915,"holdout":5916,"。":5917,"tializations":5918,"sizable":5919,"intent":5920,"inactions":5921,"Katakis":5922,"Tessera":5923,"Tranah":5924,"label2image":5925,"meetings":5926,"375":5927,"增强":5928,"cmap":5929,"Challenges":5930,"5177":5931,"20256":5932,"193":5933,"site":5934,"Empirically":5935,"sors":5936,"chapters":5937,"Precise":5938,"haustive":5939,"weibull":5940,"17780":5941,"流程图":5942,"Wetzel":5943,"Timeframe":5944,"StructGPT":5945,"month":5946,"historical":5947,"Interactive":5948,"mechanisms":5949,"Speech":5950,"hsneto":5951,"049":5952,"pledging":5953,"415":5954,"pretrain":5955,"translations":5956,"10022":5957,"ea":5958,"Gebiete":5959,"fauna":5960,"emulate":5961,"bi":5962,"pre":5963,"Steps":5964,"stretches":5965,"Form":5966,"brands":5967,"2v":5968,"会":5969,"collaborating":5970,"PopVsSoda":5971,"Unallocated":5972,"Series":5973,"moni":5974,"888":5975,"written":5976,"Toolcoder":5977,"allowances":5978,"propelled":5979,"Rowland":5980,"plementation":5981,"automotive":5982,"erations":5983,"smoothed":5984,"Parsing":5985,"Demonstration":5986,"tricks":5987,"1550":5988,"seminal":5989,"seeds":5990,"clarifying":5991,"R1000":5992,"tinuous":5993,"10.5":5994,"Scalability":5995,"Ingest":5996,"automated":5997,"ciation":5998,"span":5999,"tomography":6000,"确保":6001,"ESEC":6002,"realm":6003,"1840":6004,"Cubed":6005,"stitches":6006,"Nonstationary":6007,"Transferring":6008,"4334":6009,"Chiao":6010,"tuner":6011,"limiting":6012,"GH200":6013,"hash":6014,"serialization":6015,"Lessons":6016,"8889":6017,"Wallis":6018,"casualty":6019,"834":6020,"projection":6021,"motivates":6022,"attracted":6023,"2023b":6024,"canonical":6025,"mining":6026,"RSUs":6027,"1.9":6028,"Datar":6029,"DRNN":6030,"Greater":6031,"Mem":6032,"imputation":6033,"entries":6034,"Concrete":6035,"sig":6036,"render":6037,"Cher":6038,"Keane":6039,"without":6040,"Simple":6041,"Stadium":6042,"Qx":6043,"fore":6044,"negation":6045,"Subsidiaries":6046,"residential":6047,"8238":6048,"R216":6049,"Python":6050,"haystack":6051,"dominant":6052,"24":6053,"133":6054,"prompt":6055,"tools":6056,"creditworthiness":6057,"reconcile":6058,"0.010599":6059,"predicated":6060,"numer":6061,"1701":6062,"alleging":6063,"ancy":6064,"perparameters":6065,"transformative":6066,"shrinking":6067,"1147":6068,"diseased":6069,"lane":6070,"merit":6071,"differed":6072,"purely":6073,"pressed":6074,"takeaway":6075,"compilation":6076,"lkelihood":6077,"Latent":6078,"matically":6079,"281":6080,"Minimizing":6081,"Sha":6082,"behind":6083,"adver":6084,"Fahrenheit":6085,"getting":6086,"mindful":6087,"colorbar":6088,"35":6089,"Waibel":6090,"Phuc":6091,"03188":6092,"Token":6093,"pandemics":6094,"heav":6095,"Duy":6096,"1A":6097,"1977":6098,"coat":6099,"Altman":6100,"confidence":6101,"Can":6102,"MLP":6103,"portion":6104,"k1":6105,"insider":6106,"assigns":6107,"ate":6108,"outsize":6109,"Incorporated":6110,"Discussions100":6111,"LogNormal":6112,"8000":6113,"gz":6114,"Grover":6115,"899917":6116,"TinyMemBench":6117,"headquarters":6118,"∩":6119,"codypenta":6120,"flexibly":6121,"codeblock":6122,"170":6123,"10.0":6124,"incurring":6125,"Pontil":6126,"memorize":6127,"Law":6128,"1015":6129,"EXHIBIT":6130,"standing":6131,"bunch":6132,"address":6133,"chooses":6134,"091":6135,"com":6136,"hears":6137,"Strategic":6138,"Covx":6139,"variety":6140,"adaptable":6141,"889":6142,"safeguarding":6143,"1447":6144,"vectors":6145,"ENDED":6146,"Andersen":6147,"Labor":6148,"mediocre":6149,"Back":6150,"cues":6151,"adds":6152,"CLAP":6153,"diverse":6154,"morphology":6155,"48":6156,"1805":6157,"derived":6158,"01ada507287d82875905620988597833ad4e0903":6159,"anything":6160,"2016":6161,"examining":6162,"deviation":6163,"200k":6164,"1435":6165,"54":6166,"diffusion":6167,"BEING":6168,"Gopher":6169,"4.6":6170,"thrilling":6171,"Preliminaries":6172,"901730":6173,"manageable":6174,"booming":6175,"Analysis":6176,"kits":6177,"presentations":6178,"Gated":6179,"GPUs":6180,"earned":6181,"serendipitous":6182,"173428":6183,"EPYC":6184,"展示":6185,"Employment":6186,"DeSantis":6187,"fight":6188,"2310":6189,"restrictive":6190,"Maximilian":6191,"private":6192,"Relating":6193,"mitigates":6194,"Doll":6195,"Oftentimes":6196,"Substitute":6197,"tip":6198,"037":6199,"resnet18":6200,"658":6201,"pithy":6202,"supercomputing":6203,"Essentially":6204,"Institute":6205,"vary":6206,"explained":6207,"Rachel":6208,"keyboard":6209,"kvpairs":6210,"Zis":6211,"0.95":6212,"dimensional":6213,"large":6214,"issuer":6215,"9793":6216,"Zhong":6217,"1409":6218,"x":6219,"fid":6220,"r1":6221,"stationarity":6222,"GPyTorch":6223,"possessing":6224,"etc":6225,"Illinois":6226,"curring":6227,"342":6228,"crime":6229,"Director":6230,"use":6231,"never":6232,"减少":6233,"blistering":6234,"considerably":6235,"gpu":6236,"CSP":6237,"Bach":6238,"0.125":6239,"ements":6240,"1KB":6241,"assumes":6242,"Addendum":6243,"partly":6244,"lowers":6245,"trajectories":6246,"runtime":6247,"occasional":6248,"Reaches":6249,"evidenced":6250,"falling":6251,"Kuksa":6252,"6583":6253,"1324":6254,"Healthcare":6255,"modifying":6256,"subsequences":6257,"harmonized":6258,"objects":6259,"ms":6260,"matmul":6261,"Druzhinin":6262,"智能":6263,"happened":6264,"strengths":6265,"C4":6266,"radiation":6267,"rattler":6268,"discharged":6269,"ax":6270,"cosine":6271,"fluctuate":6272,"Gym":6273,"synchronized":6274,"idea":6275,"truly":6276,"inds":6277,"Likewise":6278,"usr":6279,"challenging":6280,"949":6281,"semiannually":6282,"sported":6283,"Affine":6284,"Saudi":6285,"prepositions":6286,"replaces":6287,"milder":6288,"Distributed":6289,"niglich":6290,"___.":6291,"casting":6292,"insurmountable":6293,"suppres":6294,"Fluid":6295,"increased":6296,"amended":6297,"releases":6298,"leftward":6299,"𝑁":6300,"726":6301,"2th":6302,"foundation":6303,"debt":6304,"Marginal":6305,"suite":6306,"145":6307,"Receives":6308,"Print":6309,"Horanic":6310,"Julia":6311,"Apricot":6312,"Eight":6313,"comments":6314,"audio":6315,"此行":6316,"hyperscale":6317,"Ruslan":6318,"ConvolutionBackward0":6319,"intervals":6320,"palette":6321,"Kolter":6322,"endless":6323,"others":6324,"2.8290698237936858":6325,"MedTech":6326,"perpetual":6327,"6378":6328,"Weinberger":6329,"regres":6330,"estimating":6331,"Invariance":6332,"organize":6333,"06815":6334,"Effectiveness":6335,"Morris":6336,"USA":6337,"subplots":6338,"callback":6339,"blocked":6340,"arise":6341,"prides":6342,"Poland":6343,"caller":6344,"DuBois":6345,"Shoquist":6346,"same":6347,"pars":6348,"Successive":6349,"latter":6350,"suffi":6351,"magnum":6352,"202352":6353,"attends":6354,"Sign":6355,"numerical":6356,"15097":6357,"1e":6358,"density":6359,"Watch":6360,"correctness":6361,"Franceschi":6362,"Reader":6363,"binding":6364,"terrific":6365,"malization":6366,"wireframe":6367,"conscious":6368,"198453":6369,"CLOUD":6370,"dialogues":6371,"abs":6372,"FCN":6373,"overloading":6374,"Bowman":6375,"stopped":6376,"Geometric":6377,"BiRNN":6378,"bytes":6379,"ness":6380,"Cov":6381,"Tay":6382,"rule":6383,"rerankers":6384,"width":6385,"MLPScratch":6386,"Earlier":6387,"huge":6388,"908":6389,"evolutionary":6390,"NA":6391,"Best":6392,"bed":6393,"unanticipated":6394,"hungry":6395,"prehensive":6396,"ending":6397,"unlocking":6398,"571":6399,"5805":6400,"reopen":6401,"Terminating":6402,"memorizing":6403,"injective":6404,"happening":6405,"subtrees":6406,"amd64":6407,"theft":6408,"696":6409,"40GB":6410,"ë":6411,"suffers":6412,"Mastering":6413,"craftsmanship":6414,"arising":6415,"wildfire":6416,"intentionally":6417,"继续":6418,"Fabrinet":6419,"Plus":6420,"Joulin":6421,"burn":6422,"switching":6423,"actions":6424,"tax":6425,"learner":6426,"Rag":6427,"𝑍":6428,"keys":6429,"transitions":6430,"expertise":6431,"Discussions225":6432,"A.29":6433,"Grab":6434,"forecasts":6435,"First":6436,"wait":6437,"impeded":6438,"System":6439,"A.31":6440,"verbs":6441,"08053":6442,"包含":6443,"rcParams":6444,"niently":6445,"few":6446,"Investigation":6447,"ANN":6448,"Fern":6449,"Guilherme":6450,"Suggest":6451,"earnings":6452,"Nobel":6453,"Experiments":6454,"Poodles":6455,"Bruss":6456,"adapted":6457,"reincorporated":6458,"similarity":6459,"Negative":6460,"7558":6461,"alistic":6462,"exponentially":6463,"Convention":6464,"tic":6465,"Dealer":6466,"concatenation":6467,"Harvard":6468,"Richardson":6469,"struc":6470,"bears":6471,"Ghosh":6472,"terested":6473,"ditioner":6474,"YFC100M":6475,"7z":6476,"caojilin":6477,"plants":6478,"≠":6479,"RNN":6480,"rstride":6481,"plicates":6482,"cutting":6483,"subtracts":6484,"181":6485,"4f":6486,"principles":6487,"matters":6488,"Initialization":6489,"Retrieving":6490,"balanced":6491,"SuccessiveHalvingScheduler":6492,"Quorum":6493,"Mavadia":6494,"A4":6495,"surprisingly":6496,"subjects":6497,"Require":6498,"provides":6499,"13259":6500,"precomputed":6501,"verbally":6502,"viewing":6503,"Yet":6504,"terior":6505,"Oper":6506,"orig":6507,"consumes":6508,"attendance":6509,"Catanzaro":6510,"Move":6511,"3440":6512,"contracted":6513,"114":6514,"Iter":6515,"Github":6516,"brilliant":6517,"able":6518,"3064":6519,"1005":6520,"has":6521,"tried":6522,"glad":6523,"Labour":6524,"Fran":6525,"heuristics":6526,"Zamani":6527,"stage":6528,"259":6529,"honorary":6530,"Tsoumakas":6531,"triggered":6532,"summariza":6533,"RM":6534,"fittingly":6535,"Practice":6536,"countered":6537,"rephrasing":6538,"BiGRU":6539,"unshared":6540,"772":6541,"CRAG":6542,"equi":6543,"MacArthur":6544,"Macroeconomic":6545,"Significantly":6546,"megapixel":6547,"nonnegativity":6548,"overseas":6549,"lazy":6550,"6641":6551,"946":6552,"Decomposed":6553,"gunk":6554,"tagged":6555,"throttling":6556,"chapter":6557,"witness":6558,"doc1":6559,"004":6560,"rows":6561,"Nair":6562,"appropri":6563,"outsides":6564,"synthesize":6565,"excel":6566,"motherboards":6567,"burning":6568,"hear":6569,"Penghao":6570,"impede":6571,"court":6572,"Cambridge":6573,"vars":6574,"MacKenzie":6575,"Evaluates":6576,"angles":6577,"insolvency":6578,"America":6579,"Application":6580,"Yankees":6581,"apply":6582,"optimizer":6583,"reinvented":6584,"co":6585,"ciently":6586,"multibox":6587,"vkramdev":6588,"Toutanova":6589,"snack":6590,"disabling":6591,"depends":6592,"Sen":6593,"neigh":6594,"called":6595,"Gnn":6596,"th":6597,"iteration":6598,"Host":6599,"plots":6600,"Harvey":6601,"herent":6602,"discrete":6603,"Sufficed":6604,"cholesterol":6605,"ultra":6606,"Consolidated":6607,"transitioned":6608,"Line":6609,"SSRN":6610,"authorities":6611,"degrades":6612,"模块":6613,"retained":6614,"sfermigier":6615,"Technologies":6616,"propagation":6617,"responds":6618,"14.1":6619,"prowess":6620,"inferring":6621,"glove":6622,"list":6623,"organizations":6624,"radar":6625,"tional":6626,"Entropy":6627,"masked":6628,"divisible":6629,"è":6630,"multichannel":6631,"nils":6632,"coreference":6633,"NetDynamics":6634,"red":6635,"nboer":6636,"303261":6637,"quently":6638,"Much":6639,"inquiry":6640,"14.12":6641,"Zachary":6642,"prac":6643,"Vincent":6644,"BCom":6645,"search":6646,"ingly":6647,"Alleva":6648,"statement":6649,"815":6650,"Fu":6651,"段落":6652,"3781":6653,"Position":6654,"propriate":6655,"Passages":6656,"noun":6657,"491":6658,"deletion":6659,"Truncating":6660,"033363":6661,"manual":6662,"resold":6663,"E":6664,"restricts":6665,"scattered":6666,"Mohammad":6667,"interested":6668,"proves":6669,"packaged":6670,"college":6671,"Numerous":6672,"85189199447632":6673,"compatability":6674,"992":6675,"13G":6676,"opti":6677,"𝝈":6678,"David":6679,"APICoder":6680,"Kinds":6681,"bedrock":6682,"Differential":6683,"polytechnic":6684,"game252":6685,"0.8307":6686,"packed":6687,"----------":6688,"having":6689,"0298":6690,"2794":6691,"729758":6692,"putational":6693,"cash":6694,"0.0022":6695,"engi":6696,"descriptive":6697,"virtual":6698,"lifted":6699,"distributed":6700,"CenterCrop":6701,"Oxfords":6702,"Koller":6703,"numerous":6704,"NMT":6705,"aftermarket":6706,"Emphasis":6707,"490":6708,"1.4883":6709,"Education":6710,"373":6711,"Jones":6712,"soning":6713,"resellers":6714,"SATA":6715,"66":6716,"677":6717,"python3":6718,"登录":6719,"held":6720,"139":6721,"Iglovikov":6722,"appeared":6723,"assembled":6724,"phones":6725,"NumRooms":6726,"Naor":6727,"1901":6728,"definitive":6729,"200W":6730,"Aru":6731,"rep":6732,"4448":6733,"vGPU":6734,"elicits":6735,"evolutions":6736,"c3":6737,"classifica":6738,"meets":6739,"1952":6740,"acquisition":6741,"hw2":6742,"664848":6743,"514":6744,"Lots":6745,"preapproval":6746,"0.83":6747,"mechanism":6748,"4167":6749,"hue":6750,"circle":6751,"longest":6752,"Jerzy":6753,"canceling":6754,"friend":6755,"loc":6756,"frivolous":6757,"descriptions":6758,"floor":6759,"Apache":6760,"Minor":6761,"convolutional":6762,"845":6763,"impression":6764,"Handling":6765,"Getty":6766,"Tian":6767,"AVX2195":6768,"BA":6769,"arrivals":6770,"royalty":6771,"band":6772,"subfield":6773,"RAPTOR":6774,"pets":6775,"deeper":6776,"iou":6777,"029":6778,"Generation":6779,"modes":6780,"Gaussians":6781,"11":6782,"periodically":6783,"Locate":6784,"641":6785,"10466":6786,"Malartic":6787,"Maclaurin":6788,"Positionwise":6789,"randomly":6790,"Multivariable":6791,"matches":6792,"603":6793,"xxxix":6794,"PPL":6795,"associated":6796,"multitask":6797,"Eshelman204":6798,"fundamentally":6799,"706":6800,"2401":6801,"GeForce":6802,"3482":6803,"repeatedly":6804,"prototypical":6805,"dominate":6806,"Chat":6807,"Expected":6808,"VMware":6809,"encouraging":6810,"Chaumond":6811,"cla":6812,"builtins":6813,"ideal":6814,"er":6815,"Bilingual":6816,"nomials":6817,"guess":6818,"unto":6819,"repeal":6820,"marry":6821,"acted":6822,"Monitors":6823,"priors":6824,"fi":6825,"hundreds":6826,"Directly":6827,"reducing":6828,"overwhelm":6829,"species":6830,"ranging":6831,"horizontal":6832,"di":6833,"Additionally":6834,"cameras":6835,"experiments":6836,"counters":6837,"sinh":6838,"concatenates":6839,"Malkov":6840,"tracks":6841,"interacts":6842,"QOS":6843,"supplier":6844,"Discussions207":6845,"unravel":6846,"to":6847,"mally":6848,"overseeing":6849,"radial":6850,"salary":6851,"beginners":6852,"any":6853,"XBRL":6854,"3269":6855,"disclosed":6856,"Peilin":6857,"02767":6858,"Chinchilla":6859,"manifestations":6860,"Cornell":6861,"mum":6862,"ver":6863,"19.1":6864,"801":6865,"parallelize":6866,"dividing":6867,"defers":6868,"culmination":6869,"polarities":6870,"dots":6871,"noon":6872,"Zheng":6873,"execute":6874,"Cross":6875,"disgorgement":6876,"distorting":6877,"Updates":6878,"black":6879,"innermost":6880,"tency":6881,"StackTransform":6882,"References":6883,"wb":6884,"Dynamical":6885,"836":6886,"restarts":6887,"09%":6888,"implied":6889,"Doan":6890,"unstable":6891,"Often":6892,"connectivity":6893,"Discussions284":6894,"10403":6895,"travel":6896,"intellectual":6897,"unimaginable":6898,"105":6899,"XOR":6900,"309%":6901,"Somekh":6902,"compositional":6903,"warehouse":6904,"supervised":6905,"abrupt":6906,"Mao":6907,"0130":6908,"wasting":6909,"Yes":6910,"Choices":6911,"prevailed":6912,"activate":6913,"preprocessed":6914,"deeplearningbook":6915,"Chiu":6916,"determination":6917,"devise":6918,"connectors":6919,"accomplish":6920,"300000":6921,"patience":6922,"Impatient":6923,"TSR":6924,"Explain":6925,"tecture":6926,"百分比":6927,"cessation":6928,"Miller":6929,"believed":6930,"Third":6931,"Arithmetic":6932,"Contractual":6933,"highest":6934,"Capitalized":6935,"SANTA":6936,"Pro":6937,"discuss":6938,"inherit":6939,"ONE":6940,"Vector":6941,"clicks":6942,"Vest":6943,"Psychonomic":6944,"posable":6945,"pounds":6946,"smoother":6947,"merge":6948,"annually":6949,"sich":6950,"aws":6951,"legacy":6952,"JOHN":6953,"deficient":6954,"Omniverse":6955,"92":6956,"promotions":6957,"21":6958,"Network":6959,"politics":6960,"addnorm2":6961,"Lan":6962,"zeros":6963,"306":6964,"Dr":6965,"594":6966,"123379":6967,"iterated":6968,"discriminators":6969,"wanting":6970,"subscription":6971,"spec":6972,"detects":6973,"only":6974,"Adriaan":6975,"Ji":6976,"F6":6977,"prin":6978,"Discussions173":6979,"near":6980,"Oren":6981,"Nearly":6982,"BERTBASE":6983,"Gosset":6984,"researchers":6985,"B.7":6986,"crop":6987,"amend":6988,"cropped":6989,"entertainment":6990,"listwise":6991,"fillna":6992,"loves":6993,"Gan":6994,"Hastorun":6995,"安全":6996,"researches":6997,"infinitesimally":6998,"Baciu":6999,"causality":7000,"ready":7001,"1800012997":7002,"delineation":7003,"Dataset":7004,"emphasize":7005,"Tomer":7006,"06864":7007,"2.9":7008,"infused":7009,"monitoring":7010,"Three":7011,"CEDAR":7012,"underlying":7013,"Moschella":7014,"FiD":7015,"GRetriever":7016,"Litigation":7017,"lent":7018,"intend":7019,"systematically":7020,"ods":7021,"UniK":7022,"dic":7023,"guoweis":7024,"confirms":7025,"Wxo":7026,"radius":7027,"solvable":7028,"pool":7029,"2024":7030,"Immorlica":7031,"licensed":7032,"0.023":7033,"Committees":7034,"MSZoning":7035,"unaccounted":7036,"decrease":7037,"regulatory":7038,"revolutionary":7039,"3119":7040,"lecturer":7041,"log":7042,"constructing":7043,"Barbaros":7044,"sole":7045,"harness":7046,"Great":7047,"Armed":7048,"1312":7049,"terrorism":7050,"548M":7051,"3177549":7052,"Accelerators":7053,"lstm":7054,"horizon":7055,"Gender":7056,"TNNLS":7057,"babies":7058,"7549":7059,"Zhengren":7060,"tably":7061,"prove":7062,"rarely":7063,"Post":7064,"afterthought":7065,"ozgur":7066,"Continuing":7067,"Highway":7068,"Summarize":7069,"deductibles":7070,"comprise":7071,"HOG":7072,"ornithology":7073,"snapshots":7074,"Dating":7075,"scolio":7076,"caching":7077,"B200":7078,"Locatello":7079,"gorithm":7080,"CLARA":7081,"minimally":7082,"completed":7083,"undesirable":7084,"expressing":7085,"Indexing":7086,"4965":7087,"collateral":7088,"Speelpenning":7089,"base":7090,"ECCV":7091,"972":7092,"cus":7093,"Offsets":7094,"ter":7095,"INT8":7096,"Baptista":7097,"M60":7098,"exploding":7099,"95051":7100,"sick":7101,"Neighbours":7102,"budgeted":7103,"Whr":7104,"Histograms":7105,"Fukushima":7106,"further":7107,"72%":7108,"incorporate":7109,"Amgen":7110,"entangled":7111,"70":7112,"manufacturing":7113,"Comprehensive":7114,"assem":7115,"Model":7116,"414":7117,"passing":7118,"four":7119,"Staten":7120,"docs":7121,"alleviate":7122,"Bollacker":7123,"independent":7124,"publishing":7125,"ssurvey":7126,"2440":7127,"673":7128,"1459":7129,"clock":7130,"第":7131,"53.576584339141846":7132,"SUBMIT":7133,"cision":7134,"Pennington":7135,"solvers":7136,"wallclock":7137,"KISS":7138,".+":7139,"considerations":7140,"compresses":7141,"Broker":7142,"UDP":7143,"BlueField":7144,"synthe":7145,"252":7146,"exceedingly":7147,"能手":7148,"versa":7149,"act":7150,"React":7151,"1y":7152,"unilateral":7153,"wrote":7154,"deduced":7155,"RAPIDS":7156,"362":7157,"Idle":7158,"share":7159,"un":7160,"supplanted":7161,"Multi":7162,"capacities":7163,"684":7164,"v2":7165,"1012":7166,"miller":7167,"accruals":7168,"Kouzelis":7169,"merging":7170,"340":7171,"indemnity":7172,"1814":7173,"wasted":7174,"disruptive":7175,"Colossal":7176,"618":7177,"1016":7178,"Bay":7179,"nal":7180,"outcomes":7181,"merged":7182,"tarfile":7183,"proposal":7184,"infinitely":7185,"Peripherals":7186,"Kaluarachchi":7187,"ous":7188,"稳固":7189,"Consult":7190,"771":7191,"3460":7192,"false":7193,"deck":7194,"至关":7195,"∼":7196,"da":7197,"using":7198,"4279":7199,"classified":7200,"3e":7201,"resembled":7202,"selects":7203,"ence":7204,"203":7205,"SNLI":7206,"convinced":7207,"website8":7208,"Guo":7209,"vested":7210,"1040.8":7211,"Pleiss":7212,"Flickr":7213,"embedding":7214,"gym":7215,"find":7216,"NEON":7217,"invite":7218,"10790":7219,"ages":7220,"coerce":7221,"Discussions280":7222,"entrants":7223,"cut":7224,"Virtually":7225,"Intangible":7226,"spectively":7227,"𝑦":7228,"NotedownContentsManager":7229,"819":7230,"cancelled":7231,"shatter":7232,"watched":7233,"tightly":7234,"309":7235,"straightforward":7236,"transactions":7237,"harmful":7238,"bowen0701":7239,"给":7240,"680":7241,"Ramesh":7242,"announce":7243,"occurrences":7244,"kbqa":7245,"nately":7246,"介绍":7247,"African":7248,"invited":7249,"390":7250,"structure":7251,"impairing":7252,"Lafeez":7253,"Dosovitskiy":7254,"final":7255,"172":7256,"Hebbian":7257,"kernel2matrix":7258,"Hoekstra":7259,"Colab":7260,"rooms":7261,"surrounded":7262,"provisions":7263,"269":7264,"teacher":7265,"Inspiration":7266,"beam":7267,"customers":7268,"literal":7269,"Hall":7270,"ena":7271,"7542":7272,"supercharged":7273,"fancy":7274,"tread":7275,"overestimate":7276,"Kernion":7277,"menace":7278,"351":7279,"Enriching":7280,"Kalman":7281,"spending":7282,"cantly":7283,"Framework":7284,"manipulated":7285,"revolve":7286,"variability":7287,"hope":7288,"Plot":7289,"algo":7290,"716":7291,"Indian":7292,"120000":7293,"cnn":7294,"torchscript":7295,"tical":7296,"Leake":7297,"Timothy":7298,"Lane":7299,"194":7300,"whistles":7301,"webcast":7302,"environmental":7303,"7944":7304,"characteristic":7305,"thresholding":7306,"Don":7307,"upon":7308,"du":7309,"46th":7310,"ve":7311,"multiplex":7312,"conveyed":7313,"Stepback":7314,"went":7315,"34":7316,"oper":7317,"avenue":7318,"SalePrice":7319,"process":7320,"Chain":7321,"productivity":7322,"Academy":7323,"models":7324,"Privacy":7325,"detaching":7326,"Devvrit":7327,"versatile":7328,"ionMNIST":7329,"chines":7330,"Pool":7331,"natively":7332,"So":7333,"1904":7334,"Gym256":7335,"fleets":7336,"1030":7337,"Improve":7338,"sfilip":7339,"incredible":7340,"Jamieson":7341,"8949":7342,"Abhinav":7343,"Expressed":7344,"eligibility":7345,"Potts":7346,"电脑":7347,"01795":7348,"1.3":7349,"imaginary":7350,"Stolcke":7351,"em":7352,"CAPTION":7353,"depending":7354,"1403":7355,"triangles":7356,"impossible":7357,"469":7358,"8748":7359,"logged":7360,"Skylake":7361,"Optimal":7362,"tailor":7363,"intercepts":7364,"predefine":7365,"visualizes":7366,"repayment":7367,"freqs":7368,"heck":7369,"Besides":7370,"broadens":7371,"issuing":7372,"strategy":7373,"Results":7374,"Seo":7375,"should":7376,"forgo":7377,"Regan":7378,"ni":7379,"communicated":7380,"pair":7381,"repatriation":7382,"stakeholder":7383,"hey":7384,"regular":7385,"v":7386,"stances":7387,"2203":7388,"Discussions246":7389,"dchoi77":7390,"Namely":7391,"Bottou":7392,"Sintaha":7393,"contract":7394,"50770":7395,"20":7396,"absence":7397,"Extract":7398,"𝑅":7399,"Drain":7400,"Gulati":7401,"Satisfying":7402,"rehearing":7403,"WITHHOLD":7404,"invoice":7405,"generalist":7406,"Mavromatis":7407,"bygone":7408,"UCI":7409,"Rae":7410,"06211":7411,"minutes":7412,"tween":7413,"6772":7414,"merical":7415,"Controls":7416,"Discussions133":7417,"807":7418,"𝜎":7419,"Zisser":7420,"track":7421,"Augmented":7422,"insufficiently":7423,"Cun":7424,"purchases":7425,"Dive":7426,"Zentropa":7427,"gpyotrch":7428,"NL":7429,"Isaac":7430,"Objective":7431,"7540":7432,"externally":7433,"Naive":7434,"317":7435,"Abdelzaher":7436,"respon":7437,"strives":7438,"Arab":7439,"characterize":7440,"Choi":7441,"connection":7442,"complied":7443,"noised":7444,"Abid":7445,"uncontroversial":7446,"coincided":7447,"concurrent":7448,"apocalypse":7449,"2322":7450,"minimax":7451,"taught":7452,"decod":7453,"1943":7454,"minimum":7455,"organization":7456,"VioNet":7457,"Upadhyay":7458,"intends":7459,"Coming":7460,"suffix":7461,"shoes":7462,"tied":7463,"samples":7464,"足够":7465,"affording":7466,"TanhTransform":7467,"1719":7468,"082577":7469,"sorts":7470,"imposed":7471,"dozen":7472,"marginalize":7473,"Wengert":7474,"Label":7475,"Prince":7476,"academia":7477,"tasked":7478,"Supporting":7479,"B":7480,"prioritizing":7481,"Discussions55":7482,"positionwise":7483,"ogy":7484,"With":7485,"respirator":7486,"cur":7487,"Forums":7488,"rst":7489,"622":7490,"investigations":7491,"preceding":7492,"Effectively":7493,"xxv":7494,"detect":7495,"enough":7496,"litigation":7497,"xxxiv":7498,"ATTN":7499,"proximate":7500,"twenty":7501,"03888":7502,"Tipping":7503,"manuel":7504,"narrative":7505,"farmers":7506,"sgn":7507,"Notices":7508,"131072":7509,"15939":7510,"Norick":7511,"upgrades":7512,"Stockholders":7513,"claiming":7514,"274":7515,"DRAMs":7516,"xxxii":7517,"tron":7518,"6.4":7519,"24202494130424274":7520,"6712":7521,"Hyperion":7522,"Acceptability":7523,"2420":7524,"W1":7525,"economists":7526,"backend":7527,"Dynamic":7528,"Rieck":7529,"resize":7530,"Competitive":7531,"disambiguate":7532,"386":7533,"ImageNet":7534,"636":7535,"harder":7536,"trouble":7537,"Shreshtha13":7538,"20%":7539,"Recommending":7540,"layman":7541,"Jurafsky":7542,"Granholm":7543,"mediating":7544,"mild":7545,"备注":7546,"haviour":7547,"rho":7548,"一些":7549,"699":7550,"PyTorch":7551,"medications":7552,"Except":7553,"˜":7554,"minich":7555,"A.6":7556,"under":7557,"diversification":7558,"Investor":7559,"vances":7560,"AlexSauer":7561,"hl":7562,"N2":7563,"error":7564,"evidence":7565,"KF":7566,"RAT":7567,"Pulse":7568,"detour":7569,"sequentially":7570,"2307":7571,"Kan":7572,"6840":7573,"meaningful":7574,"horrible":7575,"fake":7576,"grounded":7577,"Approximate":7578,"vol":7579,"stylization":7580,"transforma":7581,"5561":7582,"672":7583,"Kimeldorf":7584,"Convert":7585,"ratios":7586,"1.0022":7587,"masks":7588,"x1":7589,"contour":7590,"contrived":7591,"living":7592,"8584":7593,"Two":7594,"cloze":7595,"recommender":7596,"Carlyle":7597,"Remediation":7598,"eating":7599,"Distinct":7600,"465":7601,"lin":7602,"Alayrac":7603,"0.8635":7604,"?":7605,"617":7606,"Sablayrolles":7607,"certifications":7608,"freq":7609,"Having":7610,"Treebank":7611,"positively":7612,"scoring":7613,"appendix":7614,"semantical":7615,"programmer":7616,"2690v4":7617,"Lakshman":7618,"consolidated":7619,"demonstrated":7620,"mzz2017":7621,"ndim":7622,"deb":7623,"google":7624,"Foxconn":7625,"la":7626,"cortex":7627,"mantissa":7628,"definitions":7629,"liter":7630,"140000":7631,"0687":7632,"interpreted":7633,"691":7634,"bolstered":7635,"biguously":7636,"10063":7637,"Codet5mix":7638,"Truth":7639,"Angeli":7640,"icsdelivery":7641,"ASC":7642,"Feel":7643,"Perceptrons":7644,"Albumentations":7645,"weaker":7646,"freedom":7647,"NSP":7648,"extreme":7649,"cmd":7650,"underpin":7651,"E1":7652,"dual":7653,"drives":7654,"093798":7655,"Billion":7656,"emphasizing":7657,"sents":7658,"Inheriting":7659,"loan":7660,"Metz":7661,"Turner":7662,"aggressively":7663,"217":7664,"collapses":7665,"II":7666,"DotProductAttention":7667,"Broadwell":7668,"crops":7669,"literals":7670,"receipt":7671,"Subramanya":7672,"alleviating":7673,"emulating":7674,"Comput":7675,"Amounts":7676,"observe":7677,"Conv1d":7678,"vocabularies":7679,"Fellow":7680,"switch":7681,"obsolete":7682,"says":7683,"nar":7684,"1873":7685,"53":7686,"continuations":7687,"inches":7688,"repre":7689,"JD":7690,"firms":7691,"33":7692,"unavailable":7693,"eigendecompositions":7694,"Srivastav":7695,"477":7696,"810":7697,"example263":7698,"LLAMA":7699,"purges":7700,"Along":7701,"Bai":7702,"0000001":7703,"detach":7704,"robustness":7705,"Learn":7706,"Biesial":7707,"tokenization":7708,"roi":7709,"⊕":7710,"professional":7711,"uninterpretable":7712,"xmin":7713,"diagnos":7714,"Framewise":7715,"1357":7716,"classifi":7717,"Wishart":7718,"Competencies":7719,"Dalmedigos":7720,"Der":7721,"relation":7722,"achievements":7723,"00789":7724,"protocols":7725,"205":7726,"Hutter":7727,"notably":7728,"Fei":7729,"at":7730,"Semantic":7731,"tie":7732,"multistep":7733,"sultats":7734,"self":7735,"asserted":7736,"chunks":7737,"1.7773":7738,"Scales":7739,"Chishti":7740,"clerical":7741,"matic":7742,"350W":7743,"Prompt":7744,"Ronald":7745,"breeze":7746,"perspectives":7747,"161":7748,"educating":7749,"␣":7750,"recall":7751,"wished":7752,"mani2106":7753,"pinpoint":7754,"Codex":7755,"pz":7756,"captions":7757,"resource":7758,"Dependency":7759,"Sys":7760,"counted":7761,"Das":7762,"disadvantaged":7763,"Predict":7764,"Observational":7765,"06476":7766,"ℰ":7767,"855":7768,"brain":7769,"Website":7770,"196842":7771,"complexity":7772,"filter":7773,"fulfilled":7774,"displaced":7775,"contrastive":7776,"oversight":7777,"plenty":7778,"optimum":7779,"keyphrase":7780,"Pappas":7781,"comment":7782,"commissioning":7783,"administer":7784,"999746":7785,"administration":7786,"Chapman":7787,"beneficially":7788,"Suffice":7789,"specified":7790,"anti":7791,"Retriever":7792,"talented":7793,"sparked":7794,"infected":7795,"donkey":7796,"Sutskever":7797,"ascertain":7798,"subsidiary":7799,"tem":7800,"matures":7801,"instagram":7802,"signal":7803,"Upad":7804,"instance":7805,"Discussions122":7806,"rvs":7807,"nature":7808,"VGGish":7809,"Muller":7810,"differentiation":7811,"𝐸":7812,"4036":7813,"Ranges":7814,"Macy":7815,"381":7816,"Masking":7817,"variances":7818,"xxxi":7819,"231":7820,"Leon":7821,"explodes":7822,"crisis":7823,"OVERVIEW":7824,"wasteful":7825,"preferable":7826,"functionalities":7827,"centuries":7828,"lus":7829,"Andreassen":7830,"NLTK":7831,"deems":7832,"corner":7833,"580s":7834,"communica":7835,"slid":7836,"Wojna":7837,"subsampled":7838,"01":7839,"hind":7840,"Rerun":7841,"variation":7842,"Calculated":7843,"positions":7844,"century":7845,"ostensible":7846,"segmentation":7847,"Pandit":7848,"Achieving":7849,"051":7850,"795":7851,"biometrics":7852,"注册":7853,"ª":7854,"answered":7855,"discrepancy":7856,"Invertibility":7857,"generally":7858,"ably":7859,"14A":7860,"pack":7861,"Smaller":7862,"211":7863,"SN2":7864,"simplification":7865,"And":7866,"returnable":7867,"Cryptology":7868,"Numbers":7869,"blends":7870,"4979":7871,"GloVe":7872,"teach":7873,"Chassis":7874,"website236":7875,"Maximum":7876,"Radosavovic":7877,"mp":7878,"sectors":7879,"Anything":7880,"like":7881,"1089":7882,"470":7883,"sponding":7884,"factorization":7885,"consumption":7886,"Complete":7887,"arena":7888,"01933":7889,"disruptions":7890,"investigation":7891,"scans":7892,"⊙":7893,"restate":7894,"signify":7895,"WeChat":7896,"reassemble":7897,"12xlarge":7898,"existed":7899,"tance":7900,"Cohan":7901,"planned":7902,"batchify":7903,"genetics":7904,"reclassified":7905,"helping":7906,"Function":7907,"Driessche":7908,"Denoising":7909,"MELISSA":7910,"760":7911,"syne":7912,"underrep":7913,"stockholders":7914,"chronological":7915,"3609":7916,"investment":7917,"027":7918,"deployments":7919,"frivolously":7920,"speech":7921,"surpasses":7922,"folk":7923,"technol":7924,"6%":7925,"lez":7926,"功能":7927,"covered":7928,"LeNet":7929,"demonstrating":7930,"Group":7931,"awakening":7932,"represen":7933,"8080":7934,"Discussions240":7935,"4542":7936,"Cock":7937,"∫":7938,"MSEE":7939,"Greek":7940,"JAX42":7941,"sit":7942,"Discussions56":7943,"Ignoring":7944,"substantially":7945,"Discussions206":7946,"Click":7947,"combination":7948,"图表":7949,"Accessing":7950,"954":7951,"crafted":7952,"terminating":7953,"executable":7954,"1000010":7955,"5908":7956,"suspect":7957,"Confidence":7958,"Shares":7959,"17070001363754272":7960,"PDT":7961,"regularly":7962,"20235201406823256":7963,"executed":7964,"0.1306":7965,"Heavily":7966,"5684e":7967,"Math":7968,"rented":7969,"100m2":7970,"gigaflops":7971,"OVERSIGHT":7972,"Chakraborty":7973,"brute":7974,"Benchmarking":7975,"unintuitive":7976,"geography":7977,"A.8":7978,"correspond":7979,"shipment":7980,"READSUM":7981,"endowed":7982,"collaborative":7983,"Closely":7984,"⌉":7985,"seems":7986,"Ashual":7987,"¶":7988,"149":7989,"45000":7990,"strikingly":7991,"alarm":7992,"covariograms":7993,"ground":7994,"exotic":7995,"645":7996,"Discussions264":7997,"Sequoia":7998,"activations":7999,"blades":8000,"Insertion":8001,"york":8002,"inconclusive":8003,"perceived":8004,"311":8005,"rbfkernel":8006,"05":8007,"Talwalkar":8008,"Highly":8009,"libraries":8010,"Whitehead":8011,"产品":8012,"prepayment":8013,"tioned":8014,"stay":8015,"marks":8016,"invents":8017,"diagnoses":8018,"2043":8019,"invention":8020,"Aerospace":8021,"xv":8022,"图像":8023,"china":8024,"weights":8025,"accounts":8026,"Carl":8027,"infor":8028,"Discussions120":8029,"DEF":8030,"pixel":8031,"AI":8032,"coming":8033,"unla":8034,"bors":8035,"urban":8036,"autopilots":8037,"science":8038,"𝝐":8039,"(":8040,"Elkan":8041,"Bosma":8042,"emergence":8043,"pulse":8044,"0b8703943ccdb6eb788e6f091b8946e82231bc4d":8045,"maximimum":8046,"Saddle":8047,"outlay":8048,"segment":8049,"clhm":8050,"Ming":8051,"Kahl":8052,"Thanh":8053,"BART":8054,"634":8055,"unmistakably":8056,"icient":8057,"const":8058,"instead":8059,"2673":8060,"Ao":8061,"reactively":8062,"hinder":8063,"una":8064,"2265":8065,"Novel":8066,"against":8067,"KGs":8068,"Microservices":8069,"customizable":8070,"399520":8071,"exploitable":8072,"repeat":8073,"Skipping":8074,"Conv2d":8075,"1828":8076,"Instruments":8077,"George":8078,"satellite":8079,"Droppo":8080,"ResNeXtBlock":8081,"BASHEX":8082,"Makena":8083,"5569":8084,"barrier":8085,"guidance":8086,"2018":8087,"1098":8088,"846":8089,"569":8090,"712":8091,"Canim":8092,"09055":8093,"swim":8094,"hibits":8095,"ORCL":8096,"consist":8097,"grateful":8098,"careers":8099,"6105":8100,"rections":8101,"0.9486":8102,"contin":8103,"88%":8104,"networking":8105,"☐":8106,"Trier":8107,"plications":8108,"browsing":8109,"suppress":8110,"naming":8111,"818":8112,"Wolf":8113,"headquartered":8114,"Gutenberg":8115,"Discussions83":8116,"prevalent":8117,"timeframes":8118,"symmetry":8119,"Nevertheless":8120,"raises":8121,"Backend":8122,"avenues":8123,"preconditioning":8124,"staffed":8125,"g4dn":8126,"PERFORMANCE":8127,"LowRankMultivariateNormal":8128,"Clearly":8129,"dictionary":8130,"precision":8131,"formed":8132,"As":8133,"xytext":8134,"Customers":8135,"vertically":8136,"qa":8137,"00426":8138,"952":8139,"pile":8140,"28th":8141,"crite":8142,"MIRAGE":8143,"indefinitely":8144,"Unvested":8145,"GBit":8146,"shift":8147,"DCASE":8148,"0.786":8149,"Probab":8150,"Out":8151,"Stop":8152,"Oguz":8153,"ization":8154,"lowercase":8155,"Urbana":8156,"desiderata":8157,"fa19780a7b011d9b009e8bff8e99922a8ee2eb90":8158,"Ramasesh":8159,"extractive":8160,"uppercase":8161,"Micro":8162,"120":8163,"intelligent":8164,"tab":8165,"crash":8166,"subtracting":8167,"trouser":8168,"Dear":8169,"simplistic":8170,"Paliwal":8171,"arately":8172,"Technically":8173,"1b1":8174,"artificial":8175,"phrases":8176,"contrast":8177,"significantly":8178,"incredibly":8179,"Graves":8180,"chaotic":8181,"glass":8182,"Watson":8183,"recognizer":8184,"一节":8185,"stall":8186,"EXPRESSWAY":8187,"Supermicro":8188,"actuator":8189,"Manufacturing":8190,"ParNet":8191,"Kluwer":8192,"RNNLM":8193,"generat":8194,"supplements":8195,"categorized":8196,"397":8197,"Hawthorne":8198,"rejected":8199,"Massively":8200,"featurizing":8201,"isotonic":8202,"Around":8203,"734":8204,"Program":8205,"main":8206,"preprocess":8207,"Order":8208,"am":8209,"05085":8210,"Ju":8211,"voted":8212,"Targeted":8213,"Trust":8214,"ll":8215,"899955":8216,"5968":8217,"hibit":8218,"conv1":8219,"Gumbel":8220,"Head":8221,"smartest":8222,"wastage":8223,"remedial":8224,"LORA":8225,"1GHz":8226,"discoveries":8227,"interpret":8228,"smooth":8229,"economical":8230,"decouples":8231,"spatiotemporal":8232,"sider":8233,"Observation":8234,"提取":8235,"enc":8236,"Empirical":8237,"ensured":8238,"intelligence":8239,"gab":8240,"agents":8241,"systolic":8242,"unhappy":8243,"curse":8244,"cosmetic":8245,"import":8246,"115752065.81":8247,"mod":8248,"executive":8249,"extractor":8250,"statisticians":8251,"RandomSearch":8252,"设":8253,"0.00036":8254,"talking":8255,"L40S":8256,"minimizing":8257,"strategic":8258,"reparametrizes":8259,"Ferrell":8260,"ndarrays":8261,"SSH":8262,"LESS":8263,"RATP":8264,"SGI":8265,"illiquid":8266,"nempirical":8267,"tFig":8268,"lihood":8269,"argue":8270,"predominantly":8271,"Chan":8272,"eigenspaces":8273,"413":8274,"decompositions":8275,"735":8276,"mouth":8277,"𝜷":8278,"opposed":8279,"approaches":8280,"rainier":8281,"485":8282,"GDDR6194":8283,"abstracts":8284,"encrypted":8285,"Vocab":8286,"conservation":8287,"izontally":8288,"prompts":8289,"dizzying":8290,"compartmentalizes":8291,"Interfaces":8292,"Georgetown":8293,"insidiously":8294,"Amplify":8295,"NIO":8296,"Johns":8297,"interpretations":8298,"Product":8299,"neurally":8300,"curves":8301,"recycled":8302,"chipset":8303,"organizes":8304,"56":8305,"2130":8306,"Questions":8307,"Blackwell":8308,"vergence":8309,"9C":8310,"had":8311,"employs":8312,"v1":8313,"Limitations":8314,"Valuation":8315,"mixtures":8316,"nations":8317,"rabbit":8318,"achine":8319,"promptly":8320,"Hx":8321,"shipments":8322,"mainstay":8323,"Chowdhury":8324,"Lussier":8325,"slower":8326,"securities":8327,"379":8328,"boost":8329,"reasonably":8330,"Barriers":8331,"149425":8332,"disastrous":8333,"432":8334,"1511":8335,"produced":8336,"DOWN":8337,"Cord":8338,"dirname":8339,"expedient":8340,"A.47":8341,"ones":8342,"348":8343,"Leyton":8344,"661":8345,"measuring":8346,"Dilutive":8347,"Strided":8348,"Undiscovered":8349,"Providence":8350,"pennystock":8351,"C++":8352,"Configuration":8353,"vehicle":8354,"Bias":8355,"change":8356,"imaging":8357,"Gazpio":8358,"fontweight":8359,"designs":8360,"founda":8361,"Minibatches":8362,"unfounded":8363,"intangible":8364,"Statements":8365,"factorial":8366,"Discussions97":8367,"encodes":8368,"inf":8369,"DARTS":8370,"590":8371,"McCulloch":8372,"Arbuthnot":8373,"0545":8374,"principal":8375,"Omnicom":8376,"subsystems":8377,"687":8378,"tionship":8379,"recognition82":8380,"Macs":8381,"Test":8382,"photograph":8383,"1%":8384,"told":8385,"survives":8386,"grasp":8387,"uncer":8388,"dethroned":8389,"AttentionDecoder":8390,"Signal":8391,"660":8392,"0cb91d09b814ecdc07b50f31f8dcad3e81d6a86d":8393,"L":8394,"je":8395,"NEC":8396,"disam":8397,"hospital":8398,"RandomResizedCrop":8399,"superficial":8400,"4914":8401,"Communication":8402,"viously":8403,"expects":8404,"Costs":8405,"Descent":8406,"modelling":8407,"particle":8408,"header":8409,"0.9940":8410,"expenditures":8411,"19":8412,"underrepresented":8413,"safe":8414,"Arrangements":8415,"qualification":8416,"Lm":8417,"identifier":8418,"adap":8419,"Drives":8420,"Amin":8421,"skipping":8422,"paddings":8423,"wiggliness":8424,"Rencos":8425,"Ginsburg":8426,"emphasized":8427,"Volume":8428,"strengthen":8429,"1974":8430,"regu":8431,"bos":8432,"9486":8433,"Training":8434,"Shot":8435,"Resurrecting":8436,"cognitive":8437,"sums":8438,"1020.0":8439,"asks":8440,"Compile":8441,"threaten":8442,"147":8443,"Key":8444,"Benefits":8445,"129569":8446,"Discourse":8447,"distill":8448,"097":8449,"allow":8450,"0.7071":8451,"annotate":8452,"MountainCar251":8453,"’":8454,"Madry":8455,"2311":8456,"return":8457,"lifecycles":8458,"ishes":8459,"Liabilities":8460,"451":8461,"encode":8462,"hyperparam":8463,"populations":8464,"encapsulated":8465,"closely":8466,"jour":8467,"84":8468,"Hewitt":8469,"nil":8470,"2.4018":8471,"updated":8472,"Welch":8473,"observable":8474,"Chair":8475,"confirm":8476,"hv2":8477,"全面":8478,"DropoutMLP":8479,"Goyal":8480,"tify":8481,"star":8482,"nonstationary":8483,"Warm":8484,"rounding":8485,"cryptocurrencies":8486,"learn":8487,"IOS":8488,"894":8489,"San":8490,"Exactly":8491,"recalibrate":8492,"Multiscale":8493,"Markovitz":8494,"aware":8495,"quantitative":8496,"attened":8497,"construct":8498,"markets":8499,"1011":8500,"XRICL":8501,"769":8502,"0.090":8503,"416":8504,"Truncation":8505,"Determines":8506,"child":8507,"Commonsense":8508,"reformulating":8509,"bn4":8510,"plemented":8511,"Guestrin":8512,"Exemplary":8513,"1073":8514,"axvline":8515,"dam":8516,"Calculation":8517,"warmup":8518,"4800":8519,"Chris":8520,"------------":8521,"Stevens":8522,"1.2":8523,"A.12":8524,"multivariate":8525,"Williams":8526,"abbreviated":8527,"Innovation":8528,"GbE":8529,"ﬄ":8530,"Dou":8531,"并":8532,"critique":8533,"PhD":8534,"Variable":8535,"Jing":8536,"probabil":8537,"QLC":8538,"racks":8539,"She":8540,"completeness":8541,"TechRadar":8542,"hazard":8543,"μ":8544,"Retain":8545,"thumbs":8546,"turbocharging":8547,"quality":8548,"generous":8549,"optics":8550,"changed":8551,"Patidar":8552,"XX":8553,"stalled":8554,"stock":8555,"Graviton":8556,"4354e":8557,"exponenti":8558,"1532":8559,"guardrail":8560,"pin":8561,"828":8562,"Lugosi":8563,"acid":8564,"12627":8565,"Alcoa":8566,"detector":8567,"31st":8568,"Readings":8569,"2010189":8570,"1996":8571,"Tyagi":8572,"Discussions273":8573,"Max":8574,"going":8575,"eye":8576,"external":8577,"member":8578,"Lamblin":8579,"continuing":8580,"Issued":8581,"choreographed":8582,"contributing":8583,"49%":8584,"logistic":8585,"important":8586,"Self":8587,"emails":8588,"overwhelming":8589,"spaCy":8590,"FP8":8591,"premises":8592,"Langchain":8593,"trying":8594,"tricky":8595,"Lab":8596,"triangle":8597,"LazyBatchNorm1d":8598,"Words":8599,"conditioned":8600,"busy":8601,"Indicate":8602,"579":8603,"Contex":8604,"FLARE":8605,"assets":8606,"enthusiastically":8607,"leon":8608,"05326":8609,"223":8610,"Gross":8611,"Partnering":8612,"grant":8613,"chitecture":8614,"行":8615,"determines":8616,"evaluate":8617,"Dividend":8618,"184":8619,"limitations":8620,"jonathanhrandall":8621,"promise":8622,"Popper":8623,"Expressing":8624,"January":8625,"describing":8626,"Biology":8627,"online":8628,"Topic":8629,"Frisoni":8630,"996":8631,"gone":8632,"kcal":8633,"absorb":8634,"性能":8635,"shutil":8636,"capability":8637,"7170":8638,"182500":8639,"iamorphen":8640,"Edge":8641,"terrifying":8642,"Lausen":8643,"Hadjis":8644,"ultimately":8645,"AST":8646,"05150":8647,"interact":8648,"squished":8649,"Deep":8650,"nient":8651,"Jiyang":8652,"Reviewing":8653,"Gardner":8654,"interpretably":8655,"superior":8656,"Chellappa":8657,"3431":8658,"exceeded":8659,"42b":8660,"Macromedia":8661,"11049":8662,"Aligning":8663,"guiding":8664,"Jayaram":8665,"negatively":8666,"9th":8667,"A.34":8668,"percentage":8669,"843":8670,"lows":8671,"Chevedden":8672,"Predictive":8673,"Open":8674,"Cours":8675,"上传":8676,"isomorphisms":8677,"*":8678,"676":8679,"628":8680,"exploited":8681,"35th":8682,"R210":8683,"Levonian":8684,"P3608":8685,"Webson":8686,"biol":8687,"Refsql":8688,"necessitate":8689,"advancement":8690,"Tang":8691,"tap":8692,"Packaged":8693,"Discussions178":8694,"heights":8695,"simulators":8696,"1060":8697,"Outreach":8698,"raw":8699,"Purchase":8700,"regularizers":8701,"California":8702,"Conveniently":8703,"nate":8704,"consists":8705,"csv":8706,"mentwise":8707,"guaranteeing":8708,"heterogeneous":8709,"leases":8710,"frontend":8711,"ma":8712,"292":8713,"comprehen":8714,"670":8715,"9912422":8716,"Strip":8717,"entirely":8718,"06":8719,"stalls":8720,"NASA":8721,"Domain":8722,"Variances":8723,"2104":8724,"162":8725,"hindsight":8726,"pku":8727,"SoftmaxRegression":8728,"998":8729,"entropy":8730,"Benz":8731,"OK":8732,"parser":8733,"isolate":8734,"quantify":8735,"terms":8736,"security":8737,"0759":8738,"conspiring":8739,"7545":8740,"fleet":8741,"biggest":8742,"ok":8743,"533380":8744,"BSE":8745,"Llama":8746,"approach":8747,"Socher":8748,"trigger":8749,"granted":8750,"Listen":8751,"drafts":8752,"Nations":8753,"Russell":8754,"slot":8755,"timeframe":8756,"Mizutani":8757,"tags":8758,"hyay":8759,"quaternions":8760,"Tenable":8761,"outbreak":8762,"pausing":8763,"Hopkins":8764,"925":8765,"562":8766,"conceivably":8767,"9645419978270817":8768,"_%":8769,"explicitly":8770,"2292.7":8771,"phi":8772,"Faloutsos":8773,"Farm":8774,"计划":8775,"buffer":8776,"Nimes":8777,"worthwhile":8778,"iPhone":8779,"adjustment":8780,"semantically":8781,"Rakib":8782,"13.4":8783,"walk":8784,"bag":8785,"FFW":8786,"Rectangle":8787,"ImputBlaster":8788,"Resilient":8789,"Levine":8790,"yuqinhan":8791,"ByteTrack":8792,"grace":8793,"LocalSearcher":8794,"rb":8795,"Monitoring":8796,"hotdogs":8797,"degenerate":8798,"Hopefully":8799,"483":8800,"BatchNorm2d":8801,"satisfaction":8802,"TensorCores":8803,"RAGAS":8804,"Discussions116":8805,"indemnifications":8806,"assembly":8807,"slicing":8808,"mix":8809,"implemented":8810,"Alsallakh":8811,"engaging":8812,"gathered":8813,"provided":8814,"Biometrika":8815,"Kahane":8816,"discarding":8817,"adult":8818,"DPU":8819,"comparatively":8820,"fine":8821,"249007":8822,"earn":8823,"med":8824,"Barr":8825,"lame":8826,"38":8827,"Representa":8828,"responsibilities":8829,"Internal":8830,"img":8831,"用户":8832,"Reporting":8833,"Switching":8834,"Graphical":8835,"Whenever":8836,"proposes":8837,"trainings":8838,"scholarship":8839,"Chang":8840,"infer":8841,"towards":8842,"histograms":8843,"Anyone":8844,"frustrated":8845,"cancellations":8846,"Kumaraswamy":8847,"Examples":8848,"conventional":8849,"retailer":8850,"ferring":8851,"iterative":8852,"history":8853,"go":8854,"banks":8855,"Relevant":8856,"{":8857,"coats":8858,"honest":8859,"Trading":8860,"affirmed":8861,"3319":8862,"sold":8863,"scalars":8864,"midrange":8865,"linewidth":8866,"Keqing":8867,"format311":8868,"blood":8869,"letter":8870,"14.13":8871,"Weibull":8872,"Pretrained":8873,"antitrust":8874,"gather":8875,"expecta":8876,"visits":8877,"algorithm":8878,"Wiesel":8879,"salient":8880,"spatio":8881,"Massa":8882,"6557":8883,"SAN":8884,"11th":8885,"glossing":8886,"GlobalP":8887,"bot":8888,"instantiate":8889,"Thinking":8890,"Means":8891,"somehow":8892,"EMNLP":8893,"Aleatory":8894,"descending":8895,"probabilities":8896,"tex":8897,"particu":8898,"SHARES":8899,"Discussions248":8900,"deposit":8901,"05108":8902,"580":8903,"findings":8904,"unrealistically":8905,"popped":8906,"AMI":8907,"725821":8908,"either":8909,"stress":8910,"States":8911,"Levy":8912,"distilling":8913,"spoke":8914,"Utility":8915,"Gerschgorin":8916,"Perez":8917,"philosophy":8918,"invari":8919,"1223":8920,"sharpening":8921,"262":8922,"read":8923,"native":8924,"mation":8925,"wafers":8926,"reposi":8927,"B.3":8928,"ﬂ":8929,"O3":8930,"eliminate":8931,"sis":8932,"demo":8933,"scholar":8934,"Lai":8935,"partials":8936,"changer":8937,"deductions":8938,"196":8939,"L1Loss":8940,"1025":8941,"string":8942,"1b":8943,"1003":8944,"referrals":8945,"fas":8946,"Standardize":8947,"ethernet":8948,"cease":8949,"methodology":8950,"emits":8951,"StevenJokes":8952,"17043":8953,"groundedness":8954,"York":8955,"discretization":8956,"reproduce":8957,"illustrating":8958,"Nishihara":8959,"Cai":8960,"thereto":8961,"tuples":8962,"deduce":8963,"formidable":8964,"Show":8965,"Epsilon":8966,"Electro":8967,"suring":8968,"Interestingly":8969,"lambd":8970,"Yi":8971,"Proceed":8972,"1.45":8973,"0679":8974,"quantization":8975,"SYSTEMS":8976,"dividend":8977,"426":8978,"today":8979,"catalyst":8980,"devices":8981,"tern":8982,"introduced":8983,"barometer":8984,"ingenuity":8985,"ify":8986,"Nominations":8987,"Bradbury":8988,"TOMAS":8989,"geographic":8990,"pivot":8991,"Invent":8992,"manufacture":8993,"societal":8994,"outlandish":8995,"hackers":8996,"858":8997,"deferral":8998,"BNLeNet":8999,"Rozi":9000,"1602":9001,"superlative":9002,"ysraell":9003,"achieve":9004,"Llmlingua":9005,"overhead":9006,"uniquely":9007,"mattered":9008,"Meanwhile":9009,"LIPTON":9010,"ment":9011,"412":9012,"surprised":9013,"Haben":9014,"mizing":9015,"described":9016,"siderations":9017,"dedicate":9018,"stopping":9019,"rithm":9020,"arrays":9021,"toddler":9022,"pip":9023,"352":9024,"Twice":9025,"suffer":9026,"consideration":9027,"DATACENTERS":9028,"November":9029,"shocking":9030,"tationally":9031,"STEVENS":9032,"Treating":9033,"allocated":9034,"queried":9035,"cls":9036,"suboptimal":9037,"1470":9038,"zticks":9039,"Complexity":9040,"suitable":9041,"apparent":9042,"COLETTE":9043,"amongst":9044,"involved":9045,"cmp":9046,"ysis":9047,"Raptor":9048,"上":9049,"Islander":9050,"Have":9051,"philosopher":9052,"2504":9053,"Qualifying":9054,"Cancellation":9055,"Cancer":9056,"think":9057,"extraneous":9058,"Core":9059,"损坏":9060,"462":9061,"Exclude":9062,"staple":9063,"fulfilling":9064,"mixing":9065,"Ducharme":9066,"1910":9067,"ndez":9068,"clipped":9069,"leftwards":9070,"machines":9071,"Hartley":9072,"Similarity":9073,"2.6727":9074,"hy":9075,"amenable":9076,"xxxvii":9077,"systematize":9078,"笔记本电脑":9079,"EditSum":9080,"Lei":9081,"appended":9082,"purportedly":9083,"flows":9084,"resentations":9085,"posts":9086,"modularity":9087,"reasonable":9088,"Elemente":9089,"storage":9090,"whereby":9091,"044":9092,"3002":9093,"observer":9094,"endeavor":9095,"ReLKT":9096,"1568":9097,"Ni":9098,"chinery":9099,"499":9100,"5200":9101,"deference":9102,"finetunes":9103,"Singular":9104,"Rasool":9105,"395":9106,"Evidence":9107,"Each":9108,"quorum":9109,"instability":9110,"spe":9111,"slice":9112,"door":9113,"Mechanisms":9114,"Jacobians":9115,"ij":9116,"availabil":9117,"cov":9118,"074":9119,"Response":9120,"discourse":9121,"Opinions":9122,"transformers":9123,"attorneys":9124,"mentations":9125,"unspecified":9126,"versus":9127,"Markdown":9128,"now":9129,"sporadically":9130,"2384":9131,"Officer":9132,"Exercises":9133,"calculus":9134,"timelines":9135,"Labradors":9136,"Hipporag":9137,"sharp":9138,"seemed":9139,"Plugging":9140,"Neyman":9141,"solid":9142,"multaneously":9143,"RAMKG":9144,"redress":9145,"boats":9146,"maps":9147,"253":9148,"Beutel":9149,"spam":9150,"Arcy":9151,"enriching":9152,"200":9153,"virtualshareholdermeeting":9154,"Semi":9155,"12b":9156,"upsample":9157,"chips":9158,"proportionally":9159,"2737092907":9160,"surprising":9161,"gymlibrary":9162,"LLP":9163,"0.00019":9164,"expectations":9165,"Ditlevsen":9166,"⊤":9167,"entitled":9168,"geriatric":9169,"Khamy":9170,"resentation":9171,"hearing":9172,"™":9173,"256":9174,"discriminator":9175,"Private":9176,"wiring":9177,"arrives":9178,"c1":9179,"Golde":9180,"climate":9181,"hid":9182,"folding":9183,"fabric":9184,"Important":9185,"0.92":9186,"supervision":9187,"Kapoor":9188,"vendors":9189,",":9190,"evolved":9191,"AW":9192,"conversely":9193,"Unseen":9194,"fr":9195,"4532":9196,"9142e":9197,"HUB":9198,"Askell":9199,"frames":9200,"recurse":9201,"LowerCholeskyTransform":9202,"undoes":9203,"flags":9204,"missed":9205,"MT":9206,"banc":9207,"walkthrough":9208,"focal":9209,"mips":9210,"Sharma":9211,"overly":9212,"Intersections":9213,"2.4724":9214,"8377":9215,"throw":9216,"1.3537":9217,"chebyshev":9218,"CLIP":9219,"voracious":9220,"Fangcheng":9221,"conceptual":9222,"needles":9223,"identifying":9224,"pedestrians":9225,"reimagined":9226,"Others":9227,"caused":9228,"Covenant":9229,"neglecting":9230,"Optics":9231,"GitHub9":9232,"63":9233,"exclude":9234,"8037":9235,"covmat":9236,"1155":9237,"Plaintiffs":9238,"590407":9239,"pushing":9240,"filt":9241,"sheer":9242,"success":9243,"hedged":9244,"Lim":9245,"patents":9246,"Horovod210":9247,"coordinating":9248,"pain":9249,"normal":9250,"reinventing":9251,"924182":9252,"dz":9253,"Yiming":9254,"𝑢":9255,"viii":9256,"randomized":9257,"solu":9258,"register":9259,"Optical":9260,"RandomSearcher":9261,"0741039859356903":9262,"internalize":9263,"5289":9264,"reinforcement":9265,"participating":9266,"hasoadd":9267,"eration":9268,"dark":9269,"319d85e578af0cdc590547f26231e4e31cdf1e42":9270,"analytically":9271,"Sahoo":9272,"Discussions104":9273,"830":9274,"T4":9275,"Brendel":9276,"Responsible":9277,"hopped":9278,"107":9279,"myopically":9280,"659":9281,"venture":9282,"5163":9283,"StackSpotAI":9284,"pressing":9285,"skills":9286,"billed":9287,"demystify":9288,"selections":9289,"10789":9290,"attr":9291,"thermore":9292,"fans":9293,"recip":9294,"jupyter":9295,"University":9296,"calling":9297,"Katarzyna":9298,"Zhu":9299,"Chinese":9300,"Officers":9301,"Optimizing":9302,"Y2":9303,"Economic":9304,"control":9305,"shareholdermeeting":9306,"hindering":9307,"Kolmogorov":9308,"Visa":9309,"broke":9310,"defeat":9311,"ASCII":9312,"Cash":9313,"𝑀":9314,"reconstructed":9315,"0767":9316,"scoliosis":9317,"Metzler":9318,"illusion":9319,"0.0017270983662456274":9320,"architectural":9321,"Be":9322,"computershare":9323,"curtain":9324,"YouTube":9325,"shell":9326,"ratably":9327,"anonymously":9328,"4697":9329,"booster":9330,"Mention":9331,"-":9332,"mental":9333,"quoted":9334,"shazbot":9335,"Discussions102":9336,"LeanDojo":9337,"devote":9338,"navigating":9339,"fallacy":9340,"visit":9341,"subscripts":9342,"distribuzione":9343,"imdb":9344,"-------":9345,"Derive":9346,"drunk":9347,"Organiza":9348,"1013":9349,"visualizations":9350,"1.3746":9351,"389":9352,"documentations":9353,"Zhao":9354,"capitalizations":9355,"outputs":9356,"0.5":9357,"detec":9358,"INT4":9359,"Hyeong":9360,"Rao":9361,"transportation":9362,"prevailing":9363,"illuminating":9364,"项":9365,"qualitative":9366,"Terminal":9367,"Preparation":9368,"conjunction":9369,"contractions":9370,"ushered":9371,"disputes":9372,"unrepresented":9373,"norm":9374,"trial":9375,"rested":9376,"overlay":9377,"Shah":9378,"workers":9379,"9099":9380,"discriminate":9381,"Transfer":9382,"pro":9383,"Alignment":9384,"gent":9385,"Adilov":9386,"plague":9387,"mysterious":9388,"8598":9389,"RATT":9390,"spatialized":9391,"979":9392,"platter":9393,"avatar":9394,"":9395,"Ott":9396,"generators":9397,"Fan":9398,"2008":9399,"emit":9400,"languished":9401,"concatenated":9402,"identity":9403,"Notation":9404,"Lomeli":9405,"ADBE":9406,"block4":9407,"backpropagate":9408,"3112":9409,"148":9410,"identified":9411,"07843":9412,"Py":9413,"local":9414,"PowerTransform":9415,"html":9416,"6305":9417,"NLRSE":9418,"271":9419,"Gate":9420,"newer":9421,"934":9422,"recognizes":9423,"synthetic":9424,"PaddlePaddle":9425,"MSELoss":9426,"gradients":9427,"13%":9428,"prehension":9429,"7903":9430,"fortuitous":9431,"mecha":9432,"transcript":9433,"cube":9434,"newest":9435,"bc":9436,"decomposing":9437,"posed":9438,"1RealSoftMax":9439,"application":9440,"96%":9441,"organizing":9442,"Artetxe":9443,"Chetlur":9444,"15556":9445,"propose":9446,"nums":9447,"markedly":9448,"109":9449,"shortcut":9450,"taxable":9451,"Pronouncements":9452,"Solutions":9453,"Discussions215":9454,"Mack":9455,"satisfied":9456,"Simply":9457,"REQUESTS":9458,"Board":9459,"backwards":9460,"Stripe":9461,"bodies":9462,"530":9463,"728":9464,"deferred":9465,"4.2":9466,"Liu":9467,"1.2275":9468,"variance":9469,"DTD":9470,"nor":9471,"concatenate":9472,"flaw":9473,"frogs":9474,"Merger":9475,"coverage":9476,"Letham":9477,"factoring":9478,"mergers":9479,"ˆ":9480,"770":9481,"Nasdaq100":9482,"\u0002":9483,"forked":9484,"lovely":9485,"07":9486,"efficiently":9487,"Josifovski":9488,"0.000000":9489,"Discussions238":9490,"jaccard":9491,"configured":9492,"Adeli":9493,"succession":9494,"adapt":9495,"041688":9496,"188":9497,"Appendix":9498,"architect":9499,"brand":9500,"mode":9501,"090":9502,"ID":9503,"Check":9504,"Corpus":9505,"DDR4":9506,"barring":9507,"might":9508,"profoundly":9509,"405":9510,"WARNING":9511,"1088":9512,"lizing":9513,"826":9514,"":9515,"feasibility":9516,"lessly":9517,"6132":9518,"Miech":9519,"Though":9520,"Choosing":9521,"GRUs":9522,"denominator":9523,"info":9524,"5x":9525,"likened":9526,"walked":9527,"Advancements":9528,"19%":9529,"geNet":9530,"Gbit":9531,"XWxh":9532,"truck":9533,"books":9534,"Worlds":9535,"Starcraft":9536,"goes":9537,"Redefining":9538,"protect":9539,"Executive":9540,"09844872561810249":9541,"Ahuja":9542,"streamline":9543,"RDM":9544,"pendent":9545,"tightened":9546,"judging":9547,"Lastly":9548,"responding":9549,"suc":9550,"19473v6":9551,"pem":9552,"PepsiCo":9553,"xstar":9554,"Punkt":9555,"Patwary":9556,"whole":9557,"cumbersome":9558,"ternative":9559,"Sequen":9560,"1992":9561,"workforce":9562,"relationship":9563,"Adapting":9564,"47":9565,"yy":9566,"Slice":9567,"filtered":9568,"ć":9569,"Money":9570,"Vu":9571,"Uelwer":9572,"RAID":9573,"fac":9574,"things":9575,"refine":9576,"increas":9577,"1.14":9578,"pendix":9579,"questionable":9580,"controller":9581,"Desjardins":9582,"bet":9583,"rungs":9584,"streamers":9585,"inspiring":9586,"obviously":9587,"microseconds":9588,"959":9589,"136":9590,"dilemma":9591,"ExactGPModel":9592,"Investment":9593,"sessing":9594,"C5":9595,"redirect":9596,"fluidly":9597,"unidirectional":9598,"giants":9599,"Initial":9600,"we":9601,"symbols":9602,"useful":9603,"Experience":9604,"Partner":9605,"427":9606,"actual":9607,"“":9608,"vivid":9609,"23%":9610,"concurrently":9611,"downwards":9612,"1050":9613,"tenders":9614,"silicon":9615,"Hailin":9616,"Copilots":9617,"propagate":9618,"trader":9619,"br":9620,"classifying":9621,"banana":9622,"nextstate":9623,"filled":9624,"333":9625,"contact":9626,"Ouyang":9627,"knobs":9628,"sewage":9629,"24xlarge":9630,"transposing":9631,"SSDs":9632,"Hebb":9633,"Q2K2":9634,"sonous":9635,"decide":9636,"expanded":9637,"401":9638,"6B":9639,"woman":9640,"engineer":9641,"languages":9642,"baked":9643,"IRS":9644,"overparametrized":9645,"nonexistence":9646,"Representations":9647,"requested":9648,"R2":9649,"girl":9650,"advent":9651,"fixed":9652,"accomplishes":9653,"Validation":9654,"Encoder":9655,"uation":9656,"withhold":9657,"KnowledGPT":9658,"searcher":9659,"chronicles":9660,"5193e":9661,"prod":9662,"Addi":9663,"USG":9664,"\u001b":9665,"comprehend":9666,"Riffel":9667,"事项":9668,"Instance":9669,"March":9670,"comfortable":9671,"posits":9672,"4070":9673,"individually":9674,"Bashexplainer":9675,"Put":9676,"newsgroups":9677,"concreteness":9678,"modernizing":9679,"1056":9680,"grounds":9681,"symbol":9682,"Yan":9683,"swaths":9684,"1.6486":9685,"violated":9686,"formalized":9687,"00387":9688,"safety":9689,"0.0993":9690,"02210":9691,"contradict":9692,"00001":9693,"hw1":9694,"regressors":9695,"bank":9696,"Cucchiara":9697,"susceptible":9698,"Uncer":9699,"Biden":9700,"Conversational":9701,"respond":9702,"Moro":9703,"Jang":9704,"5940":9705,"980305":9706,"Federal":9707,"0591":9708,"clinically":9709,"onto":9710,"carryforwards":9711,"𝑥":9712,"对于":9713,"jurisdiction":9714,"Convnets":9715,"xxix":9716,"synetune":9717,"parsing":9718,"style":9719,"attention2":9720,"inadvertently":9721,"automakers":9722,"Altos":9723,"referred":9724,"approximated":9725,"25th":9726,"Licensing":9727,"field":9728,"stumbled":9729,"listen":9730,"poured":9731,"Traditionally":9732,"00300":9733,"8707":9734,"Cornelia":9735,"assemble":9736,"longer":9737,"adaptation":9738,"results":9739,"monochro":9740,"Apple":9741,"434274":9742,"supporting":9743,"fusing":9744,"794":9745,"230":9746,"Defects":9747,"everyone":9748,"ReACC":9749,"behaved":9750,"printing":9751,"Aggregating":9752,"solver":9753,"说明":9754,"expenses":9755,"61":9756,"nicate":9757,"A.5":9758,"Chanan":9759,"blank":9760,"About":9761,"compositionality":9762,"relates":9763,"Frobenius":9764,"disparities":9765,"processing":9766,"tropical":9767,"Goal":9768,"Ravi":9769,"nary":9770,"der":9771,"Sukhbaatar":9772,"ured":9773,"posit":9774,"analyzing":9775,"presents":9776,"malfunctions":9777,"Koren":9778,"DC":9779,"1109":9780,"892":9781,"variate":9782,"fault":9783,"deprecated":9784,"luxury":9785,"1408":9786,"Mooij":9787,"Ownership":9788,"workload":9789,"CAE":9790,"benefitting":9791,"97":9792,"displaying":9793,"eight":9794,"minibatch":9795,"Shakeri":9796,"report":9797,"1890":9798,"GB":9799,"ditional":9800,"immune":9801,"Aarti":9802,"535":9803,"北京":9804,"shorten":9805,"tack":9806,"colleague":9807,"runners":9808,"kriz":9809,"quirks":9810,"atgctg":9811,"compensated":9812,"approximation":9813,"Siciliano":9814,"dismissal":9815,"streamlined":9816,"utiliza":9817,"extents":9818,"program":9819,"Wenxiang":9820,"queued":9821,"176B":9822,"02940":9823,"208500":9824,"started":9825,"larities":9826,"surpassing":9827,"compromising":9828,"Hewlett":9829,"900026":9830,"Chi":9831,"4151":9832,"Monolingual":9833,"caches":9834,"ator":9835,"TransformerEncoder":9836,"unexercised":9837,"Ç":9838,"repetitively":9839,"Aronszajn":9840,"Wall":9841,"reliance":9842,"LION":9843,"1945":9844,"418":9845,"definitely":9846,"TinyImages":9847,"dose":9848,"majors":9849,"BURGESS":9850,"632":9851,"1510":9852,"dropping":9853,"Slots":9854,"decoding":9855,"Frostig":9856,"mdp":9857,"tention":9858,"Bad":9859,"SuperNIC":9860,"Nan":9861,"soft":9862,"official":9863,"e03d187e043d2a17cae636d6af164015":9864,"941":9865,"tackles":9866,"Bioengineering":9867,"equations":9868,"boasts":9869,"approval":9870,"rendered":9871,"ascendance":9872,"population":9873,"lev":9874,"cooperate":9875,"kb":9876,"award":9877,"combina":9878,"loga":9879,"Economist":9880,"Celsius":9881,"K2":9882,"F":9883,"Adoption":9884,"Combine":9885,"Alon":9886,"Typical":9887,"exceptional":9888,"warrants":9889,"Its":9890,"Packard":9891,"filename":9892,"⌊":9893,"𝑞":9894,"1351":9895,"authors":9896,"75th":9897,"consolidations":9898,"xviii":9899,"Speculative":9900,"numel":9901,"2795":9902,"example":9903,"retailers":9904,"二节":9905,"Clip":9906,"streams":9907,"blks":9908,"VisualGPT":9909,"0.0015":9910,"xx":9911,"Riverside":9912,"arranged":9913,"concise":9914,"algorithmic":9915,"ery":9916,"district":9917,"preferentially":9918,"273898":9919,"状态":9920,"contemplate":9921,"2439":9922,"5840":9923,"deliberation":9924,"speeded":9925,"1997":9926,"Yida":9927,"suppose":9928,"achieving":9929,"Ishan":9930,"learnable":9931,"Bahdanau":9932,"Contribution":9933,"larly":9934,"concisely":9935,"SKURG":9936,"Sachs":9937,"eventualities":9938,"gou":9939,"Covs":9940,"Tensorflow":9941,"966":9942,"week":9943,"CONCURRENTLY":9944,"525":9945,"Pham":9946,"RECODE":9947,"reply":9948,"Headquartered":9949,"REDCODER":9950,"Things":9951,"simi":9952,"798":9953,"294430":9954,"dims":9955,"ModuleList":9956,"4510":9957,"Transformation":9958,"Introduction":9959,"uphold":9960,"magnitudes":9961,"Imagenet":9962,"7195":9963,"dating":9964,"Who":9965,"dle":9966,"halving":9967,"99%":9968,"Zoe":9969,"plugin":9970,"transforms":9971,"0.277":9972,"12th":9973,"IGX":9974,"studied":9975,"461":9976,"Unearned":9977,"Bajwa":9978,"plan":9979,"Lou":9980,"Owners":9981,"see":9982,"Finding":9983,"zipfile":9984,"valued":9985,"15657":9986,"Kilt":9987,"controlled":9988,"Reflects":9989,"也":9990,"312":9991,"8kHz":9992,"认证":9993,"neurophysiology":9994,"exists":9995,"primarily":9996,"lesser":9997,"u":9998,"loading":9999,"Extracting":10000,"METRICS":10001,"ASTs":10002,"leftChild":10003,"easily":10004,"2672e":10005,"downloads":10006,"Certainly":10007,"recoverability":10008,"witnessed":10009,"312770":10010,"Experts":10011,"primitives":10012,"nel":10013,"likelihood":10014,"alerts":10015,"careless":10016,"Securities":10017,"faithfulness":10018,"balance":10019,"𝚲":10020,"jurisdictional":10021,"088129":10022,"CIKM":10023,"pillar":10024,"SVM":10025,"indemnities":10026,"513":10027,"average":10028,"00":10029,"TransformedDistribution":10030,"Lagrangian":10031,"Beginning":10032,"W4":10033,"hands":10034,"Financial":10035,"Associated":10036,"优化":10037,"factorizing":10038,"scalability":10039,"Fash":10040,"exponent":10041,"03299":10042,"statutory":10043,"six":10044,"mount":10045,"数据":10046,"ablation":10047,"RA":10048,"integrals":10049,"book":10050,"Matthias":10051,"Discussions160":10052,"motions":10053,"90%":10054,"day":10055,"superseded":10056,"bigrams":10057,"05407":10058,"Approximators":10059,"Surbhi":10060,"intuition":10061,"Weights":10062,"BCEWithLogitsLoss":10063,"plays":10064,"sentinel":10065,"Faculty":10066,"1879":10067,"involving":10068,"delivered":10069,"ECBRF":10070,"994":10071,"Where":10072,"Airlines":10073,"knows":10074,"Hey":10075,"Control":10076,"item":10077,"hypotenuse":10078,"402":10079,"summed":10080,"sentations":10081,"broadcast":10082,"electronics":10083,"participation":10084,"Investments":10085,"Personal":10086,"783":10087,"Polyak":10088,"0.186632":10089,"archival":10090,"liability":10091,"ACTIVERAG":10092,"aggre":10093,"Howard":10094,"13":10095,"下面":10096,"speeds":10097,"audited":10098,"massive":10099,"Bindel":10100,"K":10101,"emissions":10102,"fitness":10103,"UDAPDR":10104,"disclaims":10105,"COG":10106,"micrometers":10107,"9B":10108,"Exercise":10109,"xxii":10110,"faulty":10111,"manifesting":10112,"plethora":10113,"266%":10114,"drug":10115,"MB":10116,"Underfitting":10117,"页眉":10118,"995":10119,"starts":10120,"esti":10121,"banking":10122,"3009":10123,"2159":10124,"EfficientNets":10125,"nA":10126,"Schlessinger":10127,"on":10128,"573":10129,"Providing":10130,"discovering":10131,"Reserved":10132,"Jenatton":10133,"justifiably":10134,"learns":10135,"wildfires":10136,"hits":10137,"fraud":10138,"class":10139,"Telco":10140,"100GbE":10141,"Likely":10142,"Discussions64":10143,"discussed":10144,"explore":10145,"virus":10146,"Laskin":10147,"Yutaro":10148,"bootcamp":10149,"Sparse":10150,"POS":10151,"branching316":10152,"100":10153,"finitely":10154,"Durable":10155,"hyperparameters":10156,"sequences":10157,"unions":10158,"argminx":10159,"Prediction":10160,"899995":10161,"signals":10162,"governmental":10163,"statistic":10164,"transform":10165,"SGDR":10166,"ply":10167,"wealth":10168,"Center":10169,"1017":10170,"travels":10171,"Pharmaceuticals":10172,"autoregression":10173,"Servers":10174,"deriva":10175,"Asynchrony":10176,"assumed":10177,"diagnosis":10178,"suggested":10179,"advise":10180,"meme":10181,"index":10182,"widened":10183,"jacent":10184,"manages":10185,"retainer":10186,"Classifier":10187,"5402":10188,"guishing":10189,"Lost":10190,"Tallec":10191,"2511":10192,"Anatomical":10193,"2679":10194,"elevations":10195,"docx":10196,"amino":10197,"dispersed":10198,"Titan":10199,"ious":10200,"require":10201,"007":10202,"2.3":10203,"Contingent":10204,"echo":10205,"Dartmouth":10206,"primed":10207,"Pu":10208,"unaffected":10209,"scripts":10210,"dissipate":10211,"relied":10212,"Signals":10213,"Mutual":10214,"map":10215,"Competition":10216,"epidemics":10217,"Convinse":10218,"North":10219,"Imple":10220,"paywalls":10221,"Compliance":10222,"buggy":10223,"1802":10224,"conformity":10225,"Drug":10226,"nominee":10227,"AND":10228,"524":10229,"instant":10230,"robot":10231,"0437":10232,"max":10233,"Terriers":10234,"exposition":10235,"alternation":10236,"movies":10237,"intuit":10238,"Rajpurkar":10239,"Elementary":10240,"foundries":10241,"30X":10242,"141":10243,"0577":10244,"PubMed":10245,"Hard":10246,"ml":10247,"0.224":10248,"binomial":10249,"exploiting":10250,"abbreviation":10251,"rescal":10252,"sql":10253,"698":10254,"revive":10255,"coordinates":10256,"declaring":10257,"Termination":10258,"invested":10259,"exaggeration":10260,"fascinating":10261,"1.974459321485":10262,"Siriwardhana":10263,"295":10264,"Stakeholders":10265,"958":10266,"warehousing":10267,"7892434579795236":10268,"Cambria":10269,"638":10270,"household":10271,"reconstructing":10272,"Architectures":10273,"stu":10274,"Acquired":10275,"Rosemary":10276,"Bridging":10277,"3.5":10278,"Wiki":10279,"alpha":10280,"Qin":10281,"reasonableness":10282,"Farri":10283,"0.9713101437890875":10284,"2.2461":10285,"detriment":10286,"den":10287,"draws":10288,"impaired":10289,"INCORPORATED":10290,"g4":10291,"Chen":10292,"Hallacy":10293,"Stage":10294,"314":10295,"wget":10296,"441":10297,"entirety":10298,"Pai":10299,"182764":10300,"Channels":10301,"Genegpt":10302,"Meduri":10303,"prioritize":10304,"FrozenLake":10305,"⌋":10306,"Kulkarni":10307,"Hedging":10308,"trades":10309,"loaders":10310,"GB200":10311,"353":10312,"NYSE":10313,"subscript":10314,"declines":10315,"divestitures":10316,"adoption":10317,"neighboring":10318,"theoretically":10319,"BERTLARGE":10320,"Zip":10321,"barely":10322,"N1":10323,"American":10324,"∅":10325,"899801":10326,"inelegant":10327,"rewriting":10328,"cases":10329,"coherence":10330,"entrypoint":10331,"liked":10332,"pipelines":10333,"prepay":10334,"WD":10335,"827":10336,"Manning":10337,"origi":10338,"repurchase":10339,"Just":10340,"properties":10341,"SelfMemory":10342,"anchor":10343,"sqrt":10344,"comedies":10345,"words":10346,"Aston":10347,"PC":10348,"room":10349,"marketing":10350,"unzipping":10351,"certify":10352,"ALU":10353,"Automotive":10354,"student":10355,"gauges":10356,"mimic":10357,"evolution":10358,"Union":10359,"consequently":10360,"required":10361,"与":10362,"Accuracy":10363,"Cola":10364,"sustaining":10365,"outages":10366,"autograd":10367,"SliceBackward0":10368,"780":10369,"reviewed":10370,"incentive":10371,"Universal":10372,"Parthenon":10373,"delivers":10374,"A.9":10375,"Torch":10376,"complicated":10377,"cancer":10378,"injunction":10379,"revisions":10380,"Xiao":10381,"Achievement":10382,"vn09":10383,"responsive":10384,"Scribner":10385,"structurally":10386,"503":10387,"modified":10388,"CenteredLayer":10389,"1029":10390,"Square":10391,"Luckily":10392,"communicates":10393,"folio63":10394,"know":10395,"Sketch":10396,"housekeeping":10397,"测试":10398,"needing":10399,"prospect":10400,"sl7423":10401,"deterministic":10402,"offset":10403,"Raman":10404,"reports":10405,"flexibility":10406,"CHROMESH":10407,"10559888854748693":10408,"seem":10409,"viewpoint":10410,"drop":10411,"Classical":10412,"variant":10413,"DECAF":10414,"NVDA2024":10415,"gently":10416,"Verbeek":10417,"maliciously":10418,"Optimize":10419,"stabilizing":10420,"headwinds":10421,"elem":10422,"granularity":10423,"neat":10424,"Juergen":10425,"1935":10426,"quick":10427,"Bases":10428,"distilled":10429,"Cojocaru":10430,"Laine":10431,"0624":10432,"1533":10433,"mutu":10434,"0.91":10435,"asymptotes":10436,"31%":10437,"Built":10438,"speedups":10439,"进展":10440,"5f":10441,"secondary":10442,"ballroom":10443,"playing":10444,"Shannon":10445,"Synopsys":10446,"breadth":10447,"¨":10448,"Adopts":10449,"fidelity":10450,"REPLUG":10451,"examples":10452,"economic":10453,"demystified":10454,"9249":10455,"sexual":10456,"simu":10457,"sprites":10458,"awkward":10459,"12246":10460,"initializer":10461,"Parallel":10462,"MLLM":10463,"tlenecks":10464,"Dehghani":10465,"5e":10466,"repeatable":10467,"Stemming":10468,"\\":10469,"manifestation":10470,"04333":10471,"mathresearch":10472,"handles":10473,"Gonz":10474,"Connected":10475,"Pitts":10476,"018":10477,"Distinctive":10478,"continuation":10479,"array":10480,"fin":10481,"Vapnik":10482,"3872":10483,"explosive":10484,"pop":10485,"Theoretical":10486,"quent":10487,"899988":10488,"flywheel":10489,"tenure":10490,"625":10491,"modernized":10492,"Sentence":10493,"wells":10494,"Jean":10495,"tackling":10496,"Kuzovkin":10497,"Bepler":10498,"mapping":10499,"Pittsburgh":10500,"chapeter":10501,"Labs":10502,"Based":10503,"completion":10504,"Compared":10505,"preinstalled":10506,"folder":10507,"reconstruction":10508,"GELU":10509,"Principles":10510,"Benchmark":10511,"postprocess":10512,"Martens":10513,"net":10514,"August":10515,"serve":10516,"708":10517,"Boards":10518,"RGB":10519,"PayPal":10520,"destabilizing":10521,"Windows":10522,"conclusion":10523,"leverag":10524,"8086":10525,"Arguably":10526,"Struc":10527,"check":10528,"equaled":10529,"720":10530,"Mar":10531,"816103":10532,"erably":10533,"licensors":10534,"holding":10535,"Transitions":10536,"solicited":10537,"Loukas":10538,"clever":10539,"Sakamoto":10540,"1940":10541,"Tiep":10542,"Absent":10543,"Vaessen":10544,"Not":10545,"boolean":10546,"defaults":10547,"gure":10548,"relieving":10549,"Nobody":10550,"Reputation":10551,"repayments":10552,"10th":10553,"linguistic":10554,"917":10555,"xs":10556,"HNSW":10557,"Provide":10558,"BLLM":10559,"gyms":10560,"INTC":10561,"agreements":10562,"nearly":10563,"Vested":10564,"Translation":10565,"Richard":10566,"benchmarking":10567,"1708":10568,"Leasehold":10569,"windows":10570,"demon":10571,"Hui":10572,"674":10573,"StackedRNNScratch":10574,"abilities":10575,"内存":10576,"formulation":10577,"Ex":10578,"implementations":10579,"wavelength":10580,"Gaurav":10581,"gating":10582,"ha":10583,"absolute":10584,"32":10585,"Swish":10586,"sixty":10587,"whet":10588,"dists":10589,"Mechanism":10590,"Rastogi":10591,"481":10592,"leads":10593,"topecongiro":10594,"vo":10595,"bbox":10596,"corrupted":10597,"allocat":10598,"catch":10599,"combinatorial":10600,"preserving":10601,"multiplications":10602,"two":10603,"scaling":10604,"thousands":10605,"cryptocurrency":10606,"Lewis":10607,"separation":10608,"typically":10609,"several":10610,"examine":10611,"obligate":10612,"enhancements":10613,"pi":10614,"chairpersons":10615,"navigation":10616,"corresponding":10617,"takes":10618,"013":10619,"IJCNN":10620,"ically":10621,"Kaddour":10622,"contrib":10623,"mansion":10624,"FOR":10625,"⊮":10626,"about":10627,"899922":10628,"nbatch":10629,"lower":10630,"encoder":10631,"notional":10632,"773":10633,"discourage":10634,"Provides":10635,"Novikov":10636,"philosophies":10637,"specifies":10638,"frozen":10639,"router":10640,"3000":10641,"stretch":10642,"Larochelle":10643,"21.1":10644,"Galac":10645,"494":10646,"front":10647,"Case":10648,"Hundreds":10649,"heat":10650,"sigma":10651,"nin":10652,"rare":10653,"parameterized":10654,"Membership":10655,"belonged":10656,"German":10657,"saturation":10658,"144":10659,"tually":10660,"suggest":10661,"8462":10662,"1.76":10663,"hypotheses":10664,"Services":10665,"Jie":10666,"Nauk":10667,"intermediate":10668,"unfortu":10669,"offers":10670,"discount":10671,"HDDs":10672,"independence":10673,"CRSR":10674,"mutually":10675,"illustrations":10676,"FMR":10677,"à":10678,"intrinsically":10679,"138":10680,"ypandya":10681,"--":10682,"Optuna":10683,"waiting":10684,"explored":10685,"insist":10686,"outreach":10687,"long":10688,"notion":10689,"sections":10690,"dependence":10691,"1492":10692,"resources":10693,"Mathematicians":10694,"sented":10695,"-.":10696,"055":10697,"lowerrights":10698,"nition":10699,"Recall":10700,"studentT":10701,"preoccupy":10702,"E5":10703,"autoregressive":10704,"Up":10705,"Lapse":10706,"Term":10707,"moving":10708,"obtained":10709,"Δ":10710,"capturing":10711,"Cocomic":10712,"Knowledgenavigator":10713,"checkout":10714,"node":10715,"accompany":10716,"consuming":10717,"Converting":10718,"Discussions234":10719,"Being":10720,"courses":10721,"Ali":10722,"inform":10723,"copyfile":10724,"Chapter":10725,"philosophical":10726,"host":10727,"Bahri":10728,"Waterman":10729,"young":10730,"strat":10731,"Oliver":10732,"Blue":10733,"applicants":10734,"wet":10735,"seg":10736,"undermines":10737,"cycles":10738,"prohibited":10739,"Stand":10740,"diligently":10741,"http":10742,"d2":10743,"4822":10744,"augment":10745,"softmin":10746,"bonds":10747,"Voting":10748,"advance":10749,"Unresolved":10750,"Proceedings":10751,"cancels":10752,"rigorous":10753,"publicly":10754,"candidate":10755,"Xu":10756,"intelligently":10757,"keyrings":10758,"87":10759,"964542":10760,"DKS":10761,"26":10762,"eyeballed":10763,"COCO":10764,"Chip":10765,"688":10766,"project":10767,"mathematics":10768,"Reveal":10769,"robotics":10770,"270":10771,"dubious":10772,"RTLFixer":10773,"Aim":10774,"selective":10775,"Tatoeba":10776,"11761":10777,"𝝓":10778,"finely":10779,"course":10780,"dismissing":10781,"CNNs":10782,"aligned":10783,"Gibbs":10784,"项目":10785,"rnn":10786,"Convexity":10787,"LSTM":10788,"exhaustive":10789,"by":10790,"is":10791,"lousy":10792,"tolist":10793,"Guggenheim":10794,"Deepak":10795,"Synthesized":10796,"ylim":10797,"Lift":10798,"planning":10799,"pairings":10800,"plumbing":10801,"vi":10802,"il":10803,"egy":10804,"o":10805,"counsel":10806,"courtesy":10807,"highlighting":10808,"Lisa":10809,"among":10810,"elementary":10811,"arrowprops":10812,"namely":10813,"weave":10814,"Founder":10815,"962":10816,"accrues":10817,"328":10818,"submitted":10819,"Stable":10820,"files":10821,"Correctly":10822,"Forget":10823,"captioned":10824,"divides":10825,"reveals":10826,"文件":10827,"InferFix":10828,"mea":10829,"reads":10830,"Nonparametric":10831,"intro":10832,"012271314788363914":10833,"oil":10834,"tualized":10835,"𝜕":10836,"Maas":10837,"datapoint":10838,"maker1":10839,"0.243":10840,"razor":10841,"64MB":10842,"determinazione":10843,"Multidimensional":10844,"789":10845,"upsampling":10846,"nlp":10847,"innovation":10848,"Graduate":10849,"leverage":10850,"leans":10851,"SCH":10852,"little":10853,"immense":10854,"col":10855,"ethics":10856,"experience":10857,"admission":10858,"perplexity":10859,"underutilization":10860,"0000000000001":10861,"proposed":10862,"awareness":10863,"title":10864,"ReLUs":10865,"vidually":10866,"delay":10867,"McCann":10868,"reorg":10869,"knowl":10870,"Insert":10871,"2430":10872,"O":10873,"8730":10874,"6411":10875,"appearance":10876,"graytowne":10877,"diverges":10878,"war":10879,"puzzle":10880,"aim":10881,"champion":10882,"xlim":10883,"suddenly":10884,"encapsulate":10885,"Decomposition":10886,"expresses":10887,"verb":10888,"2002":10889,"dom":10890,"anyone":10891,"pertinent":10892,"stops":10893,"typesetter":10894,"retrieves":10895,"reseller":10896,"positives":10897,"arnav":10898,"gaming":10899,"shallow":10900,"sector":10901,"closes":10902,"2027":10903,"parametric":10904,"Norelli":10905,"ramble":10906,"lung":10907,"27":10908,"Is":10909,"puting":10910,"Tu":10911,"fried":10912,"bling":10913,"discovery":10914,"Fur":10915,"fuses":10916,"comput":10917,"keynote":10918,"beverage":10919,"intercompany":10920,"regress":10921,"McCandlish":10922,"ful":10923,"multidimen":10924,"HiFi":10925,"premature":10926,"Discussions237":10927,"Cer":10928,"researcher":10929,"Man":10930,"1933":10931,"Gemini":10932,"Broadcom":10933,"Nerveux":10934,"Write":10935,"crease":10936,"denotes":10937,"Kipf":10938,"yd":10939,"3274":10940,"caliber":10941,"reinvested":10942,"Statisti":10943,"Survey":10944,"Stone":10945,"attack":10946,"averages":10947,"throwing":10948,"Joe":10949,"remeasured":10950,"shine":10951,"unsatisfy":10952,"Thus":10953,"differentiates":10954,"disclose":10955,"Reshape":10956,"charges":10957,"photos":10958,"Responsibilities":10959,"10000":10960,"All":10961,"sigmas":10962,"MultiStepLR":10963,"anyway":10964,"Predicting":10965,"predictions":10966,"bylaws":10967,"amortization":10968,"suppressed":10969,"14198":10970,"absurdity":10971,"aeroplane":10972,"TRANX":10973,"tapestry":10974,"evenly":10975,"qui":10976,"neuron":10977,"approachable":10978,"≻":10979,"poses":10980,"Items":10981,"International":10982,"tiveness":10983,"ality":10984,"although":10985,"assembling":10986,"R1":10987,"III":10988,"Tree":10989,"FD":10990,"isolation":10991,"differs":10992,"RepoFusion":10993,"strategies":10994,"0.009948714636266232":10995,"universities":10996,"Splitting":10997,"Moment":10998,"Vietnam":10999,"Karypis":11000,"caveat":11001,"Donhauser":11002,"signatures":11003,"outside":11004,"thumbnails":11005,"Emphasize":11006,"627":11007,"obtaining":11008,"invented":11009,"Shaw":11010,"2.0148":11011,"new":11012,"356":11013,"HBM":11014,"channel":11015,"maintained":11016,"eclipse":11017,"Discussions281":11018,"Electing":11019,"SIGGRAPH":11020,"Expressway":11021,"maximal":11022,"1.1":11023,"items":11024,"raised":11025,"NCCL":11026,"advice":11027,"ingre":11028,"relax":11029,"regularized":11030,"Trappolini":11031,"Deshmukh":11032,"3368":11033,"Saving":11034,"trebeljahr":11035,"ele":11036,"rules":11037,"Nouvelles":11038,"nonlinearities":11039,"surge":11040,"basicConfig":11041,"value":11042,"dynamically":11043,"done":11044,"Ellie":11045,"Sa":11046,"restricted":11047,"11990":11048,"mil":11049,"into":11050,"Discussions58":11051,"ble":11052,"Elsken":11053,"Processors":11054,"seconds":11055,"footprint":11056,"jroberayalas":11057,"receives":11058,"Warren":11059,"abundant":11060,"RetrieveGAN":11061,"Verification":11062,"contri":11063,"employ":11064,"Divvala":11065,"Oversight":11066,"nbextensions":11067,"become":11068,"coherent":11069,"callable":11070,"Irrevocable":11071,"clas":11072,"experimented":11073,"culating":11074,"Gerson":11075,"346":11076,"791":11077,"autocomplete":11078,"Launched":11079,"Skcoder":11080,"negligible":11081,"Nicolas":11082,"split":11083,"Tangent":11084,"portions":11085,"transmitting":11086,"478":11087,"China":11088,"label":11089,"Affiliated":11090,"042":11091,"calm":11092,"256MB":11093,"10428":11094,"composition":11095,"velocity":11096,"linestyle":11097,"Child":11098,"auditing":11099,"Enhancements":11100,"restricting":11101,"installation":11102,"786":11103,"m":11104,"lest":11105,"𝑋":11106,"memorable":11107,"026":11108,"Philosophy":11109,"0.24":11110,"Representation":11111,"rounded":11112,"engineering":11113,"voltage":11114,"malevolent":11115,"ent":11116,"plifying":11117,"Ren":11118,"reliable":11119,"compliant":11120,"Alessandro":11121,"MasterCard":11122,"suspiciously":11123,"Tikhonov":11124,"Solution":11125,"speedier":11126,"448":11127,"ensuing":11128,"misclassifying":11129,"rotations":11130,"lists":11131,"Cohen":11132,"Replug":11133,"Neu":11134,"insight":11135,"Greece":11136,"extracts":11137,"Techniques":11138,"Werbos":11139,"cance":11140,"viewable":11141,"3836":11142,"On":11143,"Arun":11144,"lkj":11145,"params":11146,"773950":11147,"Nishiyama":11148,"missing":11149,"0.846":11150,"services":11151,"skipped":11152,"Inspired":11153,"April":11154,"Accumulator":11155,"Supplementary":11156,"Greff":11157,"Civil":11158,"Islam":11159,"Yasunaga":11160,"RAG":11161,"impactful":11162,"40%":11163,"heightened":11164,"float":11165,"Median":11166,"Fact":11167,"metamind":11168,"Miniconda13":11169,"Defining":11170,"Nik":11171,"manipulating":11172,"makes":11173,"Wi":11174,"Remarkably":11175,"SK":11176,"12.6":11177,"12024":11178,"substance":11179,"wondering":11180,"914":11181,"Discussions317":11182,"lobbying":11183,"pretraining":11184,"venience":11185,"grapples":11186,"Miotto":11187,"IDF":11188,"criticized":11189,"Peng":11190,"English":11191,"Transform":11192,"entire":11193,"mountain":11194,"encounters":11195,"realize":11196,"104831":11197,"1092":11198,"6750":11199,"buttons":11200,"AdaptiveAvgPool1d":11201,"JANUARY":11202,"218":11203,"Amsterdam":11204,"Gaosheng":11205,"Element":11206,"cover":11207,"business":11208,"Jacot":11209,"Assessments":11210,"Visualization":11211,"Y":11212,"Continuous":11213,"Lachaux":11214,"Proposal":11215,"Isola":11216,"pa":11217,"symmetrical":11218,"killer":11219,"gained":11220,"Tools":11221,"Minibatch":11222,"241":11223,"Discussions151":11224,"Standardization":11225,"Attendance":11226,"superiority":11227,"733":11228,"Testing":11229,"Siri":11230,"price":11231,"AGAINST":11232,"homes":11233,"summaries":11234,"capital":11235,"textrm":11236,"1Whf":11237,"popularly":11238,"amalgamates":11239,"participant":11240,"SuccessiveHalvingSched":11241,"Zettlemoyer":11242,"Orthopedist":11243,"Objects":11244,"fea":11245,"PICa":11246,"Dot":11247,"terrible":11248,"12950":11249,"consolidation":11250,"pos":11251,"ecution":11252,"drawbacks":11253,"Pushing":11254,"Date":11255,"Lawsuits":11256,"Addressing":11257,"131":11258,"signi":11259,"24%":11260,"sons":11261,"Rajbhandari":11262,"portfolios":11263,"competencies":11264,"redesigned":11265,"Struct":11266,"means":11267,"Volinsky":11268,"zyhazwraith":11269,"subclasses":11270,"Zhuang":11271,"Indebtedness":11272,"vention":11273,"matical":11274,"Ecker":11275,"cautionary":11276,"engulfs":11277,"2048":11278,"afforded":11279,"1954":11280,"canyon":11281,"814":11282,"Importantly":11283,"undo":11284,"organic":11285,"cyclical":11286,"modular":11287,"refundable":11288,"Patents":11289,"initially":11290,"Memorizing":11291,"Stock":11292,"wrap":11293,"duced":11294,"AACL":11295,"6855":11296,"Markovian":11297,"vote":11298,"310":11299,"1505":11300,"num":11301,"amazing":11302,"3454":11303,"extractors":11304,"multipli":11305,"Illustration":11306,"PID":11307,"Arp":11308,"asymptotics":11309,"corporates":11310,"diagonal":11311,"uneven":11312,"Micron":11313,"测试阶段":11314,"receivable":11315,"unduly":11316,"tutorial7":11317,"relaxed":11318,"Johnson":11319,"Hopper":11320,"troducing":11321,"electric":11322,"hallucinations":11323,"Sellers":11324,"Discussions85":11325,"Maddison":11326,"Frasconi":11327,"reduces":11328,"emerging":11329,"Executing":11330,"vii":11331,"Cookbook":11332,"often":11333,"robust":11334,"speak":11335,"emphasizes":11336,"scalar":11337,"offs":11338,"Simard":11339,"Initialize":11340,"V":11341,"111483":11342,"encoders":11343,"𝝃":11344,"sometimes":11345,"":11346,"978":11347,"fol":11348,"Sand":11349,"bundled":11350,"conciseness":11351,"screenful":11352,"lottery":11353,"Rossiello":11354,"goals":11355,"parallelism":11356,"LIBRARIES":11357,"709":11358,"NVLink":11359,"inventories":11360,"certificates":11361,"Naively":11362,"prevents":11363,"introduces":11364,"xxxviii":11365,"eigenvalues":11366,"forfeited":11367,"011548051321691994":11368,"Reed":11369,"t10k":11370,"separately":11371,"3190":11372,"binocular":11373,"体验":11374,"权限":11375,"viewpoints":11376,"interactivity":11377,"advantages":11378,"bursting":11379,"unemployment":11380,"Schilling":11381,"2406":11382,"paral":11383,"conditionally":11384,"btae075":11385,"158":11386,"Deploy":11387,"Conducting":11388,"kaggle":11389,"505":11390,"butions":11391,"Theoria":11392,"Ozair":11393,"Aly":11394,"applications":11395,"foot":11396,"community":11397,"Class":11398,"Dell":11399,"truncating":11400,"833":11401,"additive":11402,"cum":11403,"ℱ":11404,"Bordes":11405,"Golmant":11406,"Sum":11407,"determine":11408,"Buy":11409,"voc2012":11410,"Helmbold":11411,"defense":11412,"Specifically":11413,"Amd":11414,"980":11415,"Method":11416,"Jaccard":11417,"syllogism":11418,"lifting":11419,"088457":11420,"intersegment":11421,"next":11422,"modulated":11423,"arXiv":11424,"textualized":11425,"13038":11426,"Sancak":11427,"DGX":11428,"13179":11429,"999":11430,"PromptDiff":11431,"boiler":11432,"id":11433,"Seq2SeqAttentionDecoder":11434,"Sentiment":11435,"9113":11436,"lates":11437,"AWS":11438,"MLPs":11439,"P1":11440,"Alphabet":11441,"ining":11442,"charter":11443,"Defined":11444,"2117":11445,"Regression":11446,"504":11447,"and":11448,"640":11449,"CoCoMic":11450,"shapes":11451,"930":11452,"explanation":11453,"asking":11454,"0.02":11455,"justified":11456,"Amsalem":11457,"Carlini":11458,"1495":11459,"552":11460,"quency":11461,"0.0967":11462,"Leandojo":11463,"mentioning":11464,"INTRODUCTION":11465,"indivisible":11466,"11.9":11467,"sophisticated":11468,"cable":11469,"animals":11470,"notebooks":11471,"Oxford":11472,"rics":11473,"pleasant":11474,"outlook":11475,"revised":11476,"Abadi":11477,"Squared":11478,"modders":11479,"3251":11480,"inputs":11481,"2.1":11482,"LLM":11483,"rebroadcast":11484,"acting":11485,"vitskiy":11486,"Alliance":11487,"stories":11488,"整个":11489,"forest":11490,"Gou":11491,"predictable":11492,"31":11493,"eig":11494,"Behavior":11495,"Aliasing":11496,"rescaling":11497,"batch":11498,"家具":11499,"desirable":11500,"pointwise":11501,"Kg":11502,"vectorized":11503,"distant":11504,"aggregating":11505,"y":11506,"livelihood":11507,"nonsensical":11508,"ration":11509,"Workers":11510,"qingfengtommy":11511,"Article":11512,"ubiquitous":11513,"Discussions158":11514,"Region":11515,"3200":11516,"tracing":11517,"orders":11518,"djliden":11519,"hatch":11520,"8828.3095703125":11521,"Baidu":11522,"Availability":11523,"beings":11524,"526":11525,"606":11526,"reduction":11527,"agree":11528,"𝜽":11529,"lim":11530,"behave":11531,"005":11532,"1603":11533,"retrievals":11534,"unusual":11535,"gelu":11536,"intersections":11537,"gpu1":11538,"cept":11539,"Clean":11540,"150":11541,"Dohan":11542,"Second":11543,"braries":11544,"linearly":11545,"Baid":11546,"wiped":11547,"doubles":11548,"whom":11549,"doodle":11550,"85":11551,"leader":11552,"sampleSubmission":11553,"Taylor":11554,"0.17802":11555,"supported":11556,"Shifting":11557,"point":11558,"869":11559,"File":11560,"nevertheless":11561,"disable":11562,"discretize":11563,"copy":11564,"buck":11565,"uations":11566,"8":11567,"6851":11568,"adequately":11569,"50%":11570,"hotfix":11571,"impressive":11572,"enlarges":11573,"inconsistency":11574,"Kakulapati":11575,"figure":11576,"trates":11577,"039":11578,"json":11579,"visualizing":11580,"Sanh":11581,"generation":11582,"cheaper":11583,"implies":11584,"virtue":11585,"multiset":11586,"547":11587,"Schlatter":11588,"halve":11589,"0.060466":11590,"cousin":11591,"cols":11592,"harmonizing":11593,"painting":11594,"日期":11595,"Least":11596,"Karayev":11597,"ddof":11598,"chairman":11599,"Shamma":11600,"properly":11601,"Buses":11602,"Peters":11603,"REFERENCE":11604,"biject":11605,"BasicScheduler":11606,"expanding":11607,"1934":11608,"s1":11609,"Nogueira":11610,"6294":11611,"384":11612,"Ctrl":11613,"lacked":11614,"InProgress":11615,"chi":11616,"small":11617,"长":11618,"«":11619,"Download":11620,"hedge":11621,"supplying":11622,"1473":11623,"1.9390":11624,"Com":11625,"42B":11626,"Discussions255":11627,"compose":11628,"中":11629,"decaying":11630,"Textual":11631,"Challenge":11632,"Implications":11633,"assigning":11634,"scape":11635,"NiN":11636,"CARP":11637,"223500":11638,"materializing":11639,"manipulate":11640,"coding":11641,"splitext":11642,"machinery":11643,"mortality":11644,"establishing":11645,"Jobs":11646,"exposures":11647,"MyLinear":11648,"dense":11649,"ITER":11650,"instants":11651,"monoton":11652,"measure":11653,"Discussions223":11654,"9597":11655,"Values":11656,"val":11657,"joseppinilla":11658,"factual":11659,"Blocks":11660,"fre":11661,"RESDSQL":11662,"becoming":11663,"tbaums":11664,"ranges":11665,"TinySSD":11666,"12.11":11667,"invent":11668,"Step":11669,"Supervised":11670,"executions":11671,"remeasurement":11672,"invaluable":11673,"unrealistic":11674,"Vote":11675,"4659":11676,"applica":11677,"solicit":11678,"reproduced":11679,"Linnaeus":11680,"representations":11681,"Ethereum":11682,"commenced":11683,"1990":11684,"Transforming":11685,"Bioreader":11686,"Shuster":11687,"Tanh":11688,"outperforms":11689,"4399":11690,"cast":11691,"disutility":11692,"Matena":11693,"severe":11694,"subject":11695,"theoretical":11696,"inferior":11697,"Penn":11698,"abstrac":11699,"sumed":11700,"Matter":11701,"introducing":11702,"rectified":11703,"94304":11704,"convolution":11705,"RegNetY":11706,"ballpark":11707,"operate":11708,"Simulators":11709,"answers":11710,"verify":11711,"Symbolic":11712,"Ranzato":11713,"Nadaraya":11714,"Frequently":11715,"3526":11716,"1075":11717,">":11718,"vent":11719,"megabytes":11720,"EWMA":11721,"★":11722,"10347":11723,"hairier":11724,"Minimum":11725,"localized":11726,"dubbed":11727,"bounded":11728,"bouncing":11729,"drawn":11730,"516":11731,"advisory":11732,"Mann":11733,"CG":11734,"but":11735,"complement":11736,"factor":11737,"regression":11738,"01274":11739,"Earned":11740,"Jain":11741,"Agrawal":11742,"mentation":11743,"beta2":11744,"worldwide":11745,"reimbursement":11746,"binom":11747,"656":11748,"Linguistics":11749,"Derivatives":11750,"benefit":11751,"confers":11752,"Localize":11753,"engage":11754,"unobserved":11755,"uct":11756,"documentation":11757,"displacement":11758,"":11759,"PRESENTS":11760,"cisco":11761,"10.8":11762,"2988":11763,"LotFrontage":11764,"queries":11765,"Park":11766,"CNN":11767,"Scientists":11768,"Markets":11769,"But":11770,"investments":11771,"Sanjar":11772,"EmilyOng":11773,"Excitation":11774,"multiplicative":11775,"workshops":11776,"750M":11777,"excludes":11778,"op":11779,"commonalities":11780,"brings":11781,"must":11782,"7.359":11783,"evry":11784,"2402":11785,"uating":11786,"Lewkowycz":11787,"instruments":11788,"attribute":11789,"Reading":11790,"abstracting":11791,"Hoi":11792,"secrets":11793,"Hou":11794,"列表":11795,"Breuleux":11796,"phenomena":11797,"erates":11798,"exempt":11799,"931":11800,"unwilling":11801,"isinstance":11802,"recap":11803,"Zeitschrift":11804,"Discussions286":11805,"Amodei":11806,"Extended":11807,"08144":11808,"isolated":11809,"outcome":11810,"weeds":11811,"973":11812,"Un":11813,"Shrimali":11814,"entrenching":11815,"regard":11816,"Prepaid":11817,"top":11818,"Lehrwerk":11819,"ANNUAL":11820,"attach":11821,"confronted":11822,"adjusts":11823,"729":11824,"⊂":11825,"fixing":11826,"app":11827,"collectability":11828,"A.27":11829,"Because":11830,"integers":11831,"8370":11832,"Ryan":11833,"schedulers":11834,"29":11835,"Provided":11836,"Together":11837,"1422":11838,"My":11839,"tour":11840,"tips":11841,"55%":11842,"other":11843,"Salimans":11844,"Flora":11845,"lab":11846,"3422":11847,"Rasmussen":11848,"Ornstein":11849,"Deviations":11850,"stretching":11851,"Gijs":11852,"ALL":11853,"RUN":11854,"TensorFlow34":11855,"Gunrock":11856,"Hwang":11857,"Parisotto":11858,"semination":11859,"test274":11860,"United":11861,"mu":11862,"roost":11863,"mainboards":11864,"BN":11865,"Blended":11866,"Shampoo":11867,"HyDE":11868,"Ha":11869,"delete":11870,"Tsai":11871,"concept":11872,"capping":11873,"EXTRA":11874,"ally":11875,"Ray":11876,"1035":11877,"3.3":11878,"Gershgorin":11879,"opt":11880,"predicting":11881,"embodies":11882,"By":11883,"holidays":11884,"bilin":11885,"06175":11886,"Teo":11887,"liusy182":11888,"gression":11889,"Rosenblatt":11890,"Right":11891,"C2":11892,"07812":11893,"fun":11894,"segmenting":11895,"quotient":11896,"suffixes":11897,"unaware":11898,"modification":11899,"A2":11900,"rifts":11901,"25":11902,"Trees":11903,"Claims":11904,"2211":11905,"HDD":11906,"hypoth":11907,"528166":11908,"moderately":11909,"recovered":11910,"placed":11911,"exhibit":11912,"Barak":11913,"classical":11914,"TST":11915,"Registrar":11916,"descent":11917,"aclImdb":11918,"effected":11919,"Employees":11920,"input":11921,"joining":11922,"different":11923,"reproducible":11924,"1804":11925,"LangChain":11926,"eter":11927,"hiccup":11928,"\u0015":11929,"enduring":11930,"Feed":11931,"Ratio":11932,"162754":11933,"Whx":11934,"e":11935,"02219":11936,"resizing":11937,"Caffe":11938,"speaking":11939,"firm":11940,"diamond":11941,"waving":11942,"784":11943,"spends":11944,"tence":11945,"onboarding":11946,"prominent":11947,"lamest":11948,"Indianapolis":11949,"Ma":11950,"kxxt":11951,"usu":11952,"bankruptcy":11953,"vlines":11954,"restoring":11955,"advising":11956,"proactive":11957,"办公椅":11958,"replaced":11959,"Discussions244":11960,"watches":11961,"2040":11962,"treatment":11963,"Chaudhuri":11964,"recurrent":11965,"Explor":11966,"tangible":11967,"efficiencies":11968,"activity":11969,"015":11970,"supplemental":11971,"variations":11972,"693":11973,"table":11974,"peer":11975,"summarized":11976,"1980s":11977,"纯":11978,"ho":11979,"interpolation":11980,"Investors":11981,"reshape":11982,"Prove":11983,"unique":11984,"REST":11985,"tik":11986,"displayed":11987,"sampled":11988,"1590":11989,"punkt":11990,"occurs":11991,"occasion":11992,"Li":11993,"originally":11994,"replicates":11995,"hat":11996,"Supercharging":11997,"Conversely":11998,"exemplified":11999,"subsequence":12000,"16347":12001,"inclusion":12002,"right":12003,"loaded":12004,"RandomGenerator":12005,"23445":12006,"rest":12007,"extended":12008,"obtain":12009,"Make":12010,"clearly":12011,"themself":12012,"bb":12013,"pseudorandom":12014,"tribution":12015,"connects":12016,"third":12017,"invocation":12018,"lem":12019,"organized":12020,"Mangal":12021,"operators":12022,"membership":12023,"increases":12024,"leaves":12025,"reschedule":12026,"simulta":12027,"seamlessly":12028,"257636":12029,"city":12030,"245":12031,"meant":12032,"1902":12033,"license":12034,"CIFAR10":12035,"bend":12036,"Neither":12037,"vast":12038,"heralded":12039,"15%":12040,"impinge":12041,"rectangle":12042,"William":12043,"TF32":12044,"Greedy":12045,"disproportionate":12046,"fier":12047,"681":12048,"haria":12049,"Kirkland":12050,"160MB":12051,"Yunteng":12052,"mathematician":12053,"Press":12054,"biology":12055,"𝑟":12056,"readability":12057,"0365":12058,"hopefully":12059,"2798":12060,"acceptability":12061,"maintains":12062,"03184":12063,"blunt":12064,"Objectives":12065,"ingesting":12066,"Nonconvex":12067,"Needless":12068,"timer":12069,"Bentley":12070,"around":12071,"INDUSTRIAL":12072,"0.03":12073,"authorizations":12074,"Proposals":12075,"chance":12076,"omitted":12077,"overlapping":12078,"stems":12079,"JPEG":12080,"nest":12081,"roads":12082,"8%":12083,"Thomee":12084,"Maron":12085,"intuitively":12086,"2z":12087,"wreck":12088,"lters":12089,"vertical":12090,"Zolna":12091,"treaties":12092,"Hamilton":12093,"muddier":12094,"Quite":12095,"advised":12096,"Alex":12097,"Gray":12098,"5226":12099,"Sarto":12100,"reweigh":12101,"instantiations":12102,"iv":12103,"sounds":12104,"user":12105,"Selective":12106,"7640":12107,"523":12108,"director":12109,"collections":12110,"deliver":12111,"Balance":12112,"Oriented":12113,"latent":12114,"arrival":12115,"Possible":12116,"CED":12117,"concerned":12118,"Jen":12119,"lambdas":12120,"Despite":12121,"consumer":12122,"argsort":12123,"2.0":12124,"67":12125,"MNIST":12126,"Yime":12127,"Radford":12128,"distractions":12129,"featuring":12130,"insertion":12131,"75":12132,"\u0003":12133,"RegNets":12134,"requesting":12135,"astute":12136,"Posed":12137,"trap":12138,"732":12139,"megawatts":12140,"infamous":12141,"attention1":12142,"SciPy":12143,"0398":12144,"modifies":12145,"announcements":12146,"coffee":12147,"17325432986021042":12148,"indebtedness":12149,"mydict2":12150,"Roadmap":12151,"totaled":12152,"equally":12153,"infusing":12154,"rates":12155,"Expert":12156,"painful":12157,"chiplet":12158,"utf":12159,"Sheng":12160,"mechanical":12161,"ray":12162,"Overlooking":12163,"15880":12164,"coe":12165,"topics":12166,"Relevance":12167,"unsu":12168,"nbextension":12169,"options":12170,"评估":12171,"margins":12172,"Auli":12173,"Lecture":12174,"requisite":12175,"1014":12176,"____________________________________________________":12177,"栏":12178,"Refreshment":12179,"encountered":12180,"MARK":12181,"restrictions":12182,"unambiguously":12183,"someone":12184,"Tolstikhin":12185,"specially":12186,"Follow":12187,"K1":12188,"Locally":12189,"coordination":12190,"Wierstra":12191,"Dean":12192,"information":12193,"Israel":12194,"temperature":12195,"discriminant":12196,"inte":12197,"174":12198,"send":12199,"repeats":12200,"567":12201,"487762":12202,"oversee":12203,"CF":12204,"nondecreasing":12205,"Pointwise":12206,"perturbed":12207,"REQUEST":12208,"Lightning":12209,"Documentation":12210,"CODEAGENT":12211,"cased":12212,"combines":12213,"Lavril":12214,"782":12215,"Cloud":12216,"baselines":12217,"instructs":12218,"sises":12219,"utili":12220,"Exupery":12221,"revolutions":12222,"operations":12223,"inefficient":12224,"Association":12225,"vecs":12226,"Feb":12227,"orthog":12228,"Noise":12229,"Registrant":12230,"commercial":12231,"agreement":12232,"demonstration":12233,"timize":12234,"Nonlinear":12235,"Jeong":12236,"aims":12237,"offsetting":12238,"weakness":12239,"lever":12240,"robotic":12241,"Bergstra":12242,"Quantization":12243,"wellness":12244,"egou":12245,"hires":12246,"experiences":12247,"Limits":12248,"avoids":12249,"kNM":12250,"Discussions260":12251,"recovery":12252,"Query2doc":12253,"Warranty":12254,"led":12255,"nested":12256,"Internet":12257,"MedWriter":12258,"foods":12259,"1.5723":12260,"graduate":12261,"unexpected":12262,"339":12263,"RQ":12264,"Cooperation":12265,"controllable":12266,"Garg":12267,"◦":12268,"inequalities":12269,"speed":12270,"1655":12271,"Database":12272,"Discussions232":12273,"Recaptioning":12274,"296":12275,"433":12276,"Modern":12277,"Operation":12278,"Attending":12279,"cn":12280,"Linkbase":12281,"geom":12282,"404":12283,"announcement":12284,"lw":12285,"delivering":12286,"Denominator":12287,"Ability":12288,"1734":12289,"deserve":12290,"acids":12291,"geometrically":12292,"tially":12293,"Platform":12294,"applying":12295,"3538":12296,"519":12297,"81":12298,"BioNeMo":12299,"Tibshirani":12300,"pools":12301,"DATA":12302,"ranking":12303,"gd":12304,"Oxley":12305,"90th":12306,"concatenating":12307,"consistent":12308,"TSMC":12309,"LKJCholesky":12310,"trick":12311,"consequences":12312,"disrupted":12313,"2529":12314,"undue":12315,"subset":12316,"wide":12317,"animal":12318,"microprocessor":12319,"measurements":12320,"call":12321,"schedules":12322,"Gradient":12323,"NativeBatchNormBackward0":12324,"correlated":12325,"tainty":12326,"Poisson":12327,"formance":12328,"ature":12329,"932":12330,"meanwhile":12331,"slots":12332,"dominated":12333,"likelihoods":12334,"trolVAE":12335,"imagen":12336,"projecting":12337,"frequency":12338,"treme":12339,"Specifications":12340,"Knowledgpt":12341,"Patch":12342,"Settlements":12343,"than":12344,"fetches":12345,"1259":12346,"890":12347,"Closing":12348,"talk":12349,"obj":12350,"SSSR":12351,"clothes":12352,"akin":12353,"collected":12354,"xc":12355,"457":12356,"petition":12357,"mismatch":12358,"shortened":12359,"deter":12360,"Whether":12361,"765":12362,"17081":12363,"searches":12364,"ICCV":12365,"进入":12366,"edit":12367,"original":12368,"GANs":12369,"figuring":12370,"intel":12371,"0318":12372,"crack":12373,"528":12374,"BC":12375,"COVID":12376,"assessed":12377,"angle":12378,"testament":12379,"Xw":12380,"orderings":12381,"distributors":12382,"1987":12383,"SuperPOD":12384,"intractable":12385,"encodings":12386,"9539":12387,"anc":12388,"epsilons":12389,"Csisz":12390,"medicine":12391,"adolescents":12392,"Alibaba":12393,"struct":12394,"Quantized":12395,"Turbo":12396,"supply":12397,"15.10":12398,"broadcasting":12399,"partic":12400,"finetuning":12401,"2004":12402,"WordNet":12403,"minor":12404,"668":12405,"skies":12406,"posteriori":12407,"Parameters":12408,"Utilization":12409,"Ranvier":12410,"WITH":12411,"rela":12412,"payouts":12413,"reasoners":12414,"Death":12415,"385970":12416,"kumaraswamy":12417,"Crawled":12418,"Exculpation":12419,"Gardent":12420,"AUC":12421,"tables":12422,"Convolutional":12423,"resnet34":12424,"GBE":12425,"857":12426,"incorporating":12427,"Access":12428,"escape":12429,"06450":12430,"infinite":12431,"Gordon":12432,"antecedents":12433,"necessarily":12434,"713":12435,"defending":12436,"resemblance":12437,"Tune":12438,"knowing":12439,"algebra":12440,"idx3":12441,"Advancement":12442,"MaoMao":12443,"complish":12444,"LogSoftmax":12445,"MIS":12446,"suspicious":12447,"ATMs":12448,"multimodal":12449,"AnyNetX":12450,"dreaded":12451,"需要":12452,"Recogni":12453,"glish":12454,"charac":12455,"Most":12456,"charms":12457,"gories":12458,"signifi":12459,"backgrounds":12460,"inequality":12461,"Roller":12462,"ignored":12463,"roadmap":12464,"timation":12465,"HPOTuner":12466,"unfortunate":12467,"freedoms":12468,"percentiles":12469,"Book312":12470,"8397":12471,"Quantitative":12472,"behaviors":12473,"HUANG":12474,"hypothesis":12475,"gressBoard":12476,"clich":12477,"197":12478,"Last":12479,"046":12480,"sleeping":12481,"oscillations":12482,"marization":12483,"Evuru":12484,"reflecting":12485,"profit":12486,"dieses":12487,"MLL":12488,"pattern":12489,"execution":12490,"𝐿":12491,"datum":12492,"Vial":12493,"Focal":12494,"earliest":12495,"recommends":12496,"querys":12497,"YaYaB":12498,"reimbursements":12499,"unavoidably":12500,"475":12501,"successively":12502,"EMB":12503,"exclusions":12504,"dynamical":12505,"1809":12506,"differential":12507,"subwindows":12508,"incorporated":12509,"outnumber":12510,"file52":12511,"portfolio":12512,"vexing":12513,"spond":12514,"2223e":12515,"exited":12516,"800":12517,"progressive":12518,"upstream":12519,"ü":12520,"est":12521,"synapses":12522,"ture":12523,"roof":12524,"partitioning":12525,"Barkan":12526,"515":12527,"wonder":12528,"771699":12529,"Ground":12530,"attempt":12531,"matplotlib":12532,"Thought":12533,"cot":12534,"centerpiece":12535,"putes":12536,"offices":12537,"2e":12538,"donations":12539,"numeric":12540,"CPG":12541,"covering":12542,"obscure":12543,"gducharme":12544,"indenture":12545,"documents":12546,"Cadence":12547,"profound":12548,"implementa":12549,"informal":12550,"reached":12551,"Murat":12552,"responsibility":12553,"Softmax":12554,"perdu":12555,"1086":12556,"Saint":12557,"911":12558,"matched":12559,"deals":12560,"06541275978088379":12561,"axioms":12562,"Shakespeare":12563,"3228":12564,"tial":12565,"Bhagavatula":12566,"9.2":12567,"procedures":12568,"3.2":12569,"pecking":12570,"ideas":12571,"patterns":12572,"easiest":12573,"finding":12574,"gridwold":12575,"mat":12576,"precious":12577,"290000":12578,"extraordinarily":12579,"love":12580,"students":12581,"ellipsoid":12582,"objec":12583,"dictive":12584,"rejections":12585,"counterfactual":12586,"327":12587,"reciprocal":12588,"Manhattan":12589,"shared":12590,"flipped":12591,"localhost":12592,"Differentiation":12593,"incorrectly":12594,"202":12595,"Qiao":12596,"6000":12597,"Trigonom":12598,"contested":12599,"Hao":12600,"locally":12601,"deliberate":12602,"initializations":12603,"ops":12604,"1st":12605,"12.8":12606,"14451533555984497":12607,"enforce":12608,"succeeding":12609,"Government":12610,"ematical":12611,"opus":12612,"fixes":12613,"DeepBlue":12614,"754":12615,"0055":12616,"Persis":12617,"A.36":12618,"322":12619,"db":12620,"rediscovered":12621,"dpkg":12622,"Millennium":12623,"xr":12624,"possibilities":12625,"facil":12626,"Pareto":12627,"Resource":12628,"Identifying":12629,"illustrate":12630,"limited":12631,"1048":12632,"estimate":12633,"Kernel":12634,"Walker":12635,"approximates":12636,"Towards":12637,"qualifications":12638,"correspondingly":12639,"ConvED":12640,"𝜸":12641,"promotes":12642,"prepaids":12643,"retaining":12644,"article":12645,"articulated":12646,"10m":12647,"879":12648,"retentive":12649,"Niu":12650,"frontier":12651,"Medal":12652,"Location":12653,"164":12654,"fetching":12655,"screening":12656,"Next":12657,"wear":12658,"fras":12659,"031":12660,"requiring":12661,"Elizabeth":12662,"0974":12663,"A.35":12664,"Cache":12665,"Kryder":12666,"Log":12667,"acquisitions":12668,"aligns":12669,"10D":12670,"46":12671,"wired":12672,"preset":12673,"01307":12674,"127500":12675,"Roberts":12676,"fba480ffa8aa7e0febbb511d181409f899b9baa5":12677,"Bilinear":12678,"Torr":12679,"master":12680,"exact":12681,"prescription":12682,"minds":12683,"ourselves":12684,"011548":12685,"dependent":12686,"Use":12687,"delimiter":12688,"Denker":12689,"creative":12690,"age":12691,"10828":12692,"TOG":12693,"19.3":12694,"Babaei":12695,"7845737934112549":12696,"overstate":12697,"timesteps":12698,"f":12699,"Adversarial":12700,"lots":12701,"amendment":12702,"Parser":12703,"1949":12704,"Hallucinator":12705,"cleanly":12706,"unstructured":12707,"batches":12708,"Betker":12709,"reproducing":12710,"00083":12711,"simulating":12712,"partially":12713,"Highlights":12714,"NRauschmayr":12715,"realizable":12716,"supervisors":12717,"Penedo":12718,"Kepler":12719,"unrelated":12720,"numpy":12721,"extraction":12722,"asserts":12723,"540":12724,"request308":12725,"Go":12726,"noelo":12727,"585":12728,"grossartige":12729,"110":12730,"Connection":12731,"lending":12732,"tin":12733,"outputted":12734,"Engineering":12735,"deployed":12736,"equity":12737,"Subword":12738,"advocate":12739,"Satheesh":12740,"aids":12741,"7924":12742,"2003":12743,"AIS":12744,"05777":12745,"A.39":12746,"flexibilty":12747,"429":12748,"aid":12749,"0825770880068151":12750,"Fall":12751,"1493":12752,"Unsu":12753,"749":12754,"coins":12755,"positive":12756,"monitor":12757,"raytune":12758,"Burgess":12759,"xvii":12760,"actually":12761,"BERTEn":12762,"4570":12763,"CL":12764,"Against":12765,"Store":12766,"COURSERA":12767,"CV":12768,"amortizable":12769,"Shoe":12770,"legislation":12771,"unrecognized":12772,"RMSProp":12773,"Automatic":12774,"Bokeh48":12775,"splitting":12776,"Benefit":12777,"Pattern":12778,"continued":12779,"idio":12780,"sign":12781,"grating":12782,"tainly":12783,"Hybridization":12784,"Encoders":12785,"2980":12786,"forward":12787,"Google":12788,"distances":12789,"1087":12790,"next1":12791,"Embeddings":12792,"attributed":12793,"SPECIFICALLY":12794,"tedious":12795,"expenditure":12796,"Hispanic":12797,"VB":12798,"adjust":12799,"modelers":12800,"Vladimir":12801,"zilliztech":12802,"signif":12803,"Throughout":12804,"678":12805,"audits":12806,"Batching":12807,"scent":12808,"Poor":12809,"traditional":12810,"makedirs":12811,"rung":12812,"nents":12813,"896":12814,"tradi":12815,"representatives":12816,"exploring":12817,"complaint":12818,"exclusion":12819,"Hennessy":12820,"Bishop":12821,"805":12822,"TensorRT":12823,"backends":12824,"discussing":12825,"chat":12826,"domestically":12827,"Light":12828,"saddle":12829,"Norlund":12830,"Corston":12831,"inspired":12832,"wildly":12833,"regardent":12834,"manipulations":12835,"‡":12836,"roadside":12837,"Limited":12838,"572":12839,"EDGE":12840,"774":12841,"OU":12842,"winning":12843,"prepaid":12844,"Direct":12845,"Legendre":12846,"gans":12847,"repetitive":12848,"CE":12849,"Kandpal":12850,"ancestors":12851,"Dhariwal":12852,"bn3":12853,"smoothly":12854,"renaming":12855,"optimally":12856,"Unified":12857,"3.4":12858,"FP32":12859,"tuned":12860,"trajecto":12861,"invoking":12862,"calculated":12863,"Microsystems":12864,"conditional":12865,"58":12866,"chopping":12867,"definition":12868,"boyfriend":12869,"facility":12870,"18678":12871,"Synchronous":12872,"0.126340":12873,"randn":12874,"contributes":12875,"2993874715754653":12876,"commencement":12877,"536":12878,"marginal":12879,"Granted":12880,"KILT":12881,"achieved":12882,"remedies":12883,"Lars":12884,"leaving":12885,"beat":12886,"survivors":12887,"eigendecom":12888,"218990":12889,"jects":12890,"Random":12891,"reinforce":12892,"sudo":12893,"recommend":12894,"NeuroNimes":12895,"weighting":12896,"reversed":12897,"arch":12898,"Indyk":12899,"cancellable":12900,"sliding":12901,"Na":12902,"fragments":12903,"alarms":12904,"addi":12905,"ruled":12906,"fabless":12907,"Homme":12908,"fig":12909,"Adding":12910,"replace":12911,"magnify":12912,"embark":12913,"127":12914,"Lopyrev":12915,"serendipitously":12916,"iterrows":12917,"examined":12918,"sponds":12919,"AS":12920,"stamp":12921,"Slate":12922,"va":12923,"review":12924,"configurations":12925,"Determinants":12926,"authorizes":12927,"、":12928,"ICSE":12929,"elab":12930,"fix":12931,"Phoneme":12932,"leaky":12933,"receive":12934,"762":12935,"executives":12936,"frame":12937,"D":12938,"albeit":12939,"notebook":12940,"cos2":12941,"complications":12942,"9":12943,"Execute":12944,"Konstan":12945,"management":12946,"Joshi":12947,"decays":12948,"st":12949,"introductions":12950,"496":12951,"forfeitures":12952,"符合":12953,"Even":12954,"noticed":12955,"A.48":12956,"Rewards":12957,"parties":12958,"indebted":12959,"Court":12960,"along":12961,"cyberneticists":12962,"sequential":12963,"productive":12964,"celebrated":12965,"Fair":12966,"Symposium":12967,"741":12968,"90":12969,"Worse":12970,"fying":12971,"convolutions":12972,"preference":12973,"Weak":12974,"910":12975,"Asking":12976,"sphere":12977,"Small":12978,"Discussions184":12979,"Feurer":12980,"critical":12981,"fabrication":12982,"务器":12983,"6364":12984,"manifests":12985,"int":12986,"boot":12987,"skier":12988,"192":12989,"greatest":12990,"relying":12991,"time":12992,"successive":12993,"swamped":12994,"continuously":12995,"uses":12996,"concluded":12997,"race":12998,"Hello":12999,"Upload":13000,"2305":13001,"runaway":13002,"Well":13003,"solves":13004,"025582":13005,"trivially":13006,"alike":13007,"content":13008,"Krizhevsky":13009,"037703":13010,"yellow":13011,"Nguyen":13012,"576":13013,"Asynchronous":13014,"predetermined":13015,"approx":13016,"SIBAS":13017,"telling":13018,"attain":13019,"chairs":13020,"amplify":13021,"Overfitting":13022,"Cost":13023,"padding":13024,"nrows":13025,"significance":13026,"259171":13027,"interpolate":13028,"whip":13029,"请勿":13030,"analysis":13031,"milestones":13032,"widely":13033,"2204":13034,"1000s":13035,"Tulder":13036,"Bulatov":13037,"hole":13038,"partners":13039,"founding":13040,"completing":13041,"TD":13042,"ef":13043,"492":13044,"AAL":13045,"Flow":13046,"reliably":13047,"ties":13048,"Xiaoting":13049,"pledge":13050,"formulations":13051,"herein":13052,"400":13053,"Update":13054,"terminology":13055,"linalg":13056,"2696":13057,"Big":13058,"bn":13059,"boxes1":13060,"personalized":13061,"NN":13062,"online3":13063,"0.245":13064,"particular":13065,"Raina":13066,"picking":13067,"Embedding":13068,"impulse":13069,"0.9713":13070,"561":13071,"Discussions132":13072,"强烈":13073,"outpace":13074,"RichyChen":13075,"experiment":13076,"denoting":13077,"hyperbolic":13078,"showcased":13079,"abstraction":13080,"standpoint":13081,"Crop":13082,"Absolute":13083,"your":13084,"one":13085,"244%":13086,"acc":13087,"Let":13088,"democratized":13089,"Simplicity":13090,"1500":13091,"Salesforce":13092,"Statisticians":13093,"largest":13094,"60s":13095,"Layer":13096,"4073":13097,"cuDNN":13098,"Anatomy":13099,"ReAct":13100,"Increasing":13101,"Sora":13102,"feature":13103,"providing":13104,"r2":13105,"traverses":13106,"DIGITAL":13107,"dinosaur":13108,"Hong":13109,"seq":13110,"centered":13111,"Reinforcing":13112,"applicable":13113,"16kHz":13114,"479":13115,"Regarding":13116,"contribution":13117,"Only":13118,"anchors":13119,"covar":13120,"ex":13121,"witnessing":13122,"guided":13123,"Tchebycheffian":13124,"9.7":13125,"synthesized":13126,"sci":13127,"KB":13128,"BASHEXPLAINER":13129,"surgical":13130,"indicator":13131,"binary":13132,"XPENG":13133,"diverge":13134,"488":13135,"01068":13136,"3097":13137,"Blues":13138,"RefinedWeb":13139,"regulations":13140,"Nichols":13141,"reserves":13142,"Coherent":13143,"Custom":13144,"Between":13145,"AdaGrad":13146,"dialogue":13147,"mes":13148,"Whf":13149,"dence":13150,"Fried":13151,"Circuit":13152,"putationally":13153,"These":13154,"linear":13155,"2818":13156,"Mellanox":13157,"subsets":13158,"Trial":13159,"common":13160,"Assume":13161,"Modification":13162,"foundry":13163,"0.99":13164,"spect":13165,"0.007188":13166,"940":13167,"ati":13168,"Sohl":13169,"yogi":13170,"Risk":13171,"deeply":13172,"mess":13173,"07554":13174,"Wxr":13175,"avoided":13176,"pioneer":13177,"Codeagent":13178,"erf":13179,"emergent":13180,"focuses":13181,"KL":13182,"bers":13183,"novations":13184,"bucket":13185,"Pooling":13186,"forgotten":13187,"Massachusetts":13188,"improves":13189,"IV":13190,"PCs":13191,"Formally":13192,"RE":13193,"releasing":13194,"8226":13195,"push":13196,"3167":13197,"4":13198,"1028":13199,"Bansal":13200,"RTL":13201,"1502":13202,"harmed":13203,"358":13204,"SGD":13205,"payment":13206,"covariates":13207,"homebuilding":13208,"varies":13209,"3072e":13210,"Byte":13211,"4612":13212,"0.005":13213,"Recognition":13214,"Treasury":13215,"familiar":13216,"Request":13217,"surfaced":13218,"placement":13219,"00012":13220,"equa":13221,"losses":13222,"107405":13223,"Running":13224,"noted":13225,"underestimates":13226,"Rangan":13227,"Thudumu":13228,"subplot":13229,"premiums":13230,"Basis":13231,"Actions":13232,"risk":13233,"administratively":13234,"FASB":13235,"05377":13236,"Dinh":13237,"embs":13238,"tanh2":13239,"Cour":13240,"refined":13241,"snap":13242,"prefix":13243,"ﬁ":13244,"genes":13245,"predicted":13246,"accurately":13247,"cheap":13248,"3367":13249,"train":13250,"这是":13251,"justice":13252,"ids":13253,"pwepoiut":13254,"3212":13255,"regressor":13256,"myserver":13257,"subtly":13258,"Shahriar":13259,"tag":13260,"Recalling":13261,"domain":13262,"Saffari":13263,"Bear":13264,"Wagenmakers":13265,"flux":13266,"258":13267,"hereto":13268,"dived":13269,"sugges":13270,"Prakash":13271,"expand":13272,"aspect":13273,"Alas":13274,"Several":13275,"Businesses":13276,"2201":13277,"Faithfulness":13278,"creased":13279,"GPTCache":13280,"erage":13281,"claim":13282,"whatever":13283,"Cauchy":13284,"warps":13285,"distortions":13286,"tive":13287,"Evaluation":13288,"interruption":13289,"Seasonality":13290,"bells":13291,"leaderboard":13292,"liquid":13293,"FURNISHED":13294,"competitive":13295,"looping":13296,"five":13297,"explo":13298,"functioning":13299,"Elec":13300,"DRUG":13301,"Doersch":13302,"analysts":13303,"nsp":13304,"Finally":13305,"Teoriya":13306,"trans":13307,"1773":13308,"compliance":13309,"18170":13310,"Birch":13311,"Scao":13312,"promote":13313,"multiprocessor":13314,"019823425532533637":13315,"stimuli":13316,"kilometer":13317,"blk":13318,"Pang":13319,"Exposed":13320,"summarize":13321,"Mathematics":13322,"metadata":13323,"Suppose":13324,"181600":13325,"labeling":13326,"culus":13327,"distinctions":13328,"been":13329,"prisingly":13330,"repositories":13331,"textual":13332,"ates":13333,"Native":13334,"PTBDataset":13335,"lowercased":13336,"hyperparams":13337,"Guiding":13338,"broken":13339,"compress":13340,"form":13341,"really":13342,"咖啡豆":13343,"128":13344,"criticism":13345,"ImageSets":13346,"ethnically":13347,"theme":13348,"294":13349,"somebody":13350,"xxx":13351,"用于":13352,"differentiating":13353,"Grant":13354,"A.22":13355,"perceptual":13356,"vigilance":13357,"tropy":13358,"Vogels":13359,"1960":13360,"02155":13361,"Reid":13362,"721":13363,"2468891379769198":13364,"arrowstyle":13365,"Rq":13366,"expressions":13367,"transmission":13368,"DISTANCES":13369,"Green500":13370,"Preprocess":13371,"candidates":13372,"remediate":13373,"Engineers":13374,"Demonstrate":13375,"ling":13376,"Reimplement":13377,"entists":13378,"spot":13379,"Value":13380,"hyper":13381,"coarse":13382,"premium":13383,"begin":13384,"tangent":13385,"coaching":13386,"Experiment":13387,"plurality":13388,"718":13389,"corrupt":13390,"lying":13391,"15.5":13392,"Members":13393,"petitive":13394,"Yfcc100m":13395,"Components":13396,"Optional":13397,"Operating":13398,"GMT":13399,"Qui":13400,"7739":13401,"过程":13402,"Multihop":13403,"Cedric":13404,"indi":13405,"valuation":13406,"2312":13407,"sistent":13408,"captured":13409,"SIGKDD":13410,"Under":13411,"underscored":13412,"4097":13413,"run":13414,"retired":13415,"图片":13416,"cond":13417,"deviate":13418,"cooperative":13419,"newton":13420,"embracing":13421,"Mesbah":13422,"ConvTranspose2d":13423,"385":13424,"服务":13425,"Docprompting":13426,"330":13427,"periods":13428,"unapproved":13429,"attended":13430,"faltering":13431,"difference":13432,"MDFs":13433,"Biomedical":13434,"happy":13435,"2663":13436,"special":13437,"builds":13438,"Sebag":13439,"Args":13440,"Northwestern":13441,"Mitro":13442,"924":13443,"subsampling":13444,"Amazon":13445,"published":13446,"desc":13447,"argument":13448,"723":13449,"Sarwar":13450,"Specimen":13451,"box":13452,"BSharmi":13453,"empowers":13454,"Repository50":13455,"Writing":13456,"Noises":13457,"2022":13458,"816":13459,"reusing":13460,"rigid":13461,"BFLOAT16":13462,"eugenics":13463,"supplied":13464,"exhibited":13465,"memorizes":13466,"break":13467,"bool":13468,"preter":13469,"evaluation":13470,"544":13471,"turnover":13472,"perceive":13473,"BSEE":13474,"partitioned":13475,"instantiates":13476,"cessing":13477,"Contradiction":13478,"Nonparametrics":13479,"Artificial":13480,"11278":13481,"tent":13482,"P3dn":13483,"represented":13484,"objectively":13485,"altogether":13486,"instabilities":13487,"inserting":13488,"alternative":13489,"prog":13490,"531":13491,"cause":13492,"4795":13493,"issued":13494,"annualized":13495,"7th":13496,"Teaching":13497,"0.0094":13498,"quickly":13499,"Pigs":13500,"608":13501,"𝑗":13502,"Now":13503,"arrived":13504,"Alias":13505,"tens":13506,"classifier":13507,"their":13508,"quo":13509,"its":13510,"meanings":13511,"1114":13512,"Assumption":13513,"cd43bfb07e44e6f27cbcc7bc9ae3d80284fdaf5a":13514,"commencing":13515,"Debora":13516,"dictates":13517,"blissfully":13518,"𝑙":13519,"Medical":13520,"unreasonable":13521,"cartoonish":13522,"positioned":13523,"DistBelief":13524,"Vert":13525,"Polosukhin":13526,"tences":13527,"indefinite":13528,"Discussions95":13529,"currency":13530,"Start":13531,"reserve":13532,"Playing":13533,"Rectified":13534,"dashed":13535,"CoV":13536,"Purpose":13537,"Quality":13538,"ASE":13539,"air":13540,"express":13541,"Hyperparameters":13542,"7446":13543,"0144":13544,"connected":13545,"10.7":13546,"threads":13547,"fontsize":13548,"Same":13549,"handily":13550,"travelly":13551,"RepoCoder":13552,"rigor":13553,"wzr":13554,"If":13555,"TIME":13556,"Vetrov":13557,"1420":13558,"resembling":13559,"tory":13560,"Kang":13561,"Normalizing":13562,"officers":13563,"07331":13564,"MLM":13565,"5785":13566,"747496":13567,"Decreasing":13568,"noteworthy":13569,"programs":13570,"1301":13571,"Customer":13572,"aside":13573,"neither":13574,"Hubel":13575,"prepared":13576,"Tuli":13577,"hybridization":13578,"Perform":13579,"inverses":13580,"Opportunity":13581,"analyses":13582,"possess":13583,"retrieving":13584,"localization":13585,"SEAWELL":13586,"37716809.52":13587,"Elastic":13588,"ContinuousBernoulli":13589,"总结":13590,"far":13591,"extent":13592,"Turk":13593,"Glu":13594,"alto":13595,"Airflow":13596,"u1":13597,"lawyer":13598,"prehending":13599,"Southeast":13600,"rtel":13601,"interpolated":13602,"federal":13603,"Matsuo":13604,"experi":13605,"4x":13606,"03098":13607,"Library":13608,"investing":13609,"5035":13610,"remedy":13611,"09519":13612,"NER":13613,"Kubicek":13614,"ern":13615,"pioneered":13616,"s0":13617,"accidental":13618,"fork":13619,"Contents":13620,"industry":13621,"rel":13622,"instances":13623,"publisher":13624,"09085":13625,"0.7359":13626,"reshuffle":13627,"4105":13628,"876":13629,"million":13630,"legislative":13631,"breakthroughs":13632,"Election":13633,"Walter":13634,"odd":13635,"morally":13636,"GFN":13637,"Whh":13638,"1922":13639,"abnormal":13640,"Hanazawa":13641,"GHG":13642,"allocates":13643,"Medium":13644,"higher":13645,"derstand":13646,"Describe":13647,"president":13648,"bugs":13649,"849":13650,"Intel":13651,"6293":13652,"various":13653,"M":13654,"succinctness":13655,"twins":13656,"roll":13657,"ramp":13658,"slowly":13659,"layered":13660,"delightful":13661,"Re2g":13662,"CoBaFi":13663,"activities":13664,"stacks":13665,"Pixel":13666,"ZeroMean":13667,"unconstrained":13668,"Latinx":13669,"359":13670,"0822":13671,"installed":13672,"208":13673,"trieved":13674,"Volta":13675,"1093":13676,"900263":13677,"Retrack":13678,"le":13679,"interleave":13680,"submission":13681,"873":13682,"nite":13683,"coupled":13684,"笔记":13685,"𝑧":13686,"722":13687,"beginning":13688,"attract":13689,"fuss":13690,"Intervalrank":13691,"967":13692,"Humans":13693,"Discussions289":13694,"Mirza":13695,"awarded":13696,"categorize":13697,"owner":13698,"Vijayvargeeya":13699,"P":13700,"UNITED":13701,"TPUs":13702,"sh":13703,"notation":13704,"shortest":13705,"OpenUSD":13706,"exceeding":13707,"CLASSES":13708,"900":13709,"nAI":13710,"Tracking":13711,"prepa":13712,"ditches":13713,"Lepikhin":13714,"authentication":13715,"9824":13716,"exception":13717,"705":13718,"Register":13719,"Centennial":13720,"rization":13721,"afterwards":13722,"Ramachandran":13723,"Procedures":13724,"Mathematical":13725,"format":13726,"产品名称":13727,"BERTModel":13728,"routine":13729,"decomposed":13730,"Prior":13731,"gence":13732,"15084":13733,"dead":13734,"discussion":13735,"1C":13736,"om":13737,"handwriting":13738,"Neural":13739,"∋":13740,"1069":13741,"Snoek":13742,"lh":13743,"Disc":13744,"locations":13745,"captures":13746,"2639":13747,"post":13748,"plying":13749,"586":13750,"patrol":13751,"graphics":13752,"looked":13753,"corre":13754,"biomedicine":13755,"AVGO":13756,"sun":13757,"methodologies":13758,"Intelligence":13759,"OF":13760,"blurred":13761,"irreducible":13762,"cys4":13763,"collaboratively":13764,"Publicly":13765,"Saxena":13766,"happen":13767,"ankle":13768,"lenders":13769,"Gaming":13770,"×":13771,"NLP":13772,"minimized":13773,"41":13774,"smoothness":13775,"iid":13776,"trailing":13777,"Street":13778,"Fused":13779,"designees":13780,"COLORMAP":13781,"defines":13782,"conspicuously":13783,"pth":13784,"RoofType":13785,"minimize":13786,"Restart":13787,"Bengio":13788,"Incorporating":13789,"729115":13790,"inadvisable":13791,"singular":13792,"Strategies":13793,"RelaxedOneHotCategorical":13794,"SKI":13795,"drains":13796,"packing":13797,"satis":13798,"selected":13799,"mate":13800,"burdensome":13801,"materially":13802,"Consistent":13803,"permanent":13804,"sixth":13805,"9197404791177789":13806,"--------------------":13807,"Levin":13808,"621":13809,"trigrams":13810,"involve":13811,"GDDR6":13812,"Nothing":13813,"lifecycle":13814,"12GB":13815,"forecasted":13816,"Adjustments":13817,"installers":13818,"discriminated":13819,"Komeili":13820,"attending":13821,"haps":13822,"residents":13823,"­":13824,"Ben":13825,"Estimating":13826,"411":13827,"1877":13828,"7313":13829,"948":13830,"confidently":13831,"BiRNNScratch":13832,"Option":13833,"composed":13834,"Detail":13835,"sen":13836,"nitty":13837,"Jung":13838,"844":13839,"retrospectively":13840,"Filing":13841,"f37072fd":13842,"those":13843,"discover":13844,"Exist":13845,"Models":13846,"attached":13847,"835":13848,"indexing51":13849,"Will":13850,"twin":13851,"trails":13852,"yangling0818":13853,"Multiple":13854,"consents":13855,"financing":13856,"connec":13857,"python":13858,"separated":13859,"finalization":13860,"137":13861,"Epyc":13862,"Beaumont":13863,"McGraw":13864,"Krema":13865,"c4":13866,"produc":13867,"173":13868,"ELECTRA":13869,"Labels":13870,"Reports":13871,"ple":13872,"practicality":13873,"289":13874,"Scene":13875,"Relationships":13876,"justify":13877,"Findings":13878,"Unilog":13879,"reason":13880,"pyplot":13881,"tations":13882,"657":13883,"Warmup":13884,"597":13885,"512MB":13886,"designing":13887,"rowwise":13888,"nization":13889,"Ram":13890,"Purchases":13891,"77":13892,"many":13893,"Pokemon":13894,"7802":13895,"given":13896,"commercially":13897,"noise":13898,"1600":13899,"ahmaurya":13900,"case":13901,"processor":13902,"Ungar":13903,"Tableau":13904,"343":13905,"depend":13906,"Overall":13907,"District":13908,"Minerva":13909,"Yankee":13910,"bias":13911,"nY":13912,"epoch":13913,"7606":13914,"archi":13915,"simply":13916,"Res":13917,"propel":13918,"Generalization":13919,"SQL":13920,"three":13921,"0.1022":13922,"revenues":13923,";":13924,"sequentiality":13925,"Edit":13926,"crowdsourcing":13927,"settlement":13928,"winter":13929,"Owner":13930,"agent":13931,"distributing":13932,"ping":13933,"pointed":13934,"flip":13935,"opened":13936,"constructors":13937,"undefined":13938,"hyperspectral":13939,"position":13940,"ships":13941,"mapped":13942,"3555":13943,"Discussions301":13944,"robots":13945,"doctor":13946,"5701":13947,"9734":13948,"Trans":13949,"gopher":13950,"corresponds":13951,"shaped":13952,"Cumulative":13953,"correspondence":13954,"facts":13955,"backpropagating":13956,"sponse":13957,"2996":13958,"Roots":13959,"nervous":13960,"chronized":13961,"TR":13962,"linked":13963,"scheduled":13964,"𝑆":13965,"quite":13966,"datapoints":13967,"Fumero":13968,"NVP":13969,"Treatment":13970,"03545":13971,"attainable":13972,"stays":13973,"183":13974,"752":13975,"需求":13976,"Bello":13977,"CHARGE":13978,"automating":13979,"postulated":13980,"intake":13981,"finetune":13982,"thir":13983,"cautious":13984,"synergy":13985,"ES":13986,"predeter":13987,"56%":13988,"TextDataset":13989,"0.861":13990,"equality":13991,"30%":13992,"Capital":13993,"Majority":13994,"io":13995,"Mhanics":13996,"skilled":13997,"589":13998,"emp":13999,"Rescale":14000,"242":14001,"fear":14002,"national":14003,"unfolds":14004,"beauty":14005,"shortens":14006,"illegal":14007,"sale":14008,"iciently":14009,"ments":14010,"Leaky":14011,"dibs":14012,"meteorology":14013,"753":14014,"Joji":14015,"Detected":14016,"sudden":14017,"1045":14018,"debate":14019,"surprisal":14020,"selector":14021,"autocorrelation":14022,"Valgimigli":14023,"deterioration":14024,"900002":14025,"LDMs":14026,"IP":14027,"sion":14028,"顺序":14029,"twelve":14030,"dilutive":14031,"texture":14032,"Principal":14033,"758829":14034,"the":14035,"regulator":14036,"spectro":14037,"aggregations":14038,"Discussions247":14039,"realizations":14040,"ative":14041,"Knowledge":14042,"terminals":14043,"decomposition":14044,"participants":14045,"2404":14046,"1":14047,"principled":14048,"listdir":14049,"suited":14050,"Breaking":14051,"𝑄":14052,"Dong":14053,"approxima":14054,"解析":14055,"Zha":14056,"Concise":14057,"1.2412":14058,"Complications":14059,"基础":14060,"unsatisfactory":14061,"sampling":14062,"017898854850751864":14063,"advantage":14064,"1610":14065,"Coordinates":14066,"paraphrase":14067,"2832":14068,"integrations":14069,"Opti":14070,"underlie":14071,"Horne":14072,"88":14073,"traditionally":14074,"06434":14075,"fluctuation":14076,"periodic":14077,"Superchip":14078,"are":14079,"poly":14080,"3361":14081,"Terre":14082,"talks":14083,"08209":14084,"lkopf":14085,"tions":14086,"prose":14087,"\u001a":14088,"off":14089,"commitments":14090,"trading":14091,"Issuable":14092,"term":14093,"XML":14094,"risks":14095,"reforms":14096,"7587":14097,"124003":14098,"Song":14099,"Hoffmann":14100,"":14101,"pieces":14102,"Ahmad":14103,"520":14104,"Organization":14105,"MixtureSameFamily":14106,"Gates":14107,"Account":14108,"Agarwal":14109,"20th":14110,"atop":14111,"anticipating":14112,"Rich":14113,"pondering":14114,"Lagrange":14115,"ab":14116,"outset":14117,"ated":14118,"SHAH":14119,"1694":14120,"customer":14121,"792":14122,"krahets":14123,"extension":14124,"employment":14125,"substitution":14126,"Transposition":14127,"appling":14128,"phylogenetic":14129,"clueless":14130,"singularity":14131,"1278":14132,"pred":14133,"Normally":14134,"unrolled":14135,"Extending":14136,"Singh":14137,"steam":14138,"CAC":14139,"makers":14140,"66%":14141,"triplet":14142,"regardless":14143,"sha1":14144,"Lin":14145,"fifo":14146,"Ho":14147,"√":14148,"diagonals":14149,"periment":14150,"inferred":14151,"Terry":14152,"localities":14153,"multiplying":14154,"4322":14155,"400000":14156,"Solid":14157,"Visin":14158,"balmy":14159,"尤其":14160,"closets":14161,"SSD":14162,"plicate":14163,"action2dxdy":14164,"Modules":14165,"fname":14166,"QCOM":14167,"2107":14168,"388":14169,"breathless":14170,"trends":14171,"so":14172,"SEC":14173,"disability":14174,"9A":14175,"Semerci":14176,"sustainable":14177,"1001":14178,"BERT":14179,"Taxonomy":14180,"Tokyo":14181,"Myelin":14182,"awards":14183,"State":14184,"PoS":14185,"quota":14186,"merically":14187,"Arx":14188,"Qinhan":14189,"foremost":14190,"unsuccessful":14191,"tgt":14192,"eager":14193,"pioneers":14194,"multiply":14195,"Polestar":14196,"shadows":14197,"survived":14198,"Definitions":14199,"多行":14200,"Generators":14201,"Excludes":14202,"choice":14203,"4724884":14204,"generalization":14205,"individuals":14206,"Norouzi":14207,"GRUScratch":14208,"Array":14209,"019":14210,"merges":14211,"plot":14212,"datastore":14213,"pal":14214,"bridge":14215,"equiva":14216,"Welling":14217,"flattens":14218,"forever":14219,"08812857364095393":14220,"392":14221,"counter":14222,"Ideally":14223,"post188":14224,"copies":14225,"Useful":14226,"Vishaal":14227,"distri":14228,"originates":14229,"Augmenting":14230,"RISK":14231,"simulation":14232,"Hajishirzi":14233,"Delinquent":14234,"practicing":14235,"insur":14236,"Includes":14237,"1019":14238,"YOLOv3":14239,"HPOSearcher":14240,"Oktay":14241,"Lefan":14242,"memoizer":14243,"Kim":14244,"318":14245,"continues":14246,"tice":14247,"D5":14248,"urel":14249,"115":14250,"bin":14251,"rected":14252,"Presumably":14253,"downgrade":14254,"briefly":14255,"1044":14256,"noncompliance":14257,"regularizer":14258,"Anke":14259,"hurts":14260,"indepen":14261,"inventory":14262,"Engine":14263,"Revenue":14264,"Additive":14265,"Roossin":14266,"wife":14267,"ﬃ":14268,"spectacularly":14269,"Ermon":14270,"soul":14271,"Feldman":14272,"1606":14273,"shorter":14274,"ResNet18":14275,"Shikano":14276,"Variance":14277,"solute":14278,"Worked":14279,"voc":14280,"gramming":14281,"taken":14282,"407":14283,"skins":14284,"Combination":14285,"deem":14286,"probabilistically":14287,"subtraction":14288,"indication":14289,"covs":14290,"text":14291,"1145":14292,"thereby":14293,"Integrals":14294,"tural":14295,"Avvaru":14296,"subgraphs":14297,"niche":14298,"Tailors":14299,"interchange":14300,"skewed":14301,"notions":14302,"Modify":14303,"Formal":14304,"overweighted":14305,"Ran":14306,"†":14307,"p":14308,"ALBERT":14309,"bert":14310,"Building":14311,"restric":14312,"1607":14313,"something":14314,"Loss":14315,"Link":14316,"Convex":14317,"seman":14318,"1.0957":14319,"innovate":14320,"active":14321,"Wolpert":14322,"feline":14323,"Exhaustive":14324,"09699":14325,"Reconciliation":14326,"criteria":14327,"Unrealized":14328,"Emeritus":14329,"Schuurmans":14330,"warning":14331,"WordPiece":14332,"bylaw":14333,"A.28":14334,"mouill":14335,"Length":14336,"denoted":14337,"filepath":14338,"870":14339,"Consolidation":14340,"imitating":14341,"Aila":14342,"too":14343,"buying":14344,"reap":14345,"Tukey":14346,"Characterizing":14347,"unnecessary":14348,"SigmoidBCELoss":14349,"Exequity":14350,"senior":14351,"875":14352,"nonpublic":14353,"counterparties":14354,"8516":14355,"0668":14356,"type":14357,"seven":14358,"conv2":14359,"Emmanouilidou":14360,"Arm":14361,"workable":14362,"SH":14363,"terrorist":14364,"preventing":14365,"electrons":14366,"Measures":14367,"alongside":14368,"rooster":14369,"498":14370,"cloning":14371,"communication":14372,"he":14373,"publication":14374,"":14375,"Crandall":14376,"GMD":14377,"Notebook":14378,"custom":14379,"05310":14380,"Pennsylvania":14381,"nonnegative":14382,"convincing":14383,"decoupling":14384,"01110":14385,"Reranking":14386,"InfiniBand":14387,"matrices":14388,"´":14389,"2306":14390,"handle":14391,"counterpart":14392,"especially":14393,"Variables":14394,"advertisers":14395,"lib64":14396,"mae":14397,"outlined":14398,"bottom":14399,"Amendment":14400,"Assigning":14401,"1070":14402,"places":14403,"683":14404,"surround":14405,"permutation":14406,"distribute":14407,"Anecdotes":14408,"leaders":14409,"accessible":14410,"reuses":14411,"13.3":14412,"rameter":14413,"buildings":14414,"NishantTharani":14415,"GELUs":14416,"helped":14417,"recession":14418,"Loading":14419,"Discussions129":14420,"imposing":14421,"4109":14422,"applicant":14423,"Chess":14424,"confidentially":14425,"thin":14426,"Unused":14427,"Azizpour":14428,"Comments":14429,"Visualgpt":14430,"Cell":14431,"balls":14432,"Ph":14433,"moment":14434,"video":14435,"bd":14436,"04615":14437,"台":14438,"seed":14439,"suffice":14440,"kl":14441,"Josh":14442,"spammers":14443,"EaE":14444,"Process":14445,"r":14446,"boy":14447,"respects":14448,"dtype":14449,"trave":14450,"744":14451,"stor":14452,"651":14453,"Mean":14454,"majorizes":14455,"495":14456,"Code4UIE":14457,"practice":14458,"wishart":14459,"Ap":14460,"icantly":14461,"non":14462,"nodes":14463,"Net":14464,"obligation":14465,"aspects":14466,"expensive":14467,"overlap":14468,"facilities":14469,"injection":14470,"found":14471,"bypassing":14472,"Narasimhan":14473,"leaked":14474,"Eratta":14475,"545":14476,"deteriorate":14477,"898":14478,"bicycle":14479,"替代":14480,"envisioned":14481,"women":14482,"innocuous":14483,"Bremen":14484,"prune":14485,"0.2890":14486,"consisting":14487,"THE":14488,"8203370335228594":14489,"Alemi":14490,"CIFAR":14491,"dissatisfaction":14492,"%-":14493,"interests":14494,"best":14495,"opera":14496,"销售":14497,"instruc":14498,"elide":14499,"job":14500,"ref":14501,"inherently":14502,"HPOScheduler":14503,"shedding":14504,"05799":14505,"uniques":14506,"经由":14507,"0.1124":14508,"Pratik":14509,"69%":14510,"phase":14511,"forget":14512,"PAD":14513,"suit":14514,"6300":14515,"Distill":14516,"Transactions":14517,"determinable":14518,"009999999":14519,"rerank":14520,"backgammon":14521,"disks":14522,"nice":14523,"fall":14524,"Rethinking":14525,"recover":14526,"implemen":14527,"contributors":14528,"Inception":14529,"strip":14530,"keen":14531,"01722":14532,"disregarding":14533,"baseline":14534,"Aeronautics":14535,"duties":14536,"Credible":14537,"Accountant":14538,"virtuous":14539,"126%":14540,"Mirhoseini":14541,"priced":14542,"Janzing":14543,"convnet127":14544,"Neighbor":14545,"Silver":14546,"Pascanu":14547,"defini":14548,"hence":14549,"1079":14550,"GD":14551,"excessively":14552,"Animate":14553,"Cooley":14554,"Look":14555,"frequentist":14556,"hishek":14557,"≤":14558,"cos":14559,"planet":14560,"CEG":14561,"Numerator":14562,"putting":14563,"extravagant":14564,"lowest":14565,"optim":14566,"predates":14567,"Via":14568,"©":14569,"ratification":14570,"horizontally":14571,"Ordered":14572,"Standard":14573,"AARTI":14574,"Orientation":14575,"1009":14576,"region":14577,"Starting":14578,"mary":14579,"inject":14580,"window":14581,"bly":14582,"town":14583,"ratified":14584,"pq1":14585,"GF":14586,"S3510":14587,"Discussions277":14588,"lenlabel":14589,"0058":14590,"hybrid":14591,"Jamaoui":14592,"Gomez":14593,"Commerce":14594,"proach":14595,"679":14596,"Bollob":14597,"essentially":14598,"wish":14599,"confident":14600,"underweighted":14601,"TXN":14602,"246":14603,"Ambarella":14604,"270000":14605,"ially":14606,"TRANSITION":14607,"d2lbook":14608,"3354":14609,"et":14610,"878":14611,"H":14612,"Probabilities":14613,"06302":14614,"Exhibit":14615,"1971":14616,"stands":14617,"Matern":14618,"Re2Com":14619,"2261":14620,"transposition":14621,"startswith":14622,".":14623,"201":14624,"sentient":14625,"similar":14626,"Callan":14627,"463":14628,"28":14629,"Roychowdhury":14630,"magic":14631,"machin":14632,"Risks":14633,"stream":14634,"lockdowns":14635,"disjoint":14636,"unintended":14637,"OneHotCategoricalStraightThrough":14638,"overfitting":14639,"gener":14640,"05250":14641,"2109":14642,"impatient":14643,"priori":14644,"":14645,"1100":14646,"curious":14647,"Representative":14648,"equivalently":14649,"1047":14650,"tagging":14651,"trix":14652,"dateset":14653,"𝜶":14654,"spoken":14655,"built":14656,"1090":14657,"12890":14658,"greatly":14659,"Udomcharoenchaikit":14660,"freeze":14661,"8026":14662,"AnyNetB":14663,"Created":14664,"everything":14665,"tensor":14666,"damages":14667,"1096":14668,"bester":14669,"delineate":14670,"astrophysics":14671,"yield":14672,"4465":14673,"locate":14674,"enormous":14675,"cadence":14676,"creation":14677,"primer":14678,"Community":14679,"647":14680,"augments":14681,"sense":14682,"Swin":14683,"Coins":14684,"elected":14685,"3490":14686,"includ":14687,"tungsten":14688,"approve":14689,"ReACT":14690,"prepares":14691,"erased":14692,"ON":14693,"Cyber":14694,"decoupled":14695,"Func":14696,"10012":14697,"ff7f0e":14698,"5938":14699,"Flows":14700,"unlimited":14701,"904":14702,"tokens":14703,"condition":14704,"Torralba":14705,"10007":14706,"comfort":14707,"benign":14708,"DRAM":14709,"softly":14710,"Policy":14711,"compute":14712,"diligence":14713,"chatbot":14714,"capable":14715,"squashes":14716,"purchase":14717,"Bresson":14718,"11.4":14719,"Irene":14720,"artifact":14721,"669":14722,"More":14723,"3929":14724,"tempting":14725,"11942":14726,"256245":14727,"skin":14728,"4785":14729,"plotting":14730,"demanding":14731,"FISCAL":14732,"Audit":14733,"moniker":14734,"↦":14735,"July":14736,"1b2":14737,"SAIC":14738,"stanford":14739,"142847":14740,"Devanbu":14741,"doorstep":14742,"hereby":14743,"Fold":14744,"eternal":14745,"Integrity":14746,"rethinking":14747,"https":14748,"statutes":14749,"pays":14750,"moments":14751,"savings":14752,"Within":14753,"administering":14754,"Repofuse":14755,"474":14756,"float32":14757,"MultiHop":14758,"Netherlands":14759,"Likun":14760,"3380":14761,"combatting":14762,"advisers":14763,"080":14764,"still":14765,"Bandwidth":14766,"validated":14767,"5904067586747807":14768,"smi":14769,"distribution":14770,"288%":14771,"Chandrasekaran":14772,"Meeting":14773,"232":14774,"mlm":14775,"Pillow53":14776,"Diskann":14777,"wishing":14778,"kilometers":14779,"according":14780,"pig":14781,"ambient":14782,"Zoph":14783,"hereof":14784,"𝑉":14785,"mlp":14786,"pareto":14787,"Randomized":14788,"902":14789,"Decade":14790,"quantitatively":14791,"urges":14792,"seen":14793,"2.6190":14794,"yaxis":14795,"842":14796,"cores":14797,"promoting":14798,"equivariance":14799,"07669":14800,"detailing":14801,"circuit":14802,"informa":14803,"Implement":14804,"normalization":14805,"ndarray":14806,"logically":14807,"GAC":14808,"Note":14809,"2500":14810,"Priors":14811,"Time":14812,"Senior":14813,"Stretch":14814,"Beethoven":14815,"Riesenhuber":14816,"Extractive":14817,"Situations":14818,"improvement":14819,"Adaptive":14820,"equipment":14821,"inventions":14822,"subqueries":14823,"Correlation":14824,"Describing":14825,"recommended":14826,"stakes":14827,"Sutawika":14828,"Unlike":14829,"Scheduling":14830,"Handbook":14831,"softmax":14832,"gov":14833,"MURRE":14834,"Michalewski":14835,"stronger":14836,"Informatica":14837,"Raffel":14838,"956":14839,"Mahdi":14840,"0.78":14841,"Flatten":14842,"Discussions153":14843,"Matthew":14844,"stride":14845,"Syntax":14846,"Krause":14847,"relevance":14848,"Hofst":14849,"comprised":14850,"Discussions99":14851,"681%":14852,"ken":14853,"W2":14854,"Discussions138":14855,"embed":14856,"weighted":14857,"College":14858,"elect":14859,"appears":14860,"impediment":14861,"SageMaker":14862,"deploy":14863,"Cherti":14864,"94":14865,"current":14866,"PetaFLOP":14867,"looking":14868,"fluid":14869,"Motivating":14870,"Fe":14871,"h":14872,"xa0":14873,"ethnicities":14874,"connectionist":14875,"computations":14876,"Mani":14877,"grander":14878,"Installing":14879,"perform":14880,"xo":14881,"nation":14882,"downsides":14883,"revolutionizing":14884,"collect":14885,"progressing":14886,"Online":14887,"820337":14888,"Blunsom":14889,"1970":14890,"addnorm3":14891,"disclosing":14892,"0th":14893,"Gimpel":14894,"inited":14895,"Constraining":14896,"feedback":14897,"3D":14898,"minimiz":14899,"ptimization":14900,"impact":14901,"grounding":14902,"distribu":14903,"monstrous":14904,"Efros":14905,"Withholding":14906,"Wrong":14907,"MF":14908,"spirals":14909,"stockholder":14910,"interpreting":14911,"distinctive":14912,"07220":14913,"utilization":14914,"ior":14915,"statements":14916,"undiscounted":14917,"Ltd":14918,"102143":14919,"Covariance":14920,"unsupervised":14921,"hierarchies":14922,"243":14923,"enjoying":14924,"KGQA":14925,"InstructGPT":14926,"HippoRAG":14927,"generalizations":14928,"3818":14929,"Customization":14930,"Demographic":14931,"Actual":14932,"874":14933,"Beware":14934,"iciency":14935,"strengthens":14936,"Berkowitz":14937,"suggests":14938,"restores":14939,"certifying":14940,"DISCOVERY":14941,"tered":14942,"plicable":14943,"interpretable":14944,"repos":14945,"basis":14946,"NotImplementedError":14947,"columns":14948,"labeled":14949,"licensees":14950,"Nets":14951,"ceived":14952,"hints":14953,"2960420911378594":14954,"disease":14955,"extra":14956,"sionality":14957,"CORE":14958,"21501725.47":14959,"Storage":14960,"timestep":14961,"paradigms":14962,"Understanding":14963,"triples":14964,"mxnet":14965,"tative":14966,"Neuroscience":14967,"Contrary":14968,"𝛽":14969,"coronavirus":14970,"sched":14971,"modeler":14972,"inherits":14973,"debug":14974,"aspiring":14975,"radians":14976,"navigate":14977,"ranked":14978,"shot":14979,"haozhu233":14980,"8414":14981,"bn2":14982,"802":14983,"ratifying":14984,"artifacts":14985,"strategically":14986,"predominant":14987,"capitalize":14988,"misrepresentative":14989,"Meta":14990,"onestep":14991,"uncertainty":14992,"g1":14993,"Davide":14994,"xscale":14995,"spin":14996,"Codegen4libs":14997,"NegativeLogLikelihood":14998,"burgeoning":14999,"move":15000,"twitter":15001,"Send":15002,"Simonyan":15003,"accomplished":15004,"5393":15005,"1953":15006,"backups":15007,"divergence":15008,"grids":15009,"forth":15010,"plugging":15011,"judge":15012,"equation":15013,"4X":15014,"edu":15015,"239":15016,"disagrees":15017,"occur":15018,"them":15019,"Colin":15020,"Pedapati":15021,"preferences":15022,"color":15023,"Barvaz":15024,"nltk":15025,"TKEGEN":15026,"rent":15027,"1067":15028,"Release":15029,"mid":15030,"1Whi":15031,"backprop":15032,"LinearRegressionScratch":15033,"will":15034,"geomet":15035,"biographies":15036,"meticulously":15037,"B.2":15038,"Discussions265":15039,"compromised":15040,"gave":15041,"mon":15042,"Sales":15043,"element":15044,"cope":15045,"1cm":15046,"signature":15047,"lengths":15048,"Prakhar":15049,"2080":15050,"09288":15051,"页脚":15052,"vital":15053,"Listing":15054,"BASE":15055,"Henighan":15056,"instructional":15057,"shorthand":15058,"omitting":15059,"constituencies":15060,"cation":15061,"MultiHeadAttention":15062,"SoftplusTransform":15063,"tting":15064,"89":15065,"Reduction":15066,"annotated":15067,"leaf":15068,"Gluon":15069,"suspended":15070,"boosted":15071,"18":15072,"programming":15073,"testable":15074,"299387":15075,"minibatches":15076,"costs":15077,"international":15078,"contiguous":15079,"employed":15080,"Counterfactual":15081,"Comparing":15082,"wheel":15083,"monotonicity":15084,"spare":15085,"⊆":15086,"Courville":15087,"worry":15088,"Vougiouklis":15089,"snakes":15090,"init":15091,"resolu":15092,"bidirectional":15093,"Distributions":15094,"outlining":15095,"𝜆":15096,"Hsu":15097,"favourably":15098,"Hybridizing":15099,"Schapire":15100,"Matthews":15101,"10694":15102,"adolescent":15103,"bandit":15104,"rx":15105,"arable":15106,"Tintri":15107,"Shivam":15108,"extending":15109,"Doso":15110,"decoders":15111,"Acecoder":15112,"works":15113,"recom":15114,"memorizers":15115,"Investigate":15116,"semidefinite":15117,"were":15118,"Convolution":15119,"abbreviations":15120,"forums":15121,"matplotlib49":15122,"ARCHITECTURE":15123,"853":15124,"Firat":15125,"edgecolor":15126,"accuracies":15127,"street":15128,"collaborate":15129,"centers":15130,"subscriptions":15131,"operator":15132,"Major":15133,"912":15134,"credibility":15135,"contextual":15136,"Walsh":15137,"Obtain":15138,"Composite":15139,"competitors":15140,"AnyNetD":15141,"visible":15142,"rose":15143,"539":15144,"Policies":15145,"Occupation":15146,"4132":15147,"simonwardjones":15148,"satisfy":15149,"virtually":15150,"8.2":15151,"bail":15152,"Francesco":15153,"dist":15154,"reputation":15155,"ship":15156,"modify":15157,"resubstitution":15158,"impacting":15159,"Werness":15160,"happiness":15161,"simplicity":15162,"Land":15163,"3610":15164,"interdependent":15165,"RESOLVED":15166,"scheduler":15167,"liquidity":15168,"Liquidity":15169,"Tom":15170,"relating":15171,"Continuity":15172,"95":15173,"Clauses":15174,"switches":15175,"pavelkomarov":15176,"10222":15177,"consistency":15178,"decompose":15179,"pggPL":15180,"amination":15181,"2207":15182,"mechanics":15183,"013779522851109505":15184,"RLRC":15185,"interfaces":15186,"epochs":15187,"Remodiffuse":15188,"ject":15189,"1101":15190,"11708":15191,"comparable":15192,"Prowess":15193,"impacted":15194,"waits":15195,"Practical":15196,"chen":15197,"genera":15198,"2202":15199,"zhz":15200,"Hill":15201,"efficiency":15202,"inexpensive":15203,"Energy":15204,"阶段":15205,"Set":15206,"magnitude":15207,"stechnik":15208,"tialization":15209,"Attorney":15210,"betas":15211,"Didot":15212,"vqa":15213,"NAC":15214,"practical":15215,"underway":15216,"vgg":15217,"BIOREADER":15218,"courez":15219,"filterwarnings":15220,"polygon":15221,"patchifies":15222,"Tworek":15223,"Feichtenhofer":15224,"domVerticalFlip":15225,"Passos":15226,"x0":15227,"We":15228,"accounted":15229,"here":15230,"provements":15231,"RetDream":15232,"Purchased":15233,"constituting":15234,"Algebraic":15235,"2112":15236,"suffices":15237,"Short":15238,"colleagues":15239,"Council":15240,"BayesianOptimization":15241,"nik":15242,"Udapdr":15243,"1f":15244,"GDPR":15245,"pathic":15246,"Chervonenkis":15247,"H100":15248,"0.6026":15249,"Young":15250,"buses":15251,"intangibles":15252,"away":15253,"discusses":15254,"Price":15255,"Maria":15256,"soon":15257,"opposite":15258,"taste":15259,"derivatives":15260,"Belarus":15261,"Ellis":15262,"Facebook":15263,"702":15264,"p2.16":15265,"Enterprise":15266,"vein":15267,"Za":15268,"Description":15269,"HBM2":15270,"retriever":15271,"utiliz":15272,"812":15273,"saved":15274,"DOCUMENTS":15275,"Commonly":15276,"ularization":15277,"797":15278,"á":15279,"lightweight":15280,"covers":15281,"reasoning":15282,"refreshment":15283,"directories":15284,"ConnectX":15285,"Paragraphs":15286,"illus":15287,"Superchips":15288,"evoke":15289,"Murphy":15290,"HPE":15291,"MU":15292,"ICCBR":15293,"05313":15294,"misestimated":15295,"Hsun":15296,"define":15297,"1226":15298,"relate":15299,"tapping":15300,"maxlen":15301,"quaternion":15302,"falter":15303,"Gitman":15304,"13a":15305,"charleybeller":15306,"constantly":15307,"Choice":15308,"ranks":15309,"discretionary":15310,"everywhere":15311,"BlendedRAG":15312,"Pepsi":15313,"2.7072":15314,"amplifying":15315,"Accruals":15316,"JOSA":15317,"014076038679980779":15318,"female":15319,"west":15320,"Schmidt":15321,"Siemens":15322,"Ac":15323,"A.4":15324,"&&":15325,"$":15326,"comorbidities":15327,"03653":15328,"logo":15329,"complex":15330,"less":15331,"IID":15332,"263":15333,"Maui":15334,"Formulate":15335,"Emerging":15336,"nating":15337,"Sim":15338,"sensible":15339,"putation":15340,"Paulus":15341,"famil":15342,"1055":15343,"retirement":15344,"ki":15345,"bans":15346,"TensorBoard":15347,"038":15348,"need":15349,"repurposing":15350,"communicating":15351,"tails":15352,"influence":15353,"04136":15354,"1707":15355,"flipping":15356,"problems":15357,"FMA":15358,"Teh":15359,"Dayan":15360,"modality":15361,"unknown":15362,"Promotion":15363,"sparsity":15364,"ijk":15365,"Nijkamp":15366,"EV":15367,"Schedule":15368,"partner":15369,"GQA":15370,"2171":15371,"CACM":15372,"811":15373,"Teal":15374,"Double":15375,"Additions":15376,"prepara":15377,"03181":15378,"Malik":15379,"colormap":15380,"asymptotic":15381,"Erhan":15382,"Dalal":15383,"第二季度":15384,"plateaued":15385,"43715298.68":15386,"registered":15387,"denois":15388,"745":15389,"edition":15390,"during":15391,"0473":15392,"maintaining":15393,"accompanied":15394,"2000W":15395,"Fixed":15396,"Qdiag":15397,"nonparametric":15398,"infringe":15399,"Assign":15400,"insulating":15401,"Kingdom":15402,"NCGC":15403,"crane":15404,"p3":15405,"resonance":15406,"whenever":15407,"1440":15408,"assist":15409,"frameworks":15410,"dangerous":15411,"update":15412,"Initializing":15413,"offering":15414,"retrain":15415,"Holoscan":15416,"covariate":15417,"Alan":15418,"each":15419,"entrepreneurs":15420,"erring":15421,"wa":15422,"TO":15423,"Undoubtedly":15424,"Donini":15425,"在":15426,"textCNN":15427,"C2050":15428,"radical":15429,"Dividends":15430,"surgery":15431,"im":15432,"synchronizing":15433,"predictive":15434,"duce":15435,"adhering":15436,"Primary":15437,"Santiago":15438,"following":15439,"recognition":15440,"Victor":15441,"averaging":15442,"Segment":15443,"364757":15444,"8951":15445,"integra":15446,"infotainment":15447,"surements":15448,"Vollgraf":15449,"shop":15450,"purposes":15451,"imagined":15452,"aug":15453,"1.7359":15454,"Pathak":15455,"lawsuits":15456,"ROBOTICS":15457,"nansum":15458,"searching":15459,"rotates":15460,"Bounding":15461,"squeeze":15462,"Speeded":15463,"Tuner":15464,"shrinks":15465,"03000":15466,"langchain":15467,"1.52":15468,"BY":15469,"readily":15470,"another":15471,"developing":15472,"kind":15473,"what":15474,"assignment":15475,"flips":15476,"PKU":15477,"impute":15478,"seemingly":15479,"zone":15480,"Excess":15481,"Summary":15482,"mislabeled":15483,"vastly":15484,"trust":15485,"scene":15486,"Gato":15487,"Workshop":15488," ":15489,"LinearRegression":15490,"immersive":15491,"403":15492,"necessitates":15493,"Periodic":15494,"Configurations":15495,"bandwidth":15496,"defect":15497,"Karnin":15498,"exponentiate":15499,"austinmw":15500,"Gatys":15501,"constituent":15502,"Shin":15503,"bites":15504,"calibration":15505,"pAdvisor":15506,"enforc":15507,"EXCHANGE":15508,"exporting":15509,"客户":15510,"demoted":15511,"Flipping":15512,"ā":15513,"cuting":15514,"incidents":15515,"Awards":15516,"grams":15517,"Thieme":15518,"consoles":15519,"passes":15520,"Format":15521,"averaged":15522,"decides":15523,"redundant":15524,"Blockers":15525,"subtle":15526,"Bottom":15527,"Derivative":15528,"Φ":15529,"METHODOLOGIES":15530,"Sections":15531,"Progress":15532,"ZHANG":15533,"expire":15534,"sufficient":15535,"adeptly":15536,"862":15537,"Lubin":15538,"Conventionally":15539,"timent":15540,"重要":15541,"695":15542,"manufactures":15543,"Rights":15544,"Abnorml":15545,"Intuit":15546,"APPLICATIONS":15547,"719":15548,"PuTTY":15549,"tioning":15550,"SANER":15551,"country":15552,"basic":15553,"shifting":15554,"adminis":15555,"removed":15556,"Precision":15557,"certainty":15558,"Neuristique":15559,"unit":15560,"2056":15561,"899996":15562,"firmly":15563,"KaggleHouse":15564,"ation":15565,"smiley":15566,"851":15567,"resembles":15568,"crisper":15569,"indicate":15570,"enhance":15571,"thoroughness":15572,"02913976299993913":15573,"b1":15574,"transport":15575,"receiving":15576,"637":15577,"probability":15578,"Recomputing":15579,"Furthermore":15580,"prominence":15581,"Nie":15582,"kept":15583,"Poesia":15584,"3B":15585,"construction":15586,"Trainer":15587,"2278":15588,"Options":15589,"preparing":15590,"Selection":15591,"Hoos":15592,"understands":15593,"UNA":15594,"LazyLinear":15595,"microphone":15596,"Grape":15597,"musicians":15598,"Interspeech":15599,"𝑒":15600,"WEATHER":15601,"Wells":15602,"cycling":15603,"EDITAS":15604,"chronously":15605,"zeroing":15606,"COPY":15607,"obliterating":15608,"13.2":15609,"decisions":15610,"inversion":15611,"idle":15612,"0.22":15613,"abhinavsp0730":15614,"Serialization":15615,"instructions":15616,"canvas":15617,"2405":15618,"quest":15619,"procure":15620,"conf":15621,"Pe":15622,"Moore":15623,"sized":15624,"referring":15625,"1406":15626,"short":15627,"numerics":15628,"availability":15629,"summing":15630,"tors":15631,"complex128":15632,"500000":15633,"103":15634,"leveraging":15635,"violate":15636,"Parmar":15637,"1.6":15638,"Iwasawa":15639,"𝛾":15640,"Persons":15641,"promulgated":15642,"prize":15643,"entering":15644,"sciences":15645,"ner":15646,"studying":15647,"MEETING":15648,"pursued":15649,"Jun":15650,"mentions":15651,"weather":15652,"RHO":15653,"always":15654,"85%":15655,"restating":15656,"Burges":15657,"threshold":15658,"overestimated":15659,"multiples":15660,"stackspot":15661,"concerted":15662,"listed":15663,"decorate":15664,"Wissenschaften":15665,"cancel":15666,"1419":15667,"logging":15668,"instantiating":15669,"treasury":15670,"∇":15671,"hyperplane":15672,"Discussions94":15673,"ellipses":15674,"Micchelli":15675,"Waymo":15676,"unamortized":15677,"管理":15678,"bits":15679,"insurance":15680,"Whq":15681,"inspirations":15682,"Depreciation":15683,"SHARP":15684,"succes":15685,"issuers":15686,"Larroy":15687,"Lynn":15688,"Ling":15689,"reaches":15690,"106":15691,"interruptions":15692,"counterintuitive":15693,"exp":15694,"Discussions179":15695,"nonzero":15696,"dropoutmlp":15697,"architectures":15698,"asynchronously":15699,"Pietra":15700,"Roth":15701,"fiduciary":15702,"Health":15703,"cleverly":15704,"ExecuteTime":15705,"Economics":15706,"B.9":15707,"globally":15708,"manipu":15709,"Normalize":15710,"TENCH":15711,"moire":15712,"edge":15713,"adjusting":15714,"Realized":15715,"competitions21":15716,"mere":15717,"neously":15718,"neutrality":15719,"prevention":15720,"sec":15721,"213MB":15722,"12.05":15723,"preprocessing":15724,"2206":15725,"886":15726,"Lora":15727,"safer":15728,"forgoes":15729,"dealt":15730,"hypergradi":15731,"464":15732,"0108":15733,"newly":15734,"025":15735,"PF":15736,"pesky":15737,"TU102":15738,"sensitive":15739,"′":15740,"Ward":15741,"pu":15742,"\f":15743,"hyperparameter":15744,"具体":15745,"origin":15746,"Anthony":15747,"pairwise":15748,"height":15749,"Detect":15750,"1Whh":15751,"narrowly":15752,"detail":15753,"012271":15754,"fmts":15755,"Discussions135":15756,"59e":15757,"1000":15758,"repeated":15759,"Asadi":15760,"Submission":15761,"Blog":15762,"01738":15763,"Discussions140":15764,"configurators":15765,"quantities":15766,"quarters":15767,"cautioned":15768,"experimentation":15769,"Campbell":15770,"1611.01578":15771,"eps":15772,"20139":15773,"Production":15774,"got":15775,"05856":15776,"curacy":15777,"CARE":15778,"2329":15779,"clarifications":15780,"UR":15781,"422906":15782,"trigonometry":15783,"earning":15784,"Aug":15785,"embeds":15786,"GRU":15787,"QuickTake":15788,"watching":15789,"precedence":15790,"substances":15791,"Avatar":15792,"Brian":15793,"assurance":15794,"277195":15795,"sigmoidal":15796,"gain":15797,"Colette":15798,"equivalent":15799,"Music":15800,"5882":15801,"rich":15802,"Indenture":15803,"assistants":15804,"∥":15805,"MGIC":15806,"Insider":15807,"wires":15808,"statistics":15809,"Pinto":15810,"ficiently":15811,"Fundamental":15812,"elevate":15813,"Material":15814,"449":15815,"Ye":15816,"integer":15817,"restructures":15818,"gathering":15819,"456":15820,"待办":15821,"Design":15822,"name":15823,"AlexNet":15824,"qkv":15825,"Patodia":15826,"affordable":15827,"12925":15828,"0.406":15829,"0284":15830,"dates":15831,"not":15832,"Shi":15833,"legally":15834,"ltered":15835,"这":15836,"fetch":15837,"have":15838,"Engagement":15839,"Uni":15840,"H1":15841,"obsolescence":15842,"Rung":15843,"Shape":15844,"Kulits":15845,"Martinet":15846,"Herbrich":15847,"数据表":15848,"recent":15849,"partitions":15850,"layouts":15851,"Alyafeai":15852,"Heck":15853,"actively":15854,"High":15855,"1999":15856,"Gradients":15857,"packet":15858,"unfitness":15859,"𝑤":15860,"Leveraging":15861,"TFLOPs":15862,"made":15863,"146":15864,"883":15865,"Freeman":15866,"greedy":15867,"resolved":15868,"Snowflake":15869,"Langevin":15870,"blame":15871,"2232":15872,"computation":15873,"filters":15874,"miss":15875,"Scheduler":15876,"London":15877,"Huber":15878,"400m":15879,"Schoenholz":15880,"secure":15881,"开发进展":15882,"565":15883,"An":15884,"Annex":15885,"2616":15886,"Captioning":15887,"Nikravesh":15888,"非":15889,"may":15890,"feasible":15891,"Q2":15892,"Halving":15893,"parallelizing":15894,"ChatGPT":15895,"indemnitee":15896,"Looking":15897,"minima":15898,"Allen":15899,"电子产品":15900,"statistician":15901,"Wind":15902,"interacted":15903,"routinely":15904,"Island":15905,"dispose":15906,"111":15907,"Committee":15908,"prohibitive":15909,"Special":15910,"Blenderbot":15911,"unsolvable":15912,"BIG":15913,"dir":15914,"𝐶":15915,"942":15916,"counteracts":15917,"manually":15918,"Regulatory":15919,"Melissa":15920,"Startups":15921,"30":15922,"virtualshareholder":15923,"guard":15924,"overlaps":15925,"marketed":15926,"Question":15927,"airplane":15928,"Juntian":15929,"photography":15930,"Datla":15931,"平板":15932,"axhline":15933,"096":15934,"Faster":15935,"1928":15936,"genfromtxt":15937,"chinese":15938,"joined":15939,"tral":15940,"Alokla":15941,"RETRO":15942,"Traffic":15943,"Hron":15944,"OrderedDict":15945,"obstacles":15946,"boxes":15947,"writ":15948,"metric":15949,"benefiting":15950,"speculation":15951,"Haibin":15952,"disrupt":15953,"inception":15954,"ered":15955,"Wxh":15956,"6.2":15957,"StoppingCriterion":15958,"357":15959,"units":15960,"iii":15961,"Levenshtein":15962,"543":15963,"surrogates":15964,"spans":15965,"559":15966,"arrow":15967,"Sliding":15968,"assumption":15969,"tired":15970,"Aggregated":15971,"diluted":15972,"03130":15973,"remainder":15974,"Chae":15975,"pure":15976,"Broadcast":15977,"设计":15978,"slopes":15979,"head":15980,"Corporation":15981,"subpopulation":15982,"2x":15983,"2cm":15984,"lasting":15985,"ognize":15986,"TOSEM":15987,"darts":15988,"5097":15989,"23985":15990,"8.9":15991,"removal":15992,"strength":15993,"sensors":15994,"stewardship":15995,"wall":15996,"A800":15997,"blocking":15998,"nan":15999,"poisson":16000,"versation":16001,"dou":16002,"Change":16003,"Tips":16004,"Unamortized":16005,"Alaskan":16006,"GaussianLikelihood":16007,"sofa":16008,"Count":16009,"Alexey":16010,"B.10":16011,"slight":16012,"factorizes":16013,"Pioneering":16014,"Unpaired":16015,"quarter":16016,"contractual":16017,"3311":16018,"incantations":16019,"pendently":16020,"vehicles":16021,"next2":16022,"Asus":16023,"furnished":16024,"exports":16025,"sagemaker":16026,"astype":16027,"Mead":16028,"clothing":16029,"Goldilocks":16030,"excited":16031,"polynomial":16032,"mentor":16033,"Cloudera":16034,"clogging":16035,"lite":16036,"hood":16037,"scrutiny":16038,"dair":16039,"abundance":16040,"recharge":16041,"Laboratories":16042,"848":16043,"walks":16044,"Cavallo":16045,"startup":16046,"Assets":16047,"5148":16048,"helper":16049,"lose":16050,"CMR":16051,"play":16052,"Ch":16053,"076646":16054,"macroeconomic":16055,"ski":16056,"Surprisingly":16057,"flat":16058,"IMDb":16059,"215%":16060,"Les":16061,"tor":16062,"ViTs":16063,"v4":16064,"rec":16065,"Canceled":16066,"Full":16067,"588":16068,"size":16069,"concentrate":16070,"aptitude":16071,"CNBC":16072,"Henderson":16073,"architecture":16074,"b":16075,"sive":16076,"plain":16077,"preliminary":16078,"mini":16079,"moderating":16080,"prevalence":16081,"ticks":16082,"04032":16083,"harms":16084,"Codebert":16085,"Acknowledgments":16086,"y1":16087,"EACL":16088,"sublayer":16089,"551":16090,"530.30":16091,"printoptions":16092,"17":16093,"ecuted":16094,"11.5":16095,"480":16096,"hh":16097,"inflation":16098,"version":16099,"even":16100,"imple":16101,"Germans":16102,"02966":16103,"119":16104,"8341":16105,"A":16106,"leave":16107,"Mk":16108,"Smith":16109,"plug":16110,"doubled":16111,"18.2":16112,"Crucially":16113,"drastic":16114,"tons":16115,"03963":16116,"datacenter":16117,"renewable":16118,"mll":16119,"communities":16120,"Jan":16121,"5846051207380589":16122,"emerge":16123,"hypothetical":16124,"2.01":16125,"HPC":16126,"flow":16127,"silently":16128,"operands":16129,"charitable":16130,"08894":16131,"Vanguard":16132,"Citation":16133,"attacked":16134,"1994":16135,"practicable":16136,"093":16137,"BasicBlock":16138,"estimation":16139,"enforcement":16140,"Award":16141,"spring":16142,"click":16143,"consulting":16144,"herewith":16145,"Keen":16146,"caution":16147,"108":16148,"1.1456":16149,"SProp":16150,"imizer":16151,"2020":16152,"motives":16153,"post57":16154,"Morey":16155,"1058":16156,"gered":16157,"ä":16158,"Elsevier":16159,"ReLU":16160,"i9":16161,"perturbing":16162,"Surveys":16163,"inconsistent":16164,"pecuniary":16165,"@":16166,"𝑣":16167,"packages":16168,"sume":16169,"AveragePooling":16170,"specifics":16171,"Jelinek":16172,"regularizing":16173,"Reconciling":16174,"6667":16175,"customization":16176,"Teacher":16177,"reading":16178,"generic":16179,"targeted":16180,"089688":16181,"combined":16182,"Gems":16183,"poi":16184,"substitutes":16185,"12.7":16186,"rli":16187,"syntax":16188,"least":16189,"BPE":16190,"Foundation":16191,"Hot":16192,"home":16193,"8697":16194,"predictor":16195,"Tri":16196,"plausible":16197,"Communications":16198,"sanity":16199,"Silverman":16200,"Œ":16201,"gate":16202,"803":16203,"effecting":16204,"inferencing":16205,"successes":16206,"从":16207,"stick":16208,"THEIR":16209,"Classi":16210,"Presentation":16211,"anachro":16212,"competition":16213,"thoughts":16214,"techniques":16215,"nominating":16216,"268":16217,"Zemlyanskiy":16218,"Postprocessing":16219,"electricity":16220,"annotators":16221,"despite":16222,"Martin":16223,"radically":16224,"900532":16225,"irrelevant":16226,"semiconductors":16227,"Byrne":16228,"smart":16229,"Assembly":16230,"respective":16231,"Strangely":16232,"dos":16233,"logarithmically":16234,"doc":16235,"4899e":16236,"persistent":16237,"participate":16238,"dimension":16239,"725":16240,"fabricated":16241,"biological":16242,"wonderful":16243,"empirica":16244,"b5":16245,"variational":16246,"produce":16247,"Emergence":16248,"8287":16249,"159":16250,"inaccessible":16251,"Getting":16252,"heel":16253,"Almahairi":16254,"Aaron":16255,"15217":16256,"Dogs":16257,"analyze":16258,"proportional":16259,"199":16260,"Generative":16261,"theoret":16262,"poodle":16263,"Dynamics":16264,"Lived":16265,"856":16266,"oc":16267,"flatter":16268,"shows":16269,"ö":16270,"misshapen":16271,"failure":16272,"2309":16273,"complication":16274,"ish":16275,"ings":16276,"1158":16277,"maximize":16278,"Editsum":16279,"rulings":16280,"thousand":16281,"398":16282,"Bertsch":16283,"carbon":16284,"groundwork":16285,"editors":16286,"man":16287,"gal":16288,"argmin":16289,"Makes":16290,"pean":16291,"Intersection":16292,"Different":16293,"Summer":16294,"oh":16295,"DONALD":16296,"25300":16297,"Liwicki":16298,"Inter":16299,"houses":16300,"997":16301,"depreciation":16302,"meet":16303,"TPAT":16304,"allows":16305,"Colors":16306,"withdrawal":16307,"sticking":16308,"Cordonnier":16309,"fits":16310,"habits":16311,"Keutzer":16312,"rescue":16313,"resounding":16314,"occurred":16315,"年龄":16316,"ALSO":16317,"SIGKDDg":16318,"drinks":16319,"repository299":16320,"ColorJitter":16321,"tracked":16322,"feedforward":16323,"Verilog":16324,"nll":16325,"Lee":16326,"899920":16327,"extraordinary":16328,"Afterwards":16329,"0.1":16330,"game":16331,"Technology":16332,"第二":16333,"A.20":16334,"2Cov":16335,"estimated":16336,"amplitude":16337,"Hasson":16338,"stages":16339,"Retained":16340,"1094":16341,"excise":16342,"Unif":16343,"1448":16344,"qualify":16345,"fair":16346,"relevant":16347,"Alternatively":16348,"Bag":16349,"outpaced":16350,"GTC":16351,"14323":16352,"7%":16353,"facebook":16354,"pretended":16355,"178100":16356,"remediated":16357,"respec":16358,"neurons":16359,"convenient":16360,"optional":16361,"nisms":16362,"hooks":16363,"probs":16364,"append":16365,"T":16366,"OpenCV":16367,"qualifying":16368,"14887":16369,"933":16370,"Size":16371,"care":16372,"Since":16373,"live":16374,"evaluations":16375,"longstanding":16376,"convolu":16377,"Sulla":16378,"corrupting":16379,"autoencoders":16380,"erator":16381,"treasures":16382,"207":16383,"Rubinfeld":16384,"levels":16385,"A.16":16386,"conv4":16387,"Indeed":16388,"tabu":16389,"Jaggi":16390,"1.36":16391,"screen":16392,"NEAL":16393,"fused":16394,"Cybersecurity":16395,"341":16396,"beginner":16397,"lengthened":16398,"Yamaguchi":16399,"Discussions187":16400,"Saynova":16401,"el":16402,"genuine":16403,"exported":16404,"Stoica":16405,"Rajeswaran":16406,"side":16407,"Developments":16408,"Uhlenbeck":16409,"ExactMarginalLogLikelihood":16410,"governing":16411,"land":16412,"library45":16413,"6469":16414,"notedown":16415,"GPyTorch261":16416,"logues":16417,"026036":16418,"Reviews":16419,"tokenize":16420,"condi":16421,"validate":16422,"Samoyeds":16423,"or":16424,"Strang":16425,"score":16426,"CVPR":16427,"Von":16428,"Materials":16429,"trimming":16430,"stayed":16431,"Padding":16432,"single":16433,"2nd":16434,"1970s":16435,"rolled":16436,"brightness":16437,"perparameter":16438,"appointments":16439,"1.6809":16440,"realized":16441,"heiligerl":16442,"recenters":16443,"combin":16444,"playlist":16445,"1998":16446,"eos":16447,"Given":16448,"Finished":16449,"Explore":16450,"SaleType":16451,"Luu":16452,"widen":16453,"proved":16454,"acceleration":16455,"personalization":16456,"Minerals":16457,"4.7":16458,"140":16459,"zucchinis":16460,"Scholes":16461,"Shareholders":16462,"2y":16463,"Supermajority":16464,"SynthesizedImage":16465,"days":16466,"Speed":16467,"repurchases":16468,"0757":16469,"姓名":16470,"scatter":16471,"display":16472,"sobering":16473,"Probability":16474,"creating":16475,"Monga":16476,"continuity":16477,"unless":16478,"xiv":16479,"differen":16480,"Washington":16481,"bringing":16482,"rata":16483,"struction":16484,"DQN":16485,"0.64":16486,"Approval":16487,"boom":16488,"Fid":16489,"broadcasters":16490,"PositionWiseFFN":16491,"basing":16492,"Attuari":16493,"Mining":16494,"Latin":16495,"threats":16496,"Upon":16497,"3rd":16498,"---------":16499,"1937":16500,"4096":16501,"1.0":16502,"TimeMachine":16503,"Ad":16504,"logical":16505,"18GB":16506,"ing":16507,"miles":16508,"0.3537":16509,"8TB":16510,"commonly":16511,"λ":16512,"":16513,"Overhead":16514,"Platforms":16515,"HD":16516,"constructive":16517,"giom":16518,"papers":16519,"adaptive":16520,"rectifiers":16521,"Intuitively":16522,"implementaitons":16523,"LawLLM":16524,"founded":16525,"Graphs":16526,"Standardizing":16527,"paragraph":16528,"impossibility":16529,"angeschaut":16530,"decipher":16531,"Reduce":16532,"catenated":16533,"Uniform":16534,"eigen":16535,"Seven":16536,"aver":16537,"bigram":16538,"mimicking":16539,"Sepp":16540,"script":16541,"2013":16542,"08845692598296777":16543,"pay":16544,"Finetuning":16545,"ckstr":16546,"framework":16547,"Friedland":16548,"exchanges":16549,"entities":16550,"individual":16551,"hazy":16552,"puted":16553,"night":16554,"celebrity":16555,"13.06%":16556,"1864":16557,"Berg":16558,"9991":16559,"LeCun":16560,"flexible":16561,"Jatowt":16562,"memorized":16563,"returns":16564,"9cm":16565,"perhaps":16566,"Macau":16567,"she":16568,"Obviously":16569,"eldarkurtic":16570,"manipulat":16571,"SaleCondition":16572,"4700":16573,"niplus1":16574,"larger":16575,"subfolders":16576,"Neighborhoods":16577,"2025":16578,"excels":16579,"Leg":16580,"remarkable":16581,"Wu":16582,"Zaragoza":16583,"message":16584,"Discussions266":16585,"ymax":16586,"Cucurull":16587,"581":16588,"disincentive":16589,"2424":16590,"neighbours":16591,"np":16592,"Pipelines":16593,"blanks":16594,"returning":16595,"while":16596,"Ubuntu":16597,"Operator":16598,"informatics":16599,"transformer":16600,"analytics":16601,"cooled":16602,"erywhere":16603,"Action":16604,"TKDE":16605,"question":16606,"0568":16607,"collectively":16608,"generative":16609,"continu":16610,"denote":16611,"recreated":16612,"Extraction":16613,"generated":16614,"Bge":16615,"uler":16616,"DEFINITIONS":16617,"303746":16618,"study":16619,"vocab":16620,"Opportunities":16621,"paths":16622,"extensions":16623,"tal":16624,"Izacard":16625,"Tweak":16626,"ideling":16627,"successfully":16628,"alors":16629,"Tijsseling":16630,"sliver":16631,"Select":16632,"tossed":16633,"25X":16634,"Discount":16635,"dying":16636,"uates":16637,"OpenVino":16638,"BERTClassifier":16639,"Functions":16640,"representing":16641,"merits":16642,"poorly":16643,"cholesky":16644,"parametrizations":16645,"tradeoffs":16646,"Lumsdaine":16647,"cuBLAS":16648,"Discussions80":16649,"RMSprop":16650,"spelled":16651,"augmentations":16652,"normalizing":16653,"xAI":16654,"TCP":16655,"Wednesday":16656,"usually":16657,"achieves":16658,"coefficients":16659,"0.0974":16660,"damage":16661,"rapidly":16662,"hammer":16663,"generalists":16664,"DataModule":16665,"Metric":16666,"Rat":16667,"Wasserman":16668,"chine":16669,"option":16670,"068":16671,"Knn":16672,"face":16673,"illustrative":16674,"Vectors":16675,"16xlarge":16676,"ordered":16677,"prone":16678,"Conditional":16679,"depicts":16680,"Macready":16681,"Marketable":16682,"floating":16683,"die":16684,"715":16685,"carrying":16686,"parametriza":16687,"subcontractor":16688,"Cars":16689,"inevitable":16690,"likeli":16691,"inquired":16692,"tpdi":16693,"tunable":16694,"patches":16695,"approximator":16696,"internally":16697,"Nearby":16698,"Commitments":16699,"页":16700,"interest":16701,"l1":16702,"penalize":16703,"≡":16704,"environ":16705,"telephone":16706,"Agnostic":16707,"Du":16708,"behavioral":16709,"756":16710,"Flygare":16711,"troubles":16712,"bone":16713,"Graph":16714,"cates":16715,"748":16716,"ria":16717,"simpler":16718,"typewriter":16719,"Unbiasing":16720,"indemnitees":16721,"viation":16722,"3104":16723,"06117":16724,"TOME":16725,"Preconditioning":16726,"546":16727,"3960":16728,"smaller":16729,"215050":16730,"separates":16731,"李四":16732,"OEM":16733,"finishes":16734,"passenger":16735,"discretion":16736,"uniform":16737,"C":16738,"Norvig":16739,"Quanshangze":16740,"later":16741,"exam":16742,"logs":16743,"greater":16744,"gotten":16745,"thermal":16746,"correcting":16747,"guarantee":16748,"individ":16749,"riplus1":16750,"10196":16751,"hugging":16752,"926":16753,"Default":16754,"Massive":16755,"496118":16756,"van":16757,"Learned":16758,"persisted":16759,"faithful":16760,"Every":16761,"tailored":16762,"19355":16763,"retrievable":16764,"owing":16765,"Hoa":16766,"364":16767,"specify":16768,"𝑛":16769,"bear":16770,"pinning":16771,"tialized":16772,"Clipping":16773,"473":16774,"Distributing":16775,"parametrizing":16776,"AT":16777,"encouragement":16778,"interpre":16779,"suppresses":16780,"arguments":16781,"blend":16782,"Beat":16783,"directors":16784,"Tossing":16785,"Simon":16786,"arrangements":16787,"768":16788,"antees":16789,"Flamingo":16790,"crimes":16791,"rightmost":16792,"USPS":16793,"𝑡":16794,"Summing":16795,"lecun":16796,"comes":16797,"pertaining":16798,"preconditioner":16799,"9182":16800,"处理":16801,"aterzis":16802,"mand":16803,"receipts":16804,"PRELIMINARY":16805,"prizes":16806,"00270":16807,"05772":16808,"revisit":16809,"AddmmBackward0":16810,"truncation":16811,"folded":16812,"unvested":16813,"dataset241":16814,"standalone":16815,"482":16816,"collaborations":16817,"chains":16818,"squeezing":16819,"contradicted":16820,"Discussions144":16821,"041":16822,"addnorm1":16823,"MBA":16824,"stringent":16825,"cent":16826,"bug":16827,"Taskar":16828,"EncoderDecoder":16829,"1111":16830,"memorization":16831,"0850":16832,"lengthens":16833,"outliers":16834,"frontends":16835,"rets":16836,"grammatical":16837,"Rotate":16838,"epistemic":16839,"faculty":16840,"McCul":16841,"commanded":16842,"manner":16843,"rocal":16844,"det":16845,"Dokl":16846,"1099":16847,"Baevski":16848,"Retrieval":16849,"Garry":16850,"Popescul":16851,"ctbl":16852,"shelf":16853,"conveniently":16854,"command":16855,"Triggs":16856,"eat":16857,"autoregressively":16858,"influential":16859,"Corp":16860,"CoLA":16861,"documentation65":16862,"2108":16863,"tosses":16864,"Locality":16865,"earthquakes":16866,"respectable":16867,"factories":16868,"rising":16869,"audience":16870,"Schedules":16871,"expla":16872,"extensive":16873,"Text":16874,"End":16875,"das":16876,"Zhai":16877,"truncate":16878,"chip":16879,"-------------":16880,"arccos":16881,"Gptcache":16882,"337":16883,"Unless":16884,"esoteric":16885,"1137":16886,"implicit":16887,"suspend":16888,"libgfortran3":16889,"intentional":16890,"Boureau":16891,"Selecting":16892,"Glass":16893,"bud":16894,"9905":16895,"Cao":16896,"overall":16897,"3092":16898,"via":16899,"Content":16900,"00030":16901,"classifications":16902,"eval":16903,"NAN":16904,"sneaker":16905,"motus":16906,"invertible":16907,"SHAREHOLDERMEETING":16908,"SYNCHROMESH":16909,"adjustments":16910,"nyi":16911,"183416":16912,"Rate":16913,"Mercedes":16914,"concepts":16915,"difficult":16916,"multiple":16917,"READ":16918,"600":16919,"scratches":16920,"0.8477":16921,"B.8":16922,"pedantic":16923,"Discussions231":16924,"turbing":16925,"centrifugal":16926,"mainly":16927,"250%":16928,"overfit":16929,"malicious":16930,"Holder":16931,"rebates":16932,"Queue":16933,"OR":16934,"Narrow":16935,"Taken":16936,"4695":16937,"EDIT":16938,"proteins":16939,"Nichol":16940,"translate":16941,"471":16942,"NeurIPS":16943,"counts":16944,"Forbes":16945,"utze":16946,"sight":16947,"inclined":16948,"empty":16949,"SARGAM":16950,"constitute":16951,"Cholesky":16952,"Processing":16953,"Multivariate":16954,"According":16955,"outdated":16956,"primitive":16957,"probab":16958,"899987":16959,"onal":16960,"Middle":16961,"structuring":16962,"subsequently":16963,"architec":16964,"Ghrzuzudu":16965,"014628124155727769":16966,"containing":16967,"pReLU":16968,"CRF":16969,"Lo":16970,"￿":16971,"Cui":16972,"mappings":16973,"Discussions152":16974,"refund":16975,"deduces":16976,"6375":16977,"archiving":16978,"SoftmaxRegressionScratch":16979,"nondifferentiable":16980,"expository":16981,"deemed":16982,"788":16983,"settled":16984,"Box":16985,"00449":16986,"AB":16987,"Qadir":16988,"recommending":16989,"triplets":16990,"7044":16991,"1th":16992,"ago":16993,"cubic":16994,"determinism":16995,"Finetuned":16996,"consequence":16997,"367":16998,"cheated":16999,"ignoring":17000,"Methods":17001,"Veit":17002,"649":17003,"100d":17004,"account":17005,"reclaim":17006,"pronouncements":17007,"N0":17008,"uti":17009,"1097":17010,"instructive":17011,"∗":17012,"object":17013,"leak":17014,"sparser":17015,"viewed":17016,"SUMMARY":17017,"recipe":17018,"Quadro":17019,"CCA":17020,"intimately":17021,"Paid":17022,"conversion":17023,"developers":17024,"late":17025,"Effect":17026,"avoid":17027,"guaranteed":17028,"warm":17029,"1978":17030,"censoring":17031,"Issuer":17032,"arc":17033,"disposal":17034,"99995":17035,"fashionable":17036,"blind":17037,"Income":17038,"delegated":17039,"Araki":17040,"Discussions217":17041,"officer":17042,"1805.2267129073316":17043,"meaningfully":17044,"neural":17045,"transaction":17046,"doctorate":17047,"LMs":17048,"1765":17049,"general":17050,"initialed":17051,"observations":17052,"plies":17053,"rescaled":17054,"nontrivial":17055,"resumes":17056,"activated":17057,"1033":17058,"medium":17059,"colormap2label":17060,"761":17061,"powerless":17062,"1.0308":17063,"Rabiner":17064,"fold":17065,"auditors":17066,"expiration":17067,"handled":17068,"prepended":17069,"EgoInstructor":17070,"Threshold":17071,"realistic":17072,"linking":17073,"20170":17074,"pic":17075,"evalu":17076,"Project":17077,"gpytorch":17078,"government":17079,"censored":17080,"40th":17081,"worth":17082,"approvals":17083,"ceive":17084,"987":17085,"committees":17086,"157":17087,"rights":17088,"suburbs":17089,"Layers":17090,"Addison":17091,"routes":17092,"directory":17093,"B.1":17094,"Here":17095,"dimen":17096,"mention":17097,"Borgeaud":17098,"Fortu":17099,"Student":17100,"Fisch":17101,"multi":17102,"provenance":17103,"What":17104,"neighbors":17105,"median":17106,"elegantly":17107,"bedding":17108,"injustices":17109,"0.6":17110,"cc":17111,"730":17112,"wikipedia":17113,"repair":17114,"chemical":17115,"Dollar":17116,"Mallen":17117,"tion":17118,"updating":17119,"Iowa":17120,"wave":17121,"Table":17122,"350M":17123,"cumulative":17124,"subsample":17125,"Cores":17126,"5":17127,"Poker":17128,"synchronization":17129,"namedtuple":17130,"}":17131,"exe":17132,"943":17133,"Brown":17134,"causes":17135,"misclassification":17136,"Faced":17137,"428":17138,"logit":17139,"Unremitted":17140,"aggregate":17141,"382":17142,"Correction":17143,"frequently":17144,"furthermore":17145,"Interpretations":17146,"Alma":17147,"SpanBERT":17148,"ascendence":17149,"文档":17150,"package":17151,"Japan":17152,"multiprocessing":17153,"Matters":17154,"dislodge":17155,"uncertain":17156,"2021":17157,"Hughes":17158,"sublayers":17159,"ASHA":17160,"glean":17161,"及其":17162,"Inferfix":17163,"thumb":17164,"axis":17165,"fatter":17166,"135":17167,"coordinated":17168,"pairs":17169,"refresher":17170,"occluded":17171,"completes":17172,"paradigm":17173,"closest":17174,"Manipulation":17175,"seq2tree":17176,"Iterated":17177,"substantial":17178,"mous":17179,"Bylaws":17180,"unbiased":17181,"614":17182,"necessitating":17183,"Starry":17184,"Assurance":17185,"PCI":17186,"76":17187,"leasehold":17188,"API":17189,"paid":17190,"264":17191,"sleep":17192,"Various":17193,"quadraticity":17194,"Precisely":17195,"genie":17196,"1909":17197,"add":17198,"Nasdaq":17199,"craft":17200,"Putting":17201,"HyMap":17202,"tionary":17203,"Wan":17204,"chemist":17205,"Robotics":17206,"Aligns":17207,"1.9090":17208,"thrill":17209,"Tutorial":17210,"Evaluating":17211,"Batch":17212,"budgets":17213,"Go250":17214,"impairments":17215,"Springer":17216,"stabilize":17217,"Kotthoff":17218,"yourself":17219,"converting":17220,"Equivalent":17221,"optimized":17222,"Maintenance":17223,"diving":17224,"past":17225,"mathe":17226,"monsense":17227,"bottle":17228,"linreg":17229,"PURSUANT":17230,"inspector":17231,"pretrains":17232,"ontology":17233,"cpu":17234,"C0":17235,"1916":17236,"approximations":17237,"988":17238,"vec":17239,"down":17240,"Verlag":17241,"Del":17242,"Contrast":17243,"drive":17244,"llama":17245,"Compressing":17246,"43":17247,"formalize":17248,"raise":17249,"Mishra":17250,"entry":17251,"integration":17252,"gorgeous":17253,"249":17254,"warrant":17255,"geographical":17256,"indices":17257,"income":17258,"Unik":17259,"Delving":17260,"ANY":17261,"70s":17262,"altering":17263,"Xiang":17264,"ො":17265,"Cells":17266,"whiteboard":17267,"Braud":17268,"10b":17269,"cult":17270,"timization":17271,"SDKs":17272,"Establishes":17273,"antique":17274,"Hayati":17275,"oversees":17276,"Abstractive":17277,"willingness":17278,"Lengthy":17279,"ual":17280,"offerings":17281,"economy":17282,"QUANTUM":17283,"burdens":17284,"previously":17285,"Warehousing":17286,"X2":17287,"Encoded":17288,"D1":17289,"Specifying":17290,"dcbb9e9d":17291,"Collecting":17292,"2672":17293,"多":17294,"jpg":17295,"backbone":17296,"difficulties":17297,"alchemy":17298,"Werke":17299,"reversing":17300,"1434":17301,"Stoyanov":17302,"transcoding":17303,"StarCraft":17304,"fundamental":17305,"Style":17306,"ABCs":17307,"Bidirectional":17308,"Reliable":17309,"sorted":17310,"itself":17311,"ISCA":17312,"maintenance":17313,"multiclass":17314,"visualized":17315,"G4":17316,"Easy":17317,"0.00078":17318,"credible":17319,"REACH":17320,"Decomposing":17321,"1806":17322,"Commercial":17323,"Discussions211":17324,"saves":17325,"0264":17326,"enters":17327,"allocating":17328,"pp":17329,"commu":17330,"sheath":17331,"yielding":17332,"behavior":17333,"giant":17334,"Conference":17335,"ZF":17336,"76e5be1548fd8222e5074cf0faae75edff8cf93f":17337,"distort":17338,"Latino":17339,"Differ":17340,"Units":17341,"generically":17342,"requirement":17343,"particularly":17344,"infrastructure":17345,"ranker":17346,"packaging":17347,"jerryjliu":17348,"ditioned":17349,"8th":17350,"251":17351,"SyntheticRegressionData":17352,"\u0010":17353,"DCGAN":17354,"analogously":17355,"rations":17356,"gift":17357,"vague":17358,"remaining":17359,"767":17360,"CLS":17361,"grow":17362,"slew":17363,"consume":17364,"A.25":17365,"rection":17366,"votes":17367,"depict":17368,"1139":17369,"divided":17370,"therapeutics":17371,"mastered":17372,"Disagreements":17373,"interim":17374,"Salary":17375,"Dec":17376,"task":17377,"accumulated":17378,"140381179266448":17379,"Company":17380,"Guidelines":17381,"xlist":17382,"til":17383,"hyperplanes":17384,"cre":17385,"fasttext":17386,"Bank":17387,"1975":17388,"Knowing":17389,"trainable":17390,"useless":17391,"gorithms":17392,"richer":17393,"Underlying":17394,"Pathway":17395,"MacKay":17396,"pervised":17397,"terminal":17398,"developments":17399,"DataFrame":17400,"liabilities":17401,"salaries":17402,"Ommer":17403,"Inline":17404,"formats":17405,"Cho":17406,"380":17407,"responsibly":17408,"universally":17409,"DataLoader":17410,"dations":17411,"lines":17412,"SARS":17413,"Propagation":17414,"monotonically":17415,"clipping":17416,"big":17417,"citation":17418,"Guide":17419,"anticipated":17420,"019823":17421,"9135":17422,"A.49":17423,"art":17424,"contest":17425,"clone":17426,"0.225":17427,"maxi":17428,"pr":17429,"corresponded":17430,"Started":17431,"086":17432,"Glorot":17433,"TABLE":17434,"Euro":17435,"codes":17436,"Payout":17437,"tale":17438,"Discussions114":17439,"Interacting":17440,"House":17441,"clearance":17442,"gameplay":17443,"OCR":17444,"inheriting":17445,"outs":17446,"associates":17447,"grabbing":17448,"Mordatch":17449,"conversation":17450,"spell":17451,"focusing":17452,"0010":17453,"aforementioned":17454,"1002":17455,"651K":17456,"contribute":17457,"WhiteD3vil":17458,"get":17459,"recursive":17460,"Autoregressive":17461,"takeaways":17462,"Nazih":17463,"inbox":17464,"tconv":17465,"conversations":17466,"2969":17467,"laplace":17468,"Bin":17469,"marrying":17470,"Bodla":17471,"Rockafellar":17472,"sification":17473,"utilities":17474,"CoVe":17475,"attacks":17476,"whichever":17477,"ﬀ":17478,"Erosion":17479,"gether":17480,"snow":17481,"permitted":17482,"Concatenate":17483,"offer":17484,"space":17485,"Forward":17486,"acceptable":17487,"4284":17488,"1063":17489,"378":17490,"bang":17491,"edges":17492,"subwindow":17493,"buildout":17494,"315":17495,"Residual":17496,"trillion":17497,"Teter":17498,"例如":17499,"Boucheron":17500,"Jampani":17501,"consultants":17502,"GNN":17503,"toolkits":17504,"storing":17505,"argued":17506,"CCPA":17507,"media":17508,"emerges":17509,"Generated":17510,"RNNLMScratch":17511,"compensate":17512,"Pricing":17513,"addresses":17514,"dwarfed":17515,"resourced":17516,"persistently":17517,"117":17518,"progress":17519,"certificate":17520,"GENERATIVE":17521,"policy":17522,"undetected":17523,"coherently":17524,"sible":17525,"Server":17526,"detrimental":17527,"groundbreaking":17528,"Dwivedi":17529,"exacerbated":17530,"13.1":17531,"1103":17532," ":17533,"1810":17534,"qual":17535,"reader":17536,"areas2":17537,"Gen":17538,"exceptions":17539,"motorbike":17540,"accelerate":17541,"lanes":17542,"modifications":17543,"erroneously":17544,"assessments":17545,"Datta":17546,"14a":17547,"Kalinin":17548,"converse":17549,"1966":17550,"Relatively":17551,"worryingly":17552,"Tobias":17553,"160":17554,"revoking":17555,"lacking":17556,"p2":17557,"sought":17558,"evolves":17559,"GR00T":17560,"6547":17561,"optimizing":17562,"Declassified":17563,"1995":17564,"volutions":17565,"Bylaw":17566,"𝜌":17567,"varadgunjal":17568,"Mohamed":17569,"371":17570,"426524":17571,"placeholders":17572,"visibility":17573,"Denil":17574,"De":17575,"Haffner":17576,"integrator":17577,"xy":17578,"Part":17579,"Sometimes":17580,"father":17581,"wages":17582,"Allowances":17583,"end":17584,"Primarily":17585,"asset":17586,"08e":17587,"transmitted":17588,"Weight":17589,"decided":17590,"ternatively":17591,"0.7089":17592,"dsweet":17593,"specifying":17594,"Surveillance":17595,"rid":17596,"holder":17597,"4830":17598,"Organizing":17599,"SIGIR":17600,"alleged":17601,"COXE":17602,"3359885631737537":17603,"ℓ":17604,"amples":17605,"handwritten":17606,"Hessian":17607,"with":17608,"MCCAFFERY":17609,"parents":17610,"subconscious":17611,"calculat":17612,"ShiftNet":17613,"Variation":17614,"Gain":17615,"watson":17616,"实现":17617,"TIARA":17618,"1233786579597858":17619,"larized":17620,"Interfaith":17621,"Redondo":17622,"Mine":17623,"OUR":17624,"Classification":17625,"performance":17626,"termed":17627,"Atlas":17628,"CumulativeDistributionTransform":17629,"Adopted":17630,"10998":17631,"34019846567238493":17632,"tinkering":17633,"二季":17634,"log2":17635,"Nuro":17636,"na":17637,"Hyperparameter":17638,"warranties":17639,"06990":17640,"6.3":17641,"buy":17642,"better":17643,"VOTE":17644,"Weirdly":17645,"8.5":17646,"我们":17647,"Armin":17648,"mitigate":17649,"causal":17650,"benefited":17651,"running":17652,"icon":17653,"convex":17654,"EEA":17655,"stacking":17656,"Geng":17657,"Greenawald":17658,"lemma":17659,"permits":17660,"lessons":17661,"subjectivity":17662,"includes":17663,"increments":17664,"gnawing":17665,"attentive":17666,"Imaging":17667,"tooling":17668,"Amended":17669,"tossing":17670,"cortical":17671,"fu":17672,"Symbol":17673,"rowed":17674,"Schwann":17675,"4110":17676,"utility":17677,"submit":17678,"patrols":17679,"submitting":17680,"resented":17681,"year":17682,"Groups":17683,"doubtful":17684,"Ryder":17685,"website":17686,"ext":17687,"Steering":17688,"royalties":17689,"immediate":17690,"compre":17691,"Finardi":17692,"applies":17693,"inadequate":17694,"cybersecurity":17695,"obs":17696,"corrections":17697,"augmenting":17698,"0.0030":17699,"ploys":17700,"Qualified":17701,"078":17702,"logdet":17703,"spikes":17704,"元":17705,"AMD":17706,"CVF":17707,"Normalization":17708,"PwC":17709,"Integration":17710,"rescales":17711,"effec":17712,"blur":17713,"169":17714,"Representing":17715,"Gkioxari":17716,"985":17717,"inscribe":17718,"discarded":17719,"Infrequent":17720,"disentangling":17721,"Fourier":17722,"79%":17723,"Newsl":17724,"Caption":17725,"yesterday":17726,"striking":17727,"Failure":17728,"MAY":17729,"distorted":17730,"passages":17731,"sentiment":17732,"Mr":17733,"4413":17734,"Codes":17735,"points":17736,"Industry":17737,"Confers":17738,"synthesizes":17739,"Khisamutdinov":17740,"terminate":17741,"formers":17742,"65":17743,"lib":17744,"setups":17745,"hallucination":17746,"multinational":17747,"recognizable":17748,"mization":17749,"insert":17750,"24900745354561854":17751,"2.4857":17752,"sto":17753,"worked":17754,"15370":17755,"crafting":17756,"Transition":17757,"227":17758,"circuits":17759,"𝑂":17760,"egocentric":17761,"Ampere":17762,"swift":17763,"tances":17764,"bernoulli":17765,"Enhancing":17766,"first":17767,"Due":17768,"lelism":17769,"predefined":17770,"facilitating":17771,"BananasDataset":17772,"trieval":17773,"restore":17774,"leading":17775,"chi2":17776,"out":17777,"Mucs":17778,"alleviates":17779,"serious":17780,"Medicine":17781,"entailment":17782,"prefer":17783,"arbitrarily":17784,"Connect":17785,"Shrimad":17786,"ents":17787,"Launay":17788,"inline":17789,"tok":17790,"132":17791,"accumulate":17792,"scalable":17793,"IST":17794,"possesses":17795,"MeanBackward0":17796,"millimeters":17797,"executing":17798,"DIVKNOWQA":17799,"delve":17800,"ilkermetinkursova":17801,"article174":17802,"MIT":17803,"REFSQL":17804,"tivation":17805,"named":17806,"spaces":17807,"Code":17808,"perfect":17809,"instrument":17810,"benchmark":17811,"desktop":17812,"inside":17813,"starters":17814,"eigenvector":17815,"655":17816,"Boser":17817,"join":17818,"RelaxedBernoulli":17819,"accumulates":17820,"U":17821,"firmware":17822,"old":17823,"1245":17824,"chang":17825,"inits":17826,"emitting":17827,"Image":17828,"PACs":17829,"looks":17830,"1483":17831,"initial":17832,"les":17833,"prefilter":17834,"297":17835,"m3":17836,"C3":17837,"Kaan":17838,"Spaces":17839,"Khattab":17840,"sake":17841,"upscaled":17842,"suggestions":17843,"12.10":17844,"UCSD":17845,"modules":17846,"5444":17847,"1.08":17848,"Properties":17849,"Yoder":17850,"attributable":17851,"liens":17852,"necessities":17853,"overheads":17854,"pollution":17855,"subfields":17856,"abroad":17857,"80":17858,"cancellation":17859,"saw":17860,"exercisability":17861,"占用":17862,"iden":17863,"enjoys":17864,"Malachowsky":17865,"1065":17866,"982":17867,"Proposing":17868,"Sorting":17869,"Oracle":17870,"Fechner":17871,"conversational":17872,"Ever":17873,"font":17874,"indicating":17875,"19.4":17876,"shareowners":17877,"758":17878,"transcend":17879,"tenfold":17880,"defaulting":17881,"lawfully":17882,"repository318":17883,"Supplemental":17884,"you":17885,"Uncertainty":17886,"literally":17887,"Brain":17888,"NAACL":17889,"Uniphase":17890,"expose":17891,"Discussions292":17892,"ylist":17893,"decomposable":17894,"countries":17895,"273":17896,"Abstentions":17897,"ini":17898,"Entities":17899,"pix":17900,"4660":17901,"Arista":17902,"Generalisable":17903,"meshgrid":17904,"13243":17905,"2000s":17906,"believe":17907,"Khandelwal":17908,"dictio":17909,"connoisseur":17910,"ssh":17911,"ix":17912,"671":17913,"li":17914,"tender":17915,"successful":17916,"01717":17917,"parallelization":17918,"su":17919,"Mietchen":17920,"plementing":17921,"occupies":17922,"overwrite":17923,"standardizes":17924,"Are":17925,"possibility":17926,"Divide":17927,"Numerical":17928,"ds":17929,"A100":17930,"naive":17931,"purpose":17932,"spite":17933,"stem":17934,"emb":17935,"Compute":17936,"LSTMs":17937,"setattr":17938,"distinguish":17939,"472":17940,"legal":17941,"324":17942,"appear":17943,"changing":17944,"facilitates":17945,"613":17946,"Experimental":17947,"ce":17948,"though":17949,"imizing":17950,"behav":17951,"Investigator":17952,"6912":17953,"densely":17954,"682":17955,"decom":17956,"𝑃":17957,"4573":17958,"article171":17959,"sitting":17960,"Reblitz":17961,"verbatim":17962,"652":17963,"failures":17964,"payments":17965,"dennismalmgren":17966,"encompasses":17967,"university":17968,"macOS":17969,"PATH":17970,"accordingly":17971,"0709":17972,"17th":17973,"please":17974,"这里":17975,"1076":17976,"notorious":17977,"6b":17978,"Percent":17979,"accepts":17980,"HyperPa":17981,"Monte":17982,"409A":17983,"simplify":17984,"networks":17985,"clarity":17986,"routines":17987,"Downloading":17988,"1388":17989,"accu":17990,"affiliated":17991,"Overview":17992,"restrict":17993,"How":17994,"keepdims":17995,"traverse":17996,"Vanschoren":17997,"regarding":17998,"telecom":17999,"dx":18000,"feeling":18001,"kill":18002,"metaverse":18003,"blow":18004,"Sch":18005,"Functional":18006,"Corrective":18007,"Worldwide":18008,"caring":18009,"125":18010,"approximately":18011,"D2L":18012,"transistors":18013,"发进":18014,"Shan":18015,"branching":18016,"0.485":18017,"Bowles":18018,"IoU":18019,"Meng":18020,"0.129":18021,"subdivision":18022,"ViewBackward0":18023,"A.32":18024,"633":18025,"3":18026,"doc2":18027,"plausibly":18028,"cleaning":18029,"spatially":18030,"volumes":18031,"flushed":18032,"represent":18033,"Geoff":18034,"Shun":18035,"3643":18036,"14942487313193167":18037,"CoRec":18038,"illegible":18039,"contexts":18040,"spacy":18041,"Jackel":18042,"1986":18043,"ImageFolder":18044,"FirstEnergy":18045,"hired":18046,"ppl":18047,"Decision":18048,"57":18049,"committed":18050,"pass":18051,"a0":18052,"exponentiating":18053,"Discussions17":18054,"820":18055,"punt":18056,"Handle":18057,"underneath":18058,"predic":18059,"4.1":18060,"pit":18061,"integral":18062,"Industrial":18063,"Division":18064,"corrective":18065,"remains":18066,"0.0986":18067,"48801815412811467":18068,"2000":18069,"legend":18070,"Hasan":18071,"9204":18072,"Tokenization":18073,"including":18074,"comma":18075,"265":18076,"exdb":18077,"1031":18078,"1560":18079,"encompassing":18080,"Nomination":18081,"Qualifications":18082,"consisted":18083,"Szegedy":18084,"Mangasarian":18085,"immigration":18086,"calculation":18087,"trig":18088,"Ong":18089,"Neocognitron":18090,"0740":18091,"corr":18092,"2519":18093,"fastText":18094,"ska":18095,"Metzen":18096,"052":18097,"Four":18098,"Approach":18099,"Musicians":18100,"selling":18101,"shown":18102,"manage":18103,"enabled":18104,"APIs":18105,"CorrCholeskyTransform":18106,"inplace":18107,"touches":18108,"unsecured":18109,"understanding":18110,"134":18111,"Ring":18112,"gravitational":18113,"deriving":18114,"Capitalization":18115,"cmf":18116,"reflected":18117,"A.14":18118,"peruse":18119,"min":18120,"11246":18121,"explores":18122,"mendations":18123,"276":18124,"Eds":18125,"division":18126,"Saad":18127,"72":18128,"motivation":18129,"empha":18130,"a":18131,"2990":18132,"3%":18133,"2205":18134,"Biswajit":18135,"VII":18136,"Chainer":18137,"Sheynin":18138,"wisdom":18139,"Fedus":18140,"does":18141,"Experi":18142,"Rfou":18143,"LLaMA":18144,"cuted":18145,"Agreements":18146,"\u0000":18147,"monetize":18148,"dicting":18149,"Sunday":18150,"dvincent1337":18151,"cuDF":18152,"Rabe":18153,"cifar10":18154,"Asian":18155,"getattr":18156,"Answering":18157,"2b2":18158,"uniformly":18159,"augs":18160,"ToolCoder":18161,"openai":18162,"allowing":18163,"200%":18164,"build":18165,"decouple":18166,"2346":18167,"Advancing":18168,"resilience":18169,"Excellence":18170,"Kress":18171,"452":18172,"Rahul":18173,"Hutchins":18174,"Forschungszentrum":18175,"show":18176,"nominate":18177,"agrees":18178,"ec2":18179,"brains":18180,"Filtering":18181,"appli":18182,"195":18183,"Categorical":18184,"Malladi":18185,"nonlinearity":18186,"Aksitov":18187,"ave":18188,"Lucien":18189,"AutoML":18190,"vN":18191,"interviews":18192,"phenomenon":18193,"tection":18194,"库存":18195,"349":18196,"074104":18197,"testing":18198,"Nested":18199,"rewrite":18200,"yscale":18201,"ARM":18202,"dictatorial":18203,"situations":18204,"CD":18205,"Griewank":18206,"exchange":18207,"minimizer":18208,"adopted":18209,"governments":18210,"rainfall":18211,"trucking":18212,"numerically":18213,"reopening":18214,"44th":18215,"fatal":18216,"Course":18217,"pointer":18218,"HalfCauchy":18219,"Mishkin":18220,"prescribe":18221,"746":18222,"Interpublic":18223,"Tieleman":18224,"Maintain":18225,"memories":18226,"Convolu":18227,"3.85":18228,"detectable":18229,"Parameter":18230,"Shen":18231,"differentiate":18232,"1780":18233,"traded":18234,"correc":18235,"RIGHT":18236,"Willingness":18237,"Try":18238,"Tsipras":18239,"thick":18240,"几个":18241,"smallest":18242,"2GHz":18243,"Visualizing":18244,"sharpened":18245,"6875":18246,"forwarded":18247,"terov":18248,"Foundations":18249,"commands":18250,"En":18251,"place":18252,"boring":18253,"violations":18254,"fellow":18255,"%.":18256,"downside":18257,"Standards":18258,"alluded":18259,"compactly":18260,"km":18261,"classifiers":18262,"Petersen":18263,"riding":18264,"articles":18265,"fancier":18266,"Nakkiran":18267,"Images":18268,"32nd":18269,"tomized":18270,"b4":18271,"Build":18272,"opponents":18273,"part":18274,"tiple":18275,"recommendation":18276,"seasonality":18277,"Chairperson":18278,"copyright":18279,"Bernoulli":18280,"coupon":18281,"DistilBERT":18282,"preferential":18283,"Xie":18284,"dramatic":18285,"retrieve":18286,"Linear":18287,"WikiText":18288,"Retrievegan":18289,"0.299":18290,"synchro":18291,"eSports":18292,"Canny":18293,"halt":18294,"Discovery":18295,"1404876708984375":18296,"generations":18297,"Level":18298,"doomed":18299,"Illustrating":18300,"nears":18301,"4.3":18302,"invoke":18303,"Discussions161":18304,"favor":18305,"5934":18306,"Top":18307,"0278":18308,"07697":18309,"Synchromesh":18310,"chor":18311,"lifetime":18312,"0.98":18313,"Azoulay":18314,"diffu":18315,"granting":18316,"grown":18317,"bilizes":18318,"LLMs":18319,"Administration":18320,"4003":18321,"trim":18322,"force":18323,"biased":18324,"4608":18325,"Validated":18326,"Anirudh":18327,"wise":18328,"dress":18329,"transferable":18330,"tunes":18331,"Agent":18332,"May":18333,"mize":18334,"lock":18335,"resulted":18336,"906":18337,"Conflict":18338,"Ames":18339,"structureless":18340,"trastive":18341,"q1":18342,"factorize":18343,"industrial":18344,"unreasonably":18345,"incorpo":18346,"enhances":18347,"Bousquet":18348,"Chopra":18349,"Ponwitayarat":18350,"drugs":18351,"integrating":18352,"gamma1":18353,"Very":18354,"4169e":18355,"allies":18356,"data":18357,"Extra":18358,"Editing":18359,"能力":18360,"decomposes":18361,"pothesis":18362,"P6":18363,"LIBRARY":18364,"per":18365,"understandable":18366,"Origin":18367,"RRR":18368,"Billed":18369,"download":18370,"014628":18371,"Finnish":18372,"complaints":18373,"bridging":18374,"02":18375,"Provisions":18376,"informs":18377,"concerning":18378,"Patterson":18379,"scheduling":18380,"alternates":18381,"converts":18382,"issue":18383,"elif":18384,"fiction":18385,"misplaced":18386,"Recovery":18387,"resid":18388,"0.109":18389,"reg":18390,"3GB":18391,"226":18392,"0.05":18393,"sequence":18394,"Arizona":18395,"total":18396,"l":18397,"unlikely":18398,"1105":18399,"221":18400,"gi":18401,"shortening":18402,"queue":18403,"commonplace":18404,"tackle":18405,"begs":18406,"Would":18407,"739":18408,"favorable":18409,"initiated":18410,"pathway":18411,"p3.2":18412,"left":18413,"handling":18414,"626":18415,"central":18416,"Qualitatively":18417,"null":18418,"page":18419,"报告":18420,"profitability":18421,"转发":18422,"dimensions":18423,"spitting":18424,"全面完成":18425,"171314":18426,"Discussions74":18427,"Integral":18428,"3557":18429,"Hynix":18430,"semantic":18431,"ArXiv":18432,"filing":18433,"Receptive":18434,"advantageous":18435,"2913":18436,"shortages":18437,"Y1":18438,"Loop":18439,"filtering":18440,"Hardware":18441,"Adam":18442,"acos":18443,"SEER":18444,"Install":18445,"Ulm":18446,"0GB":18447,"halves":18448,"lint":18449,"Olshausen":18450,"2074":18451,"E2":18452,"licenses":18453,"Sufficient":18454,"7":18455,"defined":18456,"anatomical":18457,"attestation":18458,"striate":18459,"subsumes":18460,"Electronics":18461,"简单":18462,"6719":18463,"dtd":18464,"own":18465,"penalizes":18466,"34th":18467,"Replacing":18468,"zhao":18469,"SPANN":18470,"exec":18471,"Stojnic":18472,"Yuanxiang":18473,"298":18474,"06317157191455719":18475,"Gu":18476,"SegmentationClass":18477,"44":18478,"munity":18479,"directional":18480,"282":18481,"racial":18482,"dtypes":18483,"9912":18484,"79":18485,"details":18486,"HLT":18487,"dated":18488,"instantiation":18489,"ciency":18490,"remember":18491,"cations":18492,"Diao":18493,"cheating":18494,"Seattle":18495,"drew":18496,"comb":18497,"xxxiii":18498,"issues":18499,"interactively":18500,"Rink":18501,"RNNScratch":18502,"704":18503,"Igor":18504,"loops":18505,"Ohio":18506,"transfers":18507,"stochastic":18508,"graphical":18509,"revisited":18510,"7.2699":18511,"tutorial":18512,"underpowered":18513,"Support":18514,"processors":18515,"interactive":18516,"460":18517,"tests":18518,"Mitliagkas":18519,"Hills":18520,"710":18521,"Lower":18522,"derive":18523,"Copying":18524,"rectangles":18525,"Leases":18526,"Fiocco":18527,"verticals":18528,"↑":18529,"personnel":18530,"hurting":18531,"Simulateur":18532,"20e":18533,"mylaptop":18534,"theorems":18535,"Rewrite":18536,"ig":18537,"Carnegie":18538,"hotline":18539,"solve":18540,"wheels":18541,"initialize":18542,"kinks":18543,"1807":18544,"monly":18545,"Resdsql":18546,"初步":18547,"historically":18548,"transit":18549,"ineffectiveness":18550,"schemes":18551,"1536":18552,"double":18553,"pervasive":18554,"Beyond":18555,"prevented":18556,"technically":18557,"Department":18558,"forgetting":18559,"arsenals":18560,"concentrated":18561,"subvert":18562,"14130.125":18563,"Lipton":18564,"corr1d":18565,"remiss":18566,"DABIRI":18567,"Reporter":18568,"16543":18569,"566":18570,"Partners":18571,"personally":18572,"console":18573,"Analytic":18574,"mizes":18575,"LDM":18576,"unweighted":18577,"surrounding":18578,"proper":18579,"conclude":18580,"23433":18581,"6071":18582,"2g":18583,"x86":18584,"Total":18585,"traveller":18586,"及":18587,"derivations":18588,"cabularies":18589,"MS":18590,"Doing":18591,"521":18592,"reveal":18593,"vests":18594,"barriers":18595,"ordinate":18596,"tick":18597,"jit":18598,"Ee":18599,"explicit":18600,"getitem":18601,"anchez":18602,"Smallcap":18603,"rationale":18604,"tonzhang":18605,"Tangarajan":18606,"garter":18607,"Perrone":18608,"beautiful":18609,"virtuoso":18610,"stimulate":18611,"0.0360":18612,"ZEEKR":18613,"convertible":18614,"196101":18615,"xxxvi":18616,"BMM":18617,"histor":18618,"Map":18619,"percentages":18620,"DGL199":18621,"computing":18622,"logits":18623,"607":18624,"ignore":18625,"14.4":18626,"implications":18627,"06471":18628,"older":18629,"paved":18630,"whereas":18631,"59874":18632,"558":18633,"represents":18634,"Jin":18635,"pricing":18636,"synchronisation":18637,"Provost":18638,"modal":18639,"SLAC":18640,"00000":18641,"Discussions290":18642,"11782":18643,"quantifica":18644,"resort":18645,"Discussions142":18646,"linspace":18647,"1041":18648,"NVDA":18649,"Apart":18650,"P7":18651,"Lovelace":18652,"Abadie":18653,"Omeiza":18654,"Kaftan":18655,"cache":18656,"synaptic":18657,"Lapata":18658,"Taking":18659,"340198":18660,"Kalchbrenner":18661,"1077":18662,"ML":18663,"taxing":18664,"diagonally":18665,"formulate":18666,"Delete":18667,"oftentimes":18668,"union":18669,"underestimate":18670,"condense":18671,"或":18672,"multipliers":18673,"hr":18674,"TOC":18675,"differentiable":18676,"10666":18677,"inaccurate":18678,"penalized":18679,"8165":18680,"realization":18681,"summary":18682,"remote":18683,"aggregated":18684,"skew":18685,"Gotmare":18686,"intu":18687,"perceptron":18688,"runs":18689,"Lisp":18690,"limits":18691,"Sheet":18692,"Discussions150":18693,"God":18694,"memory":18695,".__":18696,"∀":18697,"Koyama":18698,"documentation291":18699,"scanning":18700,"thank":18701,"indus":18702,"vendor":18703,"Dialogue":18704,"570":18705,"自动":18706,"102":18707,"081":18708,"girlfriend":18709,"SoftmaxTransform":18710,"IOPs":18711,"accountants":18712,"considering":18713,"incomplete":18714,"shade":18715,"lurking":18716,"Language":18717,"5%":18718,"Hesslow":18719,"approximate":18720,"000073":18721,"Atishay":18722,"COM":18723,"Rusu":18724,"216":18725,"83%":18726,"Codet5":18727,"mv":18728,"DeCandia":18729,"somewhat":18730,"775":18731,"Anal":18732,"Convergence":18733,"trigram":18734,"formation":18735,"Lumentum":18736,"assisting":18737,"affected":18738,"lan":18739,"↩":18740,"ESPP":18741,"A.46":18742,"Bethge":18743,"Elliott":18744,"第二段":18745,"beliefs":18746,"Fuma":18747,"cific":18748,"multidimensional":18749,"Notebooks":18750,"ylabel":18751,"0.2":18752,"highway":18753,"mirrored":18754,"husband":18755,"interspersing":18756,"spanning":18757,"Efficient":18758,"minimizes":18759,"947":18760,"Archimedes":18761,"900037":18762,"Imagnet":18763,"bidirec":18764,"Feature":18765,"children":18766,"Wiley":18767,"incurred":18768,"Return":18769,"fmap":18770,"blazingly":18771,"Define":18772,"Var":18773,"functions":18774,"Scaling":18775,"component":18776,"nutshell":18777,"Notes":18778,"happens":18779,"intricate":18780,"approves":18781,"mediated":18782,"gems":18783,"excluding":18784,"music":18785,"01000":18786,"European":18787,"grows":18788,"imag":18789,"RAMP":18790,"Sons":18791,"多栏":18792,"interme":18793,"GenTKGQA":18794,"Discussions18":18795,"Connectivity":18796,"NLLLoss":18797,"parallelogram":18798,"downturns":18799,"rmsprop":18800,"this":18801,"Liang":18802,"RohanKarthikeyan":18803,"constructed":18804,"gritty":18805,"Wave":18806,"sadly":18807,"SMOLA":18808,"Spoken":18809,"mobile":18810,"Rev":18811,"文字":18812,"redundancy":18813,"DeiT":18814,"giving":18815,"SportsTHU":18816,"PCAOB":18817,"budget":18818,"predecessor":18819,"Exhibits":18820,"persists":18821,"228":18822,"unscrupulous":18823,"PipeRAG":18824,"musical":18825,"487":18826,"iterate":18827,"amortize":18828,"COMPSTAT":18829,"replicating":18830,"peaks":18831,"biologically":18832,"RAS":18833,"volatile":18834,"conceived":18835,"Anil":18836,"nowhere":18837,"10483132064775551":18838,"maxima":18839,"digital":18840,"Physical":18841,"Society":18842,"duplicate":18843,"ables":18844,"Rawat":18845,"Historical":18846,"asymmetric":18847,"editing":18848,"12869":18849,"respect":18850,"Cornia":18851,"8.3":18852,"unearned":18853,"Beltagy":18854,"possession":18855,"Dealing":18856,"chief":18857,"BLEU":18858,"standardize":18859,"Waldo":18860,"Fully":18861,"underpinned":18862,"Similarly":18863,"capacity":18864,"49":18865,"comfortably":18866,"301":18867,"drops":18868,"−":18869,"ResNetBlock":18870,"Certificates":18871,"decoder":18872,"ASAP":18873,"lions":18874,"collate":18875,"Chowdhery":18876,"eyeball":18877,"NIVIDA":18878,"personal":18879,"MVAPICH2":18880,"dense1":18881,"associating":18882,"awesome":18883,"pursuant":18884,"PLAINER":18885,"chose":18886,"27e":18887,"Almeida":18888,"predict":18889,"violation":18890,"pause":18891,"existing":18892,"Enhancement":18893,"A.13":18894,"inhibition":18895,"Mutex":18896,"unclear":18897,"approxi":18898,"Volatility":18899,"wanted":18900,"11.93":18901,"Infusing":18902,"noncancelable":18903,"Segments":18904,"039036":18905,"autoencoder":18906,"Could":18907,"lay":18908,"RegNet":18909,"horse":18910,"relkt":18911,"Exploration":18912,"Inflation":18913,"fell":18914,"Historically":18915,"sarial":18916,"centred":18917,"55":18918,"serialize":18919,"flattening":18920,"Inc":18921,"Bol":18922,"era":18923,"shares":18924,"Calculus":18925,"shrink":18926,"aerospace":18927,"dividends":18928,"wants":18929,"involves":18930,"motivating":18931,"DDR":18932,"sim":18933,"Xing":18934,"1713":18935,"gpy":18936,"ln":18937,"lays":18938,"fly":18939,"storm":18940,"Sarbanes":18941,"22nd":18942,"CEO":18943,"convolves":18944,"UNK":18945,"accelerates":18946,"595":18947,"Brooke":18948,"splits":18949,"Intelli":18950,"Pat":18951,"route":18952,"asserting":18953,"icl":18954,"random":18955,"产品名":18956,"2%":18957,"alert":18958,"Column":18959,"DALI":18960,"stunningly":18961,"Chunk":18962,"estima":18963,"Concatenation":18964,"p4":18965,"export":18966,"Inside":18967,"Events":18968,"healthcare":18969,"desk":18970,"backing":18971,"Jaedong":18972,"Rezende":18973,"Dachshunds":18974,"sided":18975,"Vicente":18976,"arriving":18977,"21%":18978,"adjusted":18979,"last":18980,"Snappy":18981,"employees":18982,"rectangu":18983,"NIMs":18984,"low":18985,"impose":18986,"sad":18987,"This":18988,"RALM":18989,"Certification":18990,"pretending":18991,"outer":18992,"gathers":18993,"signifying":18994,"1091":18995,"codify":18996,"1.4887":18997,"interac":18998,"Reitter":18999,"6213":19000,"classification":19001,"Darrell":19002,"电子":19003,"Points":19004,"Extraordinary":19005,"Poggio":19006,"ther":19007,"Document":19008,"73":19009,"brittle":19010,"Qualitative":19011,"Techni":19012,"spotting":19013,"SURGE":19014,"tiny":19015,"whitened":19016,"widths":19017,"available":19018,"superresolution":19019,"Kingma":19020,"0899":19021,"Inves":19022,"outlet":19023,"Display":19024,"mobility":19025,"surfaces":19026,"B.4":19027,"branch":19028,"Cockpit":19029,"Eigendecompositions":19030,"Love":19031,"dont":19032,"Issuance":19033,"884":19034,"aided":19035,"i":19036,"Participation":19037,"linkedin":19038,"adaptations":19039,"ever":19040,"4032":19041,"servers":19042,"Larsen":19043,"Veness":19044,"constant":19045,"Robot":19046,"gains":19047,"neighborhood":19048,"Lead":19049,"Poloczek":19050,"relationships":19051,"Kurniawan":19052,"GoogLeNet":19053,"additional":19054,"considered":19055,"234":19056,"510":19057,"guessed":19058,"contains":19059,"Gar":19060,"NetApp":19061,"766":19062,"mlxd":19063,"discontinue":19064,"charge":19065,"memberships":19066,"RTRL":19067,"guide":19068,"512":19069,"223485":19070,"Snack":19071,"direct":19072,"posing":19073,"Help":19074,"programmable":19075,"expected":19076,"overseen":19077,"surpris":19078,"Think":19079,"AbsTransform":19080,"880":19081,"McClenny":19082,"noisiness":19083,"CRUD":19084,"ROBERTSON":19085,"Story":19086,"astray":19087,"Amortized":19088,"Mattersight":19089,"generalizing":19090,"LLC":19091,"currentframe":19092,"Concatenating":19093,"delicious":19094,"shines":19095,"weird":19096,"DSL":19097,"Multilayer":19098,"Accordingly":19099,"analogy":19100,"A.2":19101,"rejects":19102,"commonality":19103,"quadratically":19104,"186485":19105,"prohibitions":19106,"mals":19107,"combining":19108,"family":19109,"0129":19110,"hiring":19111,"02962":19112,"Report":19113,"Average":19114,"misclassified":19115,"unfavorable":19116,"EmbeddingBackward0":19117,"princi":19118,"Separate":19119,"Instead":19120,"2081":19121,"avail":19122,"WE":19123,"finish":19124,"summarizing":19125,"Storyboard":19126,"Factor":19127,"1004":19128,"Installed":19129,"molecule":19130,"supportive":19131,"∈":19132,"Wang":19133,"Optimized":19134,"751":19135,"vzlamal":19136,"particles":19137,"Xbox":19138,"9.1":19139,"Martins":19140,"Fernando":19141,"Repeat":19142,"absurd":19143,"5034":19144,"Installation":19145,"frisbees":19146,"tistical":19147,"tries":19148,"PERRY":19149,"Hossain":19150,"bz":19151,"5366":19152,"Kwasny":19153,"keeping":19154,"spatial":19155,"Kini":19156,"guesses":19157,"aleatoric":19158,"translation":19159,"®":19160,"3414":19161,"tolerance":19162,"breaches":19163,"alternatives":19164,"Kubernetes":19165,"placing":19166,"turn":19167,"known":19168,"discard":19169,"over":19170,"plane":19171,"checkpoint":19172,"Neurips":19173,"gca":19174,"cuda":19175,"1010":19176,"ROBERT":19177,"cess":19178,"temporal":19179,"redefined":19180,"OREOLM":19181,"telescopes":19182,"tured":19183,"Sifre":19184,"Gabriel":19185,"char":19186,"1.74":19187,"qualified":19188,"extrapolate":19189,"edited":19190,"SOFTWARE":19191,"bedrooms":19192,"trucks":19193,"1046":19194,"expert":19195,"PLX":19196,"TIP":19197,"cclauss":19198,"rely":19199,"HW":19200,"enjoy":19201,"Wxi":19202,"knew":19203,"Animator":19204,"212":19205,"pick":19206,"Giannis":19207,"uence":19208,"figsize":19209,"MFLOPs":19210,"1948":19211,"limitation":19212,"meticulous":19213,"Arks":19214,"ternational":19215,"436":19216,"Recently":19217,"tokenized":19218,"educator":19219,"reviews":19220,"070776":19221,"S":19222,"Validating":19223,"histogram":19224,"𝐺":19225,"FP64":19226,"List":19227,"coincidence":19228,"Confer":19229,"pharmaceutical":19230,"/":19231,"usefulness":19232,"comprises":19233,"ACM":19234,"Preface":19235,"Tra":19236,"Besta":19237,"UgurKap":19238,"strictions":19239,"Tagging":19240,"cheats":19241,"0.5723":19242,"Sigma":19243,"eigendecomposition":19244,"VGNMN":19245,"psychology":19246,"Example":19247,"participates":19248,"1983":19249,"heads":19250,"cluding":19251,"sunglasses":19252,"ethnic":19253,"drivers":19254,"ev":19255,"aptly":19256,"15":19257,"reliability":19258,"970":19259,"carry":19260,"Accrued":19261,"ters":19262,"databases":19263,"dropout1":19264,"Employer":19265,"examination":19266,"paying":19267,"tendency":19268,"contradicts":19269,"Aquarium":19270,"pool2d":19271,"A.7":19272,"991":19273,"Den":19274,"Possessing":19275,"modifi":19276,"Houlsby":19277,"logarithmic":19278,"rons":19279,"People":19280,"hard":19281,"loop":19282,"Prentice":19283,"Sym":19284,"07909":19285,"133%":19286,"indexing":19287,"𝜏":19288,"eminently":19289,"performs":19290,"powerful":19291,"3d":19292,"929":19293,"xz":19294,"Problematically":19295,"17.1":19296,"formatted":19297,"Visit":19298,"caps":19299,"0.884":19300,"z":19301,"Hebrew":19302,"clustering":19303,"rank":19304,"paper":19305,"Yogi":19306,"Common":19307,"Acquisition":19308,"nuisance":19309,"Causal":19310,"selec":19311,"load":19312,"gemini":19313,"diet":19314,"Shuffle":19315,"Hoeffding":19316,"gra":19317,"competent":19318,"Comma":19319,"RSU":19320,"French":19321,"loch":19322,"prioritizes":19323,"dareddygari":19324,"effect":19325,"sitional":19326,"Tencent":19327,"pedagogical":19328,"1412":19329,"US":19330,"evant":19331,"moi":19332,"tilling":19333,"Continental":19334,"Interlocks":19335,"disappear":19336,"forecast":19337,"SIMD":19338,"Web":19339,"similarities":19340,"simulated":19341,"NVLINK":19342,"scan":19343,"holdings":19344,"her":19345,"boundary":19346,"Fractional":19347,"perspective":19348,"innovative":19349,"Augmentation":19350,"specific":19351,"men":19352,"cp":19353,"Northern":19354,"compromise":19355,"erly":19356,"Gogh":19357,"compromises":19358,"publicity":19359,"Worker":19360,"imread":19361,"0522":19362,"covenants":19363,"spectral":19364,"domestic":19365,"Smola":19366,"consummate":19367,"Archambeau":19368,"316":19369,"pseudoran":19370,"experienced":19371,"reinstate":19372,"hess":19373,"used":19374,"Rtlfixer":19375,"sources":19376,"identi":19377,"Scalars":19378,"preserve":19379,"Conse":19380,"Problems":19381,"ligands":19382,"recast":19383,"Secrets":19384,"elicit":19385,"069":19386,"flatten":19387,"pitfall":19388,"ambitious":19389,"institutional":19390,"18450":19391,"Care":19392,"excellent":19393,"clude":19394,"strong":19395,"0.85":19396,"undergraduate":19397,"shortly":19398,"FitzGerald":19399,"tail":19400,"conducting":19401,"sustained":19402,"3175":19403,"Distribution":19404,"Trustworthy":19405,"guistic":19406,"mul":19407,"Laplace":19408,"diversity":19409,"Av":19410,"functionality":19411,"Iteration":19412,"invokes":19413,"Children":19414,"tackled":19415,"Imagine":19416,"STEPHEN":19417,"0.73":19418,"Basic":19419,"indexer":19420,"Alto":19421,"gual":19422,"broadly":19423,"inflected":19424,"Performance":19425,"374":19426,"foreign":19427,"进行":19428,"fast":19429,"bandits":19430,"ap":19431,"Multinomial":19432,"module":19433,"fractional":19434,"talked":19435,"fn":19436,"recalled":19437,"calibrated":19438,"asyn":19439,"Axiomatic":19440,"984":19441,"digitalize":19442,"xxvi":19443,"prefixes":19444,"polygons":19445,"scripted":19446,"voting":19447,"parametrization":19448,"Plotting":19449,"grad":19450,"EAE":19451,"troduction":19452,"identifies":19453,"atomic":19454,"759":19455,"links":19456,"mkdir":19457,"dollars":19458,"rotated":19459,"96":19460,"1430":19461,"choosing":19462,"formulae":19463,"Stability":19464,"server":19465,"Inst":19466,"Zou":19467,"infoNCE":19468,"doesn":19469,"daunting":19470,"exist":19471,"Volatile":19472,"decreasing":19473,"Ope":19474,"Nexus":19475,"Scalar":19476,"male":19477,"toregressive":19478,"kinds":19479,"enrich":19480,"7.1":19481,"educate":19482,"Theano":19483,"acquirer":19484,"76%":19485,"Hazan":19486,"407285":19487,"Climate":19488,"Increases":19489,"brevity":19490,"profiling":19491,"2X":19492,"Vanhoucke":19493,"indexed":19494,"service":19495,"Acoustics":19496,"voluntarily":19497,"1710":19498,"amortized":19499,"trun":19500,"gas":19501,"2.7434":19502,"shipping":19503,"recursively":19504,"sauce":19505,"exponentiation":19506,"𝑠":19507,"CPU":19508,"thought":19509,"targeting":19510,"149558":19511,"len":19512,"Tim":19513,"controllers":19514,"Accumulated":19515,"commutative":19516,"key":19517,"1991":19518,"principals":19519,"familiarity":19520,"Adobe":19521,"inspect":19522,"composes":19523,"A.40":19524,"stale":19525,"PROCESSING":19526,"1308":19527,"fostering":19528,"Stories":19529,"sions":19530,"sibly":19531,"584":19532,"Available":19533,"2621":19534,"Does":19535,"grammer":19536,"Basics":19537,"titled":19538,"acknowledge":19539,"806":19540,"Sermanet":19541,"I":19542,"Synergizing":19543,"detection":19544,"9079":19545,"STOCKHOLDERS":19546,"misgivings":19547,"installing":19548,"sporting":19549,"calculate":19550,"cheat":19551,"10997":19552,"777":19553,"inference":19554,"0.456":19555,"Instances":19556,"09":19557,"2301":19558,"Reportable":19559,"Epanechikov":19560,"nothing":19561,"11000":19562,"vectorize":19563,"mature":19564,"analog":19565,"30000":19566,"preliminaries":19567,"45":19568,"located":19569,"pursue":19570,"extremes":19571,"ends":19572,"Creating":19573,"watt":19574,"archaic":19575,"allocate":19576,"internet":19577,"VOC2012":19578,"Builders":19579,"1054":19580,"joke":19581,"ace":19582,"indulge":19583,"W3":19584,"Yanase":19585,"AffineTransform":19586,"Saharia":19587,"org":19588,"net1":19589,"generating":19590,"1400":19591,"newspapers":19592,"passive":19593,"Sivic":19594,"supplies":19595,"Protection":19596,"YEAR":19597,"更":19598,"Tayal":19599,"Incrementally":19600,"4400":19601,"profile":19602,"extracted":19603,"Cohesity":19604,"reset":19605,"Enterprises":19606,"sectional":19607,"belonging":19608,"snippets":19609,"our":19610,"Judgement":19611,"simulates":19612,"2826":19613,"encourages":19614,"Ramos":19615,"DecomposableAttention":19616,"hiding":19617,"employee":19618,"Rudin":19619,"neighborhoods":19620,"COSO":19621,"startups":19622,"amazonaws":19623,"determinant":19624,"corporations":19625,"production":19626,"016":19627,"molecules":19628,"At":19629,"logarithm":19630,"Lifu":19631,"governance":19632,"stochas":19633,"Lockheed":19634,"organizational":19635,"`":19636,"quantifies":19637,"facilitate":19638,"extract":19639,"Xavier":19640,"ordinal":19641,"category":19642,"301029995.6":19643,"elers":19644,"anywhere":19645,"tiation":19646,"asymp":19647,"Gururangan":19648,"revenue":19649,"Chu":19650,"Graber":19651,"reinstall":19652,"give":19653,"chairperson":19654,"Reclassification":19655,"Bojanowski":19656,"Intervals":19657,"moderate":19658,"ease":19659,"60000":19660,"Kaggle":19661,"Discussions275":19662,"concrete":19663,"41%":19664,"Inputs":19665,"Integrating":19666,"books5":19667,"dummy":19668,"growing":19669,"端":19670,"Cisco":19671,"clicking":19672,"Discussions283":19673,"failed":19674,"customized":19675,"coin":19676,"500K":19677,"FP16":19678,"acquire":19679,"ERATTA":19680,"gan":19681,"derivate":19682,"analogue":19683,"M1":19684,"Anchor":19685,"company":19686,"exposure":19687,"coefficient":19688,"inspiration":19689,"Neubig":19690,"1.1951":19691,"filler":19692,"P3":19693,"6040":19694,"fraction":19695,"abuse":19696,"odically":19697,"Discussions43":19698,"2806":19699,"analyticsindiamag":19700,"cate":19701,"subword":19702,"gpg":19703,"regions":19704,"18.3":19705,"vis":19706,"stemming":19707,"thanks":19708,"minimizers":19709,"dropout2":19710,"食品":19711,"≈":19712,"Eigenvalues":19713,"Vencu":19714,"solving":19715,"Higher":19716,"Koltun":19717,"⊥":19718,"s2":19719,"Primeneniya":19720,"4743":19721,"HalfNormal":19722,"inter":19723,"arm":19724,"matching":19725,"1980":19726,"b2":19727,"gyu":19728,"volutional":19729,"Claude":19730,"SIGPLAN":19731,"proposing":19732,"expiring":19733,"nets":19734,"RealSoftMax":19735,"Mangram":19736,"Mechanics":19737,"power":19738,"0.8":19739,"lect":19740,"pendencies":19741,"Future":19742,"increase":19743,"meat":19744,"arbitrary":19745,"formulaic":19746,"forwarding":19747,"755":19748,"368":19749,"bution":19750,"Smoothing":19751,"warehoused":19752,"timemachine":19753,"managers":19754,"breeds":19755,"1907":19756,"07285":19757,"surged":19758,"1064":19759,"446396":19760,"ViTMLP":19761,"believes":19762,"baseball":19763,"pad":19764,"evi":19765,"Computers":19766,"126259":19767,"REQUESTED":19768,"0314":19769,"aggregates":19770,"Watkins":19771,"eral":19772,"是":19773,"salut":19774,"94646ad1522d915e7b0f9296181140edcf86a4f5":19775,"bake":19776,"denies":19777,"3588":19778,"Remaining":19779,"recur":19780,"090b5e7e70c295757f55df93cb0a180b9691891a":19781,"lake":19782,"worthiness":19783,"Ninth":19784,"Lowe":19785,"nectivity":19786,"assuming":19787,"IN":19788,"replacement":19789,"mirror":19790,"Current":19791,"photographs":19792,"quences":19793,"Discussion":19794,"ule":19795,"slices":19796,"chess":19797,"Hussien":19798,"carries":19799,"HTML":19800,"Such":19801,"permissible":19802,"01652":19803,"highlighted":19804,"multilateral":19805,"DomKM":19806,"accessing":19807,"encountering":19808,"27001":19809,"Headings":19810,"854":19811,"pmi":19812,"huddling":19813,"Determinant":19814,"rat":19815,"squares":19816,"trol":19817,"NVIDIANs":19818,"yielded":19819,"pivots":19820,"contracting":19821,"Exemplar":19822,"inclines":19823,"Ultimately":19824,"Search":19825,"Popular":19826,"correct":19827,"mountains":19828,"fuel":19829,"added":19830,"OV":19831,"submissions":19832,"statute":19833,"subregions":19834,"B.6":19835,"beneficial":19836,"liberty":19837,"platforms":19838,"typos":19839,"Peter":19840,"sigmoid":19841,"end2end":19842,"subtensor":19843,"closet":19844,"Silicon":19845,"etin":19846,"3159":19847,"adversely":19848,"jonbally":19849,"framed":19850,"Below":19851,"noisy":19852,"vice":19853,"LN":19854,"wherever":19855,"illustration":19856,"acknowledged":19857,"16714":19858,"hierarchi":19859,"537":19860,"引擎":19861,"emphasis":19862,"spacing":19863,"unsophisticated":19864,"cart":19865,"event":19866,"Princeton":19867,"95110":19868,"fresh":19869,"leather":19870,"os":19871,"Related":19872,"clip":19873,"07883":19874,"indicates":19875,"2106":19876,"standardization":19877,"scope":19878,"registering":19879,"coder":19880,"700":19881,"Pradel":19882,"derstanding":19883,"surface":19884,"Delaware":19885,"SMALLCAP":19886,"9fcde07509c7e87ec61c640c1b2753d9041758e4":19887,"Either":19888,"rea":19889,"augmentation":19890,"flowing":19891,"1803":19892,"influenced":19893,"complemented":19894,"Political":19895,"TV":19896,"bility":19897,"vanilla":19898,"subwords":19899,"semantics":19900,"varied":19901,"DataParallel":19902,"5228":19903,"allude":19904,"boxcar":19905,"hoonose":19906,"Gaussian":19907,"configuration":19908,"stored":19909,"Global":19910,"violating":19911,"re":19912,"Janmey":19913,"causing":19914,"objectivity":19915,"Students":19916,"dilemmas":19917,"1c":19918,"headlines":19919,"EVP":19920,"wM":19921,"Colab302":19922,"Maiorca":19923,"his":19924,"pushed":19925,"968":19926,"6.5":19927,"outweighs":19928,"unaligned":19929,"funds":19930,"collapse":19931,"poker":19932,"neering":19933,"错误处理":19934,"poison":19935,"665":19936,"posium":19937,"resale":19938,"obviating":19939,"ghejc":19940,"competitor":19941,"perplexing":19942,"quantity":19943,"642":19944,"blue":19945,"influences":19946,"Spanish":19947,"normaliza":19948,"solved":19949,"ultimate":19950,"transformations":19951,"Malvern":19952,"939":19953,"JMLR":19954,"authorized":19955,"carefully":19956,"inherited":19957,"obligations":19958,"latents":19959,"REPORT":19960,"FC":19961,"block3":19962,"recognizability":19963,"tight":19964,"recipient":19965,"consultation":19966,"downsampling":19967,"tunnel":19968,"Iyyer":19969,"CoT":19970,"Sigmoids":19971,"gkutiel":19972,"1036":19973,"09237":19974,"Mount":19975,"extracting":19976,"until":19977,"obtains":19978,"Companies":19979,"confront":19980,"processes":19981,"2524":19982,"Refine":19983,"theorem177":19984,"squish":19985,"2f":19986,"0.5993":19987,"ICSME":19988,"news":19989,"Side":19990,"Shoeybi":19991,"eigenvalue":19992,"Endowment":19993,"matern":19994,"structures":19995,"Empowering":19996,"sampler":19997,"24900506":19998,"611":19999,"GluonCV":20000,"verse":20001,"Cie":20002,"2019":20003,"PERSIS":20004,"0909":20005,"reported":20006,"0.17762434482574463":20007,"Constructing":20008,"Focus":20009,"Rescal":20010,"immensely":20011,"0s":20012,"1206":20013,"swapping":20014,"ephemeral":20015,"Tied":20016,"Organize":20017,"multivariable":20018,"Novak":20019,"9.6":20020,"Connecting":20021,"SEAB":20022,"SemEval":20023,"rate":20024,"temptation":20025,"lam":20026,"simplifying":20027,"damaged":20028,"sandwiched":20029,"Juang":20030,"1006":20031,"desktops":20032,"continually":20033,"将":20034,"945":20035,"anniversary":20036,"lin2":20037,"586%":20038,"11903":20039,"2005":20040,"ples":20041,"strive":20042,"result":20043,"办公":20044,"mini2":20045,"Arts":20046,"argu":20047,"Early":20048,"Deletion":20049,"adage":20050,"tuning":20051,"recov":20052,"airplanes":20053,"Weighted":20054,"creativity":20055,"office":20056,"cui":20057,"slightly":20058,"FILCO":20059,"143":20060,"priorities":20061,"tandem":20062,"7096":20063,"8755":20064,"quake2005":20065,"intuitive":20066,"distinguished":20067,"Diagnostics":20068,"optimizations":20069,"repeating":20070,"𝑘":20071,"Gurgul":20072,"inbuilt":20073,"board":20074,"behalf":20075,"240":20076,"decidedly":20077,"Yashunin":20078,"biopharmaceuticals":20079,"visualize":20080,"Guarantees":20081,"6342":20082,"suits":20083,"affine":20084,"Bendersky":20085,"Gonina":20086,"Legal":20087,"menting":20088,"benchmarks":20089,"fed":20090,"Balso":20091,"08361":20092,"40e":20093,"problematic":20094,"Narayanamurthy":20095,"907":20096,"analytical":20097,"converge":20098,"Ukraine":20099,"FILED":20100,"Tbit":20101,"ReSKGC":20102,"sensitivity":20103,"beled":20104,"Discussions131":20105,"Redmon":20106,"workloads":20107,"indispensable":20108,"restated":20109,"multiplication":20110,"stretched":20111,"gammas":20112,"450":20113,"Science":20114,"workstation":20115,"13.7":20116,"sunset":20117,"405545":20118," ":20119,"mirac":20120,"dis":20121,"Commitment":20122,"KRG":20123,"imposes":20124,"forecasting":20125,"within":20126,"absolutely":20127,"a1":20128,"Lopez":20129,"literature":20130,"expect":20131,"scaled":20132,"instrumentation":20133,"effective":20134,"supports":20135,"addressing":20136,"Preferred":20137,"impacts":20138,"help":20139,"Markowitz":20140,"𝜼":20141,"Contributing":20142,"designed":20143,"ISVs":20144,"majority":20145,"Goetz":20146,"exercised":20147,"ramping":20148,"Annuity":20149,"Ventures":20150,"confusingly":20151,"taxation":20152,"ExponentialFamily":20153,"0.3":20154,"upwards":20155,"014076":20156,"adopting":20157,"composing":20158,"AddNorm":20159,"354":20160,"Expanded":20161,"Fusion":20162,"truth":20163,"Classifying":20164,"weigh":20165,"137080":20166,"catastrophic":20167,"Multiplication":20168,"10":20169,"frugal":20170,"derlying":20171,"Subsampling":20172,"motherboard":20173,"optical":20174,"350":20175,"urge":20176,"214":20177,"which":20178,"NLG":20179,"Teach":20180,"Keras":20181,"advanced":20182,"HGNN":20183,"347":20184,"underdetermined":20185,"reimagine":20186,"passage":20187,"Seawell":20188,"R":20189,"Nat":20190,"quires":20191,"humans":20192,"payout":20193,"monitors":20194,"73%":20195,"fmt":20196,"3544906":20197,"sum":20198,"Perhaps":20199,"Sequences":20200,"Samples":20201,"sneakers":20202,"1833":20203,"1508":20204,"3am":20205,"Pretrain":20206,"linguistics":20207,"nantekoto":20208,"behaving":20209,"Discussions121":20210,"xi":20211,"Ent":20212,"significant":20213,"Lucid":20214,"Jason":20215,"Removing":20216,"291":20217,"maker":20218,"”":20219,"strict":20220,"Pay":20221,"animator":20222,"25000":20223,"compete":20224,"Identi":20225,"faster":20226,"04487":20227,"563":20228,"featured":20229,"83":20230,"4783":20231,"08":20232,"shareholders":20233,"Acceleration":20234,"dotted":20235,"einsum":20236,"VC":20237,"Scale":20238,"916":20239,"Gonzalez":20240,"formerly":20241,"Java":20242,"lecture":20243,"SELF":20244,"\n":20245,"wiki":20246,"corrector":20247,"succinct":20248,"skip":20249,"gous":20250,"Enter":20251,"15391":20252,"py39":20253,"…":20254,"finality":20255,"Propose":20256,"World":20257,"indirectly":20258,"Preussische":20259,"disadvantages":20260,"5267":20261,"Spline":20262,"442":20263,"inappropriate":20264,"Read":20265,"Discussions218":20266,"Hon":20267,"0.0003":20268,"noting":20269,"Outside":20270,"944":20271,"grid":20272,"xii":20273,"trapped":20274,"redefines":20275,"code":20276,"collection":20277,"flavors":20278,"eta":20279,"Entering":20280,"Implementing":20281,"Weikum":20282,"BlenderBot3":20283,"Multimodal":20284,"optionally":20285,"speedup":20286,"lie":20287,"preconceived":20288,"pmf":20289,"relative":20290,"surfing":20291,"severance":20292,"03983":20293,"crosslingual":20294,"7.5":20295,"ai":20296,"leaps":20297,"ahead":20298,"dogs":20299,"MXNET":20300,"validation":20301,"DISCUSSION":20302,"bus":20303,"fullest":20304,"peri":20305,"identify":20306,"laptops":20307,"stalling":20308,"7072":20309,"role":20310,"diate":20311,"progressed":20312,"counteract":20313,"han":20314,"repository":20315,"conference":20316,"noticeable":20317,"Water":20318,"corr2d":20319,"2.5":20320,"Good":20321,"Advanced":20322,"warnings":20323,"A.38":20324,"directly":20325,"solution":20326,"humanity":20327,"SEP":20328,"7354":20329,"dialog":20330,"became":20331,"associations":20332,"Transformer":20333,"bullet":20334,"programmers":20335,"Rasul":20336,"Visual":20337,"proofs":20338,"hwM":20339,"www":20340,"pe":20341,"00610":20342,"would":20343,"calculations":20344,"diversifying":20345,"cater":20346,"conflicting":20347,"2029":20348,"Generator":20349,"cooling":20350,"6190":20351,"maximised":20352,"degree":20353,"Averages":20354,"Graham":20355,"ToPILImage":20356,"Wistuba":20357,"Switches":20358,"Achievements":20359,"Vin":20360,"contractually":20361,"Discussions306":20362,"8888":20363,"health":20364,"Recap":20365,"underflow":20366,"department":20367,"SURF":20368,"developement":20369,"device":20370,"throughout":20371,"avoca":20372,"lob":20373,"Correspondingly":20374,"15.2":20375,"computer":20376,"err":20377,"marking":20378,"0519":20379,"Eghbali":20380,"geopolitical":20381,"620":20382,"2088":20383,"gamers":20384,"isnan":20385,"ological":20386,"Sundar":20387,"Bradley":20388,"115584":20389,"Gregory":20390,"broker":20391,"holm":20392,"costly":20393,"2U":20394,"Identity":20395,"nisation":20396,"Ganguli":20397,"Projections":20398,"976":20399,"550":20400,"Number":20401,"rewrites":20402,"prevalences":20403,"ODIS":20404,"1.7":20405,"reranks":20406,"qualitatively":20407,"controlling":20408,"1114831485450576":20409,"collapsed":20410,"255":20411,"obtainable":20412,"exchanged":20413,"produces":20414,"trieves":20415,"combinations":20416,"deploying":20417,"Market":20418,"NegativeBinomial":20419,"unnormalized":20420,"axes":20421,"furniture":20422,"pcm":20423,"Recommends":20424,"probit":20425,"preparation":20426,"members":20427,"Multimedia":20428,"debugger":20429,"2235":20430,"839":20431,"equals":20432,"hosts":20433,"shocked":20434,"calcu":20435,"covari":20436,"appreciation":20437,"Geometry":20438,"assignments":20439,"unification":20440,"619":20441,"s":20442,"Shelhamer":20443,"shepherd":20444,"33GB":20445,"furthered":20446,"ß":20447,"Dual":20448,"Bayes":20449,"timely":20450,"tayfununal":20451,"Character":20452,"seeming":20453,"stricter":20454,"api":20455,"sheds":20456,"hire":20457,"arranging":20458,"monetary":20459,"prompting":20460,"90278":20461,"Beginner":20462,"Bioinspired":20463,"strongest":20464,"Formalize":20465,"equal":20466,"across":20467,"else":20468,"jake221":20469,"Discussions136":20470,"629":20471,"Exploding":20472,"1957":20473,"Mikhail":20474,"Placeholder":20475,"murky":20476,"CBMI":20477,"757":20478,"restriction":20479,"Flex":20480,"infeasible":20481,"meaningless":20482,"Almaden":20483,"subprocess":20484,"reparametrization":20485,"franchises":20486,"conceivable":20487,"5378":20488,"Sooner":20489,"WikiTextDataset":20490,"brush":20491,"png":20492,"Adjusting":20493,"lawllm":20494,"𝐽":20495,"928907":20496,"winners":20497,"perplexingly":20498,"sets":20499,"detecting":20500,"upside":20501,"shoulder":20502,"tagger":20503,"12652":20504,"shut":20505,"cat1":20506,"ransomware":20507,"299":20508,"section":20509,"resultant":20510,"effectively":20511,"motivated":20512,"checked":20513,"Evgeniy":20514,"contingent":20515,"layout":20516,"ani":20517,"Recognizing":20518,"nS":20519,"NAS":20520,"Statis":20521,"mini1":20522,"UniMS":20523,"Ax":20524,"865":20525,"Discussions229":20526,"Hint":20527,"sweet":20528,"747":20529,"draw":20530,"foreseeable":20531,"spiel":20532,"successor":20533,"Unsupervised":20534,"1936":20535,"ALEXANDER":20536,"Huskies":20537,"4d7d5b85e4537ad0c5d0a202623dcec5":20538,"The":20539,"321":20540,"duly":20541,"misconduct":20542,"cen":20543,"maximizing":20544,"ratio":20545,"Measure":20546,"alently":20547,"Divergence":20548,"intended":20549,"02811":20550,"pathways":20551,"engines":20552,"Zen":20553,"transformation":20554,"jumps":20555,"stinks":20556,"knn":20557,"ABSTAIN":20558,"Partly":20559,"contents":20560,"word2vec":20561,"liaison":20562,"Shift":20563,"predictors":20564,"Pena":20565,"Reducing":20566,"Stacking":20567,"STATES":20568,"817":20569,"Ioffe":20570,"UV":20571,"dollar":20572,"Draw":20573,"222814":20574,"performing":20575,"Statistical":20576,"Georg":20577,"popularized":20578,"2060":20579,"restarting":20580,"toolkit":20581,"Interpreters":20582,"fares":20583,"Payments":20584,"internalizing":20585,"rm":20586,"lookup":20587,"Ann":20588,"genomics":20589,"ReMoDiffuse":20590,"summation":20591,"texts":20592,"0805e":20593,"435":20594,"observed":20595,"iluu":20596,"pts":20597,"LaySum":20598,"ECC":20599,"0.693":20600,"9449":20601,"Parallelism":20602,"Encoding":20603,"approving":20604,"Discussions245":20605,"9923":20606,"birth":20607,"shifts":20608,"unwieldy":20609,"CNTK":20610,"nomination":20611,"Gram":20612,"Latency":20613,"studies":20614,"Readsum":20615,"Nonetheless":20616,"harm":20617,"rating":20618,"Petroni":20619,"7.2":20620,"Schwenk":20621,"FLOPs":20622,"nat":20623,"Summarization":20624,"Node":20625,"2.4754":20626,"582":20627,"Notably":20628,"Dwork":20629,"acts":20630,"sponsored":20631,"Going":20632,"stack":20633,"painfully":20634,"switched201":20635,"chez":20636,"Statistics":20637,"5cm":20638,"considerable":20639,"05578":20640,"ferentiate":20641,"CORPORATION":20642,"fewer":20643,"Synchronization":20644,"0.2353":20645,"modernize":20646,"variable":20647,"computa":20648,"5de26c8fce5ccdea9f91267273464dc968d20d72":20649,"2212":20650,"685":20651,"Rowen":20652,"enforced":20653,"accesses":20654,"rois":20655,"presentation":20656,"0.352":20657,"injected":20658,"592":20659,"mizations":20660,"Yuhong":20661,"parsimonious":20662,"11511":20663,"phone":20664,"Export":20665,"scientists":20666,"1059":20667,"144834":20668,"notes169":20669,"Liao":20670,"counting":20671,"eagerly":20672,"reducible":20673,"101":20674,"readable":20675,"ries":20676,"cord":20677,"1107":20678,"mol":20679,"Visually":20680,"signifies":20681,"Bapat":20682,"Khoshgoftaar":20683,"extractions":20684,"affiliates":20685,"12956867470224812":20686,"Tseng":20687,"13.5":20688,"unambiguous":20689,"Zhou":20690,"mark":20691,"dreds":20692,"Messrs":20693,"identically":20694,"1982":20695,"Channel":20696,"648":20697,"成果":20698,"0022":20699,"times":20700,"weeks":20701,"goal":20702,"physics96":20703,"Vanishing":20704,"SY":20705,"235":20706,"folds":20707,"chal":20708,"gracefully":20709,"Baraldi":20710,"Yue":20711,"provisioned":20712,"met":20713,"zebras":20714,"Saravia":20715,"Effective":20716,"Inclusive":20717,"diagram61":20718,"Fujimoto":20719,"choose":20720,"2d":20721,"lost":20722,"Discussions288":20723,"Kale":20724,"Town":20725,"168":20726,"p1":20727,"的":20728,"dismiss":20729,"flawed":20730,"Discriminator":20731,"±":20732,"concat":20733,"rect":20734,"dancing":20735,"In":20736,"𝑎":20737,"8115":20738,"cs":20739,"profiler":20740,"Package":20741,"mas":20742,"9328":20743,"integrated":20744,"Retirement":20745,"Karras":20746,"Wentao":20747,"Consequently":20748,"modern":20749,"getargvalues":20750,"02178":20751,"2302":20752,"1x":20753,"Lang":20754,"引言":20755,"writing":20756,"upsampled":20757,"Hamas":20758,"YOLO":20759,"0286":20760,"Bornea":20761,"mixture":20762,"Did":20763,"dat":20764,"tensorloader":20765,"pq2":20766,"2508":20767,"17.3":20768,"Practices":20769,"Pursuant":20770,"Gulwani":20771,"PCIe":20772,"2050":20773,"shuffled":20774,"67%":20775,"Partitioning":20776,"Troubling":20777,"bolded":20778,"assume":20779,"布局":20780,"06732":20781,"streaming":20782,"procurement":20783,"demand":20784,"astonishing":20785,"Prevent":20786,"pillars":20787,"Cambrian":20788,"inferences":20789,"0.6930":20790,"exhibits":20791,"11074":20792,"Gupta":20793,"Vandenberghe":20794,"embraced":20795,"Counter":20796,"freely":20797,"anticipation":20798,"epanechikov":20799,"stumble":20800,"CuDNN":20801,"wing":20802,"pasricha":20803,"progres":20804,"2805":20805,"0.1044":20806,"reflections":20807,"systematic":20808,"experimental":20809,"Applied":20810,"encryption":20811,"177":20812,"laborious":20813,"7500":20814,"57e":20815,"markdown":20816,"Terminate":20817,"cleaner":20818,"Rafael":20819,"obstacle":20820,"encompass":20821,"morphological":20822,"arises":20823,"tuition":20824,"equilibrates":20825,"arno":20826,"dog":20827,"landscape":20828,"Daniel":20829,"more":20830,"optimizes":20831,"half":20832,"converted":20833,"Share":20834,"well":20835,"578":20836,"Relations":20837,"Keep":20838,"alternatively":20839,"SECTION":20840,"675":20841,"encounter":20842,"refractions":20843,"imitation":20844,"asked":20845,"Shazeer":20846,"9.5":20847,"Wx":20848,"hi":20849,"protocol":20850,"dioxide":20851,"dec":20852,"Discussions183":20853,"2434":20854,"Too":20855,"untuned":20856,"Decreases":20857,"Catastrophic":20858,"embrace":20859,"formalism":20860,"3516":20861,"inability":20862,"conform":20863,"observing":20864,"labels":20865,"affection":20866,"1102":20867,"bidi":20868,"Venn":20869,"Road":20870,"recording":20871,"Property":20872,"GitHub":20873,"Algebra":20874,"RL":20875,"crucially":20876,"7544":20877,"eventual":20878,"ViTBlock":20879,"Rostamizadeh":20880,"chain":20881,"Ramachandra":20882,"2D":20883,"car":20884,"583":20885,"mitigation":20886,"bq":20887,"ncols":20888,"interconnected":20889,"Disclosure":20890,"Credit":20891,"84s":20892,"resistor":20893,"gentle":20894,"precedented":20895,"depth":20896,"annotation":20897,"6th":20898,"coercing":20899,"Nearest":20900,"Broadly":20901,"MatteoFerrara":20902,"association":20903,"ubyte":20904,"623":20905,"Missing":20906,"multitude":20907,"defer":20908,"scenes":20909,"diagram":20910,"rein":20911,"observant":20912,"Supreme":20913,"prob":20914,"Maheswaranathan":20915,"Discussions170":20916,"hoc":20917,"Reinwald":20918,"MICHAEL":20919,"Ð":20920,"张三":20921,"mainstays":20922,"类别":20923,"enthusiast":20924,"Arabia":20925,"controversy":20926,"modeling":20927,"operates":20928,"Whi":20929,"imitate":20930,"unbelievable":20931,"greedily":20932,"suitably":20933,"rush":20934,"global":20935,"sales":20936,"violence":20937,"hq":20938,"downfall":20939,"Pretraining":20940,"Conversation":20941,"Breed":20942,"imation":20943,"517":20944,"notational":20945,"Hotline":20946,"License":20947,"tenth":20948,"tripod":20949,"conferences":20950,"redeem":20951,"Discussions12":20952,"Psychophysik":20953,"Medina":20954,"reminder":20955,"Rrgcode":20956,"mlls":20957,"8877":20958,"Probabilistic":20959,"waldoness":20960,"LeakyReLU":20961,"Wainwright":20962,"avg":20963,"Discussions168":20964,"sophisti":20965,"iar":20966,"since":20967,"ups":20968,"imitations":20969,"Compensation":20970,"1609":20971,"Joint":20972,"early":20973,"sta":20974,"industries":20975,"960":20976,"naturally":20977,"sensing":20978,"422":20979,"0cm":20980,"outermost":20981,"theses":20982,"Tran":20983,"progresses":20984,"compilers":20985,"Identifies":20986,"Duvenaud":20987,"307":20988,"731":20989,"ues":20990,"decodings":20991,"SYN":20992,"Wei":20993,"communicate":20994,"AN":20995,"Goh":20996,"algebraic":20997,"bottleneck":20998,"2414":20999,"P8":21000,"hairy":21001,"heavily":21002,"Visualize":21003,"say":21004,"status":21005,"CI":21006,"Feedback":21007,"Salinas":21008,"690":21009,"029140":21010,"363":21011,"Eck":21012,"Amortization":21013,"orous":21014,"El":21015,"TagLM":21016,"prevent":21017,"Sample":21018,"mathematicians":21019,"7141":21020,"SCG":21021,"440":21022,"resilient":21023,"Generally":21024,"048":21025,"surpassed":21026,"feat":21027,"number":21028,"Independent":21029,"duet":21030,"PSUs":21031,"Discussions92":21032,"Shuangchi":21033,"walls":21034,"paramteres":21035,"Cuconasu":21036,"pan":21037,"Degree":21038,"600W":21039,"transfor":21040,"recoverable":21041,"spend":21042,"Pouplin":21043,"完成":21044,"Chaudhari":21045,"undone":21046,"burden":21047,"RandomCrop":21048,"sigmd":21049,"kg":21050,"potential":21051,"apparel":21052,"narrow":21053,"13461":21054,"alone":21055,"shall":21056,"inal":21057,"lies":21058,"forc":21059,"decade":21060,"IndependentTransform":21061,"Asso":21062,"0.5441":21063,"compressed":21064,"0.001":21065,"mediate":21066,"adversarial":21067,"4325":21068,"Riccardo":21069,"potted":21070,"holes":21071,"ere":21072,"Budget":21073,"Masked":21074,"happily":21075,"140488":21076,"Riemann":21077,"effort":21078,"目标":21079,"korean":21080,"51.633":21081,"lenient":21082,"Keyword":21083,"conse":21084,"k":21085,"trivial":21086,"picture":21087,"waste":21088,"area":21089,"Final":21090,"subtlety":21091,"244":21092,"HPOTrainer":21093,"upended":21094,"Kavosh":21095,"deci":21096,"cone":21097,"Initially":21098,"direc":21099,"Kokhlikyan":21100,"liberating":21101,"behaves":21102,"nlabel":21103,"Capacity":21104,"tying":21105,"hlines":21106,"Tables":21107,"staged":21108,"instances202":21109,"Emphasized":21110,"skeleton":21111,"governed":21112,"1732":21113,"tlesnakes":21114,"interventions":21115,"A77":21116,"views":21117,"Detection":21118,"Mercer":21119,"procedural":21120,"knock":21121,"attainment":21122,"specifications":21123,"outperform":21124,"regional":21125,"however":21126,"interconnects":21127,"multinoulli":21128,"sprinkled":21129,"Another":21130,"522":21131,"FURNISH":21132,"1061":21133,"factors":21134,"Finance":21135,"换行":21136,"obvi":21137,"utterance":21138,"fires":21139,"2223":21140,"Rule":21141,"fund":21142,"garbage":21143,"tractable":21144,"extrapolation":21145,"agg":21146,"preventative":21147,"bestows":21148,"baby":21149,"astoundingly":21150,"tencent":21151,"Ill":21152,"Gap":21153,"Dai":21154,"1882":21155,"plus":21156,"Debt":21157,"𝑏":21158,"physician":21159,"225d66f04cae318b841a13d32af3acc165f253ac":21160,"customize":21161,"policing":21162,"0614":21163,"formally":21164,"GILTI":21165,"exchanging":21166,"pipes":21167,"bypass":21168,"unrealized":21169,"821":21170,"04304":21171,"determined":21172,"RACE":21173,"lease":21174,"Medicines":21175,"vectorization":21176,"Sampling":21177,"1040":21178,"Revoke":21179,"Cajal":21180,"394":21181,"Volvo":21182,"refines":21183,"write":21184,"biomedical":21185,"RNNS":21186,"06125":21187,"13256":21188,"Interest":21189,"housing":21190,"catdog":21191,"9725":21192,"zx":21193,"10357":21194,"regulation":21195,"Inductive":21196,"associ":21197,"underscore":21198,"law":21199,"making":21200,"LaMP":21201,"17532":21202,"normalize":21203,"tr":21204,"str":21205,"technologies":21206,"nn":21207,"Storytelling":21208,"Command":21209,"Chief":21210,"C1":21211,"Rabinovich":21212,"12.4":21213,"digression":21214,"pull":21215,"replacing":21216,"Se":21217,"Devin":21218,"sary":21219,"Karpukhin":21220,"GHz":21221,"Agreement":21222,"real":21223,"zipping":21224,"crude":21225,"aiming":21226,"ubitiquous":21227,"retrospective":21228,"classify":21229,"80%":21230,"ine":21231,"10446":21232,"hensive":21233,"Or":21234,"son":21235,"508":21236,"ln2":21237,"whose":21238,"print":21239,"Eli":21240,"During":21241,"Context":21242,"Pong":21243,"nucleus":21244,"spanned":21245,"singletons":21246,"validating":21247,"xiii":21248,"esis":21249,"08252":21250,"senting":21251,"carried":21252,"omit":21253,"Logarithm":21254,"bags":21255,"perfectly":21256,"Equation":21257,"190":21258,"invoiced":21259,"behaviour":21260,"tokenizes":21261,"Discussions154":21262,"somewhere":21263,"kcj":21264,"clawback":21265,"comprehension":21266,"Unfortunately":21267,"inverted":21268,"Wqhh":21269,"Hadamard":21270,"demographics":21271,"entrepreneur":21272,"Interpretation":21273,"Bookkeeping":21274,"squared":21275,"MaskedSoftmaxCELoss":21276,"inclusive":21277,"1808":21278,"938":21279,"pertain":21280,"symmetric":21281,"prevails":21282,"2023a":21283,"dereference":21284,"799":21285,"Transparency":21286,"对":21287,"MDP":21288,"mail":21289,"categories":21290,"sentiments":21291,"07022":21292,"100k":21293,"generalizable":21294,"Discussions222":21295,"corresonding":21296,"K80":21297,"spells":21298,"Transla":21299,"Scaled":21300,"AIB":21301,"furnish":21302,"column":21303,"NOTICE":21304,"tier":21305,"Matias":21306,"vices":21307,"korfmann":21308,"VI":21309,"dynamics":21310,"3024":21311,"7539":21312,"8738":21313,"[":21314,"Houdt":21315,"illustrated":21316,"Maira":21317,"fractionally":21318,"0.7":21319,"goodwill":21320,"scenery":21321,"disposed":21322,"academic":21323,"子产":21324,"veloping":21325,"MY":21326,"TOP500":21327,"residency":21328,"Eventually":21329,"ging":21330,"Seeger":21331,"1600W":21332,"Fusing":21333,"Large":21334,"Merri":21335,"Implementation":21336,"CSUR":21337,"171":21338,"2493":21339,"rendering":21340,"avatars":21341,"13534":21342,"838":21343,"Jong":21344,"reordering":21345,"1Whz":21346,"tilde":21347,"wasn":21348,"wild":21349,"0.250":21350,"Adams":21351,"lr":21352,"erative":21353,"Eligible":21354,"reinstated":21355,"align":21356,"h3":21357,"20383":21358,"requests":21359,"didactic":21360,"2.0338":21361,"Portions":21362,"crepancy":21363,"guessing":21364,"它会":21365,"predecessors":21366,"pipeline":21367,"iteratively":21368,"CS":21369,"states":21370,"≥":21371,"plausibility":21372,"functional":21373,"adding":21374,"doctorcolossus":21375,"1950":21376,"dimensionalities":21377,"Hartshorn":21378,"reranker":21379,"overlooks":21380,"complementary":21381,"block1":21382,"Details":21383,"159662":21384,"A.17":21385,"Neal":21386,"professionalism":21387,"2324":21388,"Fortran":21389,"Until":21390,"awakened":21391,"Lowes":21392,"Simplify":21393,"121330":21394,"clunky":21395,"1286":21396,"amazon":21397,"torized":21398,"tionally":21399,"monoT5":21400,"just":21401,"Colmenarejo":21402,"8172":21403,"Chancery":21404,"parts":21405,"discounted":21406,"Fast":21407,"logarithms":21408,"Consider":21409,"participated":21410,"offsets":21411,"airfoil":21412,"row":21413,"exponentials":21414,"Gamers":21415,"trainLabels":21416,"Broadcasting":21417,"mismatches":21418,"Lukovenko":21419,"govern":21420,"277":21421,"prototypes":21422,"advances":21423,"nd":21424,"Discussions162":21425,"sharing":21426,"ball":21427,"Quantum":21428,"rearranging":21429,"developer":21430,"es":21431,"llms":21432,"wins":21433,"extensively":21434,"brokerage":21435,"OEMs":21436,"Reinvent":21437,"Sri":21438,"universal":21439,"dropped":21440,"strange":21441,"attention":21442,"Wqh":21443,"203052":21444,"test":21445,"tershed":21446,"BS":21447,"minimal":21448,"Zeshan":21449,"Stephen":21450,"Dops":21451,"reflects":21452,"00010":21453,"incorporation":21454,"strided":21455,"elle":21456,"Coordinating":21457,"–":21458,"From":21459,"Repofusion":21460,"288":21461,"0.002553":21462,"lar":21463,"unam":21464,"merely":21465,"plaguing":21466,"bmm":21467,"Specia":21468,"inde":21469,"teaches":21470,"Schockaert":21471,"Predictions":21472,"center":21473,"DLSS":21474,"438":21475,"testimony":21476,"xAx":21477,"0.86":21478,"Result":21479,"2c":21480,"simulations":21481,"WiFi":21482,"web":21483,"transact":21484,"NativeLayerNormBackward0":21485,"geometric":21486,"Repository":21487,"0.25":21488,"Guyon":21489,"whitespace":21490,"2423":21491,"ously":21492,"mal":21493,"implementing":21494,"expectation":21495,"retriev":21496,"2961":21497,"Problem":21498,"25088":21499,"introduce":21500,"debated":21501,"1756":21502,"\u0001":21503,"September":21504,"hop":21505,"currently":21506,"retrievers":21507,"multilingual":21508,"idiopathic":21509,"SIFT":21510,"excessive":21511,"Submitting":21512,"部分":21513,"chunking":21514,"ster":21515,"cow":21516,"Fifth":21517,"0.003":21518,"transforming":21519,"maximized":21520,"nistic":21521,"Mike":21522,"NotImplemented":21523,"Bench":21524,"170498071":21525,"WWW":21526,"Include":21527,"1456":21528,"both":21529,"achievable":21530,"2379":21531,"605":21532,"upload":21533,"\u0013":21534,"326":21535,"BFGS":21536,"Decoder":21537,"observes":21538,"Wood":21539,"promising":21540,"reach":21541,"gregate":21542,"lems":21543,"dimensonality":21544,"informative":21545,"Salut":21546,"vari":21547,"enabling":21548,"180":21549,"boasting":21550,"subnet":21551,"Bellman":21552,"once":21553,"sentence":21554,"enhancing":21555,"decelerated":21556,"tigation":21557,"276%":21558,"Equipment":21559,"transfer":21560,"0580":21561,"Making":21562,"slow":21563,"astonzhang":21564,"PageRank":21565,"initialization":21566,"Construction":21567,"0100":21568,"facing":21569,"moid":21570,"品名":21571,"derives":21572,"Synthetic":21573,"premise":21574,"errez":21575,"operational":21576,"Object":21577,"gear":21578,"423":21579,"electronically":21580,"asymptotically":21581,"yann":21582,"llm":21583,"Ge":21584,"advocacy":21585,"10.3":21586,"Physics":21587,"probabili":21588,"between":21589,"7.6":21590,"ucts":21591,"Pledging":21592,"N":21593,"allegedly":21594,"Lorenz":21595,"could":21596,"lin3":21597,"Heess":21598,"settle":21599,"modeled":21600,"revise":21601,"decades":21602,"476":21603,"MultivariateNormal":21604,"necessary":21605,"𝑇":21606,"643":21607,"CDFs":21608,"LazyBatchNorm2d":21609,"Sensing":21610,"Olivier":21611,"ingest":21612,"Resize":21613,"heading":21614,"Cortex":21615,"errors":21616,"ACL":21617,"sequent":21618,"hc":21619,"prev":21620,"1673":21621,"eters":21622,"0.4448":21623,"brokers":21624,"hardware":21625,"dropout":21626,"platters":21627,"recogni":21628,"Discussions233":21629,"arxiv":21630,"typified":21631,"displays":21632,"nnnnnInnnThe":21633,"23000":21634,"noring":21635,"extends":21636,"Rover":21637,"worlds":21638,"Scoring":21639,"mulates":21640,"core":21641,"Daisy":21642,"interleaving":21643,"comply":21644,"581%":21645,"06279":21646,"12317":21647,"Leary":21648,"players":21649,"Shoeb":21650,"polation":21651,"KNNLM":21652,"NIM":21653,"Robert":21654,"Pearson":21655,"Balakrishnan":21656,"France":21657,"postprocessing":21658,"pending":21659,"areas1":21660,"rw":21661,"mtn":21662,"together":21663,"Algorithms":21664,"repurchased":21665,"6.1":21666,"undersigned":21667,"Xcode":21668,"ny":21669,"Zan":21670,"computational":21671,"certain":21672,"0.194":21673,"Hudson":21674,"Keyboard":21675,"societies":21676,"univariate":21677,"Pilchin":21678,"tweaking":21679,"Elegant":21680,"V100":21681,"Disclosures":21682,"Vision":21683,"remove":21684,"129":21685,"Mellon":21686,"provide":21687,"interval":21688,"c065c0e2593b8b161a2d7873e42418bf6a21106c":21689,"laysum":21690,"movements":21691,"keyword":21692,"Kaixin":21693,"Conagra":21694,"context":21695,"Avila":21696,"Atari":21697,"NextSentencePred":21698,"Concentration":21699,"gaussian":21700,"Sticking":21701,"Eargo":21702,"Zeiler":21703,"strated":21704,"cleared":21705,"Covariate":21706,"Robertson":21707,"SPARQL":21708,"estimators":21709,"downloaded":21710,"0.242":21711,"intruders":21712,"software":21713,"Al":21714,"33%":21715,"temp":21716,"paste":21717,"benefits":21718,"society":21719,"listings":21720,"likewise":21721,"Align":21722,"SIGOPS":21723,"prospective":21724,"Discrete":21725,"Jordan":21726,"Fig":21727,"slide":21728,"1018":21729,"December":21730,"enables":21731,"Es":21732,"disclosure":21733,"mechanically":21734,"maturity":21735,"365970":21736,"Girshick":21737,"Exploring":21738,"permit":21739,"everyday":21740,"Waveform":21741,"74%":21742,"Lookup":21743,"sigmoids":21744,"Jia":21745,"foun":21746,"hand":21747,"GWM":21748,"VQA":21749,"8GB":21750,"11916":21751,"Hagstr":21752,"RECAP":21753,"Zemel":21754,"basics":21755,"Digital":21756,"trainer":21757,"picked":21758,"efforts":21759,"Venture":21760,"Engi":21761,"第一节":21762,"adjustable":21763,"Computational":21764,"shareholder":21765,"1106":21766,"GIL":21767,"bined":21768,"file":21769,"98":21770,"BLOOM":21771,"1822":21772,"Akademie":21773,"endorse":21774,"rem":21775,"mitigated":21776,"Divided":21777,"ecosystems":21778,"650":21779,"832":21780,"generaliza":21781,"abstractly":21782,"choices":21783,"intrinsic":21784,"responsible":21785,"mension":21786,"workhorse":21787,"theoretic":21788,"Newton":21789,"Nashid":21790,"Discussions239":21791,"Spam":21792,"Omnitab":21793,"Epidemiology":21794,"Repayment":21795,"aggressive":21796,"debugging":21797,"epsilon":21798,"CatTransform":21799,"belief":21800,"profits":21801,"that":21802,"grocery":21803,"lengthscale":21804,"183187":21805,"834111":21806,"1.5680":21807,"hopeful":21808,"mm":21809,"VLDB":21810,"1022":21811,"bulky":21812,"Palo":21813,"AT&T":21814,"'":21815,"unigram":21816,"Expectations":21817,"x16":21818,"niques":21819,"deal":21820,"forced":21821,"policymakers":21822,"977":21823,"Accountants":21824,"gion":21825,"Billions":21826,"adagrad":21827,"standardized":21828,"clean":21829,"nals":21830,"commerce":21831,"suis":21832,"proponent":21833,"Approximation":21834,"Kaplan":21835,"explainable":21836,"notice":21837,"occurrence":21838,"digitalization":21839,"codebert":21840,"unpack":21841,"overlapped":21842,"incentives":21843,"d2l":21844,"ers":21845,"alliances":21846,"Kansas":21847,"7132":21848,"Dyer":21849,"unified":21850,"Fees":21851,"Currency":21852,"press":21853,"Specify":21854,"readers":21855,"assump":21856,"excluded":21857,"money":21858,"hinges":21859,"NMS":21860,"loguniform":21861,"alter":21862,"bashrc":21863,"adursun":21864,"Orin":21865,"shard":21866,"expands":21867,"Ti":21868,"Copy":21869,"idx1":21870,"Duc":21871,"discriminating":21872,"10M":21873,"Influence":21874,"numbers":21875,"constraining":21876,"07641":21877,"sidering":21878,"0.97":21879,"credit":21880,"clients":21881,"created":21882,"467":21883,"parameter":21884,"movement":21885,"kwargs":21886,"Outline":21887,"degradation":21888,"zip":21889,"Updating":21890,"proportion":21891,"arguably":21892,"principally":21893,"unfold":21894,"Missiles":21895,"MONO":21896,"conferring":21897,"essential":21898,"token":21899,"bleak":21900,"altered":21901,"Advising":21902,"mystery":21903,"sheets":21904,"bboxes":21905,"549367":21906,"auditor":21907,"received":21908,"9803":21909,"Inherent":21910,"4.4":21911,"ongoing":21912,"latency":21913,"Public":21914,"Interna":21915,"7470":21916,"Express":21917,"SKR":21918,"concludes":21919,"服务器":21920,"working":21921,"slowed":21922,"921":21923,"gready":21924,"Xia":21925,"BYD":21926,"提升":21927,"institution":21928,"100359":21929,"ensemble":21930,"notices":21931,"network":21932,"Completion":21933,"Dabiri":21934,"strings":21935,"开发":21936,"Fixing":21937,"decision":21938,"NotebookApp":21939,"2121":21940,"pirical":21941,"sub":21942,"ask":21943,"Wide":21944,"reporting":21945,"Discussions221":21946,"017899":21947,"Quotient":21948,"superscripts":21949,"MSSubClass":21950,"symptoms":21951,"Activations":21952,"signed":21953,"14.7":21954,"5826":21955,"4300":21956,"licensing":21957,"^":21958,"instances300":21959,"2006":21960,"0000":21961,"most":21962,"physical":21963,"take":21964,"differing":21965,"breach":21966,"13th":21967,"practically":21968,"Requirements":21969,"misstatements":21970,"worse":21971,"concern":21972,"offered":21973,"1043":21974,"pixels":21975,"03360":21976,"colloquially":21977,"Vice":21978,"dispositions":21979,"lasso":21980,"allowance":21981,"worst":21982,"generalized":21983,"701":21984,"Chintala":21985,"10%":21986,"infinity":21987,"TDNNs":21988,"0276":21989,"thus":21990,"Co":21991,"SIGMOD":21992,"第三段":21993,"Mahalanobis":21994,"599":21995,"incompatible":21996,"attracting":21997,"parameters":21998,"quence":21999,"scarce":22000,"strokes":22001,"3819":22002,"MGX":22003,"helpfulness":22004,"exponentiated":22005,"unprecedented":22006,"ethical":22007,"validly":22008,"bare":22009,"7870":22010,"STATEMENT":22011,"slows":22012,"normalizations":22013,"ImageReadMode":22014,"Fargo":22015,"Ried":22016,"Nature":22017,"patients":22018,"observational":22019,"2014":22020,"2681":22021,"insurers":22022,"01211":22023,"0.459":22024,"¸":22025,"Philosophical":22026,"spaced":22027,"lenpred":22028,"Among":22029,"KD":22030,"Ying":22031,"understood":22032,"Judg":22033,"Jiekui":22034,"some":22035,"335":22036,"Crossover":22037,"webpage":22038,"appoints":22039,"FisherSnedecor":22040,"fluctuations":22041,"Zipfian":22042,"𝐵":22043,"plt":22044,"Discussions165":22045,"Revealing":22046,"cold":22047,"Operators":22048,"Triton":22049,"inverting":22050,"4KB":22051,"recruiting":22052,"RandomColorJitter":22053,"XXL":22054,"9245":22055,"Norm":22056,"observation":22057,"NIC":22058,"minute":22059,"substituting":22060,"☒":22061,"HWhq":22062,"defining":22063,"reimbursed":22064,"parentage":22065,"BatchNorm":22066,"debates":22067,"Impairments":22068,"frustrating":22069,"376":22070,"reluctant":22071,"McConchie":22072,"contributions":22073,"who":22074,"Tricks":22075,"classes":22076,"Pennock":22077,"conceiving":22078,"𝒔":22079,"education":22080,"PARC":22081,"4782":22082,"04":22083,"analyzed":22084,"paulaurel":22085,"engs":22086,"word":22087,"there":22088,"Lukasiewicz":22089,"Paszke":22090,"sample":22091,"refers":22092,"reimplemented":22093,"affect":22094,"parent":22095,"Book":22096,"Adadelta":22097,"6257":22098,"variables":22099,"requirements":22100,"constrains":22101,"RNNs":22102,"989":22103,"Keys":22104,"discards":22105,"1231":22106,"John":22107,"Sheets":22108,"ICL":22109,"Undeterred":22110,"afford":22111,"overfill":22112,"exerted":22113,"900009":22114,"humanoid":22115,"finance":22116,"7944855690002441":22117,"ward":22118,"ways":22119,"cables":22120,"popularity":22121,"10.1":22122,"sweaters":22123,"basically":22124,"byte":22125,"jancio":22126,"True":22127,"Motion":22128,"possi":22129,"Driving":22130,"Evoking":22131,"":22132,"Reserve":22133,"culture":22134,"Ignore":22135,"follow":22136,"enviroment":22137,"reportable":22138,"thereunder":22139,"ownership":22140,"honesty":22141,"grade":22142,"procedure":22143,"inters":22144,"Area":22145,"compare":22146,"Daylight":22147,"261541":22148,"imate":22149,"supercomputers":22150,"Abbeel":22151,"Equivalently":22152,"nizing":22153,"Frequency":22154,"directions":22155,"if":22156,"ng":22157,"3.1":22158,"Hershey":22159,"cyber":22160,"curvature":22161,"pension":22162,"facecolor":22163,"Discussions76":22164,"Pitassi":22165,"Pipeline":22166,"similarly":22167,"shrinkage":22168,"2705":22169,"Diagnostic":22170,"Diab":22171,"MTG":22172,"Ada":22173,"154":22174,"reorder":22175,"Microsoft":22176,"tune":22177,"Manufacturer":22178,"Applying":22179,"822":22180,"198":22181,"performed":22182,"，":22183,"P2":22184,"Researchers":22185,"Multiplying":22186,"tapped":22187,"Abalone":22188,"Unusual":22189,"orthogonal":22190,"1476":22191,"0.107":22192,"sional":22193,"Blume":22194,"expression":22195,"amount":22196,"placeholder":22197,"highly":22198,"updates":22199,"1.8":22200,"1072":22201,"related":22202,"1071":22203,"groups":22204,"major":22205,"engagement":22206,"Curve":22207,"Graphics":22208,"protease":22209,"deer":22210,"truncated":22211,"12253":22212,"intercept":22213,"Comparison":22214,"Casanova":22215,"contingencies":22216,"discovered":22217,"Giorn":22218,"3.0":22219,"H3":22220,"replies":22221,"schema":22222,"Restated":22223,"Melo":22224,"hold":22225,"2087":22226,"Hoyer":22227,"quan":22228,"piouw":22229,"won":22230,"Issues":22231,"salvage":22232,"sides":22233,"Complying":22234,"Tensors":22235,"Planck":22236,"Automated":22237,"again":22238,"Row":22239,"Shutterstock":22240,"adpated":22241,"Get":22242,"Resulting":22243,"Appendix4":22244,"Organizations":22245,"Asimov":22246,"Treadway":22247,"Douze":22248,"Inequality":22249,"Shared":22250,"appealing":22251,"output":22252,"Rademacher":22253,"processed":22254,"750":22255,"mnemonic":22256,"28881":22257,"RBFKernel":22258,"mistake":22259,"encouraged":22260,"us":22261,"Forcing":22262,"NFLX":22263,"composite":22264,"Human":22265,"Fiete":22266,"homework":22267,"notations":22268,"Optimization":22269,"delta":22270,"outperforming":22271,"wentao":22272,"9052":22273,"xh":22274,"Greys":22275,"mydict":22276,"Registered":22277,"IS":22278,"Aggarwal":22279,"adapting":22280,"0170":22281,"47th":22282,"alternate":22283,"69":22284,"desired":22285,"290":22286,"accessed":22287,"IF":22288,"Yann":22289,"uncontrolled":22290,"stranger":22291,"Activity":22292,"selecting":22293,"National":22294,"easy":22295,"bilingual":22296,"refinement":22297,"concretely":22298,"root":22299,"res":22300,"proud":22301,"Labeling":22302,"Jurisdictions":22303,"Budapest":22304,"removing":22305,"QARAG":22306,"HUDSON":22307,"ZACHARY":22308,"tensors":22309,"ually":22310,"⋯":22311,"789243":22312,"liweiwp":22313,"Logic":22314,"upper":22315,"respectively":22316,"elems":22317,"100s":22318,"static":22319,"Í":22320,"PROXY":22321,"MTFraEng":22322,"tran":22323,"setting":22324,"perimeter":22325,"Our":22326,"traversed":22327,"w1":22328,"conjecture":22329,"masking":22330,"needs":22331,"unsurprisingly":22332,"Rolnick":22333,"R1024":22334,"practitioner":22335,"22":22336,"terconnects":22337,"AVX2":22338,"elements":22339,"03903577426988046":22340,"b5116e234e9eb9076672cfeabf5469f3eec904fa":22341,"%%":22342,"proxies":22343,"piece":22344,"Qiu":22345,"242025":22346,"View":22347,"readers307":22348,"innovations":22349,"Hidden":22350,"declarative":22351,"417":22352,"2.8314":22353,"obvious":22354,"verifying":22355,"2238":22356,"Golub":22357,"Fuse":22358,"Patil":22359,"2537":22360,"Cyberspace":22361,"Oakley":22362,"Fumagalli":22363,"1053":22364,"Yao":22365,"Non":22366,"mammalian":22367,"Present":22368,"179":22369,"dramatically":22370,"practi":22371,"access":22372,"Whang":22373,"653":22374,"incident":22375,"ailments":22376,"10533":22377,"termediate":22378,"Tomasino":22379,"预期":22380,"automate":22381,"independently":22382,"cannot":22383,"transitioning":22384,"denominated":22385,"inner":22386,"browser":22387,"was":22388,"subroutines":22389,"ring":22390,"immediately":22391,"sine":22392,"Afterall":22393,"Otherwise":22394,"learned":22395,"Invoking":22396,"renormalizing":22397,"Annual":22398,"Assessment":22399,"1B":22400,"eleven":22401,"286225":22402,"SCCLLM":22403,"03b":22404,"gridworld":22405,"10524":22406,"staff":22407,"zhang":22408,"diffi":22409,"Security":22410,"Networking":22411,"appropriately":22412,"Schmid":22413,"steps":22414,"17e":22415,"920":22416,"104":22417,"sloppy":22418,"cal":22419,"Henri":22420,"comprising":22421,"corporation":22422,"𝐼":22423,"PAYOUTS":22424,"1110":22425,"APPROVED":22426,"Instagram":22427,"Minnesota":22428,"Path":22429,"ls":22430,"Ke":22431,"projected":22432,"optimal":22433,"smoothing":22434,"physically":22435,"None":22436,"Careful":22437,"3237":22438,"documented":22439,"543102":22440,"shopping":22441,"above":22442,"Bao":22443,"44%":22444,"critically":22445,"37th":22446,"unleashing":22447,"predicts":22448,"kicked":22449,"fit":22450,"regob":22451,"Weston":22452,"Discussions113":22453,"6580":22454,"Kolesnikov":22455,"capitalization":22456,"为":22457,"conditioning":22458,"Passage":22459,"flaps":22460,"circles":22461,"stated":22462,"Secondly":22463,"ell":22464,"PIL":22465,"Careil":22466,"tributed":22467,"escalate":22468,"Lafferty":22469,"rightward":22470,"ensuring":22471,"fool":22472,"perturbs":22473,"Assesses":22474,"tokenizer":22475,"Azure":22476,"SoftBank":22477,"Yang":22478,"Mind":22479,"imperfectly":22480,"determining":22481,"NEOs":22482,"hotels":22483,"ob":22484,"Cl":22485,"Lilly":22486,"explanations":22487,"appetite":22488,"attn":22489,"diseases":22490,"Trustees":22491,"pseudo":22492,"retain":22493,"Subbiah":22494,"Query":22495,"leverages":22496,"novella":22497,"Progressive":22498,"human":22499,"01431":22500,"981":22501,"489":22502,"GPU2":22503,"Ohishi":22504,"1912":22505,"Independence":22506,"2030":22507,"335989":22508,"mou":22509,"020":22510," ":22511,"devoting":22512,"2680":22513,"Belongie":22514,"07s":22515,"PTB":22516,"hint":22517,"A.33":22518,"612":22519,"reinforced":22520,"contentious":22521,"Retrieve":22522,"Obligations":22523,"vant":22524,"Papineni":22525,"OpenAI":22526,"namics":22527,"587":22528,"records":22529,"fruitful":22530,"hyperparame":22531,"Norms":22532,"acquiring":22533,"commonsense":22534,"ran":22535,"expense":22536,"Tests":22537,"digest":22538,"oranges":22539,"VidIL":22540,"hormone":22541,"consid":22542,"acquainted":22543,"flavor":22544,"02816":22545,"Discussions166":22546,"Qualification":22547,"689":22548,"1X":22549,"foundations":22550,"Cloning":22551,"rongruosong":22552,"Conduct":22553,"lection":22554,"established":22555,"downs":22556,"Nominees":22557,"9114":22558,"bh":22559,"CRM":22560,"__":22561,"reside":22562,"backyard":22563,"Track":22564,"2.1695":22565,"treated":22566,"setup":22567,"arbitrage":22568,"Real":22569,"Rearranging":22570,"Tuytelaars":22571,"readlines":22572,"9000097513198853":22573,"7484":22574,"heating":22575,"CrossEntropyLoss":22576,"248":22577,"Auto":22578,"rural":22579,"trices":22580,"fees":22581,"Observe":22582,"NaN":22583,"SoCs":22584,"Dickstein":22585,"mitigating":22586,"ney":22587,"through":22588,"model":22589,"For":22590,"zlim":22591,"Finamore":22592,"selectively":22593,"840":22594,"dardization":22595,"Accrual":22596,"emerged":22597,"Koley":22598,"basement":22599,"favorite":22600,"REFERENCES":22601,"arange":22602,"prerecorded":22603,"To":22604,"𝝁":22605,"cs229":22606,"1990s":22607,"providers":22608,"future":22609,"CPRA":22610,"02907":22611,"neutral":22612,"circa":22613,"abstract":22614,"32%":22615,"6.7":22616,"sic":22617,"Xiaomi":22618,"inactive":22619,"cosh":22620,"Ginibre":22621,"Clark":22622,"03374":22623,"histories":22624,"Raw":22625,"mem":22626,"quotients":22627,"Appl":22628,"undistributed":22629,"Chairman":22630,"improv":22631,"USB":22632,"harmlessness":22633,"simulator":22634,"editor":22635,"Obsolete":22636,"Methodologies":22637,"nonconvex":22638,"Jagerman":22639,"compatible":22640,"subtracted":22641,"gray":22642,"needed":22643,"digitization":22644,"Singapore":22645,"Weigh":22646,"Luby":22647,"trials":22648,"blogs":22649,"1.1025":22650,"incumbent":22651,"Cropping":22652,"generalize":22653,"Systems":22654,"og":22655,"Candidate":22656,"coarser":22657,"Taiwan":22658,"reconfigures":22659,"8763":22660,"Seq2Seq":22661,"Across":22662,"Many":22663,"Ratification":22664,"60%":22665,"topology":22666,"Diversity":22667,"intervening":22668,"1503420":22669,"summarizes":22670,"visualization":22671,"Natural":22672,"Research":22673,"1v":22674,"flush":22675,"Warde":22676,"serves":22677,"generates":22678,"Margin":22679,"Votes":22680,"finally":22681,"codified":22682,"lunch":22683,"eliminating":22684,"sanctity":22685,"difficulty":22686,"privacy":22687,"ulary":22688,"elite":22689,"succeed":22690,"round":22691,"unity":22692,"ringbus":22693,"Wahrscheinlichkeitstheorie":22694,"153":22695,"compensation":22696,"Manager":22697,"Discussions213":22698,"2Var":22699,"Discussions107":22700,"reinvent":22701,"cached":22702,"outperformed":22703,"151":22704,"unpredictable":22705,"Barnett":22706,"Structgpt":22707,"1082":22708,"clinical":22709,"roots":22710,"misuse":22711,"TF":22712,"verges":22713,"Ux":22714,"59":22715,"thou":22716,"privately":22717,"powers":22718,"zero":22719,"allowed":22720,"284":22721,"Give":22722,"lowing":22723,"Critical":22724,"095":22725,"posi":22726,"Atwood":22727,"unifies":22728,"LAB":22729,"FixedHiddenMLP":22730,"•":22731,"deployment":22732,"459":22733,"titioners":22734,"subse":22735,"unsatisfying":22736,"Probing":22737,"withholding":22738,"additions":22739,"multinomial":22740,"millions":22741,"initiatives":22742,"Sachan":22743,"1417":22744,"424":22745,"promises":22746,"Gao":22747,"Removal":22748,"07682":22749,"theta":22750,"dation":22751,"affords":22752,"TransformerDecoderBlock":22753,"Represents":22754,"Wxc":22755,"calls":22756,"VideoQA":22757,"Betty":22758,"923":22759,"ExpTransform":22760,"triques":22761,"Freeze":22762,"WRITING":22763,"Gulcehre":22764,"Title":22765,"mushroom":22766,"Peace":22767,"guidelines":22768,"Name":22769,"for":22770,"frequencies":22771,"Ba":22772,"definite":22773,"downtown":22774,"cer":22775,"dissemination":22776,"affirmative":22777,"Unit":22778,"Koh":22779,"𝑊":22780,"Tax":22781,"motivations":22782,"teaching":22783,"perfor":22784,"Growth":22785,"ResNet":22786,"2330":22787,"payroll":22788,"integrators":22789,"std":22790,"Libraries":22791,"dirichlet":22792,"persons":22793,"UP":22794,"ad":22795,"Plans":22796,"ii":22797,"Secretary":22798,"stats":22799,"motivate":22800,"1182":22801,"AnyNetC":22802,"qh":22803,"slowing":22804,"BERTEncoder":22805,"2208":22806,"笔记本":22807,"conditionals":22808,"pronouns":22809,"below":22810,"mandate":22811,"ANYWAY":22812,"infre":22813,"Chung":22814,"Tapestry":22815,"Incorporate":22816,"launch":22817,"an":22818,"RPRR":22819,"Izmailov":22820,"surviving":22821,"2788":22822,"gist205":22823,"visually":22824,"distributions":22825,"heart":22826,"PUs":22827,"roughly":22828,"Piktus":22829,"born":22830,"Rutherford":22831,"math":22832,"multiplica":22833,"Bio":22834,"dendrites":22835,"feedbacks":22836,"coelestum":22837,"1108":22838,"button":22839,"Specif":22840,"hardly":22841,"began":22842,"901":22843,"319":22844,"178":22845,"ourself":22846,"fire":22847,"639":22848,"concatenations":22849,"engages":22850,"consis":22851,"SDA":22852,"Maps":22853,"手机":22854,"Event":22855,"Improving":22856,"defend":22857,"reform":22858,"823":22859,"outfit":22860,"influ":22861,"ow":22862,"gt":22863,"743":22864,"nine":22865,"Reasoning":22866,"public":22867,"0.0137":22868,"Licenses":22869,"9131":22870,"abound":22871,"traveling":22872,"Electrical":22873,"191":22874,"2.2":22875,"Neurobiologically":22876,"PaLM":22877,"Decay":22878,"Times":22879,"interacting":22880,"subtract":22881,"073":22882,"selection":22883,"foundational":22884,"Consumer":22885,"Iterate":22886,"technology":22887,"028463":22888,"stores":22889,"804":22890,"proceed":22891,"流程":22892,"sketch":22893,"clear":22894,"mastery":22895,"closed":22896,"0821":22897,"increasingly":22898,"pmuens":22899,"530B":22900,"Automation":22901,"of":22902,"interfere":22903,"length":22904,"lambda":22905,"9026":22906,"MSE":22907,"visu":22908,"jump":22909,"applied":22910,"CUDA":22911,"Regulators":22912,"382563":22913,"patent":22914,"0.6958":22915,"divert":22916,"schedule":22917,"313873":22918,"Trustee":22919,"bent":22920,"Larger":22921,"strictest":22922,"Discussions296":22923,"Revels":22924,"isolating":22925,"explains":22926,"person":22927,"compressing":22928,"enumerating":22929,"1784":22930,"film":22931,"actors":22932,"893":22933,"nonetheless":22934,"Holdings":22935,"noiseless":22936,"requires":22937,"moves":22938,"279":22939,"polynomials":22940,"Concat":22941,"pendence":22942,"precise":22943,"investigated":22944,"reversible":22945,"Fallacy":22946,"sea":22947,"largely":22948,"astronomy":22949,"schedul":22950,"215":22951,"1021":22952,"coded":22953,"decent":22954,"hasattr":22955,"4990":22956,"assess":22957,"sizes":22958,"Chebyshev":22959,"MFLOPS":22960,"gpus":22961,"assign":22962,"James":22963,"leap":22964,"surrounds":22965,"20158":22966,"0.011485":22967,"negatives":22968,"964":22969,"outmoded":22970,"messages":22971,"extremely":22972,"Beam":22973,"good":22974,"trustees":22975,"Artisan":22976,"Jindal":22977,"document":22978,"thesized":22979,"Leonard":22980,"ADADELTA":22981,"了":22982,"REPOFUSE":22983,"长以":22984,"hexdigest":22985,"parse":22986,"types":22987,"ratify":22988,"0.15":22989,"Estimated":22990,"poral":22991,"chop":22992,"Rather":22993,"let":22994,"Timing":22995,"07597":22996,"ASTON":22997,"Discussions159":22998,"∪":22999,"timing":23000,"Akiki":23001,"𝒞":23002,"Sandholm":23003,"forcing":23004,"Denmark":23005,"vesting":23006,"artificially":23007,"opinion":23008,"grouping":23009,"catalyzed":23010,"Jacobian":23011,"certainly":23012,"829":23013,"flop":23014,"Less":23015,"implementation":23016,"Logging":23017,"版":23018,"driver":23019,"manufactured":23020,"representation":23021,"Fergus":23022,"exclusive":23023,"correlate":23024,"336183":23025,"Incorporation":23026,"ag":23027,"1963":23028,"Discussions54":23029,"subsidize":23030,"250":23031,"Prototype":23032,"⪅":23033,"iterators":23034,"---":23035,"exercise":23036,"differ":23037,"Sindhwani":23038,"Scope":23039,"王五":23040,"designation":23041,"recommendations":23042,"opinions":23043,"reduc":23044,"Their":23045,"0.0904":23046,"informational":23047,"creators":23048,"bayes":23049,"Networks":23050,"relatively":23051,"tinuously":23052,"school":23053,"A.30":23054,"Santhanam":23055,"disciplinary":23056,"nonzeros":23057,"Expense":23058,"titles":23059,"nities":23060,"Lipschitz":23061,"volume":23062,"Longformer":23063,"详细":23064,"pseudocode":23065,"kernels":23066,"exactly":23067,"Symphony":23068,"intersect":23069,"15884":23070,"accounting":23071,"trustworthy":23072,"909":23073,"14.11":23074,"图":23075,"takeover":23076,"071":23077,"14.2":23078,"said":23079,"AAAI":23080,"Accelerated":23081,"Ms":23082,"professionals":23083,"flattened":23084,"cludes":23085,"s3":23086,"2263":23087,"mentoring":23088,"laid":23089,"accountability":23090,"monotonic":23091,"marginalized":23092,"issuable":23093,"Suite":23094,"CODEGEN":23095,"ating":23096,"237":23097,"green":23098,"akash5474":23099,"ESAIM":23100,"layers":23101,"323":23102,"28%":23103,"Positional":23104,"Gasthaus":23105,"deactivate":23106,"runnable":23107,"1512":23108,"tain":23109,"Banach":23110,"trulens":23111,"portant":23112,"fc":23113,"saying":23114,"drastically":23115,"thrower":23116,"contradicting":23117,"11325":23118,"⌈":23119,"Kullback":23120,"regularizes":23121,"pace":23122,"Performing":23123,"concentrations":23124,"piecewise":23125,"AVX":23126,"publishers":23127,"AceCoder":23128,"Cognition":23129,"quel":23130,"McCaffery":23131,"repetitions":23132,"unsuitable":23133,"proprietary":23134,"953":23135,"0.0001":23136,"!":23137,"Simulation":23138,"There":23139,"camera":23140,"expansions":23141,"Friedman":23142,"lawful":23143,"Iscen":23144,"965":23145,"oriented":23146,"innocent":23147,"prime":23148,"dataloader":23149,"quantifying":23150,"steepest":23151,"disaggregate":23152,"Angles":23153,"Freebase":23154,"Cybenko":23155,"raising":23156,"Stopping":23157,"torques":23158,"connections":23159,"outdoors":23160,"Mu":23161,"1084":23162,"political":23163,"computers":23164,"5955":23165,"deliberately":23166,"families":23167,"argmax":23168,"7822":23169,"0089":23170,"jstor":23171,"Compensate":23172,"7038":23173,"Sawhney":23174,"rically":23175,"9316":23176,"MySequential":23177,"Waabi":23178,"Refinement":23179,"Gui":23180,"attributes":23181,"0003":23182,"Designed":23183,"narrower":23184,"japan":23185,"Multibox":23186,"08073":23187,"plement":23188,"ference":23189,"heatmaps":23190,"Attributes":23191,"wings":23192,"00%":23193,"dv":23194,"discretized":23195,"epicenter":23196,"followed":23197,"Yin":23198,"Wxz":23199,"occupy":23200,"meters":23201,"randint":23202,"treatments":23203,"md":23204,"527":23205,"950":23206,"fp":23207,"improving":23208,"Reuse":23209,"struggle":23210,"snippet":23211,"trains":23212,"auto":23213,"boat":23214,"Iterative":23215,"1672":23216,"template":23217,"parallelized":23218,"Dendrite":23219,"assis":23220,"Adverse":23221,"6400":23222,"pertains":23223,"pictures":23224,"record":23225,"occurence":23226,"Miao":23227,"hats":23228,"terminates":23229,"mech":23230,"0.000066":23231,"Rahimi":23232,"Nodar":23233,"KNOW":23234,"Prices":23235,"implicitly":23236,"6578e":23237,"integrand":23238,"Lenovo":23239,"invariably":23240,"database":23241,"convention":23242,"778":23243,"audit":23244,"enlarge":23245,"Booz":23246,"staled":23247,"Losses":23248,"multiplicity":23249,"868":23250,"Premise":23251,"sustainability":23252,"competitions":23253,"Kuratov":23254,"Incep":23255,"Squeeze":23256,"therefore":23257,"Proprietary":23258,"addressed":23259,"walking":23260,"stance":23261,"locality":23262,"08377":23263,"subsume":23264,"YOUR":23265,"copied":23266,"want":23267,"inseparable":23268,"Elizalde":23269,"Turing":23270,"Zhang":23271,"Switch":23272,"objectives":23273,"GP":23274,"Branch":23275,"02846298236356246":23276,"mpl":23277,"Infor":23278,"pathological":23279,"Schaar":23280,"finances":23281,"snli":23282,"overcoming":23283,"Discussions143":23284,"moreover":23285,"exclusively":23286,"substitute":23287,"involvement":23288,"keeps":23289,"randomization":23290,"lawsuit":23291,"Detecting":23292,"261":23293,"conformance":23294,"609":23295,"invok":23296,"Remove":23297,"602":23298,"419":23299,"B.11":23300,"disruption":23301,"Factors":23302,"gamma":23303,"MXNet":23304,"Wigner":23305,"Inference":23306,"Convolutions":23307,"cars":23308,"Commission":23309,"Applicable":23310,"punctuation":23311,"Σ":23312,"property":23313,"Noyce":23314,"93%":23315,"42":23316,"grants":23317,"strides":23318,"Audience":23319,"fashion":23320,"80486":23321,"relief":23322,"Swami":23323,"1MB":23324,"portability":23325,"100%":23326,"tionships":23327,"burst":23328,"Fortunately":23329,"aca":23330,"divide":23331,"associa":23332,"broad":23333,"hashing":23334,"0%":23335,"19th":23336,"tomorrow":23337,"validity":23338,"develop":23339,"B100":23340,"Age":23341,"attains":23342,"troduced":23343,"images":23344,"characteristics":23345,"Nikhil95":23346,"+.":23347,"figures":23348,"Club":23349,"lingual":23350,"Re":23351,"Copies":23352,"likes":23353,"cascading":23354,"h1":23355,"664":23356,"Thomas":23357,"Freund":23358,"meth":23359,"thesis":23360,"Shavit":23361,"adopts":23362,"deepteki":23363,"Single":23364,"perusing":23365,"allocation":23366,"poten":23367,"superpositions":23368,"bit":23369,"IDs":23370,"Rejection":23371,"culminated":23372,"Vocabulary":23373,"subcontractors":23374,"654":23375,"rand":23376,"Stopped":23377,"empirically":23378,"schemas":23379,"4648":23380,"hunches":23381,"agencies":23382,"entails":23383,"likening":23384,"natural":23385,"pages":23386,"Guti":23387,"y2":23388,"disagree":23389,"Euclidean":23390,"resemble":23391,"Õ":23392,"Koizumi":23393,"catenating":23394,"955":23395,"traced":23396,"straightfor":23397,"1981":23398,"Pull":23399,"throughput":23400,"2.8231":23401,"redeemed":23402,"Coreware":23403,"订单":23404,"SNLIBERTDataset":23405,"1608":23406,"8595":23407,"reranking":23408,"matter":23409,"intensely":23410,"RPG":23411,"seeing":23412,"Incentive":23413,"relies":23414,"stability":23415,"brettkoonce":23416,"018466":23417,"seq2seq":23418,"13178":23419,"novels":23420,"06455":23421,"RING":23422,"Farhadi":23423,"characterizations":23424,"604":23425,"races":23426,"INPUT":23427,"591":23428,"Jay":23429,"plaintiffs":23430,"cstride":23431,"frontiers":23432,"Whxx":23433,"election":23434,"TPAMI":23435,"areas":23436,"operation":23437,"filling":23438,"workflows":23439,"Genentech":23440,"creates":23441,"chimera":23442,"sin":23443,"Warranties":23444,"Mx":23445,"MAE":23446,"trajectory":23447,"adequate":23448,"culminating":23449,"Gammon":23450,"submits":23451,"WITHOUT":23452,"Wikipedia":23453,"mimicks":23454,"inserted":23455,"UninitializedParameter":23456,"Remote":23457,"3072":23458,"Rap":23459,"imperative":23460,"temperatures":23461,"Guu":23462,"maze":23463,"subsequent":23464,"pled":23465,"robustly":23466,"Hendrycks":23467,"714568":23468,"intensify":23469,"beta":23470,"b3":23471,"tasks":23472,"seldom":23473,"netyster":23474,"falsifiability":23475,"plementations":23476,"Gen3":23477,"misleading":23478,"LogisticNormal":23479,"boss":23480,"778503":23481,"protypical":23482,"4708":23483,"amarazov":23484,"param":23485,"1M2":23486,"Sedlmeyer":23487,"308":23488,"tectures":23489,"TransformerDecoder":23490,"laborative":23491,"Hierarchical":23492,"containers":23493,"Gunn":23494,"Donald":23495,"Your":23496,"¯":23497,"507":23498,"Tram":23499,"Discussions141":23500,"returned":23501,"normalized":23502,"location":23503,"victories":23504,"chain62":23505,"2308":23506,"Arste":23507,"up":23508,"Adequately":23509,"insidious":23510,"5036":23511,"001":23512,"Southern":23513,"2.4":23514,"CorrDiff":23515,"":23516}
```

    |-- test_datasets/
    |-- __init__.py

``` py

```

    |-- analyze_answer.py

``` py
# zhz_rag/evaluation/analyze_answer.py
import json
import os
import pandas as pd
from typing import List, Dict, Any, Optional
from collections import Counter
from datetime import datetime
import glob 

# --- 从项目中导入必要的模块 ---
try:
    from zhz_rag.utils.common_utils import (
        load_jsonl_file, # <--- 使用新的通用函数
        EVALUATION_RESULTS_LOGS_DIR # 导入评估日志目录常量
    )
except ImportError as e:
    print(f"ERROR: Could not import necessary modules in analyze_answer.py: {e}")
    print("Make sure this script is run in an environment where 'zhz_rag' package is accessible.")
    exit(1)

import logging

# --- 配置此脚本的logger ---
analyze_answer_logger = logging.getLogger("AnalyzeAnswerLogger")
analyze_answer_logger.setLevel(logging.INFO)
if not analyze_answer_logger.hasHandlers():
    _console_handler = logging.StreamHandler()
    _formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
    _console_handler.setFormatter(_formatter)
    analyze_answer_logger.addHandler(_console_handler)
    analyze_answer_logger.propagate = False
analyze_answer_logger.info("--- AnalyzeAnswerLogger configured ---")

# --- 核心功能函数 ---

def extract_answer_evaluation_details(log_entry: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """
    从单条已解析的答案评估日志条目中提取关键信息。
    这个函数与您之前在 analyze_answer.py 中的版本基本一致，稍作调整。
    """
    details = {}
    eval_data = log_entry.get("eval_llm_processed_output_json")

    if not eval_data or not isinstance(eval_data, dict):
        analyze_answer_logger.warning(f"Skipping log entry due to missing or invalid 'eval_llm_processed_output_json'. Interaction ID ref: {log_entry.get('original_interaction_id_ref')}")
        return None

    details["interaction_id_ref"] = log_entry.get("original_interaction_id_ref")
    details["user_question"] = log_entry.get("user_question_for_eval")
    details["generated_answer"] = log_entry.get("generated_answer_for_eval")
    
    summary = eval_data.get("evaluation_summary", {})
    dimensions = eval_data.get("dimensions", {})
    
    details["overall_answer_quality_score"] = summary.get("overall_answer_quality_score")
    details["main_strengths_answer"] = summary.get("main_strengths_answer")
    details["main_weaknesses_answer"] = summary.get("main_weaknesses_answer")
    
    faithfulness = dimensions.get("faithfulness", {})
    details["faithfulness_score"] = faithfulness.get("score")
    details["faithfulness_reasoning"] = faithfulness.get("reasoning")
    # 确保 problematic_answer_segments_faithfulness 是列表，然后 join
    problematic_segments = faithfulness.get("problematic_answer_segments_faithfulness", [])
    if isinstance(problematic_segments, list):
        details["faithfulness_problematic_segments"] = "; ".join(problematic_segments)
    else:
        details["faithfulness_problematic_segments"] = str(problematic_segments) # 以防万一不是列表

    relevance = dimensions.get("relevance", {})
    details["relevance_score"] = relevance.get("score")
    details["relevance_reasoning"] = relevance.get("reasoning")
    
    completeness = dimensions.get("completeness", {})
    details["completeness_context_sufficiency"] = completeness.get("context_sufficiency_assessment")
    details["completeness_context_reasoning"] = completeness.get("context_sufficiency_reasoning")
    details["completeness_score"] = completeness.get("score")
    details["completeness_reasoning"] = completeness.get("reasoning")
    
    coherence = dimensions.get("coherence_fluency", {}) # 键名可能与prompt中的一致
    details["coherence_score"] = coherence.get("score")
    details["coherence_reasoning"] = coherence.get("reasoning")

    actionability = dimensions.get("actionability_usability", {}) # 键名可能与prompt中的一致
    details["actionability_score"] = actionability.get("score")
    details["actionability_reasoning"] = actionability.get("reasoning")
    
    details["gemini_suggestion_answer"] = eval_data.get("suggestion_for_answer_improvement")

    return details

def perform_answer_evaluation_analysis(
    evaluation_log_filepath: str,
    output_csv_filepath: str
) -> bool:
    """
    加载答案评估日志，进行分析，并保存结果到CSV。

    Args:
        evaluation_log_filepath (str): 答案评估结果日志文件的路径。
        output_csv_filepath (str): 分析结果CSV文件的保存路径。

    Returns:
        bool: 如果分析和保存成功则返回True，否则返回False。
    """
    analyze_answer_logger.info(f"Starting Answer evaluation analysis for log file: {evaluation_log_filepath}")
    analyze_answer_logger.info(f"Analysis results will be saved to: {output_csv_filepath}")

    evaluation_logs = load_jsonl_file(evaluation_log_filepath)

    if not evaluation_logs:
        analyze_answer_logger.warning(f"No evaluation logs found or loaded from {evaluation_log_filepath}. Analysis aborted.")
        return False

    extracted_details_list = []
    for log_entry in evaluation_logs:
        if log_entry.get("task_type") == "answer_evaluation_result": # 确保是答案评估日志
            details = extract_answer_evaluation_details(log_entry)
            if details:
                extracted_details_list.append(details)
        else:
            analyze_answer_logger.debug(f"Skipping log entry with task_type '{log_entry.get('task_type')}' as it's not 'answer_evaluation_result'.")


    if not extracted_details_list:
        analyze_answer_logger.info("No valid Answer evaluation details extracted from the logs. No CSV will be generated.")
        return False

    df = pd.DataFrame(extracted_details_list)
    
    score_columns = [
        "overall_answer_quality_score", "faithfulness_score", "relevance_score",
        "completeness_score", "coherence_score", "actionability_score"
    ]
    for col in score_columns:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')

    analyze_answer_logger.info(f"\n--- Preliminary Answer Evaluation Analysis (from {len(extracted_details_list)} entries) ---")
    analyze_answer_logger.info(f"Total evaluation entries processed: {len(df)}")

    for col_name in score_columns:
        if col_name in df.columns and not df[col_name].isnull().all():
            analyze_answer_logger.info(f"\nDimension: {col_name}")
            analyze_answer_logger.info(f"{df[col_name].describe()}")
            analyze_answer_logger.info("Score Distribution:")
            analyze_answer_logger.info(f"{df[col_name].value_counts(dropna=False).sort_index()}")
        else:
            analyze_answer_logger.info(f"\nDimension: {col_name} - No data or all NaN.")
            
    if "completeness_context_sufficiency" in df.columns and not df["completeness_context_sufficiency"].isnull().all():
        analyze_answer_logger.info("\nContext Sufficiency Assessment Distribution:")
        analyze_answer_logger.info(f"{df['completeness_context_sufficiency'].value_counts(dropna=False)}")
    else:
        analyze_answer_logger.info("\nContext Sufficiency Assessment Distribution: No data.")

    try:
        os.makedirs(os.path.dirname(output_csv_filepath), exist_ok=True)
        df.to_csv(output_csv_filepath, index=False, encoding='utf-8-sig')
        analyze_answer_logger.info(f"\nAnalysis results saved to: {output_csv_filepath}")
        return True
    except Exception as e:
        analyze_answer_logger.error(f"\nFailed to save CSV file: {e}", exc_info=True)
        return False

if __name__ == "__main__":
    EVALUATION_NAME_FOR_ANSWER = "answer_gemini_flash" 
    
    # --- 动态查找最新的评估结果日志文件 ---
    eval_logs_pattern = os.path.join(EVALUATION_RESULTS_LOGS_DIR, f"eval_results_{EVALUATION_NAME_FOR_ANSWER}_*.jsonl")
    all_eval_logs = sorted(glob.glob(eval_logs_pattern), key=os.path.getmtime, reverse=True)
    
    log_file_path_answer: Optional[str] = None
    output_csv_path_answer: Optional[str] = None

    if all_eval_logs:
        log_file_path_answer = all_eval_logs[0] # 获取最新的一个
        analyze_answer_logger.info(f"Found latest Answer evaluation log for analysis: {log_file_path_answer}")
        
        # 根据找到的日志文件名构造输出的 CSV 文件名
        base_log_name = os.path.basename(log_file_path_answer)
        # 从 "eval_results_answer_gemini_flash_YYYYMMDD.jsonl" 生成 "analysis_answer_gemini_flash_YYYYMMDD.csv"
        if base_log_name.startswith(f"eval_results_{EVALUATION_NAME_FOR_ANSWER}_") and base_log_name.endswith(".jsonl"):
            date_part_from_filename = base_log_name[len(f"eval_results_{EVALUATION_NAME_FOR_ANSWER}_"):-len(".jsonl")]
            output_csv_name_answer = f"analysis_{EVALUATION_NAME_FOR_ANSWER}_{date_part_from_filename}.csv"
            output_csv_path_answer = os.path.join(EVALUATION_RESULTS_LOGS_DIR, output_csv_name_answer)
        else: # Fallback naming for CSV
            today_str = datetime.now().strftime("%Y%m%d")
            output_csv_name_answer = f"analysis_{EVALUATION_NAME_FOR_ANSWER}_{today_str}_fallback.csv"
            output_csv_path_answer = os.path.join(EVALUATION_RESULTS_LOGS_DIR, output_csv_name_answer)
        analyze_answer_logger.info(f"Analysis CSV report will be saved to: {output_csv_path_answer}")
    else:
        analyze_answer_logger.error(f"No Answer evaluation log files found matching pattern: {eval_logs_pattern}")

    if log_file_path_answer and output_csv_path_answer and os.path.exists(log_file_path_answer):
        perform_answer_evaluation_analysis(
            evaluation_log_filepath=log_file_path_answer,
            output_csv_filepath=output_csv_path_answer
        )
    else:
        analyze_answer_logger.info("Answer evaluation analysis will not run as no suitable log file was identified or output path could not be determined.")
```

    |-- analyze_cypher.py

``` py
# zhz_rag/evaluation/analyze_cypher.py
import json
import os
import pandas as pd
from typing import List, Dict, Any, Optional
from collections import Counter
from datetime import datetime
import glob 

# --- 从项目中导入必要的模块 ---
try:
    from zhz_rag.utils.common_utils import (
        load_jsonl_file, # <--- 使用新的通用函数
        EVALUATION_RESULTS_LOGS_DIR # 导入评估日志目录常量
    )
except ImportError as e:
    print(f"ERROR: Could not import necessary modules in analyze_cypher.py: {e}")
    print("Make sure this script is run in an environment where 'zhz_rag' package is accessible.")
    exit(1)

import logging

# --- 配置此脚本的logger ---
analyze_cypher_logger = logging.getLogger("AnalyzeCypherLogger")
analyze_cypher_logger.setLevel(logging.INFO)
if not analyze_cypher_logger.hasHandlers():
    _console_handler = logging.StreamHandler()
    _formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
    _console_handler.setFormatter(_formatter)
    analyze_cypher_logger.addHandler(_console_handler)
    analyze_cypher_logger.propagate = False
analyze_cypher_logger.info("--- AnalyzeCypherLogger configured ---")

# --- 核心功能函数 ---

def extract_cypher_evaluation_details(log_entry: Dict[str, Any]) -> Optional[Dict[str, Any]]:
    """
    从单条已解析的Cypher评估日志条目中提取关键信息。
    这个函数与您之前在 analyze_cypher.py 中的版本基本一致，稍作调整以适应通用加载。
    """
    details = {}
    # eval_llm_processed_output_json 字段包含了Gemini评估的JSON输出
    eval_data = log_entry.get("eval_llm_processed_output_json")

    if not eval_data or not isinstance(eval_data, dict):
        analyze_cypher_logger.warning(f"Skipping log entry due to missing or invalid 'eval_llm_processed_output_json'. Interaction ID ref: {log_entry.get('original_interaction_id_ref')}")
        return None # 如果核心评估数据缺失，则跳过此条目

    details["interaction_id_ref"] = log_entry.get("original_interaction_id_ref")
    details["user_question"] = log_entry.get("user_question_for_eval")
    details["generated_cypher"] = log_entry.get("generated_cypher_for_eval")
    
    summary = eval_data.get("evaluation_summary", {})
    dimensions = eval_data.get("dimensions", {})
    
    details["overall_quality_score"] = summary.get("overall_quality_score_cypher")
    details["main_strength"] = summary.get("main_strength_cypher")
    details["main_weakness"] = summary.get("main_weakness_cypher")
    
    syntax = dimensions.get("syntactic_correctness", {})
    details["syntax_score"] = syntax.get("score")
    details["syntax_reasoning"] = syntax.get("reasoning")
    
    schema = dimensions.get("schema_adherence", {})
    details["schema_overall_score"] = schema.get("overall_score")
    details["schema_node_label_correct"] = schema.get("node_label_correctness", {}).get("check_result")
    details["schema_entity_type_correct"] = schema.get("entity_type_property_correctness", {}).get("check_result")
    details["schema_rel_type_correct"] = schema.get("relationship_type_correctness", {}).get("check_result")
    details["schema_prop_name_correct"] = schema.get("property_name_correctness", {}).get("check_result")
    details["schema_hallucinated_present"] = schema.get("hallucinated_schema_elements", {}).get("check_result_hallucination_present")
    details["schema_hallucinated_elements"] = ", ".join(schema.get("hallucinated_schema_elements", {}).get("elements_found", []))
    details["schema_reasoning"] = schema.get("reasoning")
    
    intent = dimensions.get("intent_accuracy", {})
    details["intent_score"] = intent.get("score")
    details["intent_explanation_cypher"] = intent.get("explanation_of_cypher_retrieval")
    details["intent_alignment_notes"] = intent.get("semantic_alignment_with_question")
    details["intent_key_elements_notes"] = intent.get("key_element_coverage_notes")
    details["intent_reasoning"] = intent.get("reasoning")
    
    details["qwen_error_patterns"] = ", ".join(eval_data.get("qwen_error_patterns_identified", []))
    details["gemini_suggestion"] = eval_data.get("suggestion_for_improvement_cypher")

    return details

def perform_cypher_evaluation_analysis(
    evaluation_log_filepath: str,
    output_csv_filepath: str
) -> bool:
    """
    加载Cypher评估日志，进行分析，并保存结果到CSV。

    Args:
        evaluation_log_filepath (str): Cypher评估结果日志文件的路径。
        output_csv_filepath (str): 分析结果CSV文件的保存路径。

    Returns:
        bool: 如果分析和保存成功则返回True，否则返回False。
    """
    analyze_cypher_logger.info(f"Starting Cypher evaluation analysis for log file: {evaluation_log_filepath}")
    analyze_cypher_logger.info(f"Analysis results will be saved to: {output_csv_filepath}")

    # 使用通用函数加载评估日志
    # 注意：evaluate_cypher_with_gemini 保存的日志中 task_type 是 "cypher_evaluation_by_gemini"
    # load_jsonl_file 不关心 task_type，它会加载所有行
    evaluation_logs = load_jsonl_file(evaluation_log_filepath)

    if not evaluation_logs:
        analyze_cypher_logger.warning(f"No evaluation logs found or loaded from {evaluation_log_filepath}. Analysis aborted.")
        return False

    extracted_details_list = []
    for log_entry in evaluation_logs:
        # 确保只处理Cypher评估结果
        if log_entry.get("task_type") == "cypher_evaluation_by_gemini":
            details = extract_cypher_evaluation_details(log_entry)
            if details: # extract_cypher_evaluation_details 可能会返回 None
                extracted_details_list.append(details)
        else:
            analyze_cypher_logger.debug(f"Skipping log entry with task_type '{log_entry.get('task_type')}' as it's not 'cypher_evaluation_by_gemini'.")


    if not extracted_details_list:
        analyze_cypher_logger.info("No valid Cypher evaluation details extracted from the logs. No CSV will be generated.")
        return False

    df = pd.DataFrame(extracted_details_list)
    
    score_columns = ["overall_quality_score", "syntax_score", "schema_overall_score", "intent_score"]
    for col in score_columns:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')

    analyze_cypher_logger.info(f"\n--- Preliminary Cypher Evaluation Analysis (from {len(extracted_details_list)} entries) ---")
    analyze_cypher_logger.info(f"Total evaluation entries processed: {len(df)}")

    if "overall_quality_score" in df.columns and not df["overall_quality_score"].isnull().all():
        analyze_cypher_logger.info("\n1. Overall Quality Score:")
        analyze_cypher_logger.info(f"{df['overall_quality_score'].describe()}")
        analyze_cypher_logger.info("\nScore Distribution:")
        analyze_cypher_logger.info(f"{df['overall_quality_score'].value_counts(dropna=False).sort_index()}")
    else:
        analyze_cypher_logger.info("\n1. Overall Quality Score: No data or all NaN.")


    if "schema_overall_score" in df.columns and not df["schema_overall_score"].isnull().all():
        analyze_cypher_logger.info("\n2. Schema Adherence Overall Score:")
        analyze_cypher_logger.info(f"{df['schema_overall_score'].describe()}")
        analyze_cypher_logger.info("\nScore Distribution:")
        analyze_cypher_logger.info(f"{df['schema_overall_score'].value_counts(dropna=False).sort_index()}")
        
        schema_sub_checks = [
            "schema_node_label_correct", "schema_entity_type_correct", 
            "schema_rel_type_correct", "schema_prop_name_correct", 
            "schema_hallucinated_present"
        ]
        analyze_cypher_logger.info("\nSchema Adherence Sub-item Issues (False means issue, Hallucinated True means issue):")
        for check in schema_sub_checks:
            if check in df.columns:
                if check == "schema_hallucinated_present":
                    issue_count = df[df[check] == True].shape[0]
                    analyze_cypher_logger.info(f"  - {check} (Hallucination Present): {issue_count} entries")
                else:
                    issue_count = df[df[check] == False].shape[0]
                    analyze_cypher_logger.info(f"  - {check} (Incorrect): {issue_count} entries")
    else:
        analyze_cypher_logger.info("\n2. Schema Adherence Overall Score: No data or all NaN.")


    if "intent_score" in df.columns and not df["intent_score"].isnull().all():
        analyze_cypher_logger.info("\n3. Intent Accuracy Score:")
        analyze_cypher_logger.info(f"{df['intent_score'].describe()}")
        analyze_cypher_logger.info("\nScore Distribution:")
        analyze_cypher_logger.info(f"{df['intent_score'].value_counts(dropna=False).sort_index()}")
    else:
        analyze_cypher_logger.info("\n3. Intent Accuracy Score: No data or all NaN.")


    if "qwen_error_patterns" in df.columns and not df["qwen_error_patterns"].isnull().all():
        analyze_cypher_logger.info("\n4. Identified Qwen Error Patterns (Top 5):")
        all_patterns = []
        for pattern_list_str in df["qwen_error_patterns"].dropna():
            if pattern_list_str and isinstance(pattern_list_str, str):
                all_patterns.extend([p.strip() for p in pattern_list_str.split(",") if p.strip()])
        pattern_counts = Counter(all_patterns)
        analyze_cypher_logger.info(f"{pattern_counts.most_common(5)}")
    else:
        analyze_cypher_logger.info("\n4. Identified Qwen Error Patterns: No data.")
        
    try:
        # Ensure output directory exists
        os.makedirs(os.path.dirname(output_csv_filepath), exist_ok=True)
        df.to_csv(output_csv_filepath, index=False, encoding='utf-8-sig')
        analyze_cypher_logger.info(f"\nAnalysis results saved to: {output_csv_filepath}")
        return True
    except Exception as e:
        analyze_cypher_logger.error(f"\nFailed to save CSV file: {e}", exc_info=True)
        return False

if __name__ == "__main__":
    EVALUATION_NAME_FOR_CYPHER = "cypher_gemini_flash" 

    eval_logs_pattern = os.path.join(EVALUATION_RESULTS_LOGS_DIR, f"eval_results_{EVALUATION_NAME_FOR_CYPHER}_*.jsonl")
    all_eval_logs = sorted(glob.glob(eval_logs_pattern), key=os.path.getmtime, reverse=True)
    
    log_file_path_cypher: Optional[str] = None
    output_csv_path_cypher: Optional[str] = None

    if all_eval_logs:
        log_file_path_cypher = all_eval_logs[0]
        analyze_cypher_logger.info(f"Found latest Cypher evaluation log for analysis: {log_file_path_cypher}")
        
        base_log_name = os.path.basename(log_file_path_cypher)
        if base_log_name.startswith(f"eval_results_{EVALUATION_NAME_FOR_CYPHER}_") and base_log_name.endswith(".jsonl"):
            date_part_from_filename = base_log_name[len(f"eval_results_{EVALUATION_NAME_FOR_CYPHER}_"):-len(".jsonl")]
            output_csv_name_cypher = f"analysis_{EVALUATION_NAME_FOR_CYPHER}_{date_part_from_filename}.csv"
            output_csv_path_cypher = os.path.join(EVALUATION_RESULTS_LOGS_DIR, output_csv_name_cypher)
        else:
            today_str = datetime.now().strftime("%Y%m%d")
            output_csv_name_cypher = f"analysis_{EVALUATION_NAME_FOR_CYPHER}_{today_str}_fallback.csv"
            output_csv_path_cypher = os.path.join(EVALUATION_RESULTS_LOGS_DIR, output_csv_name_cypher)
        analyze_cypher_logger.info(f"Analysis CSV report will be saved to: {output_csv_path_cypher}")
    else:
        analyze_cypher_logger.error(f"No Cypher evaluation log files found matching pattern: {eval_logs_pattern}")

    if log_file_path_cypher and output_csv_path_cypher and os.path.exists(log_file_path_cypher):
        perform_cypher_evaluation_analysis(
            evaluation_log_filepath=log_file_path_cypher,
            output_csv_filepath=output_csv_path_cypher
        )
    else:
        analyze_cypher_logger.info("Cypher evaluation analysis will not run as no suitable log file was identified or output path could not be determined.")
```

    |-- batch_eval_answer.py

``` py
# 文件: zhz_rag/evaluation/batch_eval_answer.py
import os
import asyncio
import json
import logging
from typing import List, Dict, Any, Optional
from pathlib import Path
import argparse
import httpx # 使用httpx进行异步HTTP请求

# 确保项目根目录在sys.path中，以便正确导入
# 这部分可能需要根据你的项目结构调整
try:
    from zhz_rag.llm.llm_interface import NO_ANSWER_PHRASE_ANSWER_CLEAN
    from zhz_rag.config.pydantic_models import RetrievedDocument
    from zhz_rag.utils.gemini_api_utils import GeminiAPIClient # <--- 修改
    from zhz_rag.evaluation.evaluator import evaluate_answer_with_gemini
except ImportError:
    import sys
    # A more robust way to add the project root to the path
    project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
    if project_root not in sys.path:
        sys.path.insert(0, project_root)
    from zhz_rag.llm.llm_interface import NO_ANSWER_PHRASE_ANSWER_CLEAN
    from zhz_rag.config.pydantic_models import RetrievedDocument
    from zhz_rag.utils.gemini_api_utils import GeminiAPIClient # <--- 修正这一行
    from zhz_rag.evaluation.evaluator import evaluate_answer_with_gemini

# --- 配置日志 ---
batch_answer_eval_logger = logging.getLogger("BatchAnswerEvaluationLogger")
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)

# --- 定义常量 ---
RAG_API_URL = "http://localhost:8081/api/v1/rag/query"
DEFAULT_TEST_DATASET_PATH = Path(__file__).parent / "test_datasets" / "evaluation_questions_v1.txt"
REQUEST_TIMEOUT = 120.0  # API请求的超时时间，单位秒
DELAY_BETWEEN_REQUESTS = 5 # 每个评估请求之间的延迟，避免API过载

async def call_rag_api(client: httpx.AsyncClient, question: str) -> Optional[Dict[str, Any]]:
    """
    异步调用RAG API服务并返回结果。
    """
    payload = {
        "query": question,
        "top_k_vector": 3,
        "top_k_bm25": 3,
        "top_k_kg": 3,
        "top_k_final": 5  # 召回更多上下文给评估器
    }
    try:
        batch_answer_eval_logger.info(f"Sending request to RAG API for question: '{question}'")
        response = await client.post(RAG_API_URL, json=payload, timeout=REQUEST_TIMEOUT)
        
        if response.status_code == 200:
            batch_answer_eval_logger.info("RAG API returned a successful response.")
            return response.json()
        else:
            batch_answer_eval_logger.error(
                f"RAG API returned an error. Status: {response.status_code}, "
                f"Response: {response.text[:200]}"
            )
            return None
    except httpx.RequestError as e:
        batch_answer_eval_logger.error(f"Error calling RAG API for question '{question}': {e}", exc_info=True)
        return None

async def main_evaluation_runner(
    questions_file: Path, 
    gemini_resource: GeminiAPIClient, 
    app_version_tag: str,
    use_simulated_api: bool,
    api_call_delay: int
):
    """
    读取问题文件，调用RAG API，然后使用Gemini评估答案。
    """
    if not questions_file.exists():
        batch_answer_eval_logger.error(f"Test questions file not found at: {questions_file}")
        return

    with open(questions_file, 'r', encoding='utf-8') as f:
        # --- 优化：跳过注释行 (#) 和空行 ---
        questions = [line.strip() for line in f if line.strip() and not line.strip().startswith('#')]
    
    batch_answer_eval_logger.info(f"Loaded {len(questions)} questions from '{questions_file.name}'. Starting evaluation...")
    
    successful_evals = 0
    failed_evals = 0
    
    async with httpx.AsyncClient() as client:
        for i, question in enumerate(questions, 1):
            batch_answer_eval_logger.info(f"--- Processing question {i}/{len(questions)}: '{question}' ---")
            
            # 1. 调用RAG API获取答案
            rag_response = await call_rag_api(client, question)
            
            if rag_response is None:
                batch_answer_eval_logger.warning(f"Skipping evaluation for '{question}' due to RAG API failure.")
                failed_evals += 1
                await asyncio.sleep(api_call_delay) # 使用传入的延迟参数
                continue

            # 2. 从RAG API响应中提取评估所需的信息
            answer_text = rag_response.get("answer")
            retrieved_sources_raw = rag_response.get("retrieved_sources", [])
            
            # 即使有答案，如果没有上下文来源，对于研究型问题也认为是失败
            if not answer_text or not retrieved_sources_raw:
                batch_answer_eval_logger.warning(
                    f"RAG API response for '{question}' is incomplete. "
                    f"Answer: '{answer_text}', Sources: {len(retrieved_sources_raw)}. Skipping evaluation."
                )
                failed_evals += 1
                await asyncio.sleep(api_call_delay) # 使用传入的延迟参数
                continue

            # 3. 准备评估函数的输入
            retrieved_docs_for_eval = [RetrievedDocument(**doc) for doc in retrieved_sources_raw]
            
            # 4. 调用Gemini进行评估
            await evaluate_answer_with_gemini(
                gemini_resource_for_evaluator=gemini_resource,
                user_question=question,
                retrieved_contexts=retrieved_docs_for_eval,
                generated_answer=answer_text,
                app_version=app_version_tag,
                use_simulated_api=use_simulated_api,
                api_call_delay=api_call_delay
            )
            successful_evals += 1
            
            batch_answer_eval_logger.info(f"Successfully evaluated question {i}. Sleeping for {api_call_delay} seconds...")
            await asyncio.sleep(api_call_delay) # 使用传入的延迟参数

    batch_answer_eval_logger.info("--- Batch Answer Evaluation Finished ---")
    batch_answer_eval_logger.info(f"Summary: Successfully evaluated {successful_evals} questions, Failed/Skipped {failed_evals} questions.")

    
if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="Run batch evaluation of RAG answers using a questions file.")
    
    parser.add_argument(
        "--questions-file",
        type=Path,
        default=DEFAULT_TEST_DATASET_PATH,
        help=f"Path to the .txt file containing evaluation questions. Defaults to {DEFAULT_TEST_DATASET_PATH}"
    )
    parser.add_argument(
        "--app-version",
        type=str,
        default="0.2.0_eval_run",
        help="A version tag for this evaluation run."
    )
    parser.add_argument(
        "--simulate",
        action="store_true",
        help="Use simulated Gemini API responses for testing the evaluation script itself."
    )
    parser.add_argument(
        "--delay",
        type=int,
        default=DELAY_BETWEEN_REQUESTS,
        help=f"Delay in seconds between evaluation calls to avoid rate limiting. Defaults to {DELAY_BETWEEN_REQUESTS}."
    )
    
    args = parser.parse_args()
    
# 初始化Gemini客户端
    try:
        gemini_client = GeminiAPIClient.from_env() # <--- 修改
        batch_answer_eval_logger.info("GeminiAPIClient for Answer evaluation initialized successfully.")
    except Exception as e:
        batch_answer_eval_logger.error(f"Failed to initialize GeminiAPIClient: {e}", exc_info=True)
        gemini_client = None

    if not gemini_client:
        batch_answer_eval_logger.error("Cannot proceed with evaluation as GeminiAPIClient is not available.")
    else:
        asyncio.run(main_evaluation_runner(
            questions_file=args.questions_file,
            gemini_resource=gemini_client, # <--- 修改
            app_version_tag=args.app_version,
            use_simulated_api=args.simulate,
            api_call_delay=args.delay
        ))
```

    |-- batch_eval_cypher.py

``` py
# zhz_rag/evaluation/batch_eval_cypher.py
import asyncio
import json
import os
from typing import List, Dict, Any, Optional, TYPE_CHECKING, Union
import glob
from datetime import datetime

try:
    from zhz_rag.evaluation.evaluator import evaluate_cypher_with_gemini
    from zhz_rag.utils.common_utils import (
        find_latest_rag_interaction_log,
        load_jsonl_file,
        RAG_INTERACTION_LOGS_DIR
    )
except ImportError as e:
    print(f"ERROR: Could not import necessary modules in batch_eval_cypher.py: {e}")
    print("Make sure this script is run in an environment where 'zhz_rag' package is accessible.")
    exit(1)

if TYPE_CHECKING:
    from zhz_rag_pipeline_dagster.zhz_rag_pipeline.resources import GeminiAPIResource

import logging

batch_cypher_eval_logger = logging.getLogger("BatchCypherEvaluationLogger")
batch_cypher_eval_logger.setLevel(logging.DEBUG) # 设置为 DEBUG 以便查看详细日志
if not batch_cypher_eval_logger.hasHandlers():
    _console_handler = logging.StreamHandler()
    _formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
    _console_handler.setFormatter(_formatter)
    batch_cypher_eval_logger.addHandler(_console_handler)
    batch_cypher_eval_logger.propagate = False
batch_cypher_eval_logger.info("--- BatchCypherEvaluationLogger configured (Level: DEBUG) ---")


async def run_cypher_batch_evaluation(
    gemini_resource_for_evaluator: 'GeminiAPIResource',
    rag_interaction_log_filepath: str,
    app_version: str = "0.1.0",
    use_simulated_api: bool = False,
    api_call_delay: float = 4.1,
    target_task_types: Union[str, List[str]] = "cypher_generation_final_attempt_local_service",
    field_mapping: Optional[Dict[str, Union[str, List[str]]]] = None
) -> Dict[str, int]:
    batch_cypher_eval_logger.info(f"Starting batch Cypher evaluation for log file: {rag_interaction_log_filepath}")
    batch_cypher_eval_logger.info(f"Parameters: app_version='{app_version}', use_simulated_api={use_simulated_api}, api_call_delay={api_call_delay}s")
    batch_cypher_eval_logger.info(f"Target task types: {target_task_types}")
    batch_cypher_eval_logger.info(f"Field mapping: {field_mapping}")

    processed_count = 0
    evaluated_count = 0
    skipped_no_cypher_count = 0
    failed_to_extract_count = 0
    skipped_task_type_mismatch = 0

    if use_simulated_api:
        batch_cypher_eval_logger.warning("Batch Cypher evaluation is USING SIMULATED Gemini responses.")
    else:
        batch_cypher_eval_logger.info("Batch Cypher evaluation is using REAL Gemini API calls.")

    interaction_logs = load_jsonl_file(rag_interaction_log_filepath)

    if not interaction_logs:
        batch_cypher_eval_logger.error(f"No data loaded from RAG interaction log file: {rag_interaction_log_filepath}. Exiting.")
        return {"processed": 0, "evaluated": 0, "skipped_no_cypher":0, "failed_extract": 0, "skipped_task_type_mismatch":0, "file_not_found_or_empty": 1}

    if isinstance(target_task_types, str):
        target_task_types_list = [target_task_types]
    else:
        target_task_types_list = target_task_types

    default_field_map = {
        "user_query": ["user_query_for_task", "user_query", "original_user_query"],
        "generated_cypher": ["raw_llm_output", "processed_llm_output"], # raw_llm_output for cypher_generation_final_attempt_local_service
        "interaction_id": ["interaction_id", "original_interaction_id"]
    }
    current_field_map = default_field_map.copy()
    if field_mapping:
        for key, value in field_mapping.items():
            if isinstance(value, str):
                current_field_map[key] = [value]
            else:
                current_field_map[key] = value

    def get_field_value(log_entry: Dict[str, Any], field_key: str) -> Any:
        for actual_field_name in current_field_map.get(field_key, []):
            if actual_field_name in log_entry:
                return log_entry[actual_field_name]
        return None

    for line_number, interaction_log in enumerate(interaction_logs, 1):
        processed_count += 1
        current_task_type = interaction_log.get("task_type")

        if current_task_type not in target_task_types_list:
            skipped_task_type_mismatch +=1
            continue
            
        batch_cypher_eval_logger.debug(f"DEBUG_CYPHER_EVAL: Processing log entry {line_number} with task_type '{current_task_type}'")

        user_question = get_field_value(interaction_log, "user_query")
        generated_cypher_raw = get_field_value(interaction_log, "generated_cypher")
        original_id = get_field_value(interaction_log, "interaction_id")

        if user_question and original_id:
            generated_cypher_to_eval = None # 初始化
            if isinstance(generated_cypher_raw, str) and generated_cypher_raw.strip():
                # 对于 "kg_executed_query_for_eval" 类型的日志，"generated_query" 字段直接包含SQL语句
                # 我们假设字段映射已将 "generated_query" 映射到 generated_cypher_raw
                generated_cypher_to_eval = generated_cypher_raw.strip()
                batch_cypher_eval_logger.debug(f"Extracted query for eval (ID: {original_id}): '{generated_cypher_to_eval[:100]}...'")
            else:
                batch_cypher_eval_logger.warning(f"Log entry for ID {original_id} (task_type: {current_task_type}) has empty or non-string 'generated_query' (mapped to generated_cypher_raw). Value: {generated_cypher_raw}")

            if not generated_cypher_to_eval: # 再次检查，确保 generated_cypher_to_eval 有有效值
                batch_cypher_eval_logger.info(f"Skipping evaluation for interaction_id '{original_id}' as extracted Cypher is empty.")
                skipped_no_cypher_count += 1
                continue
            
            # We will evaluate "无法生成Cypher查询." as well, Gemini should score it appropriately.
            batch_cypher_eval_logger.info(f"Evaluating Cypher for interaction_id: {original_id} - User Question: {str(user_question)[:50]}... - Cypher: {str(generated_cypher_to_eval)[:100]}...")
            
            evaluation_result = await evaluate_cypher_with_gemini(
                gemini_resource=gemini_resource_for_evaluator,
                user_question=str(user_question),
                generated_cypher=str(generated_cypher_to_eval),
                original_interaction_id=str(original_id),
                app_version=app_version
            )

            if evaluation_result:
                evaluated_count += 1
                summary = evaluation_result.get("evaluation_summary", {})
                overall_score = summary.get("overall_quality_score_cypher", "N/A")
                batch_cypher_eval_logger.info(f"Successfully evaluated Cypher for interaction_id: {original_id}. Overall Score: {overall_score}")
            else:
                batch_cypher_eval_logger.warning(f"Cypher evaluation returned None or failed for interaction_id: {original_id}")
            
            if not use_simulated_api and evaluated_count > 0:
                batch_cypher_eval_logger.info(f"Waiting for {api_call_delay} seconds before next API call...")
                await asyncio.sleep(api_call_delay)
        else:
            failed_to_extract_count += 1
            batch_cypher_eval_logger.warning(f"Skipping cypher_generation log entry {line_number} due to missing user_query or interaction_id. Log content: {str(interaction_log)[:200]}...")
        
        if processed_count > 0 and processed_count % 10 == 0:
            batch_cypher_eval_logger.info(f"Progress: Processed {processed_count} log entries. Evaluated {evaluated_count} Cypher queries. Skipped (no cypher): {skipped_no_cypher_count}. Failed extract: {failed_to_extract_count}. Type mismatch: {skipped_task_type_mismatch}")

    summary = {
        "total_log_entries_read": processed_count,
        "target_task_type_entries_found": processed_count - skipped_task_type_mismatch,
        "cypher_queries_evaluated_successfully": evaluated_count,
        "skipped_empty_or_no_cypher": skipped_no_cypher_count,
        "failed_to_extract_fields_for_eval": failed_to_extract_count
    }
    batch_cypher_eval_logger.info(f"Batch Cypher evaluation finished. Summary: {summary}")
    return summary


if __name__ == "__main__":
    try:
        from zhz_rag_pipeline_dagster.zhz_rag_pipeline.resources import GeminiAPIResource, GeminiAPIResourceConfig
        
        gemini_model_name_env = os.getenv("GEMINI_MODEL_FOR_EVAL", "gemini/gemini-1.5-flash-latest")
        gemini_proxy_url_env = os.getenv("LITELLM_PROXY_URL") 

        gemini_resource_config = GeminiAPIResourceConfig(
            model_name=gemini_model_name_env,
            proxy_url=gemini_proxy_url_env
        )
        gemini_eval_resource = GeminiAPIResource(
            model_name=gemini_resource_config.model_name,
            proxy_url=gemini_resource_config.proxy_url,
            default_temperature=gemini_resource_config.default_temperature,
            default_max_tokens=gemini_resource_config.default_max_tokens
        )
        class MockContext: 
            def __init__(self):
                self.log = batch_cypher_eval_logger
        
        if hasattr(gemini_eval_resource, 'setup_for_execution'):
             gemini_eval_resource.setup_for_execution(MockContext())
        batch_cypher_eval_logger.info(f"GeminiAPIResource for Cypher evaluation initialized successfully.")

    except ImportError:
        batch_cypher_eval_logger.critical("CRITICAL: Could not import GeminiAPIResource. Ensure Dagster modules are in PYTHONPATH or installed.")
        gemini_eval_resource = None
    except Exception as e_res_init:
        batch_cypher_eval_logger.critical(f"CRITICAL: Error initializing GeminiAPIResource: {e_res_init}", exc_info=True)
        gemini_eval_resource = None

    log_file_to_evaluate = find_latest_rag_interaction_log(RAG_INTERACTION_LOGS_DIR)
    use_simulated_env = os.getenv("USE_SIMULATED_GEMINI_CYPHER_EVAL", "false").lower() == "true"
    api_delay_env = float(os.getenv("GEMINI_API_CALL_DELAY_SECONDS", "4.1"))
    app_version_tag_env = os.getenv("APP_VERSION_TAG", "0.1.4_batch_cypher_flexible")
    if use_simulated_env:
        app_version_tag_env += "_simulated"

    # --- 配置目标 task_type 和字段映射 ---
    cypher_gen_task_types = ["kg_executed_query_for_eval"] # <--- 查找新的 task_type
    cypher_gen_field_map = {
        "user_query": "user_query_for_task",      # 这个字段名在新的日志条目中是存在的
        "generated_cypher": "generated_query",    # 新的日志条目中，查询语句存储在 "generated_query" 字段
        "interaction_id": "interaction_id"        # interaction_id 仍然是主键
    }

    if not gemini_eval_resource:
        batch_cypher_eval_logger.error("Cannot proceed with Cypher evaluation as GeminiAPIResource is not available.")
    elif log_file_to_evaluate:
        batch_cypher_eval_logger.info(f"Found RAG interaction log to process for Cypher evaluation: {log_file_to_evaluate}")
        asyncio.run(run_cypher_batch_evaluation(
            gemini_resource_for_evaluator=gemini_eval_resource,
            rag_interaction_log_filepath=log_file_to_evaluate,
            app_version=app_version_tag_env,
            use_simulated_api=use_simulated_env,
            api_call_delay=api_delay_env,
            target_task_types=cypher_gen_task_types, # <--- 使用修改后的 task_types
            field_mapping=cypher_gen_field_map
        ))

    elif use_simulated_env:
        batch_cypher_eval_logger.warning(f"RAG interaction log file not found, but USE_SIMULATED_GEMINI_CYPHER_EVAL is true. Running with a dummy path.")
        if gemini_eval_resource:
            asyncio.run(run_cypher_batch_evaluation(
                gemini_resource_for_evaluator=gemini_eval_resource,
                rag_interaction_log_filepath="dummy_non_existent_file.jsonl", 
                app_version=app_version_tag_env + "_no_file",
                use_simulated_api=use_simulated_env,
                api_call_delay=api_delay_env,
                target_task_types=cypher_gen_task_types,
                field_mapping=cypher_gen_field_map
            ))
        else:
            batch_cypher_eval_logger.error("GeminiAPIResource for Cypher evaluation could not be initialized (even for simulated run). Aborting.")
    else:
        batch_cypher_eval_logger.warning(f"No suitable RAG interaction log file found in '{RAG_INTERACTION_LOGS_DIR}' and not using simulated responses. Batch Cypher evaluation will not run.")
```

    |-- evaluator.py

``` py
# 文件: zhz_rag/evaluation/evaluator.py
import os
import json
import traceback
import google.generativeai as genai
from typing import Dict, Any, Optional, TYPE_CHECKING, List

from zhz_rag.config.constants import NEW_KG_SCHEMA_DESCRIPTION as KG_SCHEMA_FOR_EVALUATION
from ..utils.interaction_logger import log_interaction_data

if TYPE_CHECKING:
    from ..config.pydantic_models import RetrievedDocument
    from ..utils.gemini_api_utils import GeminiAPIClient

import logging

eval_logger = logging.getLogger("EvaluationLogger")
if not eval_logger.hasHandlers():
    _eval_console_handler = logging.StreamHandler()
    _eval_formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
    _eval_console_handler.setFormatter(_eval_formatter)
    eval_logger.addHandler(_eval_console_handler)
    eval_logger.propagate = False
eval_logger.info("--- EvaluationLogger configured ---")

# (此处省略两个非常长的Prompt字符串 CYPHER_EVALUATION_PROMPT_V1 和 ANSWER_EVALUATION_PROMPT_V1，请保留您文件中的原样)
CYPHER_EVALUATION_PROMPT_V1 = """You are an expert Neo4j Cypher query evaluator and debugging assistant. Your primary task is to meticulously analyze a Cypher query that was generated by another AI model (Qwen2.5-3B, a 3 billion parameter model) in response to a user's natural language question. Your evaluation must be based on the provided knowledge graph schema and the specific evaluation criteria outlined below.

**IMPORTANT SCHEMA CONTEXT (KG_SCHEMA_DESCRIPTION):**
Use code with caution.
Python
{{KG_SCHEMA_DESCRIPTION}}
**USER'S NATURAL LANGUAGE QUESTION:**
Use code with caution.
{{USER_QUESTION}}
**GENERATED CYPHER QUERY TO EVALUATE:**
Use code with caution.
{{GENERATED_CYPHER}}
**EVALUATION TASK:**

Please evaluate the "GENERATED CYPHER QUERY" based on the following criteria. Provide your evaluation in a **valid JSON format** strictly adhering to the structure specified at the end.

**EVALUATION CRITERIA:**

1.  **Syntactic Correctness:**
    *   Is the Cypher query syntactically valid and parsable by Neo4j?
    *   Score (1-5): 1 = Major syntax errors, unparsable; 3 = Minor issues, likely parsable with warnings; 5 = Syntactically perfect.
    *   Reasoning: Explain your score. If errors exist, briefly describe them.

2.  **Schema Adherence (Strictly based on the provided KG_SCHEMA_DESCRIPTION):**
    *   **Node Label Correctness:**
        *   Check: Does the query exclusively use ':ExtractedEntity' for all node patterns? (True/False)
        *   Detail: Briefly explain.
    *   **Entity Type via `label` Property Correctness:**
        *   Check: Are entity types (e.g., 'PERSON', 'ORGANIZATION', 'TASK') correctly queried using the `label` property of ':ExtractedEntity' nodes (e.g., `WHERE n.label = 'PERSON'`)? (True/False)
        *   Detail: Briefly explain.
    *   **Relationship Type and Direction Correctness:**
        *   Check: Does the query use only defined relationship types (e.g., `:WORKS_AT`, `:ASSIGNED_TO`) and their correct directions as specified in the schema? (True/False)
        *   Detail: Briefly explain.
    *   **Property Name Correctness:**
        *   Check: Does the query use only valid property names for nodes and relationships (e.g., `text`, `label` for nodes)? (True/False)
        *   Detail: Briefly explain.
    *   **Hallucinated Schema Elements:**
        *   Check: Does the query reference any labels, relationship types, or properties NOT defined in the schema? (True/False - True if hallucination is present)
        *   Detail: List any hallucinated elements.
    *   **Overall Schema Adherence Score (1-5):** Based on the sub-checks above, provide an overall schema adherence score. Explain your reasoning, considering the severity and number of any deviations. Perfect adherence means all sub-checks are True and no hallucinations are present. 1 = Gross violations; 3 = Some minor deviations; 5 = Perfectly adheres to schema.
    *   Reasoning for Overall Schema Adherence Score: Provide the explanation here.

3.  **Intent Accuracy (Semantic Correctness):**
    *   Score (1-5): 1 = Completely misses user intent; 3 = Partially addresses intent but has significant gaps or inaccuracies; 5 = Accurately and fully captures user intent.
    *   Explanation of Cypher Retrieval: In simple natural language that an average office worker could understand, what information would this Cypher query retrieve from a graph that matches the schema?
    *   Alignment with User Question: How well does this retrieved information align with what the user was asking in their "USER'S NATURAL LANGUAGE QUESTION"?
    *   Key Element Coverage: Does the Cypher query attempt to address all key entities, relationships, and conditions mentioned in the user's question? If not, what specific parts of the question seem to be missing or misinterpreted in the Cypher query?
    *   Reasoning: Explain your intent accuracy score, considering the explanation, alignment, and key element coverage.

4.  **Identification of Potential Qwen2.5-3B Error Patterns (Optional but helpful):**
    *   Based on your knowledge of smaller LLMs, does this query exhibit any common error patterns such as:
        *   Over-simplification of complex conditions?
        *   Incorrect handling of the ':ExtractedEntity' and 'label' property convention?
        *   Ignoring multiple constraints from the user question?
        *   Other (please specify)?
    *   Observed Patterns: List any observed patterns from the predefined list, or provide an empty list [] if none of the predefined patterns are clearly observed.

**OUTPUT JSON STRUCTURE (Strictly follow this format):**
```json
{
  "evaluation_summary": {
    "overall_quality_score_cypher": "<Integer score 1-5, your overall judgment of the Cypher query's quality. **Crucially, assign higher weight to 'Schema Adherence' and 'Intent Accuracy'.** A query with perfect syntax but critical flaws in schema adherence or intent accuracy should NOT receive a high overall score.>",
    "main_strength_cypher": "<Briefly describe the main strength of this Cypher query, if any>",
    "main_weakness_cypher": "<Briefly describe the main weakness or most critical issue>"
  },
  "dimensions": {
    "syntactic_correctness": {
      "score": "<Integer score 1-5>",
      "parsable_prediction": "<Boolean: true/false>",
      "reasoning": "<Text explanation>"
    },
    "schema_adherence": {
      "overall_score": "<Integer score 1-5>",
      "node_label_correctness": { "check_result": "<Boolean>", "detail": "<Text>" },
      "entity_type_property_correctness": { "check_result": "<Boolean>", "detail": "<Text>" },
      "relationship_type_correctness": { "check_result": "<Boolean>", "detail": "<Text>" },
      "property_name_correctness": { "check_result": "<Boolean>", "detail": "<Text>" },
      "hallucinated_schema_elements": { "check_result_hallucination_present": "<Boolean>", "elements_found": ["<List of strings or empty list>"] },
      "reasoning": "<Text explanation for overall schema adherence score>"
    },
    "intent_accuracy": {
      "score": "<Integer score 1-5>",
      "explanation_of_cypher_retrieval": "<Text>",
      "semantic_alignment_with_question": "<Text>",
      "key_element_coverage_notes": "<Text describing coverage of key elements, and what's missing/misinterpreted, if any>",
      "reasoning": "<Text explanation for intent accuracy score>"
    }
  },
  "qwen_error_patterns_identified": ["<List of strings describing observed patterns, or empty list>"],
  "suggestion_for_improvement_cypher": "<Actionable suggestions to improve this specific Cypher query, if applicable>"
}
```"""

ANSWER_EVALUATION_PROMPT_V1 = """
You are an expert AI Answer Evaluator, specializing in assessing the quality of responses from a Retrieval Augmented Generation (RAG) system designed as an "Office Worker Assistant". Your evaluation must be objective, strictly based on the provided user question, the context the RAG system used, and the generated answer.

**USER'S NATURAL LANGUAGE QUESTION:**
Use code with caution.
Python
{{USER_QUESTION}}
**CONTEXT PROVIDED TO THE RAG SYSTEM'S GENERATION MODEL (this is the information the AI had to base its answer on):**
Use code with caution.
{{RETRIEVED_CONTEXTS}}
**AI-GENERATED ANSWER TO EVALUATE:**
Use code with caution.
{{GENERATED_ANSWER}}
**EVALUATION TASK:**

Please evaluate the "AI-GENERATED ANSWER" based on the following criteria. For each dimension, provide a score from 1 to 5 (where 5 is best) and a brief reasoning for your score. Also, if applicable, identify specific phrases or sentences in the answer that exemplify an issue.

**EVALUATION DIMENSIONS & SCORING GUIDELINES:**

1.  **Faithfulness/Traceability (Score 1-5):**
    *   Is all factual information in the answer directly supported by the "CONTEXT PROVIDED"?
    *   Does the answer avoid making up information (hallucinations) or contradicting the context?
    *   **5 (Completely Faithful):** All key factual claims in the answer are directly and accurately supported by the context. No external information introduced.
    *   **4 (Mostly Faithful):** The vast majority of factual claims are supported. May contain very minor, reasonable inferences closely tied to the context, but no direct contradictions.
    *   **3 (Partially Faithful):** Some key claims are supported, but there are noticeable unsupported claims, slight misinterpretations of the context, or minor, non-critical hallucinations.
    *   **2 (Minimally Faithful):** Significant portions of the answer are not supported by the context, or there are clear contradictions or misleading hallucinations.
    *   **1 (Not Faithful):** The answer is largely based on information 외부 from the context, contains severe hallucinations, or directly contradicts the context.

2.  **Relevance to User Query (Score 1-5):**
    *   Does the answer directly and precisely address the "USER'S NATURAL LANGUAGE QUESTION"?
    *   **5 (Perfectly Relevant):** Directly and fully answers the user's core question(s).
    *   **4 (Highly Relevant):** Accurately answers the main aspects of the question; minor aspects might be less directly addressed.
    *   **3 (Moderately Relevant):** Addresses parts of the question but may miss key aspects or include some less relevant information.
    *   **2 (Slightly Relevant):** Touches upon the topic of the question but largely misses the core intent.
    *   **1 (Not Relevant):** Does not answer the user's question at all.

3.  **Completeness (Score 1-5):**
    *   **First, assess Context Sufficiency:** Based *only* on the "CONTEXT PROVIDED", does it seem to contain enough information to fully answer the "USER'S NATURAL LANGUAGE QUESTION"? (Answer: "Sufficient", "Partially Sufficient", or "Insufficient"). Provide a brief reason for your assessment of context sufficiency.
    *   **Then, score Completeness based on the answer's performance given the context:**
        *   Does the answer address all aspects of the user's query, making good use of the relevant information available in the context?
        *   If the context was insufficient, does the answer appropriately acknowledge this or focus on what can be answered?
        *   **5 (Very Complete):** (If context sufficient) Fully addresses all aspects of the query using all relevant context. (If context insufficient) Makes the best use of available context and clearly indicates limitations.
        *   **4 (Mostly Complete):** (If context sufficient) Addresses main aspects, minor details from context might be missed. (If context insufficient) Good use of available context, fair indication of limitations.
        *   **3 (Partially Complete):** (If context sufficient) Misses some important aspects or underutilizes relevant context. (If context insufficient) Poor use of available context or unclear about limitations.
        *   **2 (Slightly Complete):** (If context sufficient) Addresses only a small part, much relevant context ignored. (If context insufficient) Very poor use of limited context.
        *   **1 (Not Complete):** Fails to address the query meaningfully, even if relevant context was available.

4.  **Coherence/Fluency (Score 1-5):**
    *   Is the answer well-written, grammatically correct, logically structured, and easy to understand?
    *   **5 (Very Fluent):** Perfectly written, clear, natural, and easy to understand. No grammatical errors.
    *   **4 (Fluent):** Well-written, mostly clear, minor or no grammatical errors.
    *   **3 (Moderately Fluent):** Understandable, but may have some awkward phrasing or minor grammatical errors that don't impede core understanding.
    *   **2 (Slightly Fluent):** Difficult to understand due to grammatical errors, awkward phrasing, or poor logical flow.
    *   **1 (Not Fluent):** Largely incomprehensible.

5.  **Actionability & Usability (for an Office Worker Assistant) (Score 1-5):**
    *   Does the answer provide clear, practical, and easy-to-understand steps, information, or suggestions that would directly help an office worker achieve their task or make a decision?
    *   **5 (Highly Actionable & Usable):** Provides clear, specific, and immediately applicable steps/information. Language is professional and easy for an office worker to understand. Format facilitates quick information retrieval.
    *   **4 (Mostly Actionable & Usable):** Provides generally clear guidance or useful information. Might require minor clarification for full actionability, or presentation could be slightly improved, but core content is helpful.
    *   **3 (Partially Actionable & Usable):** Offers some relevant information or suggestions, but lacks specific steps, is too vague for direct action, or requires significant effort to understand/apply.
    *   **2 (Minimally Actionable & Usable):** Contains some related information but no clear action plan, is impractical, or very difficult to understand/use. Offers little practical help.
    *   **1 (Not Actionable & Unusable):** Provides no actionable information, is irrelevant to practical office tasks, or is misleading/confusing.

**OUTPUT JSON STRUCTURE (Strictly follow this format):**
```json
{
  "evaluation_summary": {
    "overall_answer_quality_score": "<Integer score 1-5, your overall judgment of the answer's quality, considering all dimensions. Faithfulness and Relevance are most critical.>",
    "main_strengths_answer": "<Briefly describe the main strength(s) of this answer, if any. Be specific.>",
    "main_weaknesses_answer": "<Briefly describe the main weakness(es) or most critical issue(s) with this answer. Be specific.>"
  },
  "dimensions": {
    "faithfulness": {
      "score": "<Integer score 1-5>",
      "reasoning": "<Text explanation for faithfulness score. If not fully faithful, specify which parts are unsupported or hallucinated, referencing the answer text.>",
      "problematic_answer_segments_faithfulness": ["<List of specific phrases/sentences from the answer that are not faithful, or empty list if none>"]
    },
    "relevance": {
      "score": "<Integer score 1-5>",
      "reasoning": "<Text explanation for relevance score. Explain how well it addresses the user's core question.>"
    },
    "completeness": {
      "context_sufficiency_assessment": "<String: 'Sufficient', 'Partially Sufficient', or 'Insufficient'>",
      "context_sufficiency_reasoning": "<Brief reason for the context sufficiency assessment. If not sufficient, what key information is missing from the context?>",
      "score": "<Integer score 1-5, based on how well the answer uses the available context to address the query>",
      "reasoning": "<Text explanation for completeness score. If not fully complete, what aspects of the query were missed, or what relevant context was underutilized? If context was insufficient, how well did the answer handle this?>"
    },
    "coherence_fluency": {
      "score": "<Integer score 1-5>",
      "reasoning": "<Text explanation for coherence/fluency score. Note any grammatical issues or awkward phrasing.>"
    },
    "actionability_usability": {
      "score": "<Integer score 1-5>",
      "reasoning": "<Text explanation for actionability/usability score, considering an office worker context.>"
    }
  },
  "suggestion_for_answer_improvement": "<Provide specific, actionable suggestions on how this AI-generated answer could be improved, if applicable. If excellent, state 'No improvement needed'.>"
}
Use code with caution.
FEW-SHOT EXAMPLES:
Example 1: Good Answer
USER'S NATURAL LANGUAGE QUESTION:
如何申请本年度的带薪年假，并确保符合公司规定？
Use code with caution.
CONTEXT PROVIDED TO THE RAG SYSTEM'S GENERATION MODEL:
上下文片段1 (来源: 公司政策文档):
员工每年享有15天带薪年假。申请年假需通过公司内部HRM系统提交，并至少提前两周提出。所有年假申请必须得到直属经理的批准。未经批准的休假将按事假处理。

上下文片段2 (来源: HRM系统操作指南):
HRM系统年假申请流程：1. 登录HRM门户 (hrm.example-company.com)。 2. 导航至“我的假期”->“假期申请”。 3. 选择“年假”类型，填写休假起止日期和休假天数。 4. 在备注中简要说明休假事由（可选）。 5. 提交申请，等待经理审批。审批状态可在系统中实时查看。

上下文片段3 (来源: 知识图谱精确信息):
事实: 年假政策规定 -> 提前申请周期: 2周。
事实: 年假政策规定 -> 审批人: 直属经理。
事实: 年假政策规定 -> 年假天数上限: 15天/年。
Use code with caution.
AI-GENERATED ANSWER TO EVALUATE:
您可以按照以下步骤申请本年度的带薪年假并确保符合公司规定：

1.  **提前规划与提交：** 根据公司政策，您需要至少提前两周提交年假申请。
2.  **通过HRM系统申请：**
    *   登录公司HRM门户网站：hrm.example-company.com
    *   进入“我的假期”模块，然后选择“假期申请”。
    *   选择“年假”类型，并准确填写您希望休假的起止日期和天数。
3.  **获取经理批准：** 所有年假申请都必须得到您直属经理的批准。提交申请后，请留意系统中的审批状态。
4.  **年假额度：** 请注意，公司规定员工每年享有15天带薪年假。

请确保您的申请符合以上规定，以顺利安排您的休假。
Use code with caution.
EXPECTED EVALUATION JSON OUTPUT:
{
  "evaluation_summary": {
    "overall_answer_quality_score": 5,
    "main_strengths_answer": "答案准确、完整、步骤清晰，非常实用。",
    "main_weaknesses_answer": "无明显缺点。"
  },
  "dimensions": {
    "faithfulness": {
      "score": 5,
      "reasoning": "答案中的所有信息点（提前两周、HRM系统操作、经理批准、15天年假）均能在提供的上下文中找到明确依据，无任何捏造。",
      "problematic_answer_segments_faithfulness": []
    },
    "relevance": {
      "score": 5,
      "reasoning": "答案完美地回应了用户关于年假申请流程和合规性的问题。"
    },
    "completeness": {
      "context_sufficiency_assessment": "Sufficient",
      "context_sufficiency_reasoning": "上下文提供了申请年假所需的所有核心信息（流程、提前时间、审批人、额度）。",
      "score": 5,
      "reasoning": "答案全面覆盖了用户问题的各个方面，并充分利用了上下文中的所有关键信息，包括操作步骤和政策规定。"
    },
    "coherence_fluency": {
      "score": 5,
      "reasoning": "答案语言表达清晰、专业，逻辑步骤明确，易于理解。"
    },
    "actionability_usability": {
      "score": 5,
      "reasoning": "答案提供了非常清晰、可直接执行的申请步骤，并点明了关键的合规性要求，对办公室员工非常实用且易于操作。"
    }
  },
  "suggestion_for_answer_improvement": "No improvement needed."
}
Use code with caution.
Json
Example 2: Answer with Hallucination
USER'S NATURAL LANGUAGE QUESTION:
我们公司最新的季度IT安全演练是什么时候举行的，主要发现了哪些问题？
Use code with caution.
CONTEXT PROVIDED TO THE RAG SYSTEM'S GENERATION MODEL:
上下文片段1 (来源: 内部邮件):
主题：关于第三季度IT安全意识培训的通知
内容：各位同事，为提升全员信息安全意识，公司计划于8月15日举行第三季度IT安全意识培训，主要内容包括密码安全、钓鱼邮件识别等。请大家准时参加。

上下文片段2 (来源: IT部门公告):
IT服务台提醒：近期发现多起针对财务部门的仿冒邮件，请大家提高警惕，不要点击不明链接。如有疑问，请及时联系IT支持。演练活动将在下月进行。
Use code with caution.
AI-GENERATED ANSWER TO EVALUATE:
公司最新的季度IT安全演练已于上个月（7月）成功举行。演练主要发现了两个问题：一是部分员工的办公电脑未及时更新防病毒软件，二是演练中模拟的钓鱼邮件成功欺骗了约15%的员工。后续将加强相关培训。
Use code with caution.
EXPECTED EVALUATION JSON OUTPUT:
{
  "evaluation_summary": {
    "overall_answer_quality_score": 1,
    "main_strengths_answer": "答案结构尚可。",
    "main_weaknesses_answer": "答案内容完全是编造的（幻觉），与提供的上下文信息完全不符，具有严重误导性。"
  },
  "dimensions": {
    "faithfulness": {
      "score": 1,
      "reasoning": "答案中关于演练已于7月举行、发现的两个具体问题（未更新防病毒软件、15%员工被钓鱼邮件欺骗）在上下文中完全找不到任何依据，是严重的幻觉。",
      "problematic_answer_segments_faithfulness": ["演练已于上个月（7月）成功举行。", "演练主要发现了两个问题：一是部分员工的办公电脑未及时更新防病毒软件，二是演练中模拟的钓鱼邮件成功欺骗了约15%的员工。"]
    },
    "relevance": {
      "score": 2,
      "reasoning": "答案表面上回应了问题（演练时间和问题），但由于内容是虚假的，其实际相关性很低。"
    },
    "completeness": {
      "context_sufficiency_assessment": "Partially Sufficient",
      "context_sufficiency_reasoning": "上下文提到了计划中的培训和演练（下月进行），以及一些安全问题（仿冒邮件），但没有给出已完成演练的具体时间和发现的问题。",
      "score": 1,
      "reasoning": "答案完全没有利用上下文中的有效信息（如计划中的培训和演练），而是编造了内容。"
    },
    "coherence_fluency": {
      "score": 4,
      "reasoning": "答案的语言表达本身是通顺的，语法基本正确。"
    },
    "actionability_usability": {
      "score": 1,
      "reasoning": "虚假的信息完全不可用，且具有误导性，对办公室工作有害无益。"
    }
  },
  "suggestion_for_answer_improvement": "AI模型必须严格基于提供的上下文生成答案，严禁编造任何上下文中未提及的事实。如果上下文信息不足，应明确指出。"
}
Use code with caution.
Json
Example 3: Incomplete Answer
USER'S NATURAL LANGUAGE QUESTION:
请总结一下我们和ABC公司最近一次会议的主要议题和达成的三项关键共识。
Use code with caution.
CONTEXT PROVIDED TO THE RAG SYSTEM'S GENERATION MODEL:
上下文片段1 (来源: 会议纪要 - ABC公司会议_20250515.docx):
会议日期：2025年5月15日
与会方：我方（李明、王芳），ABC公司（张总、赵经理）
主要议题：
1.  回顾Q1合作项目进展。
2.  讨论Q2新产品联合推广计划。
3.  探讨长期战略合作框架。
关键共识：
1.  双方同意Q1项目按计划完成，成果符合预期。
2.  Q2新产品联合推广预算初定为50万，具体方案下周讨论。
3.  双方均表达了加强长期战略合作的意愿，将成立联合工作组进一步商议。
4.  下次会议暂定于6月初。
Use code with caution.
AI-GENERATED ANSWER TO EVALUATE:
我们和ABC公司最近一次会议（2025年5月15日）的主要议题包括回顾Q1项目进展和讨论Q2新产品联合推广计划。会议达成的一项关键共识是双方同意Q1项目按计划完成。
Use code with caution.
EXPECTED EVALUATION JSON OUTPUT:
{
  "evaluation_summary": {
    "overall_answer_quality_score": 3,
    "main_strengths_answer": "答案忠实于上下文，相关性较好，语言通顺。",
    "main_weaknesses_answer": "答案在完整性方面有明显不足，遗漏了多个重要议题和关键共识。"
  },
  "dimensions": {
    "faithfulness": {
      "score": 5,
      "reasoning": "答案中提到的信息点（会议日期、部分议题、一项共识）均能在上下文中找到准确依据。",
      "problematic_answer_segments_faithfulness": []
    },
    "relevance": {
      "score": 4,
      "reasoning": "答案回应了用户关于会议议题和共识的问题，但不够全面。"
    },
    "completeness": {
      "context_sufficiency_assessment": "Sufficient",
      "context_sufficiency_reasoning": "上下文详细列出了3个主要议题和4项关键共识，足以完整回答用户问题。",
      "score": 2,
      "reasoning": "答案严重不完整。议题方面遗漏了“探讨长期战略合作框架”。关键共识方面，用户要求三项，但答案只给出了一项，遗漏了“Q2推广预算初定”、“加强长期战略合作意愿将成立工作组”这两项重要共识（甚至还有第四项共识也未提及）。"
    },
    "coherence_fluency": {
      "score": 5,
      "reasoning": "答案语言表达清晰、语法正确。"
    },
    "actionability_usability": {
      "score": 3,
      "reasoning": "答案提供了一些信息，但由于信息不完整，其实用性打了折扣。用户可能需要再次查找才能获得全部关键信息。"
    }
  },
  "suggestion_for_answer_improvement": "答案应更全面地从上下文中提取信息。应完整列出所有主要议题，并至少满足用户要求的三个关键共识。例如，可以补充：'其他主要议题还包括探讨长期战略合作框架。达成的其他关键共识有：Q2新产品联合推广预算初定为50万；双方将成立联合工作组进一步商议加强长期战略合作的意愿。'"
}
Use code with caution.
Json
NOW, EVALUATE THE FOLLOWING:
USER'S NATURAL LANGUAGE QUESTION:
{{USER_QUESTION}}
Use code with caution.
CONTEXT PROVIDED TO THE RAG SYSTEM'S GENERATION MODEL (this is the information the AI had to base its answer on):
{{RETRIEVED_CONTEXTS}}
Use code with caution.
AI-GENERATED ANSWER TO EVALUATE:
{{GENERATED_ANSWER}}
Use code with caution.
YOUR EVALUATION (Strictly in the JSON format defined above):
// Your JSON output here
Use code with caution.
Json
"""



async def evaluate_cypher_with_gemini(
    gemini_resource_for_evaluator: "GeminiAPIClient",
    user_question: str,
    generated_cypher: str,
    original_interaction_id: Optional[str] = None,
    app_version: str = "0.1.0"
) -> Optional[Dict[str, Any]]:
    eval_logger.info(f"Starting Cypher evaluation. User question: '{user_question[:50]}...', Cypher: '{generated_cypher[:100]}...'")

    prompt_to_gemini = CYPHER_EVALUATION_PROMPT_V1.replace(
        "{{KG_SCHEMA_DESCRIPTION}}", KG_SCHEMA_FOR_EVALUATION
    ).replace(
        "{{USER_QUESTION}}", user_question
    ).replace(
        "{{GENERATED_CYPHER}}", generated_cypher
    )
    evaluation_result_json: Optional[Dict[str, Any]] = None
    raw_gemini_output: Optional[str] = None
    error_info: Optional[str] = None
    model_name_for_log = "gemini-1.5-flash-latest"

    try:
        eval_logger.info(f"Calling Gemini for Cypher evaluation. Model: {model_name_for_log}. Prompt length: {len(prompt_to_gemini)}")
        gemini_model = gemini_resource_for_evaluator.get_model(model_name_for_log)
        if not gemini_model:
            raise ValueError("Failed to get Gemini model from the client for Cypher eval.")

        response = await gemini_model.generate_content_async(
            contents=prompt_to_gemini,
            generation_config=genai.types.GenerationConfig(temperature=0.1)
        )
        raw_gemini_output = response.text

        if raw_gemini_output:
            cleaned_output = raw_gemini_output.strip()
            if cleaned_output.startswith("```json"):
                cleaned_output = cleaned_output[len("```json"):].strip()
            if cleaned_output.endswith("```"):
                cleaned_output = cleaned_output[:-len("```")].strip()
            evaluation_result_json = json.loads(cleaned_output)
            eval_logger.info("Successfully parsed Gemini evaluation result for Cypher.")
        else:
            error_info = "Gemini call returned None/empty"
    except Exception as e:
        error_info = f"Error during Gemini call or parsing: {str(e)}"
        raw_gemini_output = raw_gemini_output or str(e)
        eval_logger.error(f"Error in evaluate_cypher_with_gemini: {e}", exc_info=True)
        
    eval_log_data = {
        "task_type": "cypher_evaluation_by_gemini",
        "original_interaction_id_ref": original_interaction_id,
        "user_question_for_eval": user_question,
        "generated_cypher_for_eval": generated_cypher,
        "eval_llm_input_prompt_char_count": len(prompt_to_gemini),
        "eval_llm_model": model_name_for_log,
        "eval_llm_raw_output": raw_gemini_output,
        "eval_llm_processed_output_json": evaluation_result_json,
        "eval_error_info": error_info,
        "application_version": app_version
    }
    await log_interaction_data(
        eval_log_data,
        is_evaluation_result=True,
        evaluation_name_for_file="cypher_gemini_flash"
    )
    return evaluation_result_json

async def evaluate_answer_with_gemini(
    gemini_resource_for_evaluator: "GeminiAPIClient",
    user_question: str,
    retrieved_contexts: List["RetrievedDocument"],
    generated_answer: str,
    original_interaction_id: Optional[str] = None,
    app_version: str = "0.1.0",
    use_simulated_api: bool = False,
    api_call_delay: int = 0
) -> Optional[Dict[str, Any]]:
    context_strings = [
        f"Source Type: {doc.source_type}, Score: {f'{doc.score:.4f}' if isinstance(doc.score, float) else doc.score}\nContent: {doc.content}"
        for doc in retrieved_contexts
    ]
    fused_context_for_prompt = "\n\n---\n\n".join(context_strings)
    eval_logger.info(f"Starting Answer evaluation. User question: '{user_question[:50]}...', Answer: '{generated_answer[:50]}...'")

    prompt_to_gemini = ANSWER_EVALUATION_PROMPT_V1.replace(
        "{{USER_QUESTION}}", user_question
    ).replace(
        "{{RETRIEVED_CONTEXTS}}", fused_context_for_prompt
    ).replace(
        "{{GENERATED_ANSWER}}", generated_answer
    )
    evaluation_result_json: Optional[Dict[str, Any]] = None
    raw_gemini_output: Optional[str] = None
    error_info: Optional[str] = None
    model_name_for_log = "gemini-1.5-flash-latest"

    if use_simulated_api:
        eval_logger.warning("USING SIMULATED GEMINI RESPONSE FOR ANSWER EVALUATION")
        # ... (simulated response logic)
    else:
        try:
            eval_logger.info(f"Calling Gemini for Answer evaluation. Model: {model_name_for_log}. Prompt length: {len(prompt_to_gemini)}")
            gemini_model = gemini_resource_for_evaluator.get_model(model_name_for_log)
            if not gemini_model:
                raise ValueError("Failed to get Gemini model from the client for Answer eval.")

            response = await gemini_model.generate_content_async(
                contents=prompt_to_gemini,
                generation_config=genai.types.GenerationConfig(temperature=0.1)
            )
            raw_gemini_output = response.text

            if raw_gemini_output:
                eval_logger.info(f"Raw Gemini output for Answer eval (first 300 chars): {raw_gemini_output[:300]}...")
                cleaned_output = raw_gemini_output.strip()
                if cleaned_output.startswith("```json"):
                    cleaned_output = cleaned_output[len("```json"):].strip()
                if cleaned_output.endswith("```"):
                    cleaned_output = cleaned_output[:-len("```")].strip()
                evaluation_result_json = json.loads(cleaned_output)
                eval_logger.info("Successfully parsed Gemini evaluation result for Answer.")
            else:
                error_info = "Gemini call returned None/empty for answer"
        except Exception as e:
            error_info = f"Error during Gemini call or parsing: {str(e)}"
            raw_gemini_output = raw_gemini_output or str(e)
            eval_logger.error(f"Error in evaluate_answer_with_gemini: {e}", exc_info=True)

    eval_log_data = {
        "task_type": "answer_evaluation_result",
        "original_interaction_id_ref": original_interaction_id,
        "user_question_for_eval": user_question,
        "retrieved_contexts_for_eval_char_count": len(fused_context_for_prompt),
        "generated_answer_for_eval": generated_answer,
        "eval_llm_input_prompt_char_count": len(prompt_to_gemini),
        "eval_llm_model": model_name_for_log,
        "eval_llm_raw_output": raw_gemini_output,
        "eval_llm_processed_output_json": evaluation_result_json,
        "eval_error_info": error_info,
        "application_version": app_version
    }
    await log_interaction_data(
        log_data=eval_log_data,
        is_evaluation_result=True,
        evaluation_name_for_file="answer_gemini_flash"
    )

    if evaluation_result_json:
        eval_logger.info(f"Answer evaluation completed. Overall score: {evaluation_result_json.get('evaluation_summary', {}).get('overall_answer_quality_score')}")
    else:
        eval_logger.warning("Answer evaluation did not produce a valid JSON result.")
        
    return evaluation_result_json
```

        |-- evaluation_questions_v1.txt
        |-- evaluation_questions_v2.txt
    |-- __init__.py

``` py

```

    |-- embedding_api_service.py

``` py
# zhz_rag/api/embedding_api_service.py

import os
import sys
import logging
from typing import List, Optional
from fastapi import FastAPI, HTTPException
from pydantic import BaseModel, Field
import uvicorn
from contextlib import asynccontextmanager
from dotenv import load_dotenv # <--- 确保导入

# --- START: 覆盖这部分代码 ---
# 无论从哪里运行，都确保能找到项目根目录的 .env 文件
# __file__ -> /home/zhz/zhz_agent/zhz_rag/api/embedding_api_service.py
# .parents[0] -> .../api
# .parents[1] -> .../zhz_rag
# .parents[2] -> /home/zhz/zhz_agent
try:
    from pathlib import Path
    project_root = Path(__file__).resolve().parents[2]
    dotenv_path = project_root / '.env'
    if dotenv_path.exists():
        load_dotenv(dotenv_path=dotenv_path)
        print(f"EmbeddingApiService: Successfully loaded .env from absolute path: {dotenv_path}")
    else:
        print(f"EmbeddingApiService: .env file not found at {dotenv_path}. Relying on system environment variables.")
except Exception as e:
    print(f"EmbeddingApiService: Error loading .env file: {e}")
# --- END: 覆盖结束 ---
# --- 配置路径以导入 llama_cpp ---
# 确保 llama_cpp 可被找到
try:
    from llama_cpp import Llama
except ImportError:
    print("FATAL: llama-cpp-python is not installed. Please run 'pip install llama-cpp-python'")
    sys.exit(1)

# --- 日志配置 ---
api_logger = logging.getLogger("EmbeddingApiServiceLogger")
api_logger.setLevel(logging.INFO)
if not api_logger.hasHandlers():
    handler = logging.StreamHandler(sys.stdout)
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    handler.setFormatter(formatter)
    api_logger.addHandler(handler)
    api_logger.propagate = False

# --- Pydantic 模型定义 ---
class EmbeddingRequest(BaseModel):
    texts: List[str] = Field(..., description="A list of texts to be embedded.")

class EmbeddingResponse(BaseModel):
    embeddings: List[List[float]] = Field(description="A list of embedding vectors.")
    dimensions: int = Field(description="The dimension of the embedding vectors.")

# --- 全局变量用于缓存模型 ---
embedding_model: Optional[Llama] = None
model_dimension: Optional[int] = None

# --- FastAPI 生命周期管理 ---
@asynccontextmanager
async def lifespan(app: FastAPI):
    global embedding_model, model_dimension
    api_logger.info("--- Embedding API Service: Initializing... ---")
    
    model_path = os.getenv("EMBEDDING_MODEL_PATH")
    n_ctx = int(os.getenv("EMBEDDING_N_CTX", "2048"))
    n_gpu_layers = int(os.getenv("EMBEDDING_N_GPU_LAYERS", "0"))

    if not model_path or not os.path.exists(model_path):
        api_logger.error(f"FATAL: Embedding model path not found or not set in .env: {model_path}")
        raise FileNotFoundError(f"Embedding model not found at {model_path}")

    try:
        api_logger.info(f"Loading embedding model from: {model_path}")
        embedding_model = Llama(
            model_path=model_path,
            embedding=True,
            n_ctx=n_ctx,
            n_gpu_layers=n_gpu_layers,
            verbose=False
        )
        model_dimension = embedding_model.n_embd()
        api_logger.info(f"Embedding model loaded successfully. Dimension: {model_dimension}")
    except Exception as e:
        api_logger.error(f"Failed to load embedding model: {e}", exc_info=True)
        embedding_model = None
        model_dimension = None

    yield

    api_logger.info("--- Embedding API Service: Shutting down. ---")
    # 清理模型（如果需要）
    if embedding_model:
        del embedding_model
        embedding_model = None

# --- FastAPI 应用实例 ---
app = FastAPI(
    title="Standalone Embedding API Service",
    description="A dedicated API service for generating text embeddings using a GGUF model.",
    version="1.0.0",
    lifespan=lifespan
)

# --- API 端点 ---
@app.post("/embed", response_model=EmbeddingResponse)
async def create_embeddings(request: EmbeddingRequest):
    if not embedding_model or not model_dimension:
        raise HTTPException(status_code=503, detail="Embedding model is not available.")
    
    if not request.texts:
        return EmbeddingResponse(embeddings=[], dimensions=model_dimension)
    
    try:
        api_logger.info(f"Received request to embed {len(request.texts)} texts.")
        response = embedding_model.create_embedding(request.texts)
        
        embeddings_raw = response.get('data', [])
        final_embeddings: List[List[float]] = []
        for item in embeddings_raw:
            embedding_data = item.get('embedding', [])
            
            # --- 核心修复：处理可能的嵌套列表 ---
            # 检查embedding_data是否是列表，并且其第一个元素也是列表
            if isinstance(embedding_data, list) and embedding_data and isinstance(embedding_data[0], list):
                # 如果是嵌套列表，我们只取第一个内部列表
                embedding_vector_raw = embedding_data[0]
                api_logger.debug("Detected a nested list in embedding output, taking the first element.")
            else:
                # 否则，我们假设它已经是我们期望的扁平列表
                embedding_vector_raw = embedding_data
            
            # 确保向量中的每个元素都是float
            embedding_vector = [float(x) for x in embedding_vector_raw]
            final_embeddings.append(embedding_vector)
            # --- 修复结束 ---

        api_logger.info(f"Successfully generated {len(final_embeddings)} embeddings.")
        return EmbeddingResponse(embeddings=final_embeddings, dimensions=model_dimension)
    except Exception as e: 
        api_logger.error(f"Error during embedding creation: {e}", exc_info=True)
        raise HTTPException(status_code=500, detail=f"Failed to create embeddings: {e}")

@app.get("/health")
async def health_check():
    if embedding_model and model_dimension:
        return {"status": "ok", "model_loaded": True, "dimension": model_dimension}
    else:
        return {"status": "error", "model_loaded": False, "message": "Embedding model failed to load."}

if __name__ == "__main__":
    # 默认运行在 8089 端口，避免与现有服务冲突
    port = int(os.getenv("EMBEDDING_API_PORT", "8089"))
    api_logger.info(f"Starting Embedding API Service on port {port}...")
    uvicorn.run("zhz_rag.api.embedding_api_service:app", host="0.0.0.0", port=port, reload=False)
```

    |-- main_api.py

``` py
# zhz_agent/main.py
from fastapi import FastAPI
from contextlib import asynccontextmanager
import uvicorn
import logging
from typing import Type, List, Dict, Any, Optional, ClassVar

# --- [修改] 导入 -> 改为绝对导入 ---
from zhz_rag.utils.db_utils import database, sqlalchemy_engine, Base, get_scheduler
from zhz_rag.api.task_manager_api import router as tasks_router
from zhz_rag.task_management import db_models # 确保 SQLAlchemy 模型被导入

@asynccontextmanager
async def lifespan(app_instance: FastAPI):
    print("--- Main FastAPI 应用启动 (已集成任务管理API 和 APScheduler) ---") # [修改] 更新了描述

    # --- 数据库初始化 ---
    await database.connect()
    print("数据库已连接。")
    try:
        Base.metadata.create_all(bind=sqlalchemy_engine)
        print("数据库表已执行 create_all。")
        from sqlalchemy import inspect
        inspector = inspect(sqlalchemy_engine)
        if inspector.has_table("tasks"):
            print("'tasks' 表已成功创建/存在于数据库中。")
        else:
            print("警告: 'tasks' 表在 create_all 之后仍未找到！这通常意味着模型没有在 create_all 之前被正确导入。")
            print(f"   已知的表: {inspector.get_table_names()}") # 打印所有实际创建的表
            print(f"   Base.metadata.tables: {Base.metadata.tables.keys()}") # 打印 SQLAlchemy 元数据中已注册的表
    except Exception as e:
        print(f"创建或检查数据库表时出错: {e}")
        import traceback
        traceback.print_exc() # 打印详细的异常堆栈


    # --- [修改] APScheduler 初始化 (使用 get_scheduler) ---
    current_scheduler = get_scheduler() # <--- 获取调度器实例
    try:
        logging.getLogger('apscheduler').setLevel(logging.DEBUG) # 设置为 DEBUG 级别

        if not current_scheduler.running: # <--- 只有在未运行时才启动
            current_scheduler.start()
            print("APScheduler 已启动并使用数据库作业存储。")
        else:
            print("APScheduler 已在运行。")
    except Exception as e:
        print(f"APScheduler 启动失败: {e}")

    print("RAG 组件的初始化和管理在 zhz_agent_mcp_server.py。")
    print("任务管理API已在 /tasks 路径下可用。")

    yield # FastAPI 应用在此运行

    print("--- Main FastAPI 应用关闭 ---")
    current_scheduler_on_shutdown = get_scheduler() # <--- 再次获取以确保是同一个实例
    if current_scheduler_on_shutdown and current_scheduler_on_shutdown.running:
        current_scheduler_on_shutdown.shutdown()
        print("APScheduler 已关闭。")
    await database.disconnect()
    print("数据库已断开连接。")
    print("RAG 组件的清理在 zhz_agent_mcp_server.py。")

# --- App 定义 (保持不变) ---
app = FastAPI(
    title="Hybrid RAG Backend with Task Management",
    description="主 FastAPI 应用，负责接收请求、编排 Agent，并提供任务管理API。",
    version="0.2.1",
    lifespan=lifespan
)

app.include_router(tasks_router)

@app.get("/")
async def read_root():
    return {
        "message": "Welcome to the Hybrid RAG Backend Main App.",
        "available_services": {
            "task_management": "/tasks/docs",
            "rag_via_mcpo": "mcpo proxy at port 8006 (see mcpo_servers.json)"
        }
    }

if __name__ == "__main__":
    print("--- 启动 Main FastAPI 服务器 (包含任务管理API) ---")
    uvicorn.run("zhz_agent.main:app", host="0.0.0.0", port=8000, reload=True) # Ensure correct run command
```

    |-- rag_api_service.py

``` py
# /home/zhz/zhz_agent/zhz_rag/api/rag_api_service.py
# 版本: 3.1.0 - 手动实现 Small-to-Big Retrieval (更新异步检索调用, 修复prompts导入)

import pandas as pd
import io
import os
import asyncio
from contextlib import asynccontextmanager
from typing import List, Dict, Any, Optional
import logging
import sys
import uvicorn
import traceback
from fastapi import FastAPI, Request, HTTPException
from dataclasses import dataclass
from dotenv import load_dotenv
import uuid
from datetime import datetime, timezone
from cachetools import TTLCache
import hashlib
import json
# LangChain 相关导入 - 我们仍然需要 Document 和 InMemoryStore
from langchain.storage import InMemoryStore
from langchain.docstore.document import Document as LangchainDocument
from dataclasses import dataclass, field


# --- .env 文件加载 ---
_current_file_dir = os.path.dirname(os.path.abspath(__file__))
_project_root_dir = os.path.abspath(os.path.join(_current_file_dir, "..", ".."))
_dotenv_path = os.path.join(_project_root_dir, ".env")
if os.path.exists(_dotenv_path):
    load_dotenv(dotenv_path=_dotenv_path)
else:
    load_dotenv()

# --- 导入我们自己的模块 ---
from zhz_rag.config.pydantic_models import QueryRequest, HybridRAGResponse, RetrievedDocument
from zhz_rag.llm.llm_interface import (
    generate_answer_from_context,
    generate_query_plan,
    generate_table_lookup_instruction,
    generate_actionable_suggestion,
    generate_expanded_queries,
    generate_document_summary, 
    NO_ANSWER_PHRASE_ANSWER_CLEAN
)
from zhz_rag.utils.hardware_manager import HardwareManager

from zhz_rag.llm.rag_prompts import get_answer_generation_messages, get_table_qa_messages, get_fusion_messages
from zhz_rag.core_rag.retrievers.chromadb_retriever import ChromaDBRetriever
from zhz_rag.core_rag.retrievers.file_bm25_retriever import FileBM25Retriever
from zhz_rag.core_rag.retrievers.embedding_functions import LlamaCppEmbeddingFunction
from zhz_rag.llm.local_model_handler import LlamaCppEmbeddingFunction as LocalModelHandlerWrapper
from zhz_rag_pipeline_dagster.zhz_rag_pipeline.resources import GGUFEmbeddingResource
from zhz_rag.utils.interaction_logger import log_interaction_data


from enum import Enum
from zhz_rag.core_rag.fusion_engine import FusionEngine 
class HardwareTier(Enum):
    HIGH = "HIGH"
    MID = "MID"
    LOW = "LOW"

def _get_hardware_tier(hw_manager: HardwareManager) -> HardwareTier:
    """根据硬件信息判断硬件等级。"""
    hw_info = hw_manager.get_hardware_info()
    if not hw_info:
        return HardwareTier.LOW
    
    # 简化版策略：VRAM > 10GB 认为是高配，VRAM > 6GB认为是中配
    if hw_info.gpu_available and hw_info.gpu_vram_total_gb > 10:
        return HardwareTier.HIGH
    elif hw_info.gpu_available and hw_info.gpu_vram_total_gb > 6:
        return HardwareTier.MID
    else:
        return HardwareTier.LOW


# --- 日志配置 ---
api_logger = logging.getLogger("RAGApiServiceLogger")
api_logger.setLevel(logging.INFO)
if not api_logger.hasHandlers():
    handler = logging.StreamHandler(sys.stdout)
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(module)s:%(lineno)d - %(message)s')
    handler.setFormatter(handler)
    api_logger.addHandler(handler)
    api_logger.propagate = False

# --- 生产者-消费者队列 ---
log_queue = asyncio.Queue()

async def log_writer_task():
    """
    后台任务，用于从队列中获取日志条目并写入交互数据。
    """
    while True:
        try:
            log_entry_to_write = await log_queue.get()
            await log_interaction_data(log_entry_to_write) 
            log_queue.task_done()
        except Exception as e:
            api_logger.error(f"Critical error in log_writer_task: {e}", exc_info=True)

# --- 应用上下文 Dataclass ---
@dataclass
class RAGAppContext:
    """
    RAG应用程序的上下文，包含所有核心组件的实例。
    """
    chroma_retriever: ChromaDBRetriever 
    bm25_retriever: Optional[FileBM25Retriever]
    docstore: InMemoryStore
    gguf_embedding_resource: GGUFEmbeddingResource
    answer_cache: TTLCache
    llm_gbnf_instance: Any
    fusion_engine: FusionEngine
    hardware_tier: HardwareTier


@asynccontextmanager
async def lifespan(app: FastAPI):
    """
    FastAPI应用的生命周期管理器。
    负责在应用启动时初始化RAG组件，并在应用关闭时清理资源。
    """
    api_logger.info("--- RAG API Service (v5.4 - HAL Corrected): Initializing... ---")
    
    # --- 1. 使用 HAL 获取硬件建议 ---
    hal = HardwareManager()
    
    # --- 2. 在应用层加载 GBNF LLM 模型 ---
    gbnf_llm = None
    model_path = os.getenv("LOCAL_LLM_GGUF_MODEL_PATH")
    if not model_path or not os.path.exists(model_path):
        api_logger.error(f"LLM model path not found or invalid: {model_path}. GBNF features will be disabled.")
    else:
        try:
            model_size_gb = os.path.getsize(model_path) / (1024**3)
            model_total_layers = 28 # Qwen3-1.7B
            
            n_gpu_layers = hal.recommend_llm_gpu_layers(
                model_total_layers=model_total_layers,
                model_size_on_disk_gb=model_size_gb
            )
            
            from llama_cpp import Llama
            gbnf_llm = Llama(
                model_path=model_path,
                n_gpu_layers=n_gpu_layers,
                n_ctx=int(os.getenv("LLM_N_CTX", 4096)),
                verbose=False
            )
        except Exception as e:
            api_logger.critical(f"FATAL: Failed to pre-load GBNF LLM model: {e}", exc_info=True)
            gbnf_llm = None

    # --- 3. 初始化其他服务 ---
    embedding_api_url = os.getenv("EMBEDDING_API_URL", "http://127.0.0.1:8089")
    chroma_persist_dir = os.getenv("CHROMA_PERSIST_DIRECTORY")
    chroma_collection_name = os.getenv("CHROMA_COLLECTION_NAME", "zhz_rag_collection")
    bm25_index_dir = os.getenv("BM25_INDEX_DIRECTORY")


    class FakeDagsterContext:
        def __init__(self, logger_instance): self.log = logger_instance
    
    gguf_embed_resource = GGUFEmbeddingResource(api_url=embedding_api_url)
    await asyncio.to_thread(gguf_embed_resource.setup_for_execution, FakeDagsterContext(api_logger))
    model_handler = LocalModelHandlerWrapper(resource=gguf_embed_resource)
    chroma_embedding_function = LlamaCppEmbeddingFunction(model_handler=model_handler)
    
    try:
        # --- 初始化 ChromaDB ---
        chroma_retriever_instance = ChromaDBRetriever(
            collection_name=chroma_collection_name,
            persist_directory=chroma_persist_dir,
            embedding_function=chroma_embedding_function
        )

        # --- 初始化 BM25 检索器 ---
        bm25_retriever_instance = None
        if bm25_index_dir and os.path.isdir(bm25_index_dir):
            try:
                bm25_retriever_instance = FileBM25Retriever(index_directory=bm25_index_dir)
            except Exception as e:
                api_logger.error(f"Failed to initialize FileBM25Retriever: {e}", exc_info=True)
                bm25_retriever_instance = None 
        else:
            api_logger.warning(f"BM25_INDEX_DIRECTORY not set or is not a valid directory. BM25 search will be disabled.")


        api_logger.info("Building docstore from ChromaDB metadata upon service startup...")
        docstore = InMemoryStore()
        
        try:
            count = chroma_retriever_instance._collection.count()
            if count > 0:
                all_chunks_from_db = chroma_retriever_instance._collection.get(include=["metadatas", "documents"])
                
                parent_docs_map: Dict[str, Dict[str, Any]] = {}
                for i, metadata in enumerate(all_chunks_from_db['metadatas']):
                    parent_id = metadata.get("parent_id")
                    if parent_id:
                        if parent_id not in parent_docs_map:
                            parent_docs_map[parent_id] = {"metadata": metadata, "texts": []}
                        parent_docs_map[parent_id]["texts"].append(all_chunks_from_db['documents'][i])
                
                docs_to_store_in_docstore = [
                    LangchainDocument(
                        page_content="\n\n".join(sorted(data["texts"])), 
                        metadata={**data["metadata"], "doc_id": parent_id}
                    ) for parent_id, data in parent_docs_map.items()
                ]
                docstore.mset([(doc.metadata["doc_id"], doc) for doc in docs_to_store_in_docstore])
            else:
                api_logger.warning(f"ChromaDB collection '{chroma_collection_name}' is empty. Docstore will be empty.")

        except Exception as e:
            api_logger.error(f"Failed to build docstore during startup: {e}", exc_info=True)
            docstore = InMemoryStore() 
            
        # --- 硬件检测与策略决策 ---
        hw_manager = HardwareManager()
        tier = _get_hardware_tier(hw_manager)

        # 根据硬件等级决定是否启用再排序器
        enable_reranker_flag = (tier == HardwareTier.HIGH)

        # --- 初始化 FusionEngine (现在带有策略) ---
        fusion_engine_instance = FusionEngine(
            logger=api_logger, 
            enable_reranker=enable_reranker_flag
        )

        app.state.rag_context = RAGAppContext(
            chroma_retriever=chroma_retriever_instance,
            bm25_retriever=bm25_retriever_instance,
            docstore=docstore,
            gguf_embedding_resource=gguf_embed_resource,
            answer_cache=TTLCache(maxsize=100, ttl=900),
            llm_gbnf_instance=gbnf_llm,
            fusion_engine=fusion_engine_instance,
            hardware_tier=tier
        )
        
        asyncio.create_task(log_writer_task())
    except Exception as e:
        api_logger.critical(f"FATAL: Failed to initialize RAG components: {e}", exc_info=True)
        app.state.rag_context = None
    
    yield
    
    api_logger.info("--- RAG API Service: Cleaning up resources ---")
    
    if hasattr(app.state, 'rag_context') and app.state.rag_context and app.state.rag_context.gguf_embedding_resource:
        if hasattr(app.state.rag_context.gguf_embedding_resource, 'teardown_for_execution'):
            class FakeDagsterContextTeardown:
                def __init__(self, logger_instance):
                    self.log = logger_instance
            fake_dagster_context_teardown = FakeDagsterContextTeardown(api_logger)
            await asyncio.to_thread(app.state.rag_context.gguf_embedding_resource.teardown_for_execution, fake_dagster_context_teardown)
        else:
            api_logger.warning("GGUFEmbeddingResource does not have a teardown_for_execution method.")
    else:
        api_logger.warning("No RAGAppContext or GGUFEmbeddingResource found for teardown.")

    if hasattr(app.state, 'rag_context') and app.state.rag_context and app.state.rag_context.llm_gbnf_instance:
        app.state.rag_context.llm_gbnf_instance = None


def _fuse_results_rrf(
    vector_results: List[List[Dict[str, Any]]], 
    keyword_results: List[List[Dict[str, Any]]], 
    k: int = 60
) -> List[Dict[str, Any]]:
    """
    使用倒数排序融合（Reciprocal Rank Fusion - RRF）来合并向量和关键词搜索的结果。
    
    Args:
        vector_results: 来自向量检索器的结果列表（每个子查询一个列表）。
        keyword_results: 来自关键词检索器的结果列表（每个子查询一个列表）。
        k: RRF算法中的排名常数，用于降低低排名结果的权重。

    Returns:
        一个融合、去重并按RRF分数重新排序的文档块列表。
    """
    # 步骤1: 将来自多个子查询的结果平铺成两个总的排名列表
    flat_vector_results = [chunk for sublist in vector_results for chunk in sublist]
    flat_keyword_results = [chunk for sublist in keyword_results for chunk in sublist]
    
    scores = {}
    all_docs_map = {}

    # 步骤2: 计算RRF分数
    # 处理向量搜索结果
    for rank, doc in enumerate(flat_vector_results):
        doc_id = doc.get("id")
        if not doc_id: continue
        scores[doc_id] = 1 / (k + rank + 1)
        all_docs_map[doc_id] = doc

    # 处理关键词搜索结果
    for rank, doc in enumerate(flat_keyword_results):
        doc_id = doc.get("id")
        if not doc_id: continue
        if doc_id not in scores:
            scores[doc_id] = 0
        scores[doc_id] += 1 / (k + rank + 1)
        all_docs_map.setdefault(doc_id, doc) 
    
    # 步骤3: 按RRF分数降序排序
    sorted_doc_ids = sorted(scores.keys(), key=lambda x: scores[x], reverse=True)
    
    # 步骤4: 构建最终的融合结果列表
    fused_results = [all_docs_map[doc_id] for doc_id in sorted_doc_ids]

    return fused_results


# --- FastAPI 应用实例 ---
app = FastAPI(
    title="Advanced RAG API Service with Manual Small-to-Big Retrieval",
    description="Provides API access to the RAG framework, now with manual small-to-big retrieval.",
    version="3.1.0",
    lifespan=lifespan
)


@app.post("/api/v1/rag/query", response_model=HybridRAGResponse)
async def query_rag_endpoint(request: Request, query_request: QueryRequest):
    """
    处理RAG查询请求，执行混合检索、文档重排和答案生成。

    Args:
        request: FastAPI请求对象。
        query_request: 包含用户查询和检索参数的请求体。

    Returns:
        HybridRAGResponse: 包含生成的答案和检索到的源文档。
    """
    start_time_total = datetime.now(timezone.utc)
    app_ctx: RAGAppContext = request.app.state.rag_context
    if not app_ctx or not app_ctx.llm_gbnf_instance or not app_ctx.fusion_engine:
        raise HTTPException(status_code=503, detail="RAG service or its core components are not initialized.")

    interaction_id_for_log = str(uuid.uuid4())
    exception_occurred: Optional[Exception] = None
    response_to_return: Optional[HybridRAGResponse] = None

    try:
        # 缓存逻辑
        cache_key = hashlib.md5(query_request.model_dump_json().encode('utf-8')).hexdigest()
        cached_response = app_ctx.answer_cache.get(cache_key)
        if cached_response is not None:
            return cached_response

        # 1. 查询规划与扩展
        query_plan = await generate_query_plan(app_ctx.llm_gbnf_instance, query_request.query)
        
        # --- 智能查询扩展策略 ---
        # 只有在查询计划没有生成具体的元数据过滤器时，才执行查询扩展。
        # 这可以防止在精确查找（如表格问答）时，通用扩展查询干扰检索结果。
        if not query_plan.metadata_filter:
            api_logger.info("No specific metadata filter found in plan. Performing query expansion.")
            sub_queries = await generate_expanded_queries(app_ctx.llm_gbnf_instance, query_request.query)
            unique_queries = list(dict.fromkeys([query_plan.query] + sub_queries))
        else:
            api_logger.info("Metadata filter found in plan. Skipping query expansion to ensure precision.")
            unique_queries = [query_plan.query]

            
        # --- 构建元数据过滤器 ---
        user_filter_conditions = query_request.filters.get("must", []) if query_request.filters else []
        llm_filter = query_plan.metadata_filter if query_plan else {}
        all_conditions = []
        for condition in user_filter_conditions:
            key = condition.get("key")
            match = condition.get("match")
            if key and match and "value" in match:
                all_conditions.append({key: {"$eq": match["value"]}})
        if llm_filter:
            if "$and" in llm_filter:
                all_conditions.extend(llm_filter["$and"])
            elif llm_filter:
                all_conditions.append(llm_filter)
        final_metadata_filter = {}
        if all_conditions:
            unique_conditions_as_strings = {json.dumps(c, sort_keys=True) for c in all_conditions}
            unique_conditions = [json.loads(s) for s in unique_conditions_as_strings]
            if len(unique_conditions) == 1:
                final_metadata_filter = unique_conditions[0]
            else:
                final_metadata_filter = {"$and": unique_conditions}

        # --- 并行执行向量和关键词检索 ---
        vector_retrieval_tasks = [
            app_ctx.chroma_retriever.retrieve(
                query_text=q,
                n_results=query_request.top_k_vector,
                where_filter=final_metadata_filter or None
            ) for q in unique_queries
        ]
        
        keyword_retrieval_tasks = []
        if app_ctx.bm25_retriever:
            keyword_retrieval_tasks = [
                app_ctx.bm25_retriever.retrieve(
                    query_text=q,
                    n_results=query_request.top_k_bm25
                ) for q in unique_queries
            ]
        else:
            pass

        # 等待所有检索任务完成
        all_retrieval_results = await asyncio.gather(*(vector_retrieval_tasks + keyword_retrieval_tasks))
        
        # 分离结果
        vector_results = all_retrieval_results[:len(vector_retrieval_tasks)]
        keyword_results = all_retrieval_results[len(vector_retrieval_tasks):]
        
        # --- 使用RRF融合结果 ---
        fused_child_chunks = _fuse_results_rrf(vector_results, keyword_results)
        
        # --- 从融合后的子块中提取父文档 ---
        all_parent_ids = {chunk['metadata']['parent_id'] for chunk in fused_child_chunks if 'parent_id' in chunk.get('metadata', {})}
        parent_docs = app_ctx.docstore.mget(list(all_parent_ids))
        valid_parent_docs = [doc for doc in parent_docs if doc]

        # --- 后续步骤（重排、生成答案等）保持不变，但使用融合后的结果 ---
        # Stage 3: Reranking
        reranked_docs = await app_ctx.fusion_engine.rerank_documents(
            query=query_request.query,
            documents=[RetrievedDocument(content=doc.page_content, metadata=doc.metadata, score=0.0, source_type="fused_retrieval") for doc in valid_parent_docs],
            top_n=5
        )

        # --- Stage 4: Step-by-Step Synthesis ---
        final_answer = NO_ANSWER_PHRASE_ANSWER_CLEAN
        failure_reason = ""
        
        if not reranked_docs:
            failure_reason = "知识库中未能找到任何与您问题相关的信息。"
        else:
            is_table_query_top_ranked = (reranked_docs and reranked_docs[0].metadata.get("paragraph_type") == "table")
            
            if is_table_query_top_ranked:
                table_doc = reranked_docs[0]
                try:
                    df = pd.read_csv(io.StringIO(table_doc.content), sep='|', skipinitialspace=True).dropna(axis=1, how='all').iloc[1:]
                    df.columns = [col.replace(" ", "").strip() for col in df.columns]
                    # 清洗所有单元格数据中的空格
                    for col in df.columns:
                        if df[col].dtype == 'object': 
                            df[col] = df[col].str.strip()

                    if len(df.columns) < 2:
                        raise ValueError("Table must have at least two columns for key-value lookup.")
                    
                    key_column_for_lookup = df.columns[1]
                    instruction = await generate_table_lookup_instruction(
                        llm_instance=app_ctx.llm_gbnf_instance,
                        user_query=query_request.query,
                        table_column_names=df.columns.tolist()
                    )

                    if instruction and "row_identifier" in instruction and "column_identifier" in instruction:
                        row_id_raw = instruction.get("row_identifier", "")
                        col_id_raw = instruction.get("column_identifier", "")

                        # 尝试完全匹配
                        result_series = df.loc[df[key_column_for_lookup] == row_id_raw, col_id_raw]

                        # 如果完全匹配失败，尝试忽略大小写和空格进行模糊匹配
                        if result_series.empty:
                            match_condition = df[key_column_for_lookup].str.strip().str.lower().str.contains(row_id_raw.strip().lower(), na=False)
                            result_series = df.loc[match_condition, col_id_raw]

                        if not result_series.empty:
                            value = result_series.iloc[0]
                            final_answer = f"根据查找到的表格信息，{row_id_raw}的{col_id_raw}是{value}。"
                        else:
                             failure_reason = f"模型指令无法执行：在表格的'{key_column_for_lookup}'列中未能找到行'{row_id_raw}'，或在表头中未能找到列'{col_id_raw}'。"
                    else:
                        failure_reason = "模型未能从问题中生成有效的表格查询指令。"
                except Exception as e_pandas:
                    failure_reason = f"处理表格数据时遇到代码错误: {e_pandas}"
                
                if failure_reason:
                    pass

            if final_answer == NO_ANSWER_PHRASE_ANSWER_CLEAN:
                summary_tasks = [generate_document_summary(app_ctx.llm_gbnf_instance, user_query=query_request.query, document_content=doc.content) for doc in reranked_docs]
                summaries = await asyncio.gather(*summary_tasks)
                
                relevant_summaries = []
                for doc, summary in zip(reranked_docs, summaries):
                    if summary:
                        filename = doc.metadata.get('filename', '未知文档')
                        relevant_summaries.append(f"根据文档《{filename}》的信息：{summary}")
                
                if not relevant_summaries:
                    failure_reason = "虽然检索到了相关文档，但无法从中提炼出与您问题直接相关的核心信息。"
                else:
                    fusion_context = "\n\n".join(relevant_summaries)
                    final_answer = await generate_answer_from_context(user_query=query_request.query, context_str=fusion_context, prompt_builder=lambda q, c: get_fusion_messages(q, c))
            
        if not final_answer or NO_ANSWER_PHRASE_ANSWER_CLEAN in final_answer:
            if not failure_reason: failure_reason = "根据检索到的上下文信息，无法直接回答您的问题。"
            suggestion = await generate_actionable_suggestion(app_ctx.llm_gbnf_instance, user_query=query_request.query, failure_reason=failure_reason)
            final_answer = f"{failure_reason} {suggestion}" if suggestion else failure_reason

        response_to_return = HybridRAGResponse(answer=final_answer, original_query=query_request.query, retrieved_sources=reranked_docs)
        if not failure_reason and NO_ANSWER_PHRASE_ANSWER_CLEAN not in final_answer:
            app_ctx.answer_cache[cache_key] = response_to_return
    except Exception as e:
        exception_occurred = e
        response_to_return = HybridRAGResponse(answer=f"An internal server error occurred: {e}", original_query=query_request.query, retrieved_sources=[])
    finally:
        processing_time_seconds = (datetime.now(timezone.utc) - start_time_total).total_seconds()
        log_data_for_finally = {
            "interaction_id": interaction_id_for_log, "timestamp_utc": datetime.now(timezone.utc).isoformat(),
            "task_type": "rag_query_processing_v8_0_hybrid_search",
            "original_user_query": query_request.query,
            "final_answer_from_llm": response_to_return.answer if response_to_return else "N/A",
            "final_context_docs_full": [doc.model_dump() for doc in response_to_return.retrieved_sources] if response_to_return else [],
            "retrieval_parameters": query_request.model_dump(),
            "processing_time_seconds": round(processing_time_seconds, 3)
        }
        if exception_occurred:
            log_data_for_finally["error_details"] = f"{type(exception_occurred).__name__}: {str(exception_occurred)}"
            log_data_for_finally["error_traceback"] = traceback.format_exc()
        
        await log_queue.put(log_data_for_finally)
        
        if exception_occurred:
            raise HTTPException(status_code=500, detail=str(exception_occurred))
        
        if response_to_return is None:
            response_to_return = HybridRAGResponse(answer="An unexpected error occurred during response generation.", original_query=query_request.query, retrieved_sources=[])
        
        return response_to_return
    
if __name__ == "__main__":
    uvicorn.run("zhz_rag.api.rag_api_service:app", host="0.0.0.0", port=8081, reload=False)


```

    |-- rag_mcp_service.py

``` py
import os
import json
import asyncio
import traceback
from contextlib import asynccontextmanager
from typing import List, Dict, Any, Optional, AsyncIterator
from dataclasses import dataclass, field # 确保导入 field
import time
import logging
import sys
import hashlib # <--- 添加
from datetime import datetime, timezone # <--- 添加
import uuid # <--- 添加


# MCP 框架导入
from mcp.server.fastmcp import FastMCP, Context

# --- 配置 rag_service 的专用日志 ---
_rag_service_py_dir = os.path.dirname(os.path.abspath(__file__))
_rag_service_log_file = os.path.join(_rag_service_py_dir, 'rag_service_debug.log')

rag_logger = logging.getLogger("RagServiceLogger")
rag_logger.setLevel(logging.DEBUG)
rag_logger.propagate = False

if rag_logger.hasHandlers():
    rag_logger.handlers.clear()

try:
    _file_handler = logging.FileHandler(_rag_service_log_file, mode='w')
    _file_handler.setLevel(logging.DEBUG)
    _formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(filename)s:%(lineno)d - %(message)s')
    _file_handler.setFormatter(_formatter)
    rag_logger.addHandler(_file_handler)
    rag_logger.info("--- RagServiceLogger configured to write to rag_service_debug.log ---")
except Exception as e:
    print(f"CRITICAL: Failed to configure RagServiceLogger: {e}")


# --- 从项目内部导入所有 RAG 模块 ---
from zhz_rag.config.pydantic_models import QueryRequest, HybridRAGResponse, RetrievedDocument
from zhz_rag.llm.llm_interface import (
    generate_answer_from_context,
    generate_expanded_queries,
    generate_cypher_query,
    generate_clarification_question,
    generate_intent_classification,
    NO_ANSWER_PHRASE_ANSWER_CLEAN
)
from zhz_rag.core_rag.retrievers.chromadb_retriever import ChromaDBRetriever
from zhz_rag.core_rag.retrievers.file_bm25_retriever import FileBM25Retriever
from zhz_rag.core_rag.kg_retriever import KGRetriever
from zhz_rag.core_rag.fusion_engine import FusionEngine
from zhz_rag.utils.common_utils import log_interaction_data

from dotenv import load_dotenv

# 加载 .env 文件
# __file__ 是当前 rag_service.py 的路径: /home/zhz/zhz_agent/rag_service.py
# os.path.dirname(os.path.abspath(__file__)) 是 /home/zhz/zhz_agent 目录
# .env 文件与 rag_service.py 在同一个目录下 (zhz_agent 目录)
current_dir = os.path.dirname(os.path.abspath(__file__))
dotenv_path = os.path.join(current_dir, '.env')

if os.path.exists(dotenv_path):
    load_dotenv(dotenv_path=dotenv_path)
    rag_logger.info(f"Loaded .env file from: {dotenv_path}")
else:
    rag_logger.warning(f".env file not found at {dotenv_path}, will rely on environment variables or defaults.")
    # 仍然尝试加载，因为python-dotenv的默认行为是查找当前工作目录和上级目录的.env
    load_dotenv()

# --- 应用上下文 Dataclass ---
@dataclass
class AppContext:
    # vector_retriever: VectorRetriever # 旧的
    chroma_retriever: Optional[ChromaDBRetriever] = None # 新的
    kg_retriever: Optional[KGRetriever] = None
    # bm25_retriever: BM25Retriever # 旧的
    file_bm25_retriever: Optional[FileBM25Retriever] = None # 新的
    fusion_engine: Optional[FusionEngine] = None
    # llm_generator: Optional[Any] = None # LLMGenerator在您的代码中没有被实例化并放入AppContext

# --- MCP 服务器生命周期管理 ---
@asynccontextmanager
async def app_lifespan_for_rag_service(server: FastMCP) -> AsyncIterator[AppContext]:
    rag_logger.info("--- RAG Service (FastMCP): 正在初始化 RAG 组件 (新版) ---")
    
    chroma_retriever_instance: Optional[ChromaDBRetriever] = None
    kg_retriever_instance: Optional[KGRetriever] = None
    file_bm25_retriever_instance: Optional[FileBM25Retriever] = None
    fusion_engine_instance: Optional[FusionEngine] = None

    # 初始化 ChromaDB Retriever
    try:
        # 这些路径和名称应该与Dagster流水线中配置的一致
        # 优先从环境变量读取，如果不存在则使用默认值（如果适用）
        chroma_persist_dir = os.getenv("CHROMA_PERSIST_DIRECTORY", "/home/zhz/dagster_home/chroma_data")
        chroma_collection_name = os.getenv("CHROMA_COLLECTION_NAME", "rag_documents")
        embedding_model_path = os.getenv("EMBEDDING_MODEL_PATH", "/home/zhz/models/bge-small-zh-v1.5")

        if not os.path.isdir(chroma_persist_dir):
                rag_logger.warning(f"ChromaDB persist directory '{chroma_persist_dir}' not found. Retrieval may fail or use an in-memory DB if ChromaDBRetriever handles this.")
        if not os.path.exists(embedding_model_path): # embedding_model_path 应该是目录
            rag_logger.warning(f"Embedding model path '{embedding_model_path}' not found. ChromaDBRetriever initialization might fail.")

        chroma_retriever_instance = ChromaDBRetriever(
            collection_name=chroma_collection_name,
            persist_directory=chroma_persist_dir,
            embedding_model_name_or_path=embedding_model_path
        )
        rag_logger.info("RAG Service: ChromaDBRetriever 初始化成功。")
    except Exception as e:
        rag_logger.error(f"RAG Service: ChromaDBRetriever 初始化失败: {e}", exc_info=True)
        # 不在此处抛出异常，允许服务在部分组件失败时仍能启动（如果设计如此）
    
    # 初始化 File BM25 Retriever
    try:
        bm25_index_dir = os.getenv("BM25_INDEX_DIRECTORY", "/home/zhz/dagster_home/bm25_index_data/")
        if not os.path.isdir(bm25_index_dir):
            rag_logger.warning(f"BM25 index directory '{bm25_index_dir}' not found. FileBM25Retriever initialization might fail.")
            
        file_bm25_retriever_instance = FileBM25Retriever(
            index_directory_path=bm25_index_dir
        )
        rag_logger.info("RAG Service: FileBM25Retriever 初始化成功。")
    except Exception as e:
        rag_logger.error(f"RAG Service: FileBM25Retriever 初始化失败: {e}", exc_info=True)

    # 初始化 KG Retriever
    try:
        # KGRetriever 内部会从环境变量读取NEO4J配置
        # generate_cypher_query 是从 zhz_agent.llm 导入的
        kg_retriever_instance = KGRetriever(llm_cypher_generator_func=generate_cypher_query)
        rag_logger.info("RAG Service: KGRetriever 初始化成功。")
    except Exception as e:
        rag_logger.error(f"RAG Service: KGRetriever 初始化失败: {e}", exc_info=True)
        if kg_retriever_instance and hasattr(kg_retriever_instance, 'close'): # 确保在失败前尝试关闭
            kg_retriever_instance.close()
            
    # 初始化 Fusion Engine
    try:
        fusion_engine_instance = FusionEngine(logger=rag_logger)
        rag_logger.info("RAG Service: FusionEngine 初始化成功。")
    except Exception as e:
        rag_logger.error(f"RAG Service: FusionEngine 初始化失败: {e}", exc_info=True)

    rag_logger.info("--- RAG Service (FastMCP): RAG 组件初始化尝试完成。---")

    ctx = AppContext(
        chroma_retriever=chroma_retriever_instance,
        kg_retriever=kg_retriever_instance,
        file_bm25_retriever=file_bm25_retriever_instance,
        fusion_engine=fusion_engine_instance
    )
    try:
        yield ctx
    finally:
        rag_logger.info("--- RAG Service (FastMCP): 正在清理资源 ---")
        if kg_retriever_instance: # 确保只在成功初始化后才调用close
            kg_retriever_instance.close() 
        rag_logger.info("--- RAG Service (FastMCP): 清理完成 ---")

# --- 初始化 FastMCP 应用 ---
rag_mcp_application = FastMCP(
    name="zhz_agent_rag_service", # 修改了服务名称以区分
    description="Upgraded Hybrid RAG 服务，使用持久化知识库。",
    lifespan=app_lifespan_for_rag_service,
)

@rag_mcp_application.tool()
async def query_rag_v2( # 重命名工具函数以避免与旧的混淆 (如果需要)
    ctx: Context,
    query: str, # 直接使用 query 作为输入，而不是 QueryRequest 对象
    top_k_vector: int = 3,
    top_k_kg: int = 2,
    top_k_bm25: int = 3,
    top_k_final: int = 3 # 最终融合后返回的文档数
) -> str: 
    rag_logger.info(f"\n--- RAG Service (query_rag_v2): 接收到查询: '{query}' ---")
    rag_logger.info(f"      Params: top_k_vector={top_k_vector}, top_k_kg={top_k_kg}, top_k_bm25={top_k_bm25}, top_k_final={top_k_final}")
    start_time_total = time.time()

    app_ctx: AppContext = ctx.request_context.lifespan_context
    response_payload = {} 
    original_query_for_response = query 
    final_json_output = ""
    # --- [新增日志变量] ---
    log_all_raw_retrievals_summary: List[Dict[str, Any]] = []
    log_final_context_docs_summary: List[Dict[str, Any]] = [] # This will store the full doc model dumps
    log_fused_context_text_for_llm_snippet: str = "N/A"
    # --- [确保所有日志变量都有初始值] ---
    log_final_answer_from_llm: str = "N/A"
    log_intent_classification_result: Optional[Dict[str, Any]] = None
    log_expanded_queries: Optional[List[str]] = None
    # --- [结束确保所有日志变量都有初始值] ---

    try:
        # --- 1. LLM 驱动的意图分类和澄清触发 ---
        rag_logger.info(f"--- [TIME] 开始意图分类 at {time.time() - start_time_total:.2f}s ---")
        start_time_intent = time.time()
        intent_classification_result = await generate_intent_classification(query)
        log_intent_classification_result = intent_classification_result # <--- 记录日志
        rag_logger.info(f"--- [TIME] 结束意图分类, 耗时: {time.time() - start_time_intent:.2f}s. Result: {intent_classification_result}")

        if intent_classification_result.get("clarification_needed"):
            uncertainty_reason = intent_classification_result.get("reason", "查询可能存在歧义或信息不足。")
            clarification_question_text = await generate_clarification_question(query, uncertainty_reason)
            response_payload = {
                "status": "clarification_needed",
                "clarification_question": clarification_question_text,
                "original_query": original_query_for_response,
                "debug_info": {"uncertainty_reason": uncertainty_reason, "source": "intent_classification"}
            }
            rag_logger.info(f"--- 需要澄清，返回: {response_payload}")
            # final_json_output will be set before finally block

        else: 
             # --- 启用查询扩展 ---
            rag_logger.info(f"--- 查询清晰，无需澄清。将对原始查询 '{query}' 进行查询扩展 ---")
            start_time_expansion = time.time()
            expanded_queries = await generate_expanded_queries(query) # <--- 取消注释
            log_expanded_queries = expanded_queries # <--- 记录实际的扩展查询
            
            if not expanded_queries or query not in expanded_queries: # 确保原始查询一定在里面
                # 如果 generate_expanded_queries 返回空或不包含原始查询，至少处理原始查询
                if query not in (expanded_queries or []): # 处理 expanded_queries 可能为 None 的情况
                    expanded_queries = [query] + (expanded_queries or [])
                elif not expanded_queries:
                    expanded_queries = [query]

            rag_logger.info(f"--- 扩展后的查询列表 (共 {len(expanded_queries)} 个): {expanded_queries}. 耗时: {time.time() - start_time_expansion:.2f}s ---")
            
            all_raw_retrievals: List[RetrievedDocument] = []
            
            queries_to_process = expanded_queries # <--- 修改：现在处理所有扩展后的查询
            rag_logger.info(f"--- [TIME] 开始并行召回 for {len(queries_to_process)} queries at {time.time() - start_time_total:.2f}s ---")
            start_time_retrieval = time.time()

            for current_query_text in queries_to_process:
                rag_logger.info(f"Processing retrievals for query: '{current_query_text}'")
                
                # 向量检索 (ChromaDB)
                if app_ctx.chroma_retriever:
                    try:
                        chroma_docs_raw = app_ctx.chroma_retriever.retrieve(query_text=current_query_text, n_results=top_k_vector)
                        rag_logger.debug(f"   ChromaDB for '{current_query_text}' raw output: {chroma_docs_raw}") 
                        for doc_raw in chroma_docs_raw:
                            retrieved_doc = RetrievedDocument(
                                source_type="vector_chroma",
                                content=doc_raw.get("text", ""),
                                score=doc_raw.get("score", 0.0),
                                metadata={**doc_raw.get("metadata", {}), "original_query_part": current_query_text}
                            )
                            all_raw_retrievals.append(retrieved_doc)
                            log_all_raw_retrievals_summary.append(retrieved_doc.model_dump()) 
                        rag_logger.info(f"   ChromaDB for '{current_query_text}': found {len(chroma_docs_raw)} docs.")
                    except Exception as e_chroma:
                        rag_logger.error(f"   Error during ChromaDB retrieval for '{current_query_text}': {e_chroma}", exc_info=True)
                
                # 关键词检索 (BM25)
                if app_ctx.file_bm25_retriever:
                    try:
                        bm25_docs_raw = app_ctx.file_bm25_retriever.retrieve(query_text=current_query_text, n_results=top_k_bm25)
                        rag_logger.debug(f"   BM25 for '{current_query_text}' raw output (IDs and scores): {bm25_docs_raw}") 
                        for doc_raw_bm25 in bm25_docs_raw:
                            bm25_chunk_id = doc_raw_bm25.get("id")
                            text_content_for_bm25 = f"[BM25: Text for ID {bm25_chunk_id} pending]"
                            found_in_chroma = False
                            for existing_doc in all_raw_retrievals: 
                                if (existing_doc.metadata and (existing_doc.metadata.get("chunk_id") == bm25_chunk_id or existing_doc.metadata.get("id") == bm25_chunk_id)):
                                    text_content_for_bm25 = existing_doc.content
                                    found_in_chroma = True
                                    break
                            if not found_in_chroma and app_ctx.chroma_retriever and bm25_chunk_id: 
                                try:
                                    specific_chroma_doc = app_ctx.chroma_retriever._collection.get(ids=[bm25_chunk_id], include=["metadatas", "documents"]) # Also fetch documents for content
                                    if specific_chroma_doc:
                                        if specific_chroma_doc.get("documents") and specific_chroma_doc.get("documents")[0]:
                                            text_content_for_bm25 = specific_chroma_doc["documents"][0]
                                        elif specific_chroma_doc.get("metadatas") and specific_chroma_doc.get("metadatas")[0]: # Fallback to chunk_text in metadata
                                            text_content_for_bm25 = specific_chroma_doc["metadatas"][0].get("chunk_text", text_content_for_bm25)

                                except Exception as e_chroma_get:
                                    rag_logger.warning(f"   Failed to get text for BM25 ID {bm25_chunk_id} from Chroma: {e_chroma_get}")
                            
                            retrieved_doc = RetrievedDocument(
                                source_type="keyword_bm25s",
                                content=text_content_for_bm25,
                                score=doc_raw_bm25.get("score", 0.0),
                                metadata={"chunk_id": bm25_chunk_id, "original_query_part": current_query_text}
                            )
                            all_raw_retrievals.append(retrieved_doc)
                            log_all_raw_retrievals_summary.append(retrieved_doc.model_dump()) 
                        rag_logger.info(f"   BM25s for '{current_query_text}': found {len(bm25_docs_raw)} potential docs.")
                    except Exception as e_bm25:
                        rag_logger.error(f"   Error during BM25 retrieval for '{current_query_text}': {e_bm25}", exc_info=True)

                # 知识图谱检索
                if app_ctx.kg_retriever:
                    try:
                        rag_logger.info(f"   Performing KG retrieval for query: '{current_query_text}'")
                        kg_docs = await app_ctx.kg_retriever.retrieve_with_llm_cypher(query=current_query_text, top_k=top_k_kg)
                        rag_logger.debug(f"   KG for '{current_query_text}' raw output: {kg_docs}") 
                        for kg_doc_data in kg_docs: # kg_docs is List[Dict], needs conversion
                            retrieved_doc = RetrievedDocument(**kg_doc_data) # Convert dict to Pydantic model
                            if retrieved_doc.metadata:
                                retrieved_doc.metadata["original_query_part"] = current_query_text
                            else:
                                retrieved_doc.metadata = {"original_query_part": current_query_text}
                            all_raw_retrievals.append(retrieved_doc)
                            log_all_raw_retrievals_summary.append(retrieved_doc.model_dump()) 
                        rag_logger.info(f"   KG Retrieval for '{current_query_text}': found {len(kg_docs)} results.")
                    except Exception as e_kg:
                        rag_logger.error(f"   Error during KG retrieval for '{current_query_text}': {e_kg}", exc_info=True)
            
            rag_logger.info(f"--- [TIME] 结束所有召回, 耗时: {time.time() - start_time_retrieval:.2f}s ---")
            rag_logger.info(f"--- 总计从各路召回（所有查询处理后）的结果数: {len(all_raw_retrievals)} ---")
            if all_raw_retrievals:
                for i_doc, doc_retrieved in enumerate(all_raw_retrievals[:3]): # 日志只打印前3条摘要
                        rag_logger.debug(f"   Raw Doc {i_doc} (Summary): type={doc_retrieved.source_type}, score={doc_retrieved.score}, content='{str(doc_retrieved.content)[:50]}...'")

            if not all_raw_retrievals: 
                response_payload = {
                    "status": "success", 
                    "final_answer": "抱歉，根据您提供的查询，未能从知识库中找到相关信息。",
                    "original_query": original_query_for_response,
                    "retrieved_context_docs": [], 
                    "debug_info": {"message": "No documents retrieved from any source."}
                }
            else:
                rag_logger.info(f"--- [TIME] 开始结果融合与重排序 at {time.time() - start_time_total:.2f}s ---")
                start_time_fusion = time.time()
                final_context_docs: List[RetrievedDocument]
                if not app_ctx.fusion_engine:
                    rag_logger.error("FusionEngine not available! Skipping fusion and reranking.")
                    final_context_docs = sorted(all_raw_retrievals, key=lambda d: d.score if d.score is not None else -float('inf'), reverse=True)[:top_k_final]
                else:
                    final_context_docs = await app_ctx.fusion_engine.fuse_results(
                        all_raw_retrievals, 
                        original_query_for_response,
                        top_n_final=top_k_final
                    ) 
                log_final_context_docs_summary = [doc.model_dump() for doc in final_context_docs] 

                # --- 新增日志，检查 model_dump 的输出 ---
                if log_final_context_docs_summary:
                    rag_logger.info(f"DEBUG_MODEL_DUMP: First item of log_final_context_docs_summary (from model_dump()): {json.dumps(log_final_context_docs_summary[0], ensure_ascii=False, default=str)}")
                # --- 结束新增日志 ---

                rag_logger.info(f"--- [TIME] 结束结果融合与重排序, 耗时: {time.time() - start_time_fusion:.2f}s. Final context docs: {len(final_context_docs)} ---")
                if final_context_docs:
                    for i_fdoc, fdoc_retrieved in enumerate(final_context_docs[:3]): # 日志只打印前3条摘要
                        rag_logger.debug(f"   Fused Doc {i_fdoc} (Summary): type={fdoc_retrieved.source_type}, score={fdoc_retrieved.score}, content='{str(fdoc_retrieved.content)[:50]}...'")
                
                if not final_context_docs: 
                    fused_context_text_for_llm = "未在知识库中找到相关信息。"
                    final_answer_from_llm = "根据现有知识，未能找到您查询的相关信息。"
                    response_payload = {
                        "status": "success",
                        "final_answer": final_answer_from_llm,
                        "original_query": original_query_for_response,
                        "retrieved_context_docs": [], 
                        "debug_info": {"message": "No relevant context found after fusion."}
                    }
                else:
                    context_strings_for_llm = []
                    for doc in final_context_docs:
                        score_str = f"{doc.score:.4f}" if isinstance(doc.score, float) else str(doc.score if doc.score is not None else 'N/A')
                        context_strings_for_llm.append(
                            f"Source Type: {doc.source_type}, Score: {score_str}\nContent: {doc.content}"
                        )
                    fused_context_text_for_llm = "\n\n---\n\n".join(context_strings_for_llm)
                    log_fused_context_text_for_llm_snippet = fused_context_text_for_llm[:500] 

                    rag_logger.info(f"\n--- FUSED CONTEXT for LLM (length: {len(fused_context_text_for_llm)} chars) ---")
                    rag_logger.info(f"{fused_context_text_for_llm[:1000]}...") 
                    rag_logger.info(f"--- END OF FUSED CONTEXT ---\n")

                    rag_logger.info(f"--- [TIME] 开始最终答案生成 at {time.time() - start_time_total:.2f}s ---")
                    start_time_answer_gen = time.time()
                    final_answer_from_llm = await generate_answer_from_context(query, fused_context_text_for_llm)
                    log_final_answer_from_llm = final_answer_from_llm or "N/A" 
                    rag_logger.info(f"--- [TIME] 结束最终答案生成, 耗时: {time.time() - start_time_answer_gen:.2f}s ---")

                    if not final_answer_from_llm or final_answer_from_llm.strip() == NO_ANSWER_PHRASE_ANSWER_CLEAN:
                        final_answer_from_llm = "根据您提供的信息，我暂时无法给出明确的回答。"
                    
                    response_payload = {
                        "status": "success",
                        "final_answer": final_answer_from_llm,
                        "original_query": original_query_for_response,
                        "retrieved_context_docs": [doc.model_dump() for doc in final_context_docs], 
                        "debug_info": {"total_raw_retrievals_count": len(all_raw_retrievals)}
                    }

        final_json_output = json.dumps(response_payload, ensure_ascii=False)
        rag_logger.info(f"--- 'query_rag_v2' 逻辑执行完毕, 总耗时: {time.time() - start_time_total:.2f}s. ---")
        
    except Exception as e_main:
        rag_logger.error(f"RAG Service CRITICAL ERROR in 'query_rag_v2' (main try-except): {type(e_main).__name__} - {str(e_main)}", exc_info=True)
        user_query_for_err_log = original_query_for_response if 'original_query_for_response' in locals() and original_query_for_response else query
        response_payload = {
            "status": "error",
            "error_code": "RAG_SERVICE_INTERNAL_ERROR",
            "error_message": f"RAG服务内部发生未预期错误: {str(e_main)}",
            "original_query": user_query_for_err_log,
            "debug_info": {"exception_type": type(e_main).__name__}
        }
        final_json_output = json.dumps(response_payload, ensure_ascii=False)
    finally: 
        interaction_id_for_log = str(uuid.uuid4())
        current_app_version = "zhz_rag_mcp_service_0.2.1" 

        processed_final_context_docs_for_log = []
        temp_log_final_context_docs = locals().get('log_final_context_docs_summary') # 安全获取

        if temp_log_final_context_docs: # 如果 RAG 流程成功并且 final_context_docs 被处理了
            for doc_dict in temp_log_final_context_docs: # temp_log_final_context_docs 是 model_dump() 后的列表
                cleaned_doc = {}
                for key, value in doc_dict.items():
                    if isinstance(value, float) and (value != value or value == float('inf') or value == float('-inf')): 
                        cleaned_doc[key] = None 
                    else:
                        cleaned_doc[key] = value
                processed_final_context_docs_for_log.append(cleaned_doc)
        # 如果 temp_log_final_context_docs 为 None (例如澄清路径)，则 processed_final_context_docs_for_log 保持为 []

        full_log_entry = {
            "timestamp_utc": datetime.now(timezone.utc).isoformat(),
            "interaction_id": interaction_id_for_log,
            "original_query_interaction_id_ref": locals().get('original_query_interaction_id'), 
            "task_type": "rag_query_processing_full_log",
            "app_version": current_app_version,
            "original_user_query": locals().get('original_query_for_response', query), # query 总是有定义的
            "query_params": {
                "top_k_vector": top_k_vector, "top_k_kg": top_k_kg, 
                "top_k_bm25": top_k_bm25, "top_k_final": top_k_final
            },
            "intent_classification_result": locals().get('log_intent_classification_result'),
            "expanded_queries": locals().get('log_expanded_queries', []), # 默认为空列表
            "all_raw_retrievals_count": len(locals().get('log_all_raw_retrievals_summary', [])),
            "final_context_docs_count": len(processed_final_context_docs_for_log), # 使用清理后列表的长度
            "final_context_docs_summary": [ 
                {
                    "source_type": doc.get("source_type"), 
                    "score": doc.get("score"), 
                    "id": (doc.get("metadata",{}).get("chunk_id") or doc.get("metadata",{}).get("id")) if doc.get("metadata") else None, 
                    "content_preview": str(doc.get("content","N/A"))[:50]+"..."
                } 
                for doc in processed_final_context_docs_for_log[:5] # 使用清理后列表的摘要
            ], 
            "final_context_docs_full": processed_final_context_docs_for_log, # <--- 使用清理后的完整列表
            "fused_context_text_for_llm_snippet": locals().get('log_fused_context_text_for_llm_snippet', "N/A"),
            "final_answer_from_llm": locals().get('log_final_answer_from_llm', "N/A"),
            "final_response_payload_status": locals().get('response_payload', {}).get("status", "Unknown"),
            "total_processing_time_seconds": round(time.time() - start_time_total, 2) if 'start_time_total' in locals() else -1,
        }

        try:
            await log_interaction_data(full_log_entry) 
            rag_logger.info(f"Full RAG interaction log (ID: {interaction_id_for_log}) has been written.")
        except Exception as e_log_final:
            rag_logger.error(f"CRITICAL: Failed to write full RAG interaction log: {e_log_final}", exc_info=True)
        
        sys.stdout.flush(); sys.stderr.flush() 
    
    return final_json_output

if __name__ == "__main__":
    rag_logger.info("--- Starting RAG Service (FastMCP for mcpo via direct run) ---")
    rag_mcp_application.run()
```

    |-- rag_service_debug.log
    |-- task_manager_api.py

``` py
# zhz_agent/task_manager_service.py
from fastapi import APIRouter, HTTPException, Depends, Body, Query, Path, status
from typing import List, Optional, Any, cast
from datetime import datetime, timedelta
import uuid
import traceback # 导入 traceback
import pytz

# --- [修改] 从 pydantic_models 导入我们定义的模型 -> 改为绝对导入 ---
from zhz_rag.config.pydantic_models import TaskModel, CreateTaskRequest, UpdateTaskRequest, TaskStatus, ReminderMethod

# --- [修改] 从 database_models 导入 SQLAlchemy 表模型 -> 改为绝对导入 ---
from zhz_rag.task_management.db_models import TaskDB

# --- [修改] 从新的 database.py 导入 database 对象 和 get_scheduler -> 改为绝对导入 ---
from zhz_rag.utils.db_utils import database, get_scheduler # 将 db_utils 修改为 database

# --- [修改] 从 .task_jobs 导入作业函数 -> 改为绝对导入 ---
from zhz_rag.task_management.jobs import send_task_reminder, execute_task_action
from apscheduler.triggers.date import DateTrigger # 用于指定精确的运行时间
from apscheduler.jobstores.base import JobLookupError # <--- [修改] 导入 JobLookupError 的正确路径

# APIRouter 实例
router = APIRouter(
    prefix="/tasks",
    tags=["Task Management"],
    responses={404: {"description": "Not found"}},
)

def _ensure_utc(dt: datetime) -> datetime:
    """确保 datetime 对象是 UTC 时区感知的。"""
    if dt.tzinfo is None:
        return pytz.utc.localize(dt) # 如果是朴素时间，假定它是UTC并设为UTC
    return dt.astimezone(pytz.utc) # 如果是其他时区，转换为UTC

def _schedule_task_jobs(task: TaskModel):
    current_scheduler = get_scheduler() # 获取 scheduler 实例
    print(f"DEBUG SCHEDULER: _schedule_task_jobs called. Scheduler instance: {current_scheduler}, Is running: {current_scheduler.running if current_scheduler else 'N/A'}")
    if not current_scheduler or not current_scheduler.running:
        print("SCHEDULER_ERROR: APScheduler 未运行，无法调度作业。")
        return

    # 提醒作业
    if task.reminder_time and task.status == TaskStatus.PENDING:
        reminder_job_id = f"reminder_{task.id}"
        try:
            reminder_methods_list = task.reminder_methods
            reminder_utc = _ensure_utc(task.reminder_time)
            print(f"SCHEDULER DEBUG: Passing reminder_methods to job: {reminder_methods_list}") # 添加日志

            current_scheduler.add_job(
                send_task_reminder,
                trigger=DateTrigger(run_date=reminder_utc),
                args=[task.id, task.title, reminder_methods_list], # <--- [修复] 直接传递列表
                id=reminder_job_id,
                name=f"Reminder for task {task.title[:20]}",
                replace_existing=True
            )
            print(f"SCHEDULER: 已为任务 '{task.id}' 添加/更新提醒作业，运行于 {task.reminder_time}")
        except Exception as e:
            print(f"SCHEDULER_ERROR: 添加提醒作业失败 for task '{task.id}': {e}")
            traceback.print_exc() # 打印详细错误堆栈

    # 执行作业
    if task.due_date and task.status == TaskStatus.PENDING:
        execution_job_id = f"execution_{task.id}"
        try:
            due_utc = _ensure_utc(task.due_date) # <--- [新增] 确保时间是 UTC 感知的
            print(f"SCHEDULER DEBUG: Adding execution job at {due_utc} ({due_utc.tzinfo})") # <--- [新增] 添加时区日志
            current_scheduler.add_job(
                execute_task_action,
                trigger=DateTrigger(run_date=due_utc),
                args=[task.id, task.action_type, task.action_payload],
                id=execution_job_id,
                name=f"Execution for task {task.title[:20]}",
                replace_existing=True
            )
            print(f"SCHEDULER: 已为任务 '{task.id}' 添加/更新执行作业，运行于 {task.due_date}")
        except Exception as e:
            print(f"SCHEDULER_ERROR: 添加执行作业失败 for task '{task.id}': {e}")

def _cancel_task_jobs(task_id: str):
    """从 APScheduler 取消作业"""
    current_scheduler = get_scheduler()
    if not current_scheduler or not current_scheduler.running:
        print("SCHEDULER_ERROR: APScheduler 未运行，无法取消作业。")
        return

    reminder_job_id = f"reminder_{task_id}"
    execution_job_id = f"execution_{task_id}"

    try:
        current_scheduler.remove_job(reminder_job_id)
        print(f"SCHEDULER: 已移除提醒作业 for task '{task_id}'")
    except JobLookupError:
        print(f"SCHEDULER_INFO: 提醒作业 '{reminder_job_id}' 未找到，无需移除。")
    except Exception as e:
        print(f"SCHEDULER_ERROR: 移除提醒作业失败 for task '{task_id}': {e}")

    try:
        current_scheduler.remove_job(execution_job_id)
        print(f"SCHEDULER: 已移除执行作业 for task '{task_id}'")
    except JobLookupError:
        print(f"SCHEDULER_INFO: 执行作业 '{execution_job_id}' 未找到，无需移除。")
    except Exception as e:
        print(f"SCHEDULER_ERROR: 移除执行作业失败 for task '{task_id}': {e}")

@router.post("/", response_model=TaskModel, status_code=status.HTTP_201_CREATED)
async def create_task(task_request: CreateTaskRequest = Body(...)):
    """
    创建一个新任务。
    """
    now = datetime.utcnow()
    task_id = str(uuid.uuid4())

    reminder_time_val = None
    if task_request.due_date and task_request.reminder_offset_minutes is not None:
        reminder_time_val = task_request.due_date - timedelta(minutes=task_request.reminder_offset_minutes)

    reminder_methods_values = [
        method.value if hasattr(method, 'value') else str(method)
        for method in (task_request.reminder_methods or [ReminderMethod.NOTIFICATION])
    ]

    insert_query = TaskDB.__table__.insert().values(
        id=task_id,
        title=task_request.title,
        description=task_request.description,
        status=TaskStatus.PENDING,
        created_at=now,
        updated_at=now,
        due_date=task_request.due_date,
        reminder_time=reminder_time_val,
        reminder_offset_minutes=task_request.reminder_offset_minutes,
        reminder_methods=reminder_methods_values, # <--- 确保存入的是字符串列表
        priority=task_request.priority or 0,
        tags=task_request.tags or [],
        action_type=task_request.action_type,
        action_payload=task_request.action_payload or {}
    )

    try:
        await database.execute(insert_query)
    except Exception as e:
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail=f"Failed to create task in database: {e}")

    created_task_db = await database.fetch_one(TaskDB.__table__.select().where(TaskDB.id == task_id))
    if not created_task_db:
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to retrieve task after creation")

    response_task = TaskModel.model_validate(dict(created_task_db))

    response_task.reminder_methods = [
        m.value if hasattr(m, 'value') else str(m)
        for m in response_task.reminder_methods
    ]

    _schedule_task_jobs(response_task)
    print(f"TASK_MANAGER: Created task '{response_task.id}' with title '{response_task.title}' in DB")
    return response_task

@router.get("/", response_model=List[TaskModel])
async def list_tasks(
    status_filter: Optional[TaskStatus] = Query(None, alias="status"),
    priority_filter: Optional[int] = Query(None, alias="priority"),
    skip: int = Query(0, ge=0),
    limit: int = Query(10, ge=1, le=100)
):
    """
    获取任务列表，支持过滤和分页。
    """
    query = TaskDB.__table__.select()
    if status_filter:
        query = query.where(TaskDB.status == status_filter)
    if priority_filter is not None:
        query = query.where(TaskDB.priority == priority_filter)

    query = query.order_by(TaskDB.created_at.desc()).offset(skip).limit(limit)

    db_tasks = await database.fetch_all(query)
    return [TaskModel.model_validate(dict(task)) for task in db_tasks]

@router.get("/{task_id}", response_model=TaskModel)
async def get_task(task_id: str = Path(..., description="要获取的任务ID")):
    """
    根据ID获取单个任务的详细信息。
    """
    query = TaskDB.__table__.select().where(TaskDB.id == task_id)
    db_task = await database.fetch_one(query)
    if not db_task:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Task not found")
    return TaskModel.model_validate(dict(db_task))

@router.put("/{task_id}", response_model=TaskModel)
async def update_task(
    task_id: str = Path(..., description="要更新的任务ID"),
    task_update: UpdateTaskRequest = Body(...)
):
    """
    更新现有任务。
    """
    existing_task_query = TaskDB.__table__.select().where(TaskDB.id == task_id)
    db_task = await database.fetch_one(existing_task_query)
    if not db_task:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Task not found")

    update_data = task_update.model_dump(exclude_unset=True)
    update_data["updated_at"] = datetime.utcnow()

    if "reminder_methods" in update_data and update_data["reminder_methods"] is not None:
        update_data["reminder_methods"] = [
            method.value if hasattr(method, 'value') else str(method)
            for method in update_data["reminder_methods"]
        ]

    current_due_date = update_data.get("due_date", cast(Optional[datetime], db_task.due_date))
    current_offset = update_data.get("reminder_offset_minutes", cast(Optional[int], db_task.reminder_offset_minutes))

    if current_due_date and current_offset is not None:
        update_data["reminder_time"] = current_due_date - timedelta(minutes=current_offset)
    elif "due_date" in update_data and current_offset is None:
         update_data["reminder_time"] = None


    update_query = TaskDB.__table__.update().where(TaskDB.id == task_id).values(**update_data)
    await database.execute(update_query)

    updated_db_task = await database.fetch_one(existing_task_query)
    if not updated_db_task:
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to retrieve task after update")

    response_task = TaskModel.model_validate(dict(updated_db_task))

    response_task.reminder_methods = [
        m.value if hasattr(m, 'value') else str(m)
        for m in response_task.reminder_methods
    ]

    _cancel_task_jobs(task_id)
    _schedule_task_jobs(response_task)
    print(f"TASK_MANAGER: Updated task '{response_task.id}' in DB")
    return response_task

@router.delete("/{task_id}", status_code=status.HTTP_204_NO_CONTENT)
async def delete_task(task_id: str = Path(..., description="要删除的任务ID")):
    """
    删除一个任务。
    """
    existing_task_query = TaskDB.__table__.select().where(TaskDB.id == task_id)
    db_task = await database.fetch_one(existing_task_query)
    if not db_task:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Task not found")

    delete_query = TaskDB.__table__.delete().where(TaskDB.id == task_id)
    await database.execute(delete_query)

    _cancel_task_jobs(task_id)
    print(f"TASK_MANAGER: Deleted task '{task_id}' from DB")
    return None

@router.post("/{task_id}/complete", response_model=TaskModel)
async def mark_task_as_complete(task_id: str = Path(..., description="要标记为完成的任务ID")):
    """
    将任务标记为已完成。
    """
    existing_task_query = TaskDB.__table__.select().where(TaskDB.id == task_id)
    db_task_row = await database.fetch_one(existing_task_query)
    if not db_task_row:
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail="Task not found")

    db_task = TaskModel.model_validate(dict(db_task_row))
    if db_task.status == TaskStatus.COMPLETED:
        raise HTTPException(status_code=status.HTTP_400_BAD_REQUEST, detail="Task is already completed")

    update_data = {
        "status": TaskStatus.COMPLETED,
        "updated_at": datetime.utcnow()
    }
    update_query = TaskDB.__table__.update().where(TaskDB.id == task_id).values(**update_data)
    await database.execute(update_query)

    completed_db_task = await database.fetch_one(existing_task_query)
    if not completed_db_task:
        raise HTTPException(status_code=status.HTTP_500_INTERNAL_SERVER_ERROR, detail="Failed to retrieve task after marking complete")

    response_task = TaskModel.model_validate(dict(completed_db_task))
    _cancel_task_jobs(task_id)
    print(f"TASK_MANAGER: Marked task '{response_task.id}' as completed in DB")
    return response_task
```


==================================================

