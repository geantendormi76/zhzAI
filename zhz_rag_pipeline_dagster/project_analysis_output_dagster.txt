é¡¹ç›®åˆ†æè¾“å‡º
================================================================================

--- ç›®å½•ç»“æ„æ ‘ ---
ğŸ“‚ zhz_rag_pipeline_dagster/
â”œâ”€â”€ ğŸ“‚ parsers/
â”‚   â”œâ”€â”€ ğŸ“„ __init__.py
â”‚   â”œâ”€â”€ ğŸ“„ docx_parser.py
â”‚   â”œâ”€â”€ ğŸ“„ html_parser.py
â”‚   â”œâ”€â”€ ğŸ“„ md_parser.py
â”‚   â”œâ”€â”€ ğŸ“„ pdf_parser.py
â”‚   â”œâ”€â”€ ğŸ“„ txt_parser.py
â”‚   â””â”€â”€ ğŸ“„ xlsx_parser.py
â””â”€â”€ ğŸ“‚ zhz_rag_pipeline/
    â”œâ”€â”€ ğŸ“„ __init__.py
    â”œâ”€â”€ ğŸ“„ custom_io_managers.py
    â”œâ”€â”€ ğŸ“„ definitions.py
    â”œâ”€â”€ ğŸ“„ document_parsers.py
    â”œâ”€â”€ ğŸ“„ evaluation_assets.py
    â”œâ”€â”€ ğŸ“„ ingestion_assets.py
    â”œâ”€â”€ ğŸ“„ processing_assets.py
    â”œâ”€â”€ ğŸ“„ pydantic_models_dagster.py
    â””â”€â”€ ğŸ“„ resources.py


--- æ–‡ä»¶å†…å®¹ ---
--------------------------------------------------------------------------------
æ–‡ä»¶: zhz_rag_pipeline/__init__.py
--------------------------------------------------------------------------------
# zhz_rag_pipeline/__init__.py
# This file makes Python treat the directory as a package.
# You can also define a __version__ or import key modules here if needed.


--------------------------------------------------------------------------------
æ–‡ä»¶: zhz_rag_pipeline/custom_io_managers.py
--------------------------------------------------------------------------------
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/zhz_rag_pipeline/custom_io_managers.py
import json
import os
from typing import List, Type, Union, get_args, get_origin, Any, Optional 
from dagster import UPathIOManager, InputContext, OutputContext, DagsterInvariantViolationError
from pydantic import BaseModel as PydanticBaseModel
from upath import UPath

class PydanticListJsonIOManager(UPathIOManager):
    extension: str = ".jsonl"

    def __init__(self, base_dir: Optional[str] = None): # Changed base_path to base_dir for clarity
        resolved_base_dir: UPath
        if base_dir:
            resolved_base_dir = UPath(base_dir).resolve() # Resolve to absolute path
        else:
            # Default to <DAGSTER_HOME>/storage/pydantic_jsonl_io
            # DAGSTER_HOME defaults to ~/.dagster, but can be overridden by env var
            dagster_home_str = os.getenv("DAGSTER_HOME", os.path.join(os.path.expanduser("~"), ".dagster"))
            resolved_base_dir = UPath(dagster_home_str) / "storage" / "pydantic_jsonl_io"
        
        # Ensure the directory exists
        try:
            resolved_base_dir.mkdir(parents=True, exist_ok=True)
        except Exception as e:
            # Log this error appropriately, perhaps using a direct print if logger isn't set up yet
            # or re-raise as a Dagster-specific error.
            print(f"[PydanticListJsonIOManager __init__] ERROR: Could not create bafef __init__(self, base_dir: Optse directory {resolved_base_dir}: {e}")
            # Depending on Dagster's init sequence, context.log might not be available here.
            # It's safer to let UPathIOManager handle its own base_path or ensure dir exists before.
            # For now, we proceed, UPathIOManager might handle it or fail later.

        super().__init__(base_path=resolved_base_dir)
        # Log the final base path used by the UPathIOManager instance
        # self.log available after super().__init__() in ConfigurableIOManager context
        # For direct instantiation, we might need to pass a logger or use a global one.
        # print(f"[PydanticListJsonIOManager __init__] Initialized with resolved base_path: {self.base_path}")


    def dump_to_path(self, context: OutputContext, obj: List[PydanticBaseModel], path: UPath):
        context.log.info(f"[PydanticListJsonIOManager dump_to_path] Attempting to dump to resolved path: {path.resolve()}")
        
        if not isinstance(obj, list):
            msg = f"Expected a list of Pydantic models, got {type(obj)}"
            context.log.error(msg)
            raise TypeError(msg)
        
        # Optional: More robust type checking for list items if needed, using context.dagster_type
        # For now, assume obj is List[PydanticBaseModel] based on upstream asset's type hint.

        try:
            with path.open("w", encoding="utf-8") as f:
                for model_instance in obj:
                    if not isinstance(model_instance, PydanticBaseModel):
                        context.log.warning(f"Item in list is not a Pydantic model: {type(model_instance)}. Skipping.")
                        continue
                    json_str = model_instance.json() # Pydantic V1
                    f.write(json_str + "\n")
            context.log.info(f"[PydanticListJsonIOManager dump_to_path] Successfully dumped {len(obj)} items to {path.resolve()}")
        except Exception as e:
            context.log.error(f"[PydanticListJsonIOManager dump_to_path] Failed to dump object to {path.resolve()}: {e}", exc_info=True)
            raise

    def load_from_path(self, context: InputContext, path: UPath) -> List[PydanticBaseModel]:
        context.log.info(f"[PydanticListJsonIOManager load_from_path] Attempting to load from resolved path: {path.resolve()}")
        
        list_typing_type = context.dagster_type.typing_type
        origin = get_origin(list_typing_type)
        args = get_args(list_typing_type)

        if not (origin is list and args and issubclass(args[0], PydanticBaseModel)):
            msg = (
                f"PydanticListJsonIOManager can only handle inputs of type List[PydanticModel], "
                f"but got {list_typing_type} for input '{context.name}'."
            )
            context.log.error(msg)
            raise DagsterInvariantViolationError(msg) # Use Dagster specific error
        
        model_type: Type[PydanticBaseModel] = args[0]
        context.log.info(f"[PydanticListJsonIOManager load_from_path] Target model type for list items: {model_type.__name__}")

        loaded_models: List[PydanticBaseModel] = []
        if not path.exists():
            context.log.warning(f"[PydanticListJsonIOManager load_from_path] File not found at {path.resolve()}, returning empty list for input '{context.name}'.")
            return loaded_models

        try:
            with path.open("r", encoding="utf-8") as f:
                for line_number, line in enumerate(f, 1):
                    line_content = line.strip()
                    if not line_content:
                        continue
                    try:
                        model_instance = model_type.parse_raw(line_content) # Pydantic V1
                        loaded_models.append(model_instance)
                    except Exception as e_parse:
                        context.log.error(
                            f"[PydanticListJsonIOManager load_from_path] Failed to parse JSON line {line_number} "
                            f"into {model_type.__name__} from {path.resolve()}: {e_parse}. "
                            f"Line content (first 100 chars): '{line_content[:100]}...'",
                            exc_info=True
                        )
                        # Optionally re-raise or decide to skip problematic lines
                        # For now, we'll skip
            context.log.info(f"[PydanticListJsonIOManager load_from_path] Successfully loaded {len(loaded_models)} instances of {model_type.__name__} from {path.resolve()}")
        except Exception as e_read:
            context.log.error(f"[PydanticListJsonIOManager load_from_path] Failed to read or process file {path.resolve()}: {e_read}", exc_info=True)
            raise # Re-raise if file reading itself fails catastrophically
            
        return loaded_models


--------------------------------------------------------------------------------
æ–‡ä»¶: zhz_rag_pipeline/definitions.py
--------------------------------------------------------------------------------
import dagster as dg
import os
from dagster import Definitions

from zhz_rag_pipeline_dagster.zhz_rag_pipeline.ingestion_assets import all_ingestion_assets
from zhz_rag_pipeline_dagster.zhz_rag_pipeline.processing_assets import all_processing_assets

from zhz_rag_pipeline_dagster.zhz_rag_pipeline.resources import (
    GGUFEmbeddingResource,
    ChromaDBResource,
    LocalLLMAPIResource,
    DuckDBResource,
    GeminiAPIResource,
    SystemResource
)
from zhz_rag_pipeline_dagster.zhz_rag_pipeline.custom_io_managers import PydanticListJsonIOManager

# --- æ ¹æ®å®˜æ–¹æ–‡æ¡£ï¼Œå°†æ‰€æœ‰èµ„äº§ç»„åˆåœ¨ä¸€èµ· ---
all_assets = all_ingestion_assets + all_processing_assets

# --- æ ¹æ®å®˜æ–¹æ–‡æ¡£ï¼Œå®šä¹‰ä¸€ä¸ªåŒ…å«æ‰€æœ‰èµ„æºçš„å­—å…¸ ---
# Dagster ä¼šè‡ªåŠ¨ä¸ºæ¯ä¸ªèµ„äº§æä¾›å®ƒæ‰€éœ€è¦çš„èµ„æº
all_resources = {
    # IO ç®¡ç†å™¨ï¼Œé”®åå¿…é¡»æ˜¯ "io_manager" æ‰èƒ½è¢«é»˜è®¤ä½¿ç”¨
    "io_manager": PydanticListJsonIOManager(),
    
    # å…¶ä»–åº”ç”¨çº§èµ„æº
    "embedder": GGUFEmbeddingResource(
        api_url=os.getenv("EMBEDDING_API_URL", "http://127.0.0.1:8089")
    ),
    "chroma_db": ChromaDBResource(
        collection_name=os.getenv("CHROMA_COLLECTION_NAME", "zhz_rag_collection"),
        persist_directory=os.path.join(os.getenv("ZHZ_AGENT_PROJECT_ROOT", "/home/zhz/zhz_agent"), "zhz_rag", "stored_data", "chromadb_index")
    ),
    "LocalLLM_api": LocalLLMAPIResource(
        api_url="http://127.0.0.1:8088/v1/chat/completions",
        default_temperature=0.1,
        default_max_new_tokens=2048
    ),
    "duckdb_kg": DuckDBResource(
        db_file_path=os.path.join(os.getenv("ZHZ_AGENT_PROJECT_ROOT", "/home/zhz/zhz_agent"), "zhz_rag", "stored_data", "duckdb_knowledge_graph.db")
    ),
    "gemini_api": GeminiAPIResource(
        model_name="gemini/gemini-1.5-flash-latest",
        proxy_url=os.getenv("LITELLM_PROXY_URL"),
        default_temperature=0.1,
        default_max_tokens=2048
    ),
    "system_resource": SystemResource()
}

# --- åˆ›å»ºæœ€ç»ˆçš„ã€ç®€æ´çš„ Definitions å¯¹è±¡ ---
defs = Definitions(
    assets=all_assets,
    resources=all_resources
)


--------------------------------------------------------------------------------
æ–‡ä»¶: zhz_rag_pipeline/document_parsers.py
--------------------------------------------------------------------------------
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/zhz_rag_pipeline/document_parsers.py
import os
from markdown_it import MarkdownIt
import logging
from typing import List, Dict, Any, Optional, Union, Literal 
from bs4 import BeautifulSoup
import pandas as pd

# --- æ·»åŠ  Unstructured çš„å¯¼å…¥ ---
try:
    from unstructured.partition.docx import partition_docx
    from unstructured.documents.elements import Element as UnstructuredElement, Title, NarrativeText, ListItem, Table, Image, Header, Footer
    _UNSTRUCTURED_AVAILABLE = True
    print("INFO (document_parsers.py): Successfully imported Unstructured for DOCX.")
except ImportError:
    print("WARNING (document_parsers.py): Unstructured library not found. DOCX parsing will be a placeholder.")
    _UNSTRUCTURED_AVAILABLE = False
    class UnstructuredElement: pass # Dummy
# --- ç»“æŸæ·»åŠ  ---

# --- å¯¼å…¥æˆ‘ä»¬å®šä¹‰çš„Pydanticæ¨¡å‹ ---
# å‡è®¾è¿™ä¸ªæ–‡ä»¶å’Œ pydantic_models_dagster.py åœ¨åŒä¸€ä¸ªåŒ…ä¸‹æˆ–èƒ½é€šè¿‡PYTHONPATHæ‰¾åˆ°
try:
    from .pydantic_models_dagster import ( # ä½¿ç”¨ç›¸å¯¹å¯¼å…¥
        ParsedDocumentOutput,
        DocumentElementType, 
        TitleElement,
        NarrativeTextElement,
        ListItemElement,
        TableElement,
        CodeBlockElement,
        PageBreakElement,
        DocumentElementMetadata 
    )
    _PYDANTIC_MODELS_AVAILABLE_PARSERS = True
except ImportError:
    print("WARNING (document_parsers.py): Could not import Pydantic models. Using fallback Any/dict.")
    _PYDANTIC_MODELS_AVAILABLE_PARSERS = False
    class BaseModel: pass
    class DocumentElementMetadata(BaseModel): page_number: Optional[int] = None
    class ParsedDocumentOutput(BaseModel): parsed_text: str; elements: list; original_metadata: dict; summary: Optional[str] = None
    class TitleElement(BaseModel): element_type:str="title"; text:str; level:int; metadata: Optional[DocumentElementMetadata] = None
    class NarrativeTextElement(BaseModel): element_type:str="narrative_text"; text:str; metadata: Optional[DocumentElementMetadata] = None
    class ListItemElement(BaseModel): element_type:str="list_item"; text:str; level:int=0; ordered:bool=False; item_number:Optional[Union[int, str]]=None; metadata: Optional[DocumentElementMetadata] = None
    class TableElement(BaseModel): element_type:str="table"; markdown_representation:Optional[str]=None; metadata: Optional[DocumentElementMetadata] = None
    class CodeBlockElement(BaseModel): element_type:str="code_block"; code:str; language:Optional[str]=None; metadata: Optional[DocumentElementMetadata] = None
    class PageBreakElement(BaseModel): element_type:str="page_break"; metadata: Optional[DocumentElementMetadata] = None
    DocumentElementType = Any


logger = logging.getLogger(__name__) # æ¯ä¸ªæ¨¡å—ç”¨è‡ªå·±çš„logger

# --- Markdown è§£æé€»è¾‘ (ä» poc_md_markdown_it.py è¿ç§»å¹¶å°è£…) ---



def _get_text_from_md_inline(inline_tokens: Optional[List[Any]]) -> str:
    # (è¿™é‡Œæ˜¯ get_text_from_inline_tokens å‡½æ•°çš„å®Œæ•´ä»£ç )
    text_content = ""
    if inline_tokens is None: return ""
    for token in inline_tokens:
        if token.type == 'text':
            text_content += token.content
        elif token.type == 'code_inline':
            text_content += f"`{token.content}`"
        elif token.type == 'softbreak':
            text_content += ' ' 
        elif token.type == 'hardbreak':
            text_content += '\n'
        elif token.children: 
            text_content += _get_text_from_md_inline(token.children)
    return text_content

def _convert_md_tokens_to_elements_internal(tokens: list) -> List[Any]:
    # (è¿™é‡Œæ˜¯ convert_md_tokens_to_elements å‡½æ•°çš„å®Œæ•´ä»£ç ï¼Œä½†å°†å…¶é‡å‘½åä¸ºå†…éƒ¨å‡½æ•°)
    # (å¹¶ç¡®ä¿å®ƒåœ¨ _PYDANTIC_MODELS_AVAILABLE_PARSERS ä¸ºTrueæ—¶åˆ›å»ºPydanticå®ä¾‹ï¼Œå¦åˆ™åˆ›å»ºå­—å…¸)
    elements: List[Any] = []
    idx = 0
    list_level_stack = [] 

    while idx < len(tokens):
        token = tokens[idx]

        if token.type == 'heading_open':
            level = int(token.tag[1:])
            idx_content = idx + 1
            text = ""
            if idx_content < len(tokens) and tokens[idx_content].type == 'inline':
                text = _get_text_from_md_inline(tokens[idx_content].children).strip()
            if _PYDANTIC_MODELS_AVAILABLE_PARSERS: elements.append(TitleElement(text=text, level=level))
            else: elements.append({"element_type": "title", "text": text, "level": level})
            idx = idx_content + 2 
            continue

        elif token.type == 'paragraph_open':
            is_list_item_para = False
            if list_level_stack and token.level >= list_level_stack[-1]["level"]:
                pass 
            if not is_list_item_para or not list_level_stack: 
                idx_content = idx + 1
                text = ""
                if idx_content < len(tokens) and tokens[idx_content].type == 'inline':
                    text = _get_text_from_md_inline(tokens[idx_content].children).strip()
                if text:
                    if _PYDANTIC_MODELS_AVAILABLE_PARSERS: elements.append(NarrativeTextElement(text=text))
                    else: elements.append({"element_type": "narrative_text", "text": text})
            idx = idx + 2 
            if idx < len(tokens) and tokens[idx-1].type == 'inline': 
                idx +=1 
            continue
        
        elif token.type == 'bullet_list_open':
            list_level_stack.append({"ordered": False, "level": token.level})
            idx += 1
            continue
        elif token.type == 'ordered_list_open':
            start_num = token.attrs.get('start', 1)
            list_level_stack.append({"ordered": True, "current_num": start_num, "level": token.level})
            idx += 1
            continue
        
        elif token.type == 'list_item_open':
            item_text = ""
            li_level = token.level
            next_token_idx = idx + 1
            if next_token_idx < len(tokens):
                next_token = tokens[next_token_idx]
                if next_token.type == 'paragraph_open' and next_token.level == li_level + 1 :
                    inline_idx = next_token_idx + 1
                    if inline_idx < len(tokens) and tokens[inline_idx].type == 'inline':
                        item_text = _get_text_from_md_inline(tokens[inline_idx].children).strip()
                elif next_token.type == 'inline' and next_token.level == li_level +1 :
                    item_text = _get_text_from_md_inline(next_token.children).strip()
            
            if list_level_stack:
                list_info = list_level_stack[-1]
                item_num_val = None
                if list_info["ordered"]:
                    item_num_val = list_info["current_num"]
                    list_info["current_num"] += 1
                
                if _PYDANTIC_MODELS_AVAILABLE_PARSERS:
                    elements.append(ListItemElement(
                        text=item_text, level=token.level, ordered=list_info["ordered"],
                        item_number=str(item_num_val) if item_num_val is not None else None))
                else:
                    elements.append({"element_type": "list_item", "text": item_text, "level":token.level, 
                                     "ordered":list_info["ordered"], "item_number":str(item_num_val) if item_num_val is not None else None})

            temp_idx = idx + 1; nesting_count = 0
            while temp_idx < len(tokens):
                if tokens[temp_idx].type == 'list_item_open' and tokens[temp_idx].level == li_level:
                    if nesting_count == 0: idx = temp_idx; break
                if tokens[temp_idx].type == 'list_item_open': nesting_count +=1
                if tokens[temp_idx].type == 'list_item_close':
                    if nesting_count == 0 and tokens[temp_idx].level == li_level: idx = temp_idx + 1; break
                    nesting_count -=1
                temp_idx += 1
            else: idx = temp_idx
            continue

        elif token.type in ['bullet_list_close', 'ordered_list_close']:
            if list_level_stack: list_level_stack.pop()
            idx += 1
            continue

        elif token.type == 'table_open':
            header_content = []; body_rows_cells = []; current_row_cells = []; in_thead = False
            temp_idx = idx + 1
            while temp_idx < len(tokens) and tokens[temp_idx].type != 'table_close':
                t_token = tokens[temp_idx]
                if t_token.type == 'thead_open': in_thead = True
                elif t_token.type == 'thead_close': in_thead = False
                elif t_token.type == 'tr_open': current_row_cells = []
                elif t_token.type in ['th_open', 'td_open']:
                    content_idx = temp_idx + 1
                    if content_idx < len(tokens) and tokens[content_idx].type == 'inline':
                        current_row_cells.append(_get_text_from_md_inline(tokens[content_idx].children).strip())
                elif t_token.type == 'tr_close':
                    if current_row_cells:
                        if in_thead or (not header_content and not body_rows_cells): header_content.append(list(current_row_cells))
                        else: body_rows_cells.append(list(current_row_cells))
                temp_idx += 1
            md_table_str = ""
            if header_content:
                md_table_str += "| " + " | ".join(header_content[0]) + " |\n"
                md_table_str += "| " + " | ".join(["---"] * len(header_content[0])) + " |\n"
            for row_data_list in body_rows_cells: md_table_str += "| " + " | ".join(row_data_list) + " |\n"
            if _PYDANTIC_MODELS_AVAILABLE_PARSERS: elements.append(TableElement(markdown_representation=md_table_str.strip()))
            else: elements.append({"element_type": "table", "markdown_representation": md_table_str.strip()})
            idx = temp_idx + 1 
            continue

        elif token.type == 'fence' or token.type == 'code_block':
            code_content = token.content.strip(); lang = token.info.strip() if token.info else None
            if _PYDANTIC_MODELS_AVAILABLE_PARSERS: elements.append(CodeBlockElement(code=code_content, language=lang))
            else: elements.append({"element_type": "code_block", "code": code_content, "language": lang})
            idx += 1
            continue
        
        elif token.type == 'hr':
            if _PYDANTIC_MODELS_AVAILABLE_PARSERS: elements.append(PageBreakElement())
            else: elements.append({"element_type": "page_break"})
            idx += 1
            continue
        
        elif token.type == 'blockquote_open':
            blockquote_text_parts = []; temp_idx = idx + 1; start_level = token.level
            while temp_idx < len(tokens):
                bq_token = tokens[temp_idx]
                if bq_token.type == 'blockquote_close' and bq_token.level == start_level: idx = temp_idx; break
                if bq_token.type == 'paragraph_open':
                    para_content_idx = temp_idx + 1
                    if para_content_idx < len(tokens) and tokens[para_content_idx].type == 'inline':
                        blockquote_text_parts.append(_get_text_from_md_inline(tokens[para_content_idx].children).strip())
                    temp_idx = para_content_idx + 1 
                    if temp_idx < len(tokens) and tokens[temp_idx].type == 'paragraph_close': temp_idx +=1
                    else: temp_idx -=1
                temp_idx +=1
            else: idx = temp_idx
            if blockquote_text_parts:
                full_text = "\n".join(blockquote_text_parts)
                if _PYDANTIC_MODELS_AVAILABLE_PARSERS: elements.append(NarrativeTextElement(text=full_text)) 
                else: elements.append({"element_type": "narrative_text", "text": full_text, "_is_blockquote": True})
            idx +=1
            continue
        idx += 1 
    return elements

def _generate_parsed_text_from_elements_internal(elements: List[Any]) -> str:
    # (è¿™é‡Œæ˜¯ generate_parsed_text_from_elements å‡½æ•°çš„å®Œæ•´ä»£ç )
    # (ç¡®ä¿å®ƒåœ¨ _PYDANTIC_MODELS_AVAILABLE_PARSERS ä¸ºTrueæ—¶èƒ½å¤„ç†Pydanticå®ä¾‹ï¼Œå¦åˆ™å¤„ç†å­—å…¸)
    text_parts = []
    for el_data_any in elements:
        el_data = {}
        if _PYDANTIC_MODELS_AVAILABLE_PARSERS and hasattr(el_data_any, 'model_dump'):
            el_data = el_data_any.model_dump() 
        elif isinstance(el_data_any, dict):
            el_data = el_data_any
        else: continue

        el_type = el_data.get("element_type")
        if el_type == "title": text_parts.append(f"\n{'#' * el_data.get('level',1)} {el_data.get('text','')}\n")
        elif el_type == "narrative_text": text_parts.append(el_data.get('text','') + "\n")
        elif el_type == "list_item":
            prefix = f"{el_data.get('item_number','')}. " if el_data.get('ordered') and el_data.get('item_number') else "- "
            indent = "  " * el_data.get('level',0)
            text_parts.append(f"{indent}{prefix}{el_data.get('text','')}\n")
        elif el_type == "table":
            if el_data.get('markdown_representation'): text_parts.append(f"\n[Table: {el_data.get('caption','Unnamed Table')}]\n{el_data['markdown_representation']}\n")
            elif el_data.get('text_representation'): text_parts.append(f"\n[Table: {el_data.get('caption','Unnamed Table')}]\n{el_data['text_representation']}\n")
        elif el_type == "code_block":
            lang = el_data.get('language', "") or ""
            text_parts.append(f"\n```{lang}\n{el_data.get('code','')}\n```\n")
        elif el_type == "page_break": text_parts.append("\n---\n")
        text_parts.append("\n") 
    return "".join(text_parts).strip().replace("\n\n\n", "\n\n").replace("\n\n\n", "\n\n")

def _convert_unstructured_to_pydantic(elements: List[UnstructuredElement]) -> List[DocumentElementType]:  # type: ignore
    """Converts a list of Unstructured elements to our internal Pydantic models."""
    pydantic_elements = []
    for el in elements:
        meta = DocumentElementMetadata(page_number=getattr(el.metadata, 'page_number', None))
        
        if isinstance(el, Title):
            pydantic_elements.append(TitleElement(text=el.text, level=getattr(el.metadata, 'category_depth', 1), metadata=meta))
        elif isinstance(el, NarrativeText):
            pydantic_elements.append(NarrativeTextElement(text=el.text, metadata=meta))
        elif isinstance(el, ListItem):
            pydantic_elements.append(ListItemElement(text=el.text, metadata=meta))
        elif isinstance(el, Table):
            # Unstructured v0.12+ has built-in markdown conversion
            pydantic_elements.append(TableElement(markdown_representation=getattr(el, 'text_as_html', str(el)), metadata=meta))
        elif isinstance(el, (Header, Footer, Image)):
             # We can choose to ignore headers, footers, and images for now
             continue
        else:
            # Fallback for any other element types
            if el.text.strip():
                 pydantic_elements.append(NarrativeTextElement(text=el.text, metadata=meta))
                 
    return pydantic_elements


def parse_markdown_to_structured_output(md_content_str: str, original_metadata: Dict[str, Any]) -> Optional[ParsedDocumentOutput]:
    """
    Top-level function to parse markdown string and return ParsedDocumentOutput.
    """
    logger.info(f"Parsing Markdown content (length: {len(md_content_str)} chars)...")
    try:
        md_parser = MarkdownIt("commonmark").enable("table") # Removed "breaks":True based on last log
        tokens = md_parser.parse(md_content_str)
        
        structured_elements = _convert_md_tokens_to_elements_internal(tokens)
        linear_text = _generate_parsed_text_from_elements_internal(structured_elements)

        if _PYDANTIC_MODELS_AVAILABLE_PARSERS:
            return ParsedDocumentOutput(
                parsed_text=linear_text,
                elements=structured_elements,
                original_metadata=original_metadata
            )
        else: # Fallback if Pydantic models aren't available (e.g. PoC context)
            return {
                 "parsed_text": linear_text,
                 "elements": structured_elements,
                 "original_metadata": original_metadata
            } # type: ignore 
    except Exception as e:
        logger.error(f"Error in parse_markdown_to_structured_output: {e}", exc_info=True)
        return None

# --- Placeholder for other parsers ---
def parse_docx_to_structured_output(file_path: str, original_metadata: Dict[str, Any]) -> Optional[ParsedDocumentOutput]:
    """
    Parses a DOCX file using Unstructured.io, extracts rich metadata,
    and converts elements to our internal Pydantic models.
    """
    if not _UNSTRUCTURED_AVAILABLE:
        logger.error("Unstructured library is not available. Cannot parse DOCX files.")
        return None
        
    logger.info(f"Parsing DOCX with Unstructured: {file_path}")
    
    try:
        # ä½¿ç”¨ Unstructured è§£ææ–‡æ¡£ï¼Œè®¾ç½®ç­–ç•¥ä»¥è·å–æ›´å¹²å‡€çš„æ•°æ®
        unstructured_elements = partition_docx(
            filename=file_path,
            strategy="hi_res",  # ä½¿ç”¨é«˜åˆ†è¾¨ç‡ç­–ç•¥ä»¥æ›´å¥½åœ°å¤„ç†å¸ƒå±€
            infer_table_structure=True, # å¼€å¯è¡¨æ ¼ç»“æ„æ¨æ–­
        )
        
        # --- å…³é”®ï¼šåˆå¹¶ Unstructured æå–çš„å…ƒæ•°æ® ---
        # partition_docx è¿”å›çš„ç¬¬ä¸€ä¸ªå…ƒç´ é€šå¸¸åŒ…å«æ–‡æ¡£çº§åˆ«çš„å…ƒæ•°æ®
        doc_level_meta = {}
        if unstructured_elements:
             # Unstructured v0.12+ å°†å…ƒæ•°æ®é™„åŠ åˆ°æ¯ä¸ªå…ƒç´ ä¸Š
             # æˆ‘ä»¬ä»ç¬¬ä¸€ä¸ªå…ƒç´ è·å–é€šç”¨å…ƒæ•°æ®
             doc_level_meta = unstructured_elements[0].metadata.to_dict()

        # å°† Unstructured çš„å…ƒæ•°æ®ä¸æˆ‘ä»¬ä¼ å…¥çš„åŸå§‹å…ƒæ•°æ®åˆå¹¶
        # Unstructured çš„å…ƒæ•°æ®ä¼˜å…ˆçº§æ›´é«˜ï¼Œå› ä¸ºå®ƒæ›´å…·ä½“
        combined_metadata = {**original_metadata, **doc_level_meta}
        # ç§»é™¤ä¸€äº›ä¸éœ€è¦çš„å†…éƒ¨é”®
        combined_metadata.pop('parent_id', None)
        combined_metadata.pop('category_depth', None)

        # å°† Unstructured å…ƒç´ è½¬æ¢ä¸ºæˆ‘ä»¬è‡ªå·±çš„ Pydantic æ¨¡å‹
        pydantic_elements = _convert_unstructured_to_pydantic(unstructured_elements)
        
        if not pydantic_elements:
            logger.warning(f"No content elements were extracted from DOCX file: {file_path}")
            return None

        # ä»è½¬æ¢åçš„å…ƒç´ ç”Ÿæˆçº¿æ€§æ–‡æœ¬è¡¨ç¤º
        linear_text = _generate_parsed_text_from_elements_internal(pydantic_elements)

        if _PYDANTIC_MODELS_AVAILABLE_PARSERS:
            return ParsedDocumentOutput(
                parsed_text=linear_text,
                elements=pydantic_elements,
                original_metadata=combined_metadata # <--- ä½¿ç”¨åˆå¹¶åçš„å…ƒæ•°æ®
            )
        else:
            # Fallback for non-pydantic environment (should not happen in production)
            return {
                "parsed_text": linear_text,
                "elements": pydantic_elements,
                "original_metadata": combined_metadata
            } # type: ignore
            
    except Exception as e:
        logger.error(f"Error parsing DOCX file '{file_path}' with Unstructured: {e}", exc_info=True)
        return None
    
def parse_pdf_to_structured_output(file_path: str, original_metadata: Dict[str, Any]) -> Optional[ParsedDocumentOutput]:
    logger.info(f"Parsing PDF: {file_path} (Not yet fully implemented in document_parsers.py)")
    # Here you would integrate the PyMuPDF logic from your PoC
    text_content = f"[Placeholder: PDF content for {os.path.basename(file_path)}]"
    elements = []
    if _PYDANTIC_MODELS_AVAILABLE_PARSERS:
        elements.append(NarrativeTextElement(text=text_content))
        return ParsedDocumentOutput(parsed_text=text_content, elements=elements, original_metadata=original_metadata)
    else:
        elements.append({"element_type":"narrative_text", "text":text_content})
        return {"parsed_text":text_content, "elements":elements, "original_metadata":original_metadata} # type: ignore

def parse_xlsx_to_structured_output(file_path: str, original_metadata: Dict[str, Any]) -> Optional[ParsedDocumentOutput]:
    """
    Parses an XLSX file using pandas, converts each sheet to a Markdown table,
    and enriches metadata with filename and sheet name.
    """
    logger.info(f"Parsing XLSX with pandas: {file_path}")
    
    try:
        xls = pd.ExcelFile(file_path)
        all_elements: List[DocumentElementType] = [] # type: ignore
        full_text_representation = ""

        # --- æ ¸å¿ƒä¿®æ­£ï¼šç¡®ä¿filenameåœ¨å…ƒæ•°æ®ä¸­ ---
        # æ— è®ºä¸Šæ¸¸æ˜¯å¦æä¾›ï¼Œæˆ‘ä»¬åœ¨è¿™é‡Œéƒ½ä»¥æ–‡ä»¶è·¯å¾„ä¸ºå‡†ï¼Œå¼ºåˆ¶æ·»åŠ /è¦†ç›–
        base_metadata = original_metadata.copy()
        base_metadata['filename'] = os.path.basename(file_path)

        for sheet_name in xls.sheet_names:
            try:
                df = pd.read_excel(xls, sheet_name=sheet_name)

                # --- æ–°å¢ï¼šåœ¨å¤„ç†å‰ï¼Œæ¸…æ´—æ‰€æœ‰å­—ç¬¦ä¸²ç±»å‹å•å…ƒæ ¼çš„å‰åç©ºæ ¼ ---
                for col in df.columns:
                    if df[col].dtype == 'object':
                        df[col] = df[col].str.strip()
                # --- æ–°å¢ç»“æŸ ---

                # è¿‡æ»¤æ‰å®Œå…¨ä¸ºç©ºçš„è¡Œå’Œåˆ—ï¼Œé¿å…æ— æ•ˆçš„Markdownè¾“å‡º
                df.dropna(how='all', axis=0, inplace=True)
                df.dropna(how='all', axis=1, inplace=True)

                if df.empty:
                    logger.info(f"  Skipping empty sheet: {sheet_name}")
                    continue
                
                # å°†DataFrameè½¬æ¢ä¸ºMarkdownæ ¼å¼çš„å­—ç¬¦ä¸²
                markdown_table = df.to_markdown(index=False)
                
                # ä¸ºæ¯ä¸ªå·¥ä½œè¡¨åˆ›å»ºä¸€ä¸ªæ ‡é¢˜å’Œä¸€ä¸ªè¡¨æ ¼å…ƒç´ 
                sheet_title_text = f"Sheet: {sheet_name}"
                full_text_representation += f"## {sheet_title_text}\n\n{markdown_table}\n\n"
                
                # ä¸ºå·¥ä½œè¡¨æ ‡é¢˜åˆ›å»ºå…ƒç´ 
                all_elements.append(TitleElement(
                    text=sheet_title_text,
                    level=2,
                    metadata=DocumentElementMetadata(page_number=xls.sheet_names.index(sheet_name) + 1)
                ))
                
                # ä¸ºè¡¨æ ¼æœ¬èº«åˆ›å»ºå…ƒç´ 
                sheet_meta = base_metadata.copy()
                sheet_meta['sheet_name'] = sheet_name
                
                all_elements.append(TableElement(
                    markdown_representation=markdown_table,
                    metadata=DocumentElementMetadata(**sheet_meta) # ä¼ é€’ç‰¹å®šäºå·¥ä½œè¡¨çš„å…ƒæ•°æ®
                ))
                logger.info(f"  Successfully parsed sheet: {sheet_name}")

            except Exception as e_sheet:
                logger.error(f"  Failed to parse sheet '{sheet_name}' in file '{file_path}': {e_sheet}", exc_info=True)
                continue

        if not all_elements:
            logger.warning(f"No data parsed from any sheet in XLSX file: {file_path}")
            return None
        
        # è¿”å›åŒ…å«æ‰€æœ‰å·¥ä½œè¡¨å†…å®¹çš„å•ä¸ª ParsedDocumentOutput
        return ParsedDocumentOutput(
            parsed_text=full_text_representation,
            elements=all_elements,
            original_metadata=base_metadata # è¿”å›åŒ…å«æ­£ç¡®filenameçš„æ–‡æ¡£çº§å…ƒæ•°æ®
        )

    except Exception as e:
        logger.error(f"Error parsing XLSX file '{file_path}': {e}", exc_info=True)
        return None

def parse_html_to_structured_output(html_content_str: str, original_metadata: Dict[str, Any]) -> Optional[ParsedDocumentOutput]:
    """
    Parses an HTML string using BeautifulSoup, extracts meaningful elements,
    and converts them to our internal Pydantic models.
    """
    logger.info(f"Parsing HTML content with BeautifulSoup (length: {len(html_content_str)} chars)...")
    if not html_content_str.strip():
        return None

    try:
        soup = BeautifulSoup(html_content_str, 'lxml')
        
        for script_or_style in soup(["script", "style"]):
            script_or_style.decompose()

        elements: List[DocumentElementType] = [] # type: ignore
        
        for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'div', 'li', 'pre']):
            text = tag.get_text(separator=' ', strip=True)
            if text:
                if tag.name.startswith('h'):
                    level = int(tag.name[1:])
                    elements.append(TitleElement(text=text, level=level))
                elif tag.name == 'li':
                    elements.append(ListItemElement(text=text))
                elif tag.name == 'pre':
                    elements.append(CodeBlockElement(code=text))
                else:
                    elements.append(NarrativeTextElement(text=text))
        
        for table in soup.find_all('table'):
            header = [th.get_text(strip=True) for th in table.find_all('th')]
            rows = []
            for tr in table.find_all('tr'):
                cells = [td.get_text(strip=True) for td in tr.find_all('td')]
                if cells:
                    rows.append(cells)
            
            if header or rows:
                md_table_parts = []
                if header:
                    md_table_parts.append(f"| {' | '.join(header)} |")
                    md_table_parts.append(f"|{'---|' * len(header)}")
                for row in rows:
                    md_table_parts.append(f"| {' | '.join(row)} |")
                
                elements.append(TableElement(markdown_representation='\n'.join(md_table_parts)))

        if not elements:
            logger.warning("No structured elements found in HTML content after parsing.")
            return None

        linear_text = _generate_parsed_text_from_elements_internal(elements)

        return ParsedDocumentOutput(
            parsed_text=linear_text,
            elements=elements,
            original_metadata=original_metadata
        )
    except Exception as e:
        logger.error(f"Error parsing HTML content with BeautifulSoup: {e}", exc_info=True)
        return None


--------------------------------------------------------------------------------
æ–‡ä»¶: zhz_rag_pipeline/evaluation_assets.py
--------------------------------------------------------------------------------
import dagster as dg
import os
from typing import Dict, List, Any # Optional å¯èƒ½ä¹‹åä¼šç”¨åˆ°

# ä»é¡¹ç›®ä¸­å¯¼å…¥æˆ‘ä»¬é‡æ„çš„æ‰¹é‡è¯„ä¼°å‡½æ•°å’Œç›¸å…³å·¥å…·/å¸¸é‡
from zhz_rag.evaluation.batch_eval_cypher import run_cypher_batch_evaluation
from zhz_rag.evaluation.batch_eval_answer import run_answer_batch_evaluation
from zhz_rag.evaluation.analyze_cypher import perform_cypher_evaluation_analysis
from zhz_rag.evaluation.analyze_answer import perform_answer_evaluation_analysis
from zhz_rag.utils.common_utils import (
    find_latest_rag_interaction_log,
    RAG_INTERACTION_LOGS_DIR,
    EVALUATION_RESULTS_LOGS_DIR,
    get_evaluation_result_log_filepath
)
# å¯¼å…¥ GeminiAPIResource ä»¥å£°æ˜èµ„æºä¾èµ–
from zhz_rag_pipeline_dagster.zhz_rag_pipeline.resources import GeminiAPIResource

# --- èµ„äº§å®šä¹‰ ---

@dg.asset(
    name="latest_rag_interaction_log_for_evaluation",
    description="Provides the filepath of the latest RAG interaction log to be used for evaluation.",
    group_name="evaluation_pipeline",
    compute_kind="python" # å¯é€‰ï¼ŒæŒ‡æ˜è®¡ç®—ç±»å‹
)
def latest_rag_interaction_log_for_evaluation_asset(context: dg.AssetExecutionContext) -> str:
    """
    Finds and returns the path to the latest RAG interaction log file.
    """
    log_filepath = find_latest_rag_interaction_log(RAG_INTERACTION_LOGS_DIR)
    if not log_filepath or not os.path.exists(log_filepath):
        error_msg = f"No RAG interaction log file found in directory: {RAG_INTERACTION_LOGS_DIR}"
        context.log.error(error_msg)
        raise dg.Failure(description=error_msg)
    
    context.log.info(f"Using RAG interaction log for evaluation: {log_filepath}")
    context.add_output_metadata({"log_filepath": log_filepath, "filename": os.path.basename(log_filepath)})
    return log_filepath

@dg.asset(
    name="batch_cypher_evaluations_log", # èµ„äº§åç§°æœ€å¥½èƒ½åæ˜ å®ƒäº§å‡ºçš„æ˜¯æ—¥å¿—æ–‡ä»¶
    description="Runs batch evaluation of Cypher queries and produces an evaluation log file.",
    group_name="evaluation_pipeline",
    compute_kind="python",
    # deps=[latest_rag_interaction_log_for_evaluation_asset] # é€šè¿‡å‡½æ•°å‚æ•°è‡ªåŠ¨æ¨æ–­ä¾èµ–
)
async def batch_cypher_evaluation_log_asset(
    context: dg.AssetExecutionContext,
    gemini_api: GeminiAPIResource,
    latest_rag_interaction_log_for_evaluation: str # <--- ä¿®æ”¹å‚æ•°å
) -> dg.Output[str]:
    context.log.info(f"Starting batch Cypher evaluation using log file: {latest_rag_interaction_log_for_evaluation}") # <--- ä½¿ç”¨æ–°å‚æ•°å
    
    # ä» Dagster é…ç½®ä¸­è·å–å‚æ•°ï¼Œæˆ–ä½¿ç”¨é»˜è®¤/ç¯å¢ƒå˜é‡
    # è¿™é‡Œæˆ‘ä»¬å…ˆç”¨ä¹‹å‰è„šæœ¬ä¸­çš„æ–¹å¼ï¼Œæœªæ¥å¯ä»¥è½¬ä¸º Dagster run_config
    app_version = os.getenv("APP_VERSION_TAG", "dagster_cypher_eval_0.2")
    # å¯¹äº use_simulated_apiï¼Œåœ¨ Dagster ä¸­é€šå¸¸ä¼šé€šè¿‡èµ„æºé…ç½®æˆ– op_config æ¥æ§åˆ¶ï¼Œ
    # è€Œä¸æ˜¯ç›´æ¥ä¾èµ–ç¯å¢ƒå˜é‡ï¼Œè¿™æ ·æ›´çµæ´»ã€‚ä½†ä¸ºäº†ä¿æŒä¸è„šæœ¬ä¸€è‡´ï¼Œæš‚æ—¶ä¿ç•™ã€‚
    use_simulated = os.getenv("USE_SIMULATED_GEMINI_CYPHER_EVAL", "false").lower() == "true"
    api_delay = float(os.getenv("GEMINI_API_CALL_DELAY_SECONDS", "4.1"))

    if use_simulated:
        context.log.warning("Cypher evaluation asset is using SIMULATED Gemini API calls.")

    # è°ƒç”¨æˆ‘ä»¬é‡æ„çš„ã€ç°åœ¨æ¥å— gemini_resource çš„æ‰¹é‡è¯„ä¼°å‡½æ•°
    eval_stats = await run_cypher_batch_evaluation(
        gemini_resource_for_evaluator=gemini_api, # ä¼ é€’æ³¨å…¥çš„ Dagster èµ„æº
        rag_interaction_log_filepath=latest_rag_interaction_log_for_evaluation,
        app_version=app_version,
        use_simulated_api=use_simulated, # è¿™ä¸ªå‚æ•°ç°åœ¨ç”± run_cypher_batch_evaluation å†…éƒ¨å¤„ç†
        api_call_delay=api_delay
    )
    context.log.info(f"Batch Cypher evaluation completed. Statistics: {eval_stats}")

    # ç¡®å®šè¾“å‡ºçš„è¯„ä¼°ç»“æœæ—¥å¿—æ–‡ä»¶å (ä¸ evaluator.py ä¸­ä¸€è‡´)
    output_log_filepath = get_evaluation_result_log_filepath(evaluation_name="cypher_gemini_flash")
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨ (get_evaluation_result_log_filepath å†…éƒ¨çš„ log_interaction_data ä¼šå¤„ç†)
    # ä½†è¿™é‡Œæˆ‘ä»¬ä¹Ÿå¯ä»¥æå‰ç¡®ä¿ï¼Œæˆ–è€…ä¾èµ– log_interaction_data
    os.makedirs(os.path.dirname(output_log_filepath), exist_ok=True)
            
    metadata = {"evaluation_stats": eval_stats, "output_filepath": output_log_filepath}
    if eval_stats.get("cypher_queries_evaluated", 0) == 0:
        metadata["warning"] = "No Cypher queries were evaluated. Output log might be empty."
        context.log.warning(metadata["warning"])

    return dg.Output(output_log_filepath, metadata=metadata)


@dg.asset(
    name="batch_answer_evaluations_log", # èµ„äº§åç§°
    description="Runs batch evaluation of generated answers from RAG logs using Gemini.",
    group_name="evaluation_pipeline",
    compute_kind="python",
    # deps=[latest_rag_interaction_log_for_evaluation_asset] # é€šè¿‡å‡½æ•°å‚æ•°è‡ªåŠ¨æ¨æ–­ä¾èµ–
)
async def batch_answer_evaluation_log_asset(
    context: dg.AssetExecutionContext,
    gemini_api: GeminiAPIResource,
    latest_rag_interaction_log_for_evaluation: str # <--- ä¿®æ”¹å‚æ•°å
) -> dg.Output[str]:
    context.log.info(f"Starting batch Answer evaluation using log file: {latest_rag_interaction_log_for_evaluation}") # <--- ä½¿ç”¨æ–°å‚æ•°å
    app_version = os.getenv("APP_VERSION_TAG", "dagster_answer_eval_0.2")
    use_simulated = os.getenv("USE_SIMULATED_GEMINI_ANSWER_EVAL", "false").lower() == "true"
    api_delay = float(os.getenv("GEMINI_API_CALL_DELAY_SECONDS", "4.1"))

    if use_simulated:
        context.log.warning("Answer evaluation asset is using SIMULATED Gemini API calls.")

    eval_stats = await run_answer_batch_evaluation(
        gemini_resource_for_evaluator=gemini_api, # ä¼ é€’æ³¨å…¥çš„ Dagster èµ„æº
        rag_interaction_log_filepath=latest_rag_interaction_log_for_evaluation,
        app_version=app_version,
        use_simulated_api=use_simulated, # è¿™ä¸ªå‚æ•°ç°åœ¨ç”± run_answer_batch_evaluation å†…éƒ¨å¤„ç†
        api_call_delay=api_delay
    )
    context.log.info(f"Batch Answer evaluation completed. Statistics: {eval_stats}")

    output_log_filepath = get_evaluation_result_log_filepath(evaluation_name="answer_gemini_flash")
    os.makedirs(os.path.dirname(output_log_filepath), exist_ok=True)

    metadata = {"evaluation_stats": eval_stats, "output_filepath": output_log_filepath}
    if eval_stats.get("answers_evaluated", 0) == 0:
        metadata["warning"] = "No answers were evaluated. Output log might be empty."
        context.log.warning(metadata["warning"])
        
    return dg.Output(output_log_filepath, metadata=metadata)

@dg.asset(
    name="cypher_evaluation_analysis_report", # èµ„äº§åç§°
    description="Generates a CSV analysis report from Cypher evaluation results.",
    group_name="evaluation_pipeline",
    compute_kind="python",
    # deps=[batch_cypher_evaluation_log_asset] # é€šè¿‡å‡½æ•°å‚æ•°è‡ªåŠ¨æ¨æ–­ä¾èµ–
)
def cypher_analysis_report_asset(
    context: dg.AssetExecutionContext,
    batch_cypher_evaluations_log: str # ä¸Šæ¸¸èµ„äº§çš„è¾“å‡º (å³ cypher è¯„ä¼°æ—¥å¿—æ–‡ä»¶çš„è·¯å¾„)
) -> dg.Output[str]: # è¾“å‡º CSV æŠ¥å‘Šæ–‡ä»¶çš„è·¯å¾„
    """
    Analyzes Cypher evaluation logs and produces a CSV report.
    """
    context.log.info(f"Starting Cypher evaluation analysis using log file: {batch_cypher_evaluations_log}")

    if not os.path.exists(batch_cypher_evaluations_log):
        error_msg = f"Input Cypher evaluation log file not found: {batch_cypher_evaluations_log}"
        context.log.error(error_msg)
        raise dg.Failure(description=error_msg)

    # æ„å»ºè¾“å‡ºCSVæ–‡ä»¶çš„è·¯å¾„
    # æˆ‘ä»¬å¸Œæœ›CSVæ–‡ä»¶ä¹Ÿå­˜å‚¨åœ¨ EVALUATION_RESULTS_LOGS_DIR ç›®å½•ä¸‹
    # æ–‡ä»¶åå¯ä»¥åŸºäºè¾“å…¥æ—¥å¿—åæˆ–å›ºå®šä¸€ä¸ªæ¨¡å¼
    base_input_log_name = os.path.basename(batch_cypher_evaluations_log)
    # ä» "eval_results_cypher_gemini_flash_YYYYMMDD.jsonl" ç”Ÿæˆ "analysis_cypher_gemini_flash_YYYYMMDD.csv"
    if base_input_log_name.startswith("eval_results_") and base_input_log_name.endswith(".jsonl"):
        analysis_file_name = "analysis_" + base_input_log_name[len("eval_results_"):-len(".jsonl")] + ".csv"
    else: # Fallback naming
        analysis_file_name = f"analysis_cypher_report_{context.run_id[:8]}.csv"
    
    output_csv_filepath = os.path.join(EVALUATION_RESULTS_LOGS_DIR, analysis_file_name)
    
    success = perform_cypher_evaluation_analysis(
        evaluation_log_filepath=batch_cypher_evaluations_log,
        output_csv_filepath=output_csv_filepath
    )

    if success:
        context.log.info(f"Cypher evaluation analysis report generated: {output_csv_filepath}")
        return dg.Output(output_csv_filepath, metadata={"output_csv_filepath": output_csv_filepath, "source_log": base_input_log_name})
    else:
        error_msg = f"Cypher evaluation analysis failed for log file: {batch_cypher_evaluations_log}"
        context.log.error(error_msg)
        raise dg.Failure(description=error_msg)


@dg.asset(
    name="answer_evaluation_analysis_report", # èµ„äº§åç§°
    description="Generates a CSV analysis report from Answer evaluation results.",
    group_name="evaluation_pipeline",
    compute_kind="python",
    # deps=[batch_answer_evaluations_log_asset] # é€šè¿‡å‡½æ•°å‚æ•°è‡ªåŠ¨æ¨æ–­ä¾èµ–
)
def answer_analysis_report_asset(
    context: dg.AssetExecutionContext,
    batch_answer_evaluations_log: str # ä¸Šæ¸¸èµ„äº§çš„è¾“å‡º (å³ answer è¯„ä¼°æ—¥å¿—æ–‡ä»¶çš„è·¯å¾„)
) -> dg.Output[str]: # è¾“å‡º CSV æŠ¥å‘Šæ–‡ä»¶çš„è·¯å¾„
    """
    Analyzes Answer evaluation logs and produces a CSV report.
    """
    context.log.info(f"Starting Answer evaluation analysis using log file: {batch_answer_evaluations_log}")

    if not os.path.exists(batch_answer_evaluations_log):
        error_msg = f"Input Answer evaluation log file not found: {batch_answer_evaluations_log}"
        context.log.error(error_msg)
        raise dg.Failure(description=error_msg)

    base_input_log_name = os.path.basename(batch_answer_evaluations_log)
    if base_input_log_name.startswith("eval_results_") and base_input_log_name.endswith(".jsonl"):
        analysis_file_name = "analysis_" + base_input_log_name[len("eval_results_"):-len(".jsonl")] + ".csv"
    else: # Fallback naming
        analysis_file_name = f"analysis_answer_report_{context.run_id[:8]}.csv"
        
    output_csv_filepath = os.path.join(EVALUATION_RESULTS_LOGS_DIR, analysis_file_name)

    success = perform_answer_evaluation_analysis(
        evaluation_log_filepath=batch_answer_evaluations_log,
        output_csv_filepath=output_csv_filepath
    )

    if success:
        context.log.info(f"Answer evaluation analysis report generated: {output_csv_filepath}")
        return dg.Output(output_csv_filepath, metadata={"output_csv_filepath": output_csv_filepath, "source_log": base_input_log_name})
    else:
        error_msg = f"Answer evaluation analysis failed for log file: {batch_answer_evaluations_log}"
        context.log.error(error_msg)
        raise dg.Failure(description=error_msg)

# å°†æ‰€æœ‰è¯„ä¼°ç›¸å…³çš„èµ„äº§æ”¶é›†åˆ°ä¸€ä¸ªåˆ—è¡¨ä¸­ï¼Œæ–¹ä¾¿åœ¨ definitions.py ä¸­å¼•ç”¨
all_evaluation_assets = [
    latest_rag_interaction_log_for_evaluation_asset,
    batch_cypher_evaluation_log_asset,
    batch_answer_evaluation_log_asset,
    cypher_analysis_report_asset, # <--- æ–°å¢
    answer_analysis_report_asset, # <--- æ–°å¢
]


--------------------------------------------------------------------------------
æ–‡ä»¶: zhz_rag_pipeline/ingestion_assets.py
--------------------------------------------------------------------------------
# zhz_rag_pipeline/ingestion_assets.py
import dagster as dg
import os
from typing import List, Dict, Any, Union, Optional
from datetime import datetime, timezone

# --- ä¿®æ”¹ï¼šå¯¼å…¥åˆ†å‘å™¨å¹¶è®¾ç½®Pydanticå¯ç”¨æ€§æ ‡å¿— ---
# å°è¯•å¯¼å…¥Pydanticæ¨¡å‹ï¼Œå¹¶è®¾ç½®ä¸€ä¸ªæ ‡å¿—ï¼Œä»¥ä¾¿åœ¨æ¨¡å‹ä¸å¯ç”¨æ—¶ä»£ç å¯ä»¥ä¼˜é›…åœ°é™çº§ã€‚
try:
    from .pydantic_models_dagster import LoadedDocumentOutput, ParsedDocumentOutput, NarrativeTextElement
    _PYDANTIC_AVAILABLE = True
except ImportError:
    LoadedDocumentOutput = dict  # type: ignore
    ParsedDocumentOutput = dict  # type: ignore
    NarrativeTextElement = dict  # type: ignore
    _PYDANTIC_AVAILABLE = False

from .parsers import dispatch_parsing # <--- ä¿®æ”¹å¯¼å…¥è·¯å¾„
# --- ä¿®æ”¹ç»“æŸ ---

class LoadDocumentsConfig(dg.Config):
    documents_directory: str = "/home/zhz/zhz_agent/data/raw_documents/" # æ›´æ–°åçš„åŸå§‹æ–‡æ¡£ç›®å½•
    allowed_extensions: List[str] = [".txt", ".md", ".docx", ".pdf", ".xlsx", ".html", ".htm"] # æ‰©å¤§å…è®¸èŒƒå›´ä»¥æµ‹è¯•æ‰€æœ‰è§£æå™¨

@dg.asset(
    name="raw_documents",
    description="Loads raw documents from a specified directory.",
    group_name="ingestion"
)
def load_documents_asset(
    context: dg.AssetExecutionContext, 
    config: LoadDocumentsConfig
) -> List[LoadedDocumentOutput]:  # type: ignore
    
    loaded_docs: List[LoadedDocumentOutput] = []  # type: ignore
    target_directory = config.documents_directory
    allowed_exts = tuple(config.allowed_extensions) 

    context.log.info(f"Scanning directory: {target_directory} for files with extensions: {allowed_exts}")

    if not os.path.isdir(target_directory):
        context.log.error(f"Directory not found: {target_directory}")
        return loaded_docs

    for filename in os.listdir(target_directory):
        file_path = os.path.join(target_directory, filename)
        if os.path.isfile(file_path):
            file_name_lower = filename.lower()
            file_extension = os.path.splitext(file_name_lower)[1]

            if file_extension in allowed_exts:
                context.log.info(f"Found matching file: {file_path}")
                try:
                    file_stat = os.stat(file_path)
                    
                    # --- VITAL FIX: Do not pass raw_content ---
                    # The downstream parser will handle reading the file from the path.
                    doc_output_data = {
                        "document_path": file_path,
                        "file_type": file_extension,
                        # raw_content is intentionally omitted
                        "metadata": {
                            "filename": filename,
                            "source_directory": target_directory,
                            "size_bytes": file_stat.st_size,
                            "creation_time_utc": datetime.fromtimestamp(file_stat.st_ctime, tz=timezone.utc).isoformat(),
                            "modified_time_utc": datetime.fromtimestamp(file_stat.st_mtime, tz=timezone.utc).isoformat()
                        }
                    }

                    if _PYDANTIC_AVAILABLE:
                        loaded_docs.append(LoadedDocumentOutput(**doc_output_data))
                    else:
                        loaded_docs.append(doc_output_data)

                    context.log.info(f"Successfully created LoadedDocumentOutput for: {file_path}")
                except Exception as e:
                    context.log.error(f"Failed to process file {file_path}: {e}")
            else:
                context.log.debug(f"Skipping file with non-allowed extension: {file_path}")
        else:
            context.log.debug(f"Skipping non-file item: {file_path}")
            
    if not loaded_docs:
        context.log.warning(f"No matching documents found in {target_directory}")

    if loaded_docs:
        first_doc_path = loaded_docs[0].document_path if _PYDANTIC_AVAILABLE and loaded_docs else "N/A"
        context.add_output_metadata(
            metadata={
                "num_documents_loaded": len(loaded_docs),
                "first_document_path": first_doc_path
            }
        )
    return loaded_docs



@dg.asset(
    name="parsed_documents",
    description="Parses loaded documents into text and extracts basic structure using a dispatcher.",
    group_name="ingestion"
)
def parse_document_asset(
    context: dg.AssetExecutionContext,
    raw_documents: List[LoadedDocumentOutput] # type: ignore
) -> List[ParsedDocumentOutput]: # type: ignore
    
    parsed_docs_output_list: List[ParsedDocumentOutput] = [] # type: ignore
    context.log.info(f"Received {len(raw_documents)} documents to parse.")

    for doc_input in raw_documents:
        doc_path = doc_input.document_path
        file_ext = doc_input.file_type.lower()
        original_metadata = doc_input.metadata.copy()
        original_metadata["source_file_path"] = doc_path

        context.log.info(f"Attempting to parse document: {doc_path} (Type: {file_ext})")

        try:
            # --- VITAL REFACTOR ---
            # ç›´æ¥å°†æ–‡ä»¶è·¯å¾„ä¼ é€’ç»™è§£æå™¨ï¼Œä¸å†å¤„ç†å­—èŠ‚å†…å®¹
            # å¯¹äºæ–‡æœ¬æ–‡ä»¶ï¼Œè§£æå™¨å†…éƒ¨è‡ªå·±ä¼šç”¨ 'rt' æ¨¡å¼è¯»å–
            # å¯¹äºäºŒè¿›åˆ¶æ–‡ä»¶(pdf, docx, xlsx)ï¼Œè§£æå™¨ä¼šç”¨ 'rb' æ¨¡å¼æˆ–ç›¸åº”åº“è¯»å–
            parsed_output = dispatch_parsing(file_ext, doc_path, original_metadata)

            if not parsed_output:
                context.log.warning(f"Parser for '{file_ext}' returned no output for {doc_path}. Creating a fallback.")
                fallback_text = f"[Content Not Parsed by Specific Parser: {doc_path}]"
                elements = [NarrativeTextElement(text=fallback_text)]
                parsed_output = ParsedDocumentOutput(
                    parsed_text=fallback_text,
                    elements=elements,
                    original_metadata=original_metadata
                )

            # ç¡®ä¿è¾“å‡ºæ€»æ˜¯ Pydantic æ¨¡å‹
            if isinstance(parsed_output, dict):
                parsed_output = ParsedDocumentOutput(**parsed_output)
            
            parsed_docs_output_list.append(parsed_output)
            context.log.info(f"Successfully processed: {doc_path}")

        except Exception as e:
            context.log.error(f"Critical error during parsing asset for {doc_path}: {e}", exc_info=True)
            error_text = f"[Critical Parsing Exception for {doc_path}: {str(e)}]"
            elements = [NarrativeTextElement(text=error_text)]
            error_output = ParsedDocumentOutput(
                parsed_text=error_text,
                elements=elements,
                original_metadata=original_metadata
            )
            parsed_docs_output_list.append(error_output)

    if parsed_docs_output_list:
        context.add_output_metadata(
            metadata={
                "num_documents_processed_for_parsing": len(raw_documents),
                "num_parsed_document_outputs_generated": len(parsed_docs_output_list),
            }
        )
    return parsed_docs_output_list



all_ingestion_assets = [load_documents_asset, parse_document_asset]


--------------------------------------------------------------------------------
æ–‡ä»¶: zhz_rag_pipeline/processing_assets.py
--------------------------------------------------------------------------------
#  æ–‡ä»¶: zhz_rag_pipeline_dagster/zhz_rag_pipeline/processing_assets.py

import json
import asyncio
import re
import dagster as dg
from typing import List, Dict, Any, Optional, Union
import uuid
from langchain_text_splitters import RecursiveCharacterTextSplitter
import hashlib
import pandas as pd
from zhz_rag.utils.common_utils import normalize_text_for_id
from zhz_rag_pipeline_dagster.zhz_rag_pipeline.pydantic_models_dagster import (
    ChunkOutput,
    ParsedDocumentOutput,
    EmbeddingOutput,
    KGTripleSetOutput,
    ExtractedEntity,
    ExtractedRelation,
    # --- æ·»åŠ å¯¼å…¥æˆ‘ä»¬éœ€è¦çš„å…ƒç´ ç±»å‹ ---
    TitleElement,
    NarrativeTextElement,
    ListItemElement,
    TableElement,
    CodeBlockElement,
    ImageElement,
    PageBreakElement,
    HeaderElement,
    FooterElement,
    DocumentElementMetadata
    # --- ç»“æŸæ·»åŠ  ---
)
from zhz_rag_pipeline_dagster.zhz_rag_pipeline.resources import (
    GGUFEmbeddingResource,
    ChromaDBResource,
    DuckDBResource,
    LocalLLMAPIResource,
    SystemResource  # <--- æ·»åŠ è¿™ä¸€è¡Œä»¥å¯¼å…¥ SystemResource
)
import jieba
import bm25s
import pickle
import numpy as np
import os
from zhz_rag.utils.common_utils import normalize_text_for_id

_PYDANTIC_AVAILABLE = False
try:
    from .pydantic_models_dagster import ( # ä½¿ç”¨ç›¸å¯¹å¯¼å…¥
        ChunkOutput,
        ParsedDocumentOutput,
        EmbeddingOutput,
        KGTripleSetOutput,
        ExtractedEntity,
        ExtractedRelation,
        TitleElement,
        NarrativeTextElement,
        ListItemElement,
        TableElement,
        CodeBlockElement,
        ImageElement,
        PageBreakElement,
        HeaderElement,
        FooterElement,
        DocumentElementMetadata # <--- ç¡®ä¿è¿™é‡Œå¯¼å…¥äº† DocumentElementMetadata
    )
    _PYDANTIC_AVAILABLE = True
    # å¦‚æœ Pydantic å¯ç”¨ï¼Œæˆ‘ä»¬ä¹Ÿå¯ä»¥ç›´æ¥ä»æ¨¡å‹ä¸­è·å– DocumentElementType
    # from .pydantic_models_dagster import DocumentElementType # å¦‚æœéœ€è¦æ›´ç²¾ç¡®çš„ç±»å‹æç¤º
except ImportError:
    # å®šä¹‰å ä½ç¬¦
    class BaseModel: pass
    class ChunkOutput(BaseModel): pass
    class ParsedDocumentOutput(BaseModel): pass
    class EmbeddingOutput(BaseModel): pass
    class KGTripleSetOutput(BaseModel): pass
    class ExtractedEntity(BaseModel): pass
    class ExtractedRelation(BaseModel): pass
    class TitleElement(BaseModel): pass
    class NarrativeTextElement(BaseModel): pass
    class ListItemElement(BaseModel): pass
    class TableElement(BaseModel): pass
    class CodeBlockElement(BaseModel): pass
    class ImageElement(BaseModel): pass
    class PageBreakElement(BaseModel): pass
    class HeaderElement(BaseModel): pass
    class FooterElement(BaseModel): pass
    class DocumentElementMetadata(BaseModel): pass # <--- å®šä¹‰å ä½ç¬¦
    DocumentElementType = Any # type: ignore
# --- ç»“æŸ Pydantic æ¨¡å‹å¯¼å…¥ ---

import logging
logger = logging.getLogger(__name__)
if not logger.handlers:
    handler = logging.StreamHandler()
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    logger.setLevel(logging.DEBUG) # <--- ç¡®ä¿æ˜¯ DEBUG
    logger.info(f"Logger for {__name__} (processing_assets) configured with DEBUG level.")


class TextChunkerConfig(dg.Config):
    chunk_size: int = 1000 
    chunk_overlap: int = 100
    max_element_text_length_before_split: int = 1200 # ä¸€ä¸ª1200å­—ç¬¦çš„æ®µè½å¦‚æœè¯­ä¹‰è¿è´¯ï¼Œå¯ä»¥è€ƒè™‘ä¸åˆ‡åˆ†ã€‚
    target_sentence_split_chunk_size: int = 600    # ç•¥å¾®å¢å¤§å­å—çš„ç›®æ ‡å¤§å°ï¼Œä½¿å…¶åŒ…å«æ›´å¤šä¸Šä¸‹æ–‡ã€‚
    sentence_split_chunk_overlap_sentences: int = 2  # å¢åŠ åˆ°2å¥é‡å ï¼Œä»¥æœŸåœ¨å­å—ä¹‹é—´æä¾›æ›´å¥½çš„è¯­ä¹‰è¿æ¥ã€‚
    # --- åˆå¹¶ç­–ç•¥å‚æ•° ---
    min_chunk_length_to_avoid_merge: int = 250
    max_merged_chunk_size: int = 750


def split_text_into_sentences(text: str) -> List[str]:
    """
    Splits text into sentences using a regex-based approach.
    Handles common sentence terminators and aims to preserve meaningful units.
    """
    if not text:
        return []
    # æ”¹è¿›çš„å¥å­åˆ†å‰²æ­£åˆ™è¡¨è¾¾å¼ï¼Œè€ƒè™‘äº†ä¸­è‹±æ–‡å¥å·ã€é—®å·ã€æ„Ÿå¹å·
    # å¹¶å°è¯•å¤„ç†çœç•¥å·å’Œä¸€äº›ç‰¹æ®Šæƒ…å†µã€‚
    # (?<=[ã€‚ï¼Ÿï¼\.!\?]) ä¼šåŒ¹é…è¿™äº›æ ‡ç‚¹ç¬¦å·åé¢çš„ä½ç½® (lookbehind)
    # \s* åŒ¹é…æ ‡ç‚¹åçš„ä»»æ„ç©ºæ ¼
    # (?!$) ç¡®ä¿ä¸æ˜¯å­—ç¬¦ä¸²æœ«å°¾ (é¿å…åœ¨æœ«å°¾æ ‡ç‚¹åäº§ç”Ÿç©ºå¥å­)
    # å¯¹äºä¸­æ–‡ï¼Œå¥å·ã€é—®å·ã€æ„Ÿå¹å·é€šå¸¸ç›´æ¥ç»“æŸå¥å­ã€‚
    # å¯¹äºè‹±æ–‡ï¼Œ. ! ? åé¢é€šå¸¸æœ‰ç©ºæ ¼æˆ–æ¢è¡Œã€‚
    
    # ä¸€ä¸ªæ›´ç®€å•çš„ç‰ˆæœ¬ï¼Œç›´æ¥æŒ‰æ ‡ç‚¹åˆ†å‰²ï¼Œç„¶åæ¸…ç†
    sentences = re.split(r'([ã€‚ï¼Ÿï¼\.!\?])', text)
    result = []
    current_sentence = ""
    for i in range(0, len(sentences), 2):
        part = sentences[i]
        terminator = sentences[i+1] if i+1 < len(sentences) else ""
        current_sentence = part + terminator
        if current_sentence.strip():
            result.append(current_sentence.strip())
    
    # å¦‚æœä¸Šé¢çš„åˆ†å‰²ä¸ç†æƒ³ï¼Œå¯ä»¥å°è¯•æ›´å¤æ‚çš„ï¼Œä½†è¿™ä¸ªç®€å•ç‰ˆæœ¬é€šå¸¸å¤Ÿç”¨
    # ä¾‹å¦‚ï¼š
    # sentences = re.split(r'(?<=[ã€‚ï¼Ÿï¼\.!\?])\s*', text)
    # sentences = [s.strip() for s in sentences if s.strip()]
    return result if result else [text] # å¦‚æœæ— æ³•åˆ†å‰²ï¼Œè¿”å›åŸæ–‡æœ¬ä½œä¸ºä¸€ä¸ªå¥å­


# --- START: è¦†ç›–è¿™ä¸ªå‡½æ•° ---
def split_markdown_table_by_rows(
    markdown_table_text: str,
    target_chunk_size: int,
    max_chunk_size: int,
    context: Optional[dg.AssetExecutionContext] = None
) -> List[Dict[str, Any]]:
    """
    Splits a Markdown table string by its data rows.
    It now tries to create smaller chunks, even for short tables, by grouping a few rows together.
    """
    sub_chunks_data: List[Dict[str, Any]] = []
    lines = markdown_table_text.strip().split('\n')
    
    if len(lines) < 2:
        if context: context.log.warning(f"Markdown table has less than 2 lines. Cannot process for row splitting.")
        return [{"text": markdown_table_text, "start_row_index": -1, "end_row_index": -1}]

    header_row = lines[0]
    separator_row = lines[1]
    data_rows = lines[2:]

    if not data_rows:
        if context: context.log.warning("Markdown table has no data rows. Returning header and separator as single chunk.")
        return [{"text": f"{header_row}\n{separator_row}", "start_row_index": -1, "end_row_index": -1}]

    # --- æ–°çš„ã€æ›´æ¿€è¿›çš„åˆ†å‰²é€»è¾‘ ---
    current_sub_chunk_lines = []
    current_sub_chunk_start_row_idx = 0
    
    # å®šä¹‰æ¯ä¸ªå—çš„ç›®æ ‡è¡Œæ•°ï¼Œä¾‹å¦‚ 2-3 è¡Œï¼Œå¯ä»¥æ ¹æ®éœ€è¦è°ƒæ•´
    ROWS_PER_CHUNK = 2

    for i in range(0, len(data_rows), ROWS_PER_CHUNK):
        chunk_of_rows = data_rows[i:i + ROWS_PER_CHUNK]
        
        # æ¯ä¸ªå—éƒ½åŒ…å«è¡¨å¤´å’Œåˆ†éš”ç¬¦ï¼Œä»¥ä¿è¯ä¸Šä¸‹æ–‡å®Œæ•´
        sub_chunk_text = "\n".join([header_row, separator_row] + chunk_of_rows)
        
        start_row_index = i
        end_row_index = i + len(chunk_of_rows) - 1

        sub_chunks_data.append({
            "text": sub_chunk_text,
            "start_row_index": start_row_index,
            "end_row_index": end_row_index
        })
        if context:
            context.log.debug(f"  Table sub-chunk created: data rows index {start_row_index}-{end_row_index}")
    
    return sub_chunks_data
# --- END: è¦†ç›–ç»“æŸ ---


def split_code_block_by_blank_lines(
    code_text: str,
    target_chunk_size: int, # å¤ç”¨é…ç½®ï¼Œä½†å¯¹äºä»£ç å—ï¼Œè¿™ä¸ªæ›´åƒæ˜¯ä¸€ä¸ªä¸Šé™æŒ‡å¯¼
    max_chunk_size: int,    # ä½œä¸ºç¡¬ä¸Šé™
    context: Optional[dg.AssetExecutionContext] = None
) -> List[str]:
    """
    Splits a code block string by blank lines (one or more empty lines).
    Tries to keep resulting chunks from exceeding max_chunk_size.
    """
    if not code_text.strip():
        return []

    # ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…ä¸€ä¸ªæˆ–å¤šä¸ªè¿ç»­çš„ç©ºè¡Œä½œä¸ºåˆ†éš”ç¬¦
    # \n\s*\n åŒ¹é…ä¸€ä¸ªæ¢è¡Œç¬¦ï¼Œåè·Ÿé›¶æˆ–å¤šä¸ªç©ºç™½å­—ç¬¦ï¼Œå†è·Ÿä¸€ä¸ªæ¢è¡Œç¬¦
    potential_splits = re.split(r'(\n\s*\n)', code_text) # ä¿ç•™åˆ†éš”ç¬¦ä»¥ä¾¿åç»­å¤„ç†
    
    sub_chunks = []
    current_chunk_lines = []
    current_chunk_char_count = 0

    # ç¬¬ä¸€ä¸ªå—æ€»æ˜¯ä»å¤´å¼€å§‹
    if potential_splits:
        first_part = potential_splits.pop(0).strip()
        if first_part:
            current_chunk_lines.append(first_part)
            current_chunk_char_count += len(first_part)

    while potential_splits:
        delimiter = potential_splits.pop(0) # è¿™æ˜¯åˆ†éš”ç¬¦ \n\s*\n
        if not potential_splits: # æ²¡æœ‰æ›´å¤šå†…å®¹äº†
            if delimiter.strip(): # å¦‚æœåˆ†éš”ç¬¦æœ¬èº«ä¸æ˜¯çº¯ç©ºç™½ï¼Œä¹Ÿç®—å†…å®¹
                 current_chunk_lines.append(delimiter.rstrip()) # ä¿ç•™æœ«å°¾çš„æ¢è¡Œ
                 current_chunk_char_count += len(delimiter.rstrip())
            break 
        
        next_part = potential_splits.pop(0).strip()
        if not next_part: # å¦‚æœä¸‹ä¸€ä¸ªéƒ¨åˆ†æ˜¯ç©ºçš„ï¼Œåªå¤„ç†åˆ†éš”ç¬¦
            if delimiter.strip():
                current_chunk_lines.append(delimiter.rstrip())
                current_chunk_char_count += len(delimiter.rstrip())
            continue

        # æ£€æŸ¥åŠ å…¥ delimiter å’Œ next_part æ˜¯å¦ä¼šè¶…é•¿
        # å¯¹äºä»£ç ï¼Œæˆ‘ä»¬é€šå¸¸å¸Œæœ›åœ¨é€»è¾‘æ–­ç‚¹ï¼ˆç©ºè¡Œï¼‰å¤„åˆ†å‰²ï¼Œå³ä½¿å—è¾ƒå°
        # ä½†å¦‚æœå•ä¸ªç”±ç©ºè¡Œåˆ†éš”çš„å—æœ¬èº«å°±è¶…è¿‡ max_chunk_sizeï¼Œåˆ™éœ€è¦è¿›ä¸€æ­¥å¤„ç†ï¼ˆç›®å‰ç®€å•æˆªæ–­æˆ–æ¥å—ï¼‰
        
        # ç®€åŒ–é€»è¾‘ï¼šå¦‚æœå½“å‰å—éç©ºï¼Œå¹¶ä¸”åŠ å…¥ä¸‹ä¸€ä¸ªéƒ¨åˆ†ï¼ˆåŒ…æ‹¬åˆ†éš”çš„ç©ºè¡Œï¼‰ä¼šè¶…è¿‡ç›®æ ‡å¤§å°ï¼Œ
        # æˆ–è€…ä¸¥æ ¼è¶…è¿‡æœ€å¤§å¤§å°ï¼Œåˆ™ç»“æŸå½“å‰å—ã€‚
        # è¿™é‡Œçš„åˆ†éš”ç¬¦ï¼ˆç©ºè¡Œï¼‰æœ¬èº«ä¹Ÿåº”è¯¥è¢«è§†ä¸ºå—çš„ä¸€éƒ¨åˆ†ï¼Œæˆ–è€…ä½œä¸ºå—çš„è‡ªç„¶ç»“æŸã€‚

        # æ›´ç®€å•çš„ç­–ç•¥ï¼šæ¯ä¸ªç”± re.split åˆ†å‰²å‡ºæ¥çš„éç©ºéƒ¨åˆ†ï¼ˆå³ä»£ç æ®µï¼‰è‡ªæˆä¸€å—
        # å¦‚æœä»£ç æ®µæœ¬èº«è¿‡é•¿ï¼Œåˆ™æ¥å—å®ƒï¼Œæˆ–è€…æœªæ¥å†ç»†åˆ†
        if current_chunk_lines: # å¦‚æœå½“å‰å—æœ‰å†…å®¹
            # æ£€æŸ¥å¦‚æœåŠ ä¸Š next_part æ˜¯å¦ä¼šè¶…é•¿ï¼ˆè¿™é‡Œå¯ä»¥ç®€åŒ–ï¼Œå› ä¸ºç©ºè¡Œåˆ†å‰²é€šå¸¸æ„å‘³ç€é€»è¾‘å•å…ƒï¼‰
            # æˆ‘ä»¬å…ˆå‡è®¾æ¯ä¸ªç”±ç©ºè¡Œåˆ†å‰²çš„å—éƒ½æ˜¯ä¸€ä¸ªç‹¬ç«‹çš„å•å…ƒ
            sub_chunks.append("\n".join(current_chunk_lines))
            if context: context.log.debug(f"  Code sub-chunk created (blank line split), len: {current_chunk_char_count}")
            current_chunk_lines = []
            current_chunk_char_count = 0
        
        if next_part: # å¼€å§‹æ–°çš„å—
            current_chunk_lines.append(next_part)
            current_chunk_char_count += len(next_part)

    # æ·»åŠ æœ€åä¸€ä¸ªæ­£åœ¨æ„å»ºçš„å­å—
    if current_chunk_lines:
        sub_chunks.append("\n".join(current_chunk_lines))
        if context: context.log.debug(f"  Code sub-chunk created (blank line split, last), len: {current_chunk_char_count}")

    if not sub_chunks and code_text: # å¦‚æœå®Œå…¨æ²¡åˆ†å‰²å‡ºä»»ä½•ä¸œè¥¿ï¼ˆä¾‹å¦‚ä»£ç æ²¡æœ‰ç©ºè¡Œï¼‰
        if context: context.log.warning("Code block splitting by blank lines resulted in no sub-chunks. Returning original code block.")
        # å¯¹äºè¿™ç§æƒ…å†µï¼Œæˆ‘ä»¬å¯èƒ½éœ€è¦ä¸€ä¸ªå­—ç¬¦åˆ†å‰²å™¨ä½œä¸ºæœ€ç»ˆå›é€€
        # ä½†ä¸ºäº†ç®€å•èµ·è§ï¼Œæˆ‘ä»¬å…ˆè¿”å›åŸå§‹ä»£ç å—
        # å¦‚æœåŸå§‹ä»£ç å— > max_chunk_sizeï¼Œå®ƒä»ç„¶ä¼šæ˜¯ä¸€ä¸ªå¤§å—
        if len(code_text) > max_chunk_size:
            if context: context.log.warning(f"  Original code block (len: {len(code_text)}) exceeds max_chunk_size ({max_chunk_size}) and was not split by blank lines. Consider character splitting as fallback.")
            # è¿™é‡Œå¯ä»¥æ’å…¥ RecursiveCharacterTextSplitter é€»è¾‘
            # from langchain_text_splitters import RecursiveCharacterTextSplitter
            # char_splitter = RecursiveCharacterTextSplitter(chunk_size=max_chunk_size, chunk_overlap=0, separators=["\n", " ", ""])
            # sub_chunks = char_splitter.split_text(code_text)
            # if context: context.log.info(f"    Fallback: Code block character split into {len(sub_chunks)} parts.")
            # return sub_chunks
        return [code_text] # æš‚æ—¶è¿”å›åŸå—

    # è¿‡æ»¤æ‰å®Œå…¨æ˜¯ç©ºå­—ç¬¦ä¸²çš„å— (re.split å¯èƒ½äº§ç”Ÿ)
    final_sub_chunks = [chunk for chunk in sub_chunks if chunk.strip()]
    return final_sub_chunks if final_sub_chunks else [code_text]


def _get_element_text(element: Any, context: dg.AssetExecutionContext) -> Optional[str]:
    """
    Extracts the text content from a DocumentElement, handling various types.
    """
    # æ£€æŸ¥ element æ˜¯å¦æœ‰ text å±æ€§
    if hasattr(element, 'text') and isinstance(element.text, str) and element.text.strip():
        return element.text.strip()
    
    # å¯¹TableElementï¼Œå®ƒçš„markdownè¡¨ç¤ºæ›´æœ‰ç”¨
    if isinstance(element, TableElement) and hasattr(element, 'markdown_representation'):
        return element.markdown_representation
        
    # å¯¹CodeBlockElementï¼Œå®ƒçš„codeå±æ€§æ˜¯å†…å®¹
    if isinstance(element, CodeBlockElement) and hasattr(element, 'code'):
        return element.code

    # æœ€åçš„é˜²çº¿ï¼šå°è¯•å°†å…ƒç´ è½¬ä¸ºå­—ç¬¦ä¸²ï¼Œä½†è¿™é€šå¸¸è¡¨ç¤ºæœ‰æœªå¤„ç†çš„ç±»å‹
    # context.log.warning(f"Element of type {type(element).__name__} has no direct text attribute. Falling back to str().")
    # return str(element)
    return None # å¦‚æœæ²¡æœ‰æ˜ç¡®çš„æ–‡æœ¬å†…å®¹ï¼Œåˆ™è¿”å›Noneï¼Œé¿å…æ³¨å…¥æè¿°æ€§æ–‡å­—



@dg.asset(
    name="text_chunks",
    description="Cleans/chunks documents. Splits long elements, merges short ones, enriches with contextual metadata.",
    group_name="processing",
    deps=["parsed_documents"]
)
def clean_chunk_text_asset(
    context: dg.AssetExecutionContext,
    config: TextChunkerConfig,
    parsed_documents: List[ParsedDocumentOutput]
) -> List[ChunkOutput]:
    all_chunks: List[ChunkOutput] = []

    for doc_idx, parsed_doc in enumerate(parsed_documents):
        # --- START: æ ¸å¿ƒä¿®æ­£ - ç»Ÿä¸€æå–å¹¶ä¼ é€’æ–‡æ¡£çº§å…ƒæ•°æ® (é€»è¾‘ä¿æŒä¸å˜) ---
        doc_meta = parsed_doc.original_metadata
        doc_filename = doc_meta.get("filename") or doc_meta.get("file_name") or f"doc_{doc_idx}"
        doc_creation_date = doc_meta.get("creation_date") or doc_meta.get("creation_datetime")
        doc_last_modified = doc_meta.get("last_modified") or doc_meta.get("last_modified_datetime")
        doc_author = doc_meta.get("author") or doc_meta.get("authors")

        document_level_metadata = {
            "filename": doc_filename,
            "creation_date": doc_creation_date,
            "last_modified": doc_last_modified,
            "author": doc_author,
        }
        document_level_metadata = {k: v for k, v in document_level_metadata.items() if v is not None}
        
        context.log.info(f"Processing document for chunking: {doc_filename}")
        # --- END: æ ¸å¿ƒä¿®æ­£ ---

        # --- ã€ã€ã€æ–°å¢çš„æ ¸å¿ƒä¿®æ”¹ã€‘ã€‘ã€‘æ„å»ºå…ƒæ•°æ®å‰ç¼€ ---
        # å°†å…³é”®å…ƒæ•°æ®ï¼ˆå¦‚æ–‡ä»¶åï¼‰æ ¼å¼åŒ–ä¸ºä¸€ä¸ªå­—ç¬¦ä¸²ï¼Œå°†é™„åŠ åˆ°æ¯ä¸ªå—çš„æ–‡æœ¬å†…å®¹ä¹‹å‰ã€‚
        # è¿™ä½¿å¾—BM25ç­‰å…³é”®è¯æ£€ç´¢å™¨ä¹Ÿèƒ½â€œçœ‹åˆ°â€è¿™äº›å…ƒæ•°æ®ã€‚
        metadata_prefix_for_text = f"[Source Document: {doc_filename}] "
        # è¿˜å¯ä»¥æ·»åŠ å…¶ä»–ä½ è®¤ä¸ºå¯¹æ£€ç´¢é‡è¦çš„å…ƒæ•°æ®
        if doc_author:
            metadata_prefix_for_text += f"[Author: {doc_author}] "
        # --- ã€ã€ã€æ–°å¢ç»“æŸã€‘ã€‘ã€‘ ---

        current_title_hierarchy: Dict[int, str] = {}
        doc_internal_chunk_counter = 0

        for element_idx, element in enumerate(parsed_doc.elements):
            parent_id = str(uuid.uuid4())
            
            base_chunk_meta = document_level_metadata.copy()
            
            element_type_str = getattr(element, 'element_type', type(element).__name__)
            base_chunk_meta.update({
                "parent_id": parent_id,
                "paragraph_type": element_type_str,
                "source_element_index": element_idx,
            })

            if isinstance(element, TitleElement):
                 title_level = getattr(element, 'level', 1)
                 keys_to_remove = [lvl for lvl in current_title_hierarchy if lvl >= title_level]
                 for key in keys_to_remove:
                     del current_title_hierarchy[key]
                 current_title_hierarchy[title_level] = getattr(element, 'text', '').strip()
            
            for level, title in current_title_hierarchy.items():
                base_chunk_meta[f"title_hierarchy_{level}"] = title
            
            if hasattr(element, 'metadata') and element.metadata:
                page_num = getattr(element.metadata, 'page_number', None)
                if page_num is not None:
                    base_chunk_meta['page_number'] = page_num + 1

            sub_chunks: List[Dict[str, Any]] = []
            
            text_content = _get_element_text(element, context)
            
            if not text_content:
                context.log.debug(f"Skipping element {element_idx} in {doc_filename} due to empty content.")
                continue

            # --- ã€ã€ã€ä¿®æ­£çš„åˆ¤æ–­é€»è¾‘ã€‘ã€‘ã€‘ ---
            if element_type_str == "TableElement":
                 base_chunk_meta["paragraph_type"] = "table"
                 sub_chunks = split_markdown_table_by_rows(text_content, config.target_sentence_split_chunk_size, config.max_merged_chunk_size, context)
            else:
                if len(text_content) > config.max_element_text_length_before_split:
                    sentences = split_text_into_sentences(text_content)
                    for sent in sentences:
                        if sent.strip():
                            sub_chunks.append({"text": sent.strip()})
                else:
                    sub_chunks.append({"text": text_content})

            for sub_chunk_data in sub_chunks:
                doc_internal_chunk_counter += 1
                chunk_meta_final = base_chunk_meta.copy()
                chunk_meta_final["chunk_number_in_doc"] = doc_internal_chunk_counter
                
                if "start_row_index" in sub_chunk_data:
                    chunk_meta_final["table_original_start_row"] = sub_chunk_data["start_row_index"]
                    chunk_meta_final["table_original_end_row"] = sub_chunk_data["end_row_index"]

                # --- ã€ã€ã€æ–°å¢çš„æ ¸å¿ƒä¿®æ”¹ã€‘ã€‘ã€‘å°†å…ƒæ•°æ®å‰ç¼€å’Œå—æ–‡æœ¬å†…å®¹ç»“åˆ ---
                final_chunk_text = metadata_prefix_for_text + sub_chunk_data["text"]

                all_chunks.append(ChunkOutput(
                    chunk_text=final_chunk_text, # <--- ä½¿ç”¨ç»“åˆåçš„æ–‡æœ¬
                    source_document_id=doc_filename,
                    chunk_metadata=chunk_meta_final
                ))

    context.log.info(f"Chunking process finished. Total chunks generated: {len(all_chunks)}")
    if all_chunks:
        # å¼ºåˆ¶æ‰“å°æœ€åä¸€ä¸ªå—çš„å…ƒæ•°æ®å’Œæ–‡æœ¬ï¼Œçœ‹filenameæ˜¯å¦å­˜åœ¨
        context.log.info(f"Sample final chunk TEXT: {all_chunks[-1].chunk_text[:300]}...")
        context.log.info(f"Sample final chunk METADATA: {all_chunks[-1].chunk_metadata}")
    
    context.add_output_metadata(metadata={"total_chunks_generated": len(all_chunks)})
    return all_chunks

@dg.asset(
    name="text_embeddings",
    description="Generates vector embeddings for text chunks.",
    group_name="processing",
    deps=["text_chunks"]
)
def generate_embeddings_asset( # <--- ä¿æŒåŒæ­¥ï¼Œå› ä¸º GGUFEmbeddingResource.encode æ˜¯åŒæ­¥åŒ…è£…
    context: dg.AssetExecutionContext,
    text_chunks: List[ChunkOutput],
    embedder: GGUFEmbeddingResource
) -> List[EmbeddingOutput]:
    # +++ æ–°å¢æ‰“å°è¯­å¥ +++
    context.log.info(f"generate_embeddings_asset: Received {len(text_chunks)} text_chunks.")
    if text_chunks:
        context.log.info(f"generate_embeddings_asset: First chunk text (first 100 chars): '{text_chunks[0].chunk_text[:100]}'")
        context.log.info(f"generate_embeddings_asset: First chunk metadata: {text_chunks[0].chunk_metadata}")
    # +++ ç»“æŸæ–°å¢æ‰“å°è¯­å¥ +++

    all_embeddings: List[EmbeddingOutput] = []
    if not text_chunks:
        context.log.warning("generate_embeddings_asset: No text chunks received, returning empty list.") # æ·»åŠ ä¸€ä¸ªæ˜ç¡®çš„è­¦å‘Š
        return all_embeddings
    
    # --- ç¡®ä¿ chunk_texts_to_encode ä¸ä¸ºç©ºæ‰è°ƒç”¨ embedder.encode ---
    chunk_texts_to_encode = [chunk.chunk_text for chunk in text_chunks if chunk.chunk_text and chunk.chunk_text.strip()]
    
    if not chunk_texts_to_encode:
        context.log.warning("generate_embeddings_asset: All received text chunks are empty or whitespace after filtering. Returning empty list.")
        # å³ä½¿åŸå§‹ text_chunks éç©ºï¼Œä½†å¦‚æœæ‰€æœ‰ chunk_text éƒ½æ— æ•ˆï¼Œä¹Ÿåº”è¯¥è¿”å›ç©º embedding åˆ—è¡¨
        # å¹¶ä¸”è¦ç¡®ä¿ä¸‹æ¸¸çŸ¥é“æœŸæœ›çš„ EmbeddingOutput æ•°é‡å¯èƒ½æ˜¯0
        return all_embeddings # è¿”å›ç©ºåˆ—è¡¨æ˜¯æ­£ç¡®çš„

    vectors = embedder.encode(chunk_texts_to_encode)

    # --- ç¡®ä¿æ­£ç¡®åœ°å°†åµŒå…¥ç»“æœæ˜ å°„å›åŸå§‹çš„ text_chunks åˆ—è¡¨ï¼ˆå¦‚æœæ•°é‡å¯èƒ½ä¸ä¸€è‡´ï¼‰---
    # å½“å‰çš„é€»è¾‘æ˜¯å‡è®¾ vectors å’Œ chunk_texts_to_encode ä¸€ä¸€å¯¹åº”ï¼Œå¹¶ä¸” text_chunks çš„é¡ºåºä¸ chunk_texts_to_encode è¿‡æ»¤å‰çš„é¡ºåºç›¸å…³
    # å¦‚æœ chunk_texts_to_encode è¿›è¡Œäº†è¿‡æ»¤ï¼Œè¿™é‡Œçš„å¾ªç¯éœ€è¦æ›´å°å¿ƒ
    
    # ä¸€ä¸ªæ›´å®‰å…¨çš„æ˜ å°„æ–¹å¼æ˜¯ï¼Œåªä¸ºé‚£äº›å®é™…è¢«ç¼–ç çš„æ–‡æœ¬å—åˆ›å»º EmbeddingOutput
    # ä½†è¿™è¦æ±‚ä¸‹æ¸¸èƒ½å¤„ç† EmbeddingOutput åˆ—è¡¨é•¿åº¦å¯èƒ½å°äº ChunkOutput åˆ—è¡¨é•¿åº¦çš„æƒ…å†µï¼Œ
    # æˆ–è€…ï¼Œæˆ‘ä»¬åº”è¯¥ä¸ºé‚£äº›è¢«è¿‡æ»¤æ‰çš„ chunk ä¹Ÿåˆ›å»ºä¸€ä¸ªå¸¦æœ‰é›¶å‘é‡çš„ EmbeddingOutputã€‚
    # æˆ‘ä»¬ä¹‹å‰çš„ LocalModelHandler ä¿®æ”¹æ˜¯ä¸ºäº†å¤„ç†å•ä¸ªç©ºæ–‡æœ¬ï¼Œç°åœ¨è¿™é‡Œæ˜¯èµ„äº§å±‚é¢çš„ã€‚

    # ä¿æŒä¸ LocalModelHandler ç±»ä¼¼çš„å¥å£®æ€§ï¼šä¸ºæ‰€æœ‰ä¼ å…¥çš„ text_chunks ç”Ÿæˆ EmbeddingOutputï¼Œ
    # å¦‚æœå…¶æ–‡æœ¬ä¸ºç©ºæˆ–åµŒå…¥å¤±è´¥ï¼Œåˆ™ä½¿ç”¨é›¶å‘é‡ã€‚

    embedding_map = {text: vec for text, vec in zip(chunk_texts_to_encode, vectors)}

    for i, chunk_input in enumerate(text_chunks):
        model_name_for_log = os.getenv("EMBEDDING_MODEL_PATH", "API_Based_Embedder")
        embedding_vector_for_chunk = [0.0] * embedder.get_embedding_dimension() # é»˜è®¤ä¸ºé›¶å‘é‡

        if chunk_input.chunk_text and chunk_input.chunk_text.strip() and chunk_input.chunk_text in embedding_map:
            embedding_vector_for_chunk = embedding_map[chunk_input.chunk_text]
        elif chunk_input.chunk_text and chunk_input.chunk_text.strip(): 
            # æ–‡æœ¬æœ‰æ•ˆä½†æ²¡æœ‰åœ¨ embedding_map ä¸­æ‰¾åˆ° (å¯èƒ½å› ä¸º embedder.encode å†…éƒ¨çš„æŸäº›é—®é¢˜)
            context.log.warning(f"generate_embeddings_asset: Valid chunk text for chunk_id {chunk_input.chunk_id} was not found in embedding_map. Using zero vector.")
        else: # æ–‡æœ¬æœ¬èº«å°±æ˜¯ç©ºçš„
            context.log.info(f"generate_embeddings_asset: Chunk_id {chunk_input.chunk_id} has empty text. Using zero vector.")


        all_embeddings.append(EmbeddingOutput(
            chunk_id=chunk_input.chunk_id,
            chunk_text=chunk_input.chunk_text, # å­˜å‚¨åŸå§‹æ–‡æœ¬ï¼Œå³ä½¿å®ƒæ˜¯ç©ºçš„
            embedding_vector=embedding_vector_for_chunk,
            embedding_model_name=model_name_for_log,
            original_chunk_metadata=chunk_input.chunk_metadata
        ))
    
    context.add_output_metadata(metadata={"total_embeddings_generated": len(all_embeddings)})
    return all_embeddings


@dg.asset(
    name="vector_store_embeddings",
    description="Stores text embeddings into a ChromaDB vector store.",
    group_name="indexing",
    deps=["text_embeddings"]
)
def vector_storage_asset(
    context: dg.AssetExecutionContext,
    text_embeddings: List[EmbeddingOutput],
    chroma_db: ChromaDBResource
) -> None:
    if not text_embeddings:
        context.log.warning("vector_storage_asset: No embeddings received, nothing to store in ChromaDB.")
        context.add_output_metadata(metadata={"num_embeddings_stored": 0})
        return

    # --- START: æ ¸å¿ƒä¿®å¤ ---
    # ç­›é€‰å‡ºé‚£äº›æ‹¥æœ‰æœ‰æ•ˆï¼ˆéç©ºï¼‰åµŒå…¥å‘é‡çš„æ¡ç›®
    valid_embeddings_to_store: List[EmbeddingOutput] = []
    for emb in text_embeddings:
        if emb.embedding_vector and len(emb.embedding_vector) > 0:
            valid_embeddings_to_store.append(emb)
        else:
            context.log.warning(f"Skipping storage for chunk_id {emb.chunk_id} due to empty embedding vector.")
    
    if not valid_embeddings_to_store:
        context.log.warning("vector_storage_asset: No valid embeddings found after filtering. Nothing to store.")
        context.add_output_metadata(metadata={"num_embeddings_stored": 0, "num_invalid_embeddings_skipped": len(text_embeddings)})
        return
        
    ids_to_store = [emb.chunk_id for emb in valid_embeddings_to_store]
    embeddings_to_store = [emb.embedding_vector for emb in valid_embeddings_to_store]
    documents_to_store = [emb.chunk_text for emb in valid_embeddings_to_store]
    cleaned_metadatas: List[Dict[str, Any]] = []

    for i, emb_output in enumerate(valid_embeddings_to_store):
    # --- END: æ ¸å¿ƒä¿®å¤ ---
        original_meta = emb_output.original_chunk_metadata if isinstance(emb_output.original_chunk_metadata, dict) else {}
        meta = original_meta.copy()
        
        meta["chunk_text_in_meta"] = str(emb_output.chunk_text) if emb_output.chunk_text is not None else "[TEXT IS NULL]"

        cleaned_meta_item: Dict[str, Any] = {}
        for key, value in meta.items():
            if isinstance(value, dict):
                if key == "title_hierarchy" and not value: 
                    cleaned_meta_item[key] = "None"
                    context.log.debug(f"Metadata for chunk {emb_output.chunk_id}: Replaced empty title_hierarchy dict with 'None' string.")
                else:
                    try:
                        cleaned_meta_item[key] = json.dumps(value, ensure_ascii=False)
                    except TypeError:
                        cleaned_meta_item[key] = str(value)
                        context.log.warning(f"Metadata for chunk {emb_output.chunk_id}: Could not JSON serialize dict for key '{key}', used str(). Value: {str(value)[:100]}...")
            elif isinstance(value, list):
                try:
                    cleaned_meta_item[key] = json.dumps(value, ensure_ascii=False)
                except TypeError:
                    cleaned_meta_item[key] = str(value)
                    context.log.warning(f"Metadata for chunk {emb_output.chunk_id}: Could not JSON serialize list for key '{key}', used str(). Value: {str(value)[:100]}...")
            elif value is None:
                cleaned_meta_item[key] = "" 
            else: 
                cleaned_meta_item[key] = value
        cleaned_metadatas.append(cleaned_meta_item)

    # +++ æ–°å¢æ—¥å¿— +++
    if embeddings_to_store:
        context.log.info(f"vector_storage_asset: Sample embedding vector to be stored (first item, first 10 elements): {str(embeddings_to_store[0][:10]) if embeddings_to_store[0] else 'None'}")
        context.log.info(f"vector_storage_asset: Length of first embedding vector to be stored: {len(embeddings_to_store[0]) if embeddings_to_store[0] else 'N/A'}")
        is_first_all_zeros = all(v == 0.0 for v in embeddings_to_store[0]) if embeddings_to_store[0] else "N/A"
        context.log.info(f"vector_storage_asset: Is first sample embedding all zeros: {is_first_all_zeros}")
    # +++ ç»“æŸæ–°å¢æ—¥å¿— +++

    context.log.info(f"vector_storage_asset: Preparing to add/update {len(ids_to_store)} items to ChromaDB collection '{chroma_db.collection_name}'.")
    if ids_to_store:
        context.log.info(f"vector_storage_asset: Sample ID to store: {ids_to_store[0]}")
        # ç¡®ä¿ documents_to_store ä¹Ÿæœ‰å¯¹åº”å†…å®¹ï¼Œå¹¶ä¸”ä¸æ˜¯ None
        sample_doc_text = "[EMPTY DOCUMENT]"
        if documents_to_store and documents_to_store[0] is not None:
            sample_doc_text = str(documents_to_store[0])[:100] # æ˜¾ç¤ºå‰100å­—ç¬¦
        elif documents_to_store and documents_to_store[0] is None:
            sample_doc_text = "[DOCUMENT IS NULL]"
        context.log.info(f"vector_storage_asset: Sample document to store (from documents_to_store, first 100 chars): '{sample_doc_text}'")
        
        sample_meta_text = "[NO METADATA]"
        if cleaned_metadatas:
            sample_meta_text = str(cleaned_metadatas[0])[:200] # æ˜¾ç¤ºå…ƒæ•°æ®æ‘˜è¦
        context.log.info(f"vector_storage_asset: Sample cleaned metadata for first item: {sample_meta_text}")

    try:
        chroma_db.add_embeddings(
            ids=ids_to_store, 
            embeddings=embeddings_to_store, 
            documents=documents_to_store, # ä¼ é€’çœŸå®çš„æ–‡æœ¬å†…å®¹ç»™ChromaDBçš„documentså­—æ®µ
            metadatas=cleaned_metadatas
        )
        # å°è¯•è·å–å¹¶è®°å½•æ“ä½œåçš„é›†åˆè®¡æ•°
        # æ³¨æ„: chroma_db._collection å¯èƒ½æ˜¯ç§æœ‰å±æ€§ï¼Œç›´æ¥è®¿é—®ä¸æ¨èï¼Œä½†ä¸ºäº†è°ƒè¯•å¯ä»¥å°è¯•
        # æ›´å¥½çš„æ–¹å¼æ˜¯ ChromaDBResource æä¾›ä¸€ä¸ª get_collection_count() æ–¹æ³•
        collection_count_after_add = -1 # é»˜è®¤å€¼
        try:
            if chroma_db._collection: # ç¡®ä¿ _collection ä¸æ˜¯ None
                 collection_count_after_add = chroma_db._collection.count()
        except Exception as e_count:
            context.log.warning(f"vector_storage_asset: Could not get collection count after add: {e_count}")

        context.add_output_metadata(metadata={"num_embeddings_stored": len(ids_to_store), "collection_count_after_add": collection_count_after_add})
        context.log.info(f"vector_storage_asset: Successfully called add_embeddings. Stored {len(ids_to_store)} items. Collection count now: {collection_count_after_add}")
    except Exception as e_chroma_add:
        context.log.error(f"vector_storage_asset: Failed to add embeddings to ChromaDB: {e_chroma_add}", exc_info=True)
        raise

class BM25IndexConfig(dg.Config):
    index_file_path: str = "/home/zhz/zhz_agent/zhz_rag/stored_data/bm25_index/"


@dg.asset(
    name="keyword_index",
    description="Builds and persists a BM25 keyword index from text chunks.",
    group_name="indexing",
    deps=["text_chunks"]
)
def keyword_index_asset(
    context: dg.AssetExecutionContext,
    config: BM25IndexConfig, # ç¡®ä¿ BM25IndexConfig åœ¨æ–‡ä»¶æŸå¤„å·²å®šä¹‰
    text_chunks: List[ChunkOutput]
) -> None:
    if not text_chunks:
        context.log.warning("keyword_index_asset: No text chunks received, skipping BM25 index building.")
        context.add_output_metadata(metadata={"num_documents_indexed": 0, "index_directory_path": config.index_file_path})
        return

    # --- æ–°å¢ï¼šæ£€æŸ¥å¹¶è®°å½•ç©ºæ–‡æœ¬å— ---
    valid_chunks_for_indexing: List[ChunkOutput] = []
    for idx, chunk in enumerate(text_chunks):
        if chunk.chunk_text and chunk.chunk_text.strip():
            valid_chunks_for_indexing.append(chunk)
        else:
            context.log.warning(f"keyword_index_asset: Chunk {idx} (ID: {chunk.chunk_id}) has empty or whitespace-only text. Skipping for BM25 indexing.")
    
    if not valid_chunks_for_indexing:
        context.log.warning("keyword_index_asset: All received text chunks have empty or whitespace-only text after filtering. Skipping BM25 index building.")
        context.add_output_metadata(metadata={"num_documents_indexed": 0, "index_directory_path": config.index_file_path})
        return
    # --- ç»“æŸæ–°å¢ ---

    # ä½¿ç”¨è¿‡æ»¤åçš„æœ‰æ•ˆå—
    corpus_texts = [chunk.chunk_text for chunk in valid_chunks_for_indexing]
    document_ids = [chunk.chunk_id for chunk in valid_chunks_for_indexing] # ç¡®ä¿IDä¸æœ‰æ•ˆæ–‡æœ¬å¯¹åº”

    context.log.info(f"keyword_index_asset: Preparing to index {len(corpus_texts)} valid text chunks for BM25.")
    if corpus_texts: # ä»…åœ¨æœ‰æ•°æ®æ—¶æ‰“å°æ ·æœ¬
        context.log.info(f"keyword_index_asset: Sample document ID for BM25: {document_ids[0]}")
        context.log.info(f"keyword_index_asset: Sample document text for BM25 (first 50 chars): '{str(corpus_texts[0])[:50]}'")

    try:
        corpus_tokenized_jieba = [list(jieba.cut_for_search(text)) for text in corpus_texts]
        context.log.info(f"keyword_index_asset: Tokenized {len(corpus_tokenized_jieba)} texts for BM25.")
        
        bm25_model = bm25s.BM25() # ä½¿ç”¨é»˜è®¤å‚æ•°åˆå§‹åŒ–
        context.log.info("keyword_index_asset: BM25 model initialized.")
        
        bm25_model.index(corpus_tokenized_jieba)
        indexed_doc_count = len(bm25_model.doc_freqs) if hasattr(bm25_model, 'doc_freqs') and bm25_model.doc_freqs is not None else len(corpus_tokenized_jieba)
        context.log.info(f"keyword_index_asset: BM25 model indexing complete for {indexed_doc_count} documents.")
        
        index_directory = config.index_file_path
        context.log.info(f"keyword_index_asset: BM25 index will be saved to directory: {index_directory}")
        os.makedirs(index_directory, exist_ok=True)
        
        bm25_model.save(index_directory) 
        context.log.info(f"keyword_index_asset: bm25_model.save('{index_directory}') called.")
        
        doc_ids_path = os.path.join(index_directory, "doc_ids.pkl")
        with open(doc_ids_path, 'wb') as f_out:
            pickle.dump(document_ids, f_out)
        context.log.info(f"keyword_index_asset: doc_ids.pkl saved to {doc_ids_path} with {len(document_ids)} IDs.")
        
        # éªŒè¯æ–‡ä»¶æ˜¯å¦çœŸçš„åˆ›å»ºäº†
        expected_params_file = os.path.join(index_directory, "params.index.json") # bm25s ä¿å­˜æ—¶ä¼šåˆ›å»ºè¿™ä¸ª
        if os.path.exists(expected_params_file) and os.path.exists(doc_ids_path):
            context.log.info(f"keyword_index_asset: Verified that BM25 index files (e.g., params.index.json, doc_ids.pkl) exist in {index_directory}.")
        else:
            context.log.error(f"keyword_index_asset: BM25 index files (e.g., params.index.json or doc_ids.pkl) NOT FOUND in {index_directory} after save operations!")
            context.log.error(f"keyword_index_asset: Check - params.index.json exists: {os.path.exists(expected_params_file)}")
            context.log.error(f"keyword_index_asset: Check - doc_ids.pkl exists: {os.path.exists(doc_ids_path)}")
            # å¦‚æœæ–‡ä»¶æœªæ‰¾åˆ°ï¼Œå¯èƒ½éœ€è¦æŠ›å‡ºå¼‚å¸¸ä»¥ä½¿èµ„äº§å¤±è´¥
            # raise FileNotFoundError(f"BM25 index files not found in {index_directory} after save.")

        context.add_output_metadata(
            metadata={
                "num_documents_indexed": len(corpus_texts), 
                "index_directory_path": index_directory,
                "bm25_corpus_size_actual": indexed_doc_count
            }
        )
        context.log.info("keyword_index_asset: BM25 indexing and saving completed successfully.")
    except Exception as e_bm25:
        context.log.error(f"keyword_index_asset: Error during BM25 indexing or saving: {e_bm25}", exc_info=True)
        raise

# --- KG Extraction ç›¸å…³çš„é…ç½®å’Œèµ„äº§ ---


# class KGExtractionConfig(dg.Config):
#     extraction_prompt_template: str = KG_EXTRACTION_SINGLE_CHUNK_PROMPT_TEMPLATE_V1
#     local_llm_model_name: str = "Qwen3-1.7B-GGUF_via_llama.cpp"

# DEFAULT_KG_EXTRACTION_SCHEMA = {
#     "type": "object",
#     "properties": {
#         "entities": {
#             "type": "array",
#             "items": {
#                 "type": "object",
#                 "properties": {
#                     "text": {"type": "string", "description": "æå–åˆ°çš„å®ä½“åŸæ–‡"},
#                     "label": {"type": "string", "description": "å®ä½“ç±»å‹ (ä¾‹å¦‚: PERSON, ORGANIZATION, TASK)"}
#                 },
#                 "required": ["text", "label"]
#             },
#             "description": "ä»æ–‡æœ¬ä¸­æå–å‡ºçš„å®ä½“åˆ—è¡¨ã€‚"
#         },
#         "relations": {
#             "type": "array",
#             "items": {
#                 "type": "object",
#                 "properties": {
#                     "head_entity_text": {"type": "string", "description": "å¤´å®ä½“çš„æ–‡æœ¬"},
#                     "head_entity_label": {"type": "string", "description": "å¤´å®ä½“çš„ç±»å‹ (ä¾‹å¦‚: PERSON, TASK)"},
#                     "relation_type": {"type": "string", "description": "å…³ç³»ç±»å‹ (ä¾‹å¦‚: WORKS_AT, ASSIGNED_TO)"},
#                     "tail_entity_text": {"type": "string", "description": "å°¾å®ä½“çš„æ–‡æœ¬"},
#                     "tail_entity_label": {"type": "string", "description": "å°¾å®ä½“çš„ç±»å‹ (ä¾‹å¦‚: ORGANIZATION, PERSON)"}
#                 },
#                 "required": ["head_entity_text", "head_entity_label", "relation_type", "tail_entity_text", "tail_entity_label"]
#             },
#             "description": "ä»æ–‡æœ¬ä¸­æå–å‡ºçš„å…³ç³»ä¸‰å…ƒç»„åˆ—è¡¨ã€‚"
#         }
#     },
#     "required": ["entities", "relations"]
# }


# @dg.asset(
#     name="kg_extractions",
#     description="Extracts entities and relations from text chunks for knowledge graph construction.",
#     group_name="kg_building",
#     io_manager_key="pydantic_json_io_manager",
#     deps=["text_chunks"]
# )
# async def kg_extraction_asset(
#     context: dg.AssetExecutionContext, # Pylance æç¤º dg.AssetExecutionContext æœªå®šä¹‰ "SystemResource"
#     text_chunks: List[ChunkOutput],
#     config: KGExtractionConfig,
#     LocalLLM_api: LocalLLMAPIResource,
#     system_info: SystemResource  # <--- æˆ‘ä»¬æ·»åŠ äº† system_info
# ) -> List[KGTripleSetOutput]:
#     all_kg_outputs: List[KGTripleSetOutput] = []
#     if not text_chunks:
#         context.log.info("No text chunks received for KG extraction, skipping.")
#         return all_kg_outputs

#     total_input_chunks = len(text_chunks)
#     total_entities_extracted_overall = 0
#     total_relations_extracted_overall = 0
#     successfully_processed_chunks_count = 0
    
#     # å¹¶å‘æ§åˆ¶å‚æ•°
#     recommended_concurrency = system_info.get_recommended_concurrent_tasks(task_type="kg_extraction_llm")
#     CONCURRENT_REQUESTS_LIMIT = max(1, recommended_concurrency) # ç›´æ¥ä½¿ç”¨HALæ¨èï¼Œä½†è‡³å°‘ä¸º1
#     context.log.info(f"HAL recommended concurrency for 'kg_extraction_llm': {recommended_concurrency}. Effective limit set to: {CONCURRENT_REQUESTS_LIMIT}")
#     semaphore = asyncio.Semaphore(CONCURRENT_REQUESTS_LIMIT)


#     async def extract_kg_for_chunk(chunk: ChunkOutput) -> Optional[KGTripleSetOutput]:
#         async with semaphore:
#             # ä½¿ç”¨å•ä¸ªchunkçš„promptæ¨¡æ¿
#             prompt = config.extraction_prompt_template.format(text_to_extract=chunk.chunk_text)
#             try:
#                 context.log.debug(f"Starting KG extraction for chunk_id: {chunk.chunk_id}, Text (start): {chunk.chunk_text[:100]}...")
#                 structured_response = await LocalLLM_api.generate_structured_output(
#                     prompt=prompt, 
#                     json_schema=DEFAULT_KG_EXTRACTION_SCHEMA # ä½¿ç”¨å•ä¸ªå¯¹è±¡çš„schema
#                 )
                
#                 # ç¡®ä¿ structured_response æ˜¯å­—å…¸ç±»å‹
#                 if not isinstance(structured_response, dict):
#                     context.log.error(f"Failed KG extraction for chunk {chunk.chunk_id}: LLM response was not a dict. Got: {type(structured_response)}. Response: {str(structured_response)[:200]}")
#                     return None

#                 entities_data = structured_response.get("entities", [])
#                 extracted_entities_list = [
#                     ExtractedEntity(text=normalize_text_for_id(e.get("text","")), label=e.get("label","UNKNOWN").upper())
#                     for e in entities_data if isinstance(e, dict)
#                 ]
                
#                 relations_data = structured_response.get("relations", [])
#                 extracted_relations_list = [
#                     ExtractedRelation(
#                         head_entity_text=r.get('head_entity_text',""), 
#                         head_entity_label=r.get('head_entity_label',"UNKNOWN").upper(), 
#                         relation_type=r.get('relation_type',"UNKNOWN").upper(), 
#                         tail_entity_text=r.get('tail_entity_text',""), 
#                         tail_entity_label=r.get('tail_entity_label',"UNKNOWN").upper()
#                     ) 
#                     for r in relations_data if isinstance(r, dict) and 
#                                                r.get('head_entity_text') and r.get('head_entity_label') and
#                                                r.get('relation_type') and r.get('tail_entity_text') and
#                                                r.get('tail_entity_label')
#                 ]
                
#                 context.log.debug(f"Finished KG extraction for chunk_id: {chunk.chunk_id}. Entities: {len(extracted_entities_list)}, Relations: {len(extracted_relations_list)}")
#                 return KGTripleSetOutput(
#                     chunk_id=chunk.chunk_id,
#                     extracted_entities=extracted_entities_list,
#                     extracted_relations=extracted_relations_list,
#                     extraction_model_name=config.local_llm_model_name,
#                     original_chunk_metadata=chunk.chunk_metadata
#                 )
#             except Exception as e:
#                 context.log.error(f"Failed KG extraction for chunk {chunk.chunk_id}: {e}", exc_info=True)
#                 return None 

#     context.log.info(f"Starting KG extraction for {total_input_chunks} chunks with concurrency limit: {CONCURRENT_REQUESTS_LIMIT}.")
    
#     tasks = [extract_kg_for_chunk(chunk) for chunk in text_chunks]
    
#     results = await asyncio.gather(*tasks)
    
#     context.log.info(f"Finished all KG extraction tasks. Received {len(results)} results (including potential None for failures).")

#     for result_item in results:
#         if result_item and isinstance(result_item, KGTripleSetOutput):
#             all_kg_outputs.append(result_item)
#             total_entities_extracted_overall += len(result_item.extracted_entities)
#             total_relations_extracted_overall += len(result_item.extracted_relations)
#             successfully_processed_chunks_count +=1
#         elif result_item is None:
#             context.log.warning("A KG extraction task failed and returned None.")
            
#     context.log.info(f"KG extraction complete. Successfully processed {successfully_processed_chunks_count} out of {total_input_chunks} chunks.")
#     context.add_output_metadata(
#         metadata={
#             "total_chunks_input_to_kg": total_input_chunks, # æ¢å¤ä¸º total_input_chunks
#             "chunks_successfully_extracted_kg": successfully_processed_chunks_count,
#             "total_entities_extracted": total_entities_extracted_overall, 
#             "total_relations_extracted": total_relations_extracted_overall
#             # ç§»é™¤äº†æ‰¹å¤„ç†ç›¸å…³çš„å…ƒæ•°æ® "total_batches_processed", "batch_size_configured"
#         }
#     )
#     return all_kg_outputs


# # --- KuzuDB æ„å»ºèµ„äº§é“¾ ---

# @dg.asset(
#     name="duckdb_schema", # <--- ä¿®æ”¹èµ„äº§åç§°
#     description="Creates the base schema (node and relation tables) in DuckDB.",
#     group_name="kg_building",
#     # deps=[kg_extraction_asset] # ä¿æŒä¾èµ–ï¼Œç¡®ä¿åœ¨æå–ä¹‹ååˆ›å»ºschema (é€»è¾‘ä¸Š)
#                                  # è™½ç„¶schemaåˆ›å»ºæœ¬èº«ä¸ç›´æ¥ä½¿ç”¨æå–ç»“æœï¼Œä½†æµæ°´çº¿é¡ºåºä¸Šåˆç†
# )
# def duckdb_schema_asset(context: dg.AssetExecutionContext, duckdb_kg: DuckDBResource, embedder: GGUFEmbeddingResource): # <--- ä¿®æ”¹å‡½æ•°åå’Œèµ„æºå‚æ•°
#     context.log.info("--- Starting DuckDB Schema Creation Asset ---")
    
#     # è·å–åµŒå…¥ç»´åº¦ï¼Œä¸KuzuDBæ—¶ç±»ä¼¼
#     EMBEDDING_DIM = embedder.get_embedding_dimension()
#     if not EMBEDDING_DIM:
#         raise ValueError("Could not determine embedding dimension from GGUFEmbeddingResource.")

#     node_table_ddl = f"""
#     CREATE TABLE IF NOT EXISTS ExtractedEntity (
#         id_prop VARCHAR PRIMARY KEY,
#         text VARCHAR,
#         label VARCHAR,
#         embedding FLOAT[{EMBEDDING_DIM}]
#     );
#     """

#     relation_table_ddl = f"""
#     CREATE TABLE IF NOT EXISTS KGExtractionRelation (
#         relation_id VARCHAR PRIMARY KEY,
#         source_node_id_prop VARCHAR,
#         target_node_id_prop VARCHAR,
#         relation_type VARCHAR
#         -- Optional: FOREIGN KEY (source_node_id_prop) REFERENCES ExtractedEntity(id_prop),
#         -- Optional: FOREIGN KEY (target_node_id_prop) REFERENCES ExtractedEntity(id_prop)
#     );
#     """
#     # ä¹Ÿå¯ä»¥ä¸ºå…³ç³»è¡¨çš„ (source, target, type) åˆ›å»ºå¤åˆå”¯ä¸€ç´¢å¼•æˆ–æ™®é€šç´¢å¼•ä»¥åŠ é€ŸæŸ¥è¯¢
#     relation_index_ddl = """
#     CREATE INDEX IF NOT EXISTS idx_relation_source_target_type 
#     ON KGExtractionRelation (source_node_id_prop, target_node_id_prop, relation_type);
#     """
    
#     ddl_commands = [node_table_ddl, relation_table_ddl, relation_index_ddl]

#     try:
#         with duckdb_kg.get_connection() as conn:
#             context.log.info("Executing DuckDB DDL commands...")
#             for command_idx, command in enumerate(ddl_commands):
#                 context.log.debug(f"Executing DDL {command_idx+1}:\n{command.strip()}")
#                 conn.execute(command)
#             context.log.info("DuckDB Schema DDL commands executed successfully.")
#     except Exception as e_ddl:
#         context.log.error(f"Error during DuckDB schema creation: {e_ddl}", exc_info=True)
#         raise
#     context.log.info("--- DuckDB Schema Creation Asset Finished ---")


# @dg.asset(
#     name="duckdb_nodes", # <--- ä¿®æ”¹èµ„äº§åç§°
#     description="Loads all unique extracted entities as nodes into DuckDB.",
#     group_name="kg_building",
#     deps=[duckdb_schema_asset, kg_extraction_asset] # <--- ä¿®æ”¹ä¾èµ–
# )
# def duckdb_nodes_asset(
#     context: dg.AssetExecutionContext,
#     kg_extractions: List[KGTripleSetOutput], # æ¥è‡ª kg_extraction_asset çš„è¾“å‡º
#     duckdb_kg: DuckDBResource,               # <--- ä¿®æ”¹èµ„æºå‚æ•°
#     embedder: GGUFEmbeddingResource          # ä¿æŒå¯¹ embedder çš„ä¾èµ–ï¼Œç”¨äºç”ŸæˆåµŒå…¥
# ):
#         # --- START: ç§»åŠ¨å¹¶å¼ºåŒ–åˆå§‹æ—¥å¿— ---
#     print("<<<<< duckdb_nodes_asset FUNCTION ENTERED - PRINTING TO STDOUT >>>>>", flush=True) 
#     # å°è¯•ä½¿ç”¨ context.logï¼Œå¦‚æœå®ƒæ­¤æ—¶å¯ç”¨
#     try:
#         context.log.info("<<<<< duckdb_nodes_asset FUNCTION CALLED - VIA CONTEXT.LOG - VERY BEGINNING >>>>>")
#     except Exception as e_log_init:
#         print(f"Context.log not available at the very beginning of duckdb_nodes_asset: {e_log_init}", flush=True)
#     # --- END: ç§»åŠ¨å¹¶å¼ºåŒ–åˆå§‹æ—¥å¿— ---

#     context.log.info("--- Starting DuckDB Node Loading Asset (Using INSERT ON CONFLICT) ---")
#     if not kg_extractions:
#         context.log.warning("No KG extractions received. Skipping node loading.")
#         return

#     # +++ æ–°å¢è°ƒè¯•æ—¥å¿—ï¼šæ£€æŸ¥è¡¨æ˜¯å¦å­˜åœ¨ +++
#     try:
#         with duckdb_kg.get_connection() as conn_debug:
#             context.log.info("Attempting to list tables in DuckDB from duckdb_nodes_asset:")
#             tables = conn_debug.execute("SHOW TABLES;").fetchall()
#             context.log.info(f"Tables found: {tables}")
#             if any('"ExtractedEntity"' in str(table_row).upper() for table_row in tables) or \
#                any('ExtractedEntity' in str(table_row) for table_row in tables) : # æ£€æŸ¥å¤§å°å†™ä¸æ•æ„Ÿçš„åŒ¹é…
#                 context.log.info("Table 'ExtractedEntity' (or similar) IS visible at the start of duckdb_nodes_asset.")
#             else:
#                 context.log.warning("Table 'ExtractedEntity' IS NOT visible at the start of duckdb_nodes_asset. Schema asset might not have run correctly or changes are not reflected.")
#     except Exception as e_debug_show:
#         context.log.error(f"Error trying to list tables in duckdb_nodes_asset: {e_debug_show}")
#     # +++ ç»“æŸæ–°å¢è°ƒè¯•æ—¥å¿— +++
    
#     unique_nodes_data_for_insert: List[Dict[str, Any]] = []
#     unique_nodes_keys = set() # ç”¨äºåœ¨Pythonå±‚é¢å»é‡ï¼Œé¿å…å¤šæ¬¡å°è¯•æ’å…¥ç›¸åŒå®ä½“

#     for kg_set in kg_extractions:
#         for entity in kg_set.extracted_entities:
#             # è§„èŒƒåŒ–æ–‡æœ¬å’Œæ ‡ç­¾ï¼Œç”¨äºç”Ÿæˆå”¯ä¸€é”®å’Œå­˜å‚¨
#             normalized_text = normalize_text_for_id(entity.text)
#             normalized_label = entity.label.upper() # ç¡®ä¿æ ‡ç­¾å¤§å†™
            
#             # ä¸ºå®ä½“ç”Ÿæˆå”¯ä¸€ID (åŸºäºè§„èŒƒåŒ–æ–‡æœ¬å’Œæ ‡ç­¾çš„å“ˆå¸Œå€¼)
#             # æ³¨æ„ï¼šå¦‚æœåŒä¸€ä¸ªå®ä½“ï¼ˆç›¸åŒæ–‡æœ¬å’Œæ ‡ç­¾ï¼‰åœ¨ä¸åŒchunkä¸­è¢«æå–ï¼Œå®ƒä»¬çš„id_propä¼šä¸€æ ·
#             node_id_prop = hashlib.md5(f"{normalized_text}_{normalized_label}".encode('utf-8')).hexdigest()
            
#             node_unique_key_for_py_dedup = (node_id_prop) # ä½¿ç”¨id_propè¿›è¡ŒPythonå±‚é¢çš„å»é‡

#             if node_unique_key_for_py_dedup not in unique_nodes_keys:
#                 unique_nodes_keys.add(node_unique_key_for_py_dedup)
                
#                 # ç”ŸæˆåµŒå…¥å‘é‡ (ä¸KuzuDBæ—¶é€»è¾‘ç›¸åŒ)
#                 embedding_vector_list = embedder.encode([normalized_text]) # embedder.encodeæœŸæœ›ä¸€ä¸ªåˆ—è¡¨
#                 final_embedding_for_db: List[float]

#                 if embedding_vector_list and embedding_vector_list[0] and \
#                    isinstance(embedding_vector_list[0], list) and \
#                    len(embedding_vector_list[0]) == embedder.get_embedding_dimension():
#                     final_embedding_for_db = embedding_vector_list[0]
#                 else:
#                     context.log.warning(f"Failed to generate valid embedding for node: {normalized_text} ({normalized_label}). Using zero vector. Embedding result: {embedding_vector_list}")
#                     final_embedding_for_db = [0.0] * embedder.get_embedding_dimension()
                    
#                 unique_nodes_data_for_insert.append({
#                     "id_prop": node_id_prop,
#                     "text": normalized_text,
#                     "label": normalized_label,
#                     "embedding": final_embedding_for_db # DuckDBçš„FLOAT[]å¯ä»¥ç›´æ¥æ¥å—Pythonçš„List[float]
#                 })

#     if not unique_nodes_data_for_insert:
#         context.log.warning("No unique nodes found in extractions to load into DuckDB.")
#         return

#     nodes_processed_count = 0
#     nodes_inserted_count = 0
#     nodes_updated_count = 0

#     upsert_sql = f"""
#     INSERT INTO "ExtractedEntity" (id_prop, text, label, embedding)
#     VALUES (?, ?, ?, ?)
#     ON CONFLICT (id_prop) DO UPDATE SET
#         text = excluded.text,
#         label = excluded.label,
#         embedding = excluded.embedding;
#     """
#     # excluded.column_name ç”¨äºå¼•ç”¨è¯•å›¾æ’å…¥ä½†å¯¼è‡´å†²çªçš„å€¼

#     try:
#         with duckdb_kg.get_connection() as conn:
#             context.log.info(f"Attempting to UPSERT {len(unique_nodes_data_for_insert)} unique nodes into DuckDB ExtractedEntity table...")
            
#             # DuckDB æ”¯æŒ executemany ç”¨äºæ‰¹é‡æ“ä½œï¼Œä½†å¯¹äº ON CONFLICTï¼Œé€æ¡æ‰§è¡Œæˆ–æ„é€ å¤§å‹ VALUES åˆ—è¡¨å¯èƒ½æ›´ç›´æ¥
#             # æˆ–è€…ä½¿ç”¨ pandas DataFrame + duckdb.register + CREATE TABLE AS / INSERT INTO SELECT
#             # è¿™é‡Œä¸ºäº†æ¸…æ™°ï¼Œæˆ‘ä»¬å…ˆç”¨å¾ªç¯æ‰§è¡Œï¼Œå¯¹äºå‡ åƒåˆ°å‡ ä¸‡ä¸ªèŠ‚ç‚¹ï¼Œæ€§èƒ½å°šå¯æ¥å—
#             # å¦‚æœèŠ‚ç‚¹æ•°é‡éå¸¸å¤§ (å‡ åä¸‡ä»¥ä¸Š)ï¼Œåº”è€ƒè™‘æ›´ä¼˜åŒ–çš„æ‰¹é‡upsertç­–ç•¥

#             for node_data_dict in unique_nodes_data_for_insert:
#                 params = (
#                     node_data_dict["id_prop"],
#                     node_data_dict["text"],
#                     node_data_dict["label"],
#                     node_data_dict["embedding"]
#                 )
#                 try:
#                     # conn.execute() å¯¹äº DML (å¦‚ INSERT, UPDATE) ä¸ç›´æ¥è¿”å›å—å½±å“çš„è¡Œæ•°
#                     # ä½†æˆ‘ä»¬å¯ä»¥å‡è®¾å®ƒæˆåŠŸäº†ï¼Œé™¤éæŠ›å‡ºå¼‚å¸¸
#                     conn.execute(upsert_sql, params)
#                     # æ— æ³•ç›´æ¥åˆ¤æ–­æ˜¯insertè¿˜æ˜¯updateï¼Œé™¤éæŸ¥è¯¢å‰åå¯¹æ¯”ï¼Œè¿™é‡Œç®€åŒ–å¤„ç†
#                     nodes_processed_count += 1 
#                 except Exception as e_upsert_item:
#                     context.log.error(f"Error UPSERTING node with id_prop {node_data_dict.get('id_prop')} into DuckDB: {e_upsert_item}", exc_info=True)
            
#             # æˆ‘ä»¬å¯ä»¥æŸ¥ä¸€ä¸‹è¡¨ä¸­çš„æ€»è¡Œæ•°æ¥é—´æ¥äº†è§£æƒ…å†µ
#             total_rows_after = conn.execute('SELECT COUNT(*) FROM "ExtractedEntity"').fetchone()[0]
#             context.log.info(f"Successfully processed {nodes_processed_count} node upsert operations into DuckDB.")
#             context.log.info(f"Total rows in ExtractedEntity table after upsert: {total_rows_after}")

#     except Exception as e_db_nodes:
#         context.log.error(f"Error during DuckDB node loading: {e_db_nodes}", exc_info=True)
#         raise
    
#     context.add_output_metadata({
#         "nodes_prepared_for_upsert": len(unique_nodes_data_for_insert),
#         "nodes_processed_by_upsert_statement": nodes_processed_count,
#     })
#     context.log.info("--- DuckDB Node Loading Asset Finished ---")


# @dg.asset(
#     name="duckdb_relations", # <--- ä¿®æ”¹èµ„äº§åç§°
#     description="Loads all extracted relationships into DuckDB.",
#     group_name="kg_building",
#     deps=[duckdb_nodes_asset] # <--- ä¿®æ”¹ä¾èµ–
# )
# def duckdb_relations_asset(
#     context: dg.AssetExecutionContext, 
#     kg_extractions: List[KGTripleSetOutput], # æ¥è‡ª kg_extraction_asset
#     duckdb_kg: DuckDBResource                # <--- ä¿®æ”¹èµ„æºå‚æ•°
# ):
#     context.log.info("--- Starting DuckDB Relation Loading Asset ---")
#     if not kg_extractions:
#         context.log.warning("No KG extractions received. Skipping relation loading.")
#         return

#     relations_to_insert: List[Dict[str, str]] = []
#     unique_relation_keys = set() # ç”¨äºåœ¨Pythonå±‚é¢å»é‡

#     for kg_set in kg_extractions:
#         for rel in kg_set.extracted_relations:
#             # ä»å®ä½“æ–‡æœ¬å’Œæ ‡ç­¾ç”ŸæˆæºèŠ‚ç‚¹å’Œç›®æ ‡èŠ‚ç‚¹çš„ID (ä¸ duckdb_nodes_asset ä¸­ä¸€è‡´)
#             source_node_text_norm = normalize_text_for_id(rel.head_entity_text)
#             source_node_label_norm = rel.head_entity_label.upper()
#             source_node_id = hashlib.md5(f"{source_node_text_norm}_{source_node_label_norm}".encode('utf-8')).hexdigest()

#             target_node_text_norm = normalize_text_for_id(rel.tail_entity_text)
#             target_node_label_norm = rel.tail_entity_label.upper()
#             target_node_id = hashlib.md5(f"{target_node_text_norm}_{target_node_label_norm}".encode('utf-8')).hexdigest()
            
#             relation_type_norm = rel.relation_type.upper()

#             # ä¸ºå…³ç³»æœ¬èº«ç”Ÿæˆä¸€ä¸ªå”¯ä¸€ID
#             relation_unique_str = f"{source_node_id}_{relation_type_norm}_{target_node_id}"
#             relation_id = hashlib.md5(relation_unique_str.encode('utf-8')).hexdigest()

#             if relation_id not in unique_relation_keys:
#                 unique_relation_keys.add(relation_id)
#                 relations_to_insert.append({
#                     "relation_id": relation_id,
#                     "source_node_id_prop": source_node_id,
#                     "target_node_id_prop": target_node_id,
#                     "relation_type": relation_type_norm
#                 })
    
#     if not relations_to_insert:
#         context.log.warning("No unique relations found in extractions to load into DuckDB.")
#         return

#     relations_processed_count = 0
    
#     # ä½¿ç”¨ INSERT INTO ... ON CONFLICT DO NOTHING æ¥é¿å…æ’å…¥é‡å¤çš„å…³ç³» (åŸºäº relation_id)
#     insert_sql = """
#     INSERT INTO KGExtractionRelation (relation_id, source_node_id_prop, target_node_id_prop, relation_type)
#     VALUES (?, ?, ?, ?)
#     ON CONFLICT (relation_id) DO NOTHING;
#     """

#     try:
#         with duckdb_kg.get_connection() as conn:
#             context.log.info(f"Attempting to INSERT {len(relations_to_insert)} unique relations into DuckDB KGExtractionRelation table...")
            
#             for rel_data_dict in relations_to_insert:
#                 params = (
#                     rel_data_dict["relation_id"],
#                     rel_data_dict["source_node_id_prop"],
#                     rel_data_dict["target_node_id_prop"],
#                     rel_data_dict["relation_type"]
#                 )
#                 try:
#                     conn.execute(insert_sql, params)
#                     # DuckDBçš„executeå¯¹äºINSERT ON CONFLICT DO NOTHINGä¸ç›´æ¥è¿”å›æ˜¯å¦æ’å…¥
#                     # ä½†æˆ‘ä»¬å¯ä»¥å‡è®¾å®ƒæˆåŠŸå¤„ç†äº†ï¼ˆè¦ä¹ˆæ’å…¥ï¼Œè¦ä¹ˆå¿½ç•¥ï¼‰
#                     relations_processed_count += 1
#                 except Exception as e_insert_item:
#                     context.log.error(f"Error INSERTING relation with id {rel_data_dict.get('relation_id')} into DuckDB: {e_insert_item}", exc_info=True)
            
#             total_rels_after = conn.execute("SELECT COUNT(*) FROM KGExtractionRelation").fetchone()[0]
#             context.log.info(f"Successfully processed {relations_processed_count} relation insert (ON CONFLICT DO NOTHING) operations.")
#             context.log.info(f"Total rows in KGExtractionRelation table after inserts: {total_rels_after}")

#     except Exception as e_db_rels:
#         context.log.error(f"Error during DuckDB relation loading: {e_db_rels}", exc_info=True)
#         raise
        
#     context.add_output_metadata({
#         "relations_prepared_for_insert": len(relations_to_insert),
#         "relations_processed_by_insert_statement": relations_processed_count,
#     })
#     context.log.info("--- DuckDB Relation Loading Asset Finished ---")



# @dg.asset(
#     name="duckdb_vector_index", # <--- ä¿®æ”¹èµ„äº§åç§°
#     description="Creates the HNSW vector index on the embedding column in DuckDB.",
#     group_name="kg_building",
#     deps=[duckdb_relations_asset]  # <--- ä¿®æ”¹ä¾èµ–
# )
# def duckdb_vector_index_asset(
#     context: dg.AssetExecutionContext, 
#     duckdb_kg: DuckDBResource # <--- ä¿®æ”¹èµ„æºå‚æ•°
# ):
#     context.log.info("--- Starting DuckDB Vector Index Creation Asset ---")
    
#     table_to_index = "ExtractedEntity"
#     column_to_index = "embedding"
#     # ç´¢å¼•åå¯ä»¥è‡ªå®šä¹‰ï¼Œé€šå¸¸åŒ…å«è¡¨åã€åˆ—åå’Œç±»å‹
#     index_name = f"{table_to_index}_{column_to_index}_hnsw_idx"
#     metric_type = "l2sq" # æ¬§æ°è·ç¦»çš„å¹³æ–¹ï¼Œä¸æˆ‘ä»¬æµ‹è¯•æ—¶ä¸€è‡´

#     # DuckDB çš„ CREATE INDEX ... USING HNSW è¯­å¥
#     # IF NOT EXISTS ç¡®ä¿äº†å¹‚ç­‰æ€§
#     index_creation_sql = f"""
#     CREATE INDEX IF NOT EXISTS {index_name} 
#     ON {table_to_index} USING HNSW ({column_to_index}) 
#     WITH (metric='{metric_type}');
#     """

#     try:
#         with duckdb_kg.get_connection() as conn:
#             # åœ¨åˆ›å»ºç´¢å¼•å‰ï¼Œç¡®ä¿vssæ‰©å±•å·²åŠ è½½ä¸”æŒä¹…åŒ–å·²å¼€å¯ (è™½ç„¶DuckDBResourceçš„setupå·²åš)
#             try:
#                 conn.execute("LOAD vss;")
#                 conn.execute("SET hnsw_enable_experimental_persistence=true;")
#                 context.log.info("DuckDB: VSS extension loaded and HNSW persistence re-confirmed for index creation asset.")
#             except Exception as e_vss_setup_idx:
#                 context.log.warning(f"DuckDB: Failed to re-confirm VSS setup for index asset: {e_vss_setup_idx}. "
#                                      "Proceeding, assuming it was set by DuckDBResource.")

#             context.log.info(f"Executing DuckDB vector index creation command:\n{index_creation_sql.strip()}")
#             conn.execute(index_creation_sql)
#             context.log.info(f"DuckDB vector index '{index_name}' creation command executed successfully (or index already existed).")

#     except Exception as e_index_asset:
#         context.log.error(f"Error during DuckDB vector index creation: {e_index_asset}", exc_info=True)
#         raise
    
#     context.log.info("--- DuckDB Vector Index Creation Asset Finished ---")


# --- æ›´æ–° all_processing_assets åˆ—è¡¨ ---
all_processing_assets = [
    clean_chunk_text_asset,
    generate_embeddings_asset,
    vector_storage_asset,
    keyword_index_asset,
    # kg_extraction_asset,
    # duckdb_schema_asset,
    # duckdb_nodes_asset,
    # duckdb_relations_asset,
    # duckdb_vector_index_asset,
]


--------------------------------------------------------------------------------
æ–‡ä»¶: zhz_rag_pipeline/pydantic_models_dagster.py
--------------------------------------------------------------------------------
# zhz_rag_pipeline/pydantic_models_dagster.py
from typing import List, Dict, Any, Union, Optional, Literal
# --- ä¿®æ”¹ï¼šä» pydantic å¯¼å…¥ BaseModel å’Œ Field ---
from pydantic import BaseModel, Field
# --- ä¿®æ”¹ç»“æŸ ---
import uuid
# from typing import List # è¿™è¡Œæ˜¯å¤šä½™çš„ï¼Œå› ä¸ºä¸Šé¢å·²ç»ä» typing å¯¼å…¥äº† List

class LoadedDocumentOutput(BaseModel):
    document_path: str
    file_type: str
    # --- VITAL FIX: Make raw_content optional and always a string ---
    raw_content: Optional[str] = None # raw_content is now optional and will only hold decoded text
    metadata: Dict[str, Any]

# --- ä¿®æ”¹ï¼šåœ¨ ParsedDocumentOutput å®šä¹‰ä¹‹å‰å®šä¹‰å…¶ä¾èµ–çš„ Element ç±»å‹ ---
class DocumentElementMetadata(BaseModel):
    """é€šç”¨å…ƒæ•°æ®ï¼Œå¯é™„åŠ åˆ°ä»»ä½•æ–‡æ¡£å…ƒç´ ä¸Š"""
    page_number: Optional[int] = None
    source_coordinates: Optional[Dict[str, float]] = None # ä¾‹å¦‚ï¼ŒPDFä¸­çš„bbox
    custom_properties: Optional[Dict[str, Any]] = None # å…¶ä»–ç‰¹å®šäºå…ƒç´ çš„å±æ€§

class TitleElement(BaseModel):
    element_type: Literal["title"] = "title"
    text: str
    level: int # ä¾‹å¦‚ 1 ä»£è¡¨ H1, 2 ä»£è¡¨ H2
    metadata: Optional[DocumentElementMetadata] = None

class NarrativeTextElement(BaseModel): # æ™®é€šæ®µè½æ–‡æœ¬
    element_type: Literal["narrative_text"] = "narrative_text"
    text: str
    metadata: Optional[DocumentElementMetadata] = None

class ListItemElement(BaseModel):
    element_type: Literal["list_item"] = "list_item"
    text: str
    level: int = 0 # åˆ—è¡¨åµŒå¥—å±‚çº§ï¼Œ0ä»£è¡¨é¡¶å±‚åˆ—è¡¨é¡¹
    ordered: bool = False # Trueä»£è¡¨æœ‰åºåˆ—è¡¨é¡¹, Falseä»£è¡¨æ— åº
    item_number: Optional[Union[int, str]] = None # ä¾‹å¦‚ "1", "a", "*"
    metadata: Optional[DocumentElementMetadata] = None

class TableElement(BaseModel):
    element_type: Literal["table"] = "table"
    text_representation: Optional[str] = None 
    markdown_representation: Optional[str] = None
    html_representation: Optional[str] = None
    caption: Optional[str] = None
    metadata: Optional[DocumentElementMetadata] = None

class CodeBlockElement(BaseModel):
    element_type: Literal["code_block"] = "code_block"
    code: str
    language: Optional[str] = None
    metadata: Optional[DocumentElementMetadata] = None

class ImageElement(BaseModel): 
    element_type: Literal["image"] = "image"
    alt_text: Optional[str] = None
    caption: Optional[str] = None
    metadata: Optional[DocumentElementMetadata] = None

class PageBreakElement(BaseModel):
    element_type: Literal["page_break"] = "page_break"
    metadata: Optional[DocumentElementMetadata] = None
    
class HeaderElement(BaseModel):
    element_type: Literal["header"] = "header"
    text: str
    metadata: Optional[DocumentElementMetadata] = None

class FooterElement(BaseModel):
    element_type: Literal["footer"] = "footer"
    text: str
    metadata: Optional[DocumentElementMetadata] = None

DocumentElementType = Union[
    TitleElement, 
    NarrativeTextElement, 
    ListItemElement, 
    TableElement, 
    CodeBlockElement,
    ImageElement,
    PageBreakElement,
    HeaderElement,
    FooterElement
]

class ParsedDocumentOutput(BaseModel):
    parsed_text: str = Field(description="æ–‡æ¡£å†…å®¹çš„çº¿æ€§åŒ–çº¯æ–‡æœ¬è¡¨ç¤ºï¼Œå°½å¯èƒ½ä¿ç•™è¯­ä¹‰ã€‚") 
    elements: List[DocumentElementType] = Field(default_factory=list, description="ä»æ–‡æ¡£ä¸­è§£æå‡ºçš„ç»“æ„åŒ–å…ƒç´ åˆ—è¡¨ã€‚")
    original_metadata: Dict[str, Any] = Field(description="å…³äºåŸå§‹æ–‡æ¡£çš„å…ƒæ•°æ®ï¼Œå¦‚æ–‡ä»¶åã€è·¯å¾„ã€å¤§å°ç­‰ã€‚")
    summary: Optional[str] = None
# --- å·²æœ‰æ¨¡å‹ ---
class ChunkOutput(BaseModel):
    chunk_id: str = Field(default_factory=lambda: str(uuid.uuid4())) # ç¡®ä¿ Field è¢«å¯¼å…¥
    chunk_text: str
    source_document_id: str 
    chunk_metadata: Dict[str, Any]

class EmbeddingOutput(BaseModel):
    chunk_id: str 
    chunk_text: str 
    embedding_vector: List[float]
    embedding_model_name: str 
    original_chunk_metadata: Dict[str, Any]

class ExtractedEntity(BaseModel):
    text: str 
    label: str 

class ExtractedRelation(BaseModel):
    head_entity_text: str
    head_entity_label: str
    relation_type: str
    tail_entity_text: str
    tail_entity_label: str

class KGTripleSetOutput(BaseModel):
    chunk_id: str 
    extracted_entities: List[ExtractedEntity] = Field(default_factory=list)
    extracted_relations: List[ExtractedRelation] = Field(default_factory=list) 
    extraction_model_name: str 
    original_chunk_metadata: Dict[str, Any]


--------------------------------------------------------------------------------
æ–‡ä»¶: zhz_rag_pipeline/resources.py
--------------------------------------------------------------------------------
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/zhz_rag_pipeline/resources.py

import logging
import dagster as dg
import chromadb
from typing import List, Dict, Any, Optional, Iterator
import httpx
import json
import os
from contextlib import asynccontextmanager, contextmanager
from pydantic import Field as PydanticField, PrivateAttr
import asyncio
import time 
import duckdb
import sys
from queue import Empty
from pathlib import Path


# --- æ—¥å¿—å’Œç¡¬ä»¶ç®¡ç†å™¨å¯¼å…¥ ---
try:
    from zhz_rag.utils.interaction_logger import get_logger
except ImportError:
    import logging
    def get_logger(name: str) -> logging.Logger:
        logger = logging.getLogger(name)
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        return logger

try:
    from zhz_rag.utils.hardware_manager import HardwareManager, HardwareInfo
except ImportError as e_hal_import:
    print(f"ERROR: Failed to import HardwareManager/HardwareInfo: {e_hal_import}. HAL features will be disabled.")
    HardwareManager = None
    HardwareInfo = None


# --- GGUFEmbeddingResource: APIå®¢æˆ·ç«¯ç‰ˆæœ¬ ---

class GGUFEmbeddingResourceConfig(dg.Config):
    """
    GGUFEmbeddingResource çš„é…ç½®ç±»ã€‚
    """
    api_url: str = PydanticField(
        default="http://127.0.0.1:8089",
        description="URL of the standalone embedding API service."
    )

class GGUFEmbeddingResource(dg.ConfigurableResource):
    """
    ç”¨äºä¸GGUFåµŒå…¥APIæœåŠ¡äº¤äº’çš„Dagsterèµ„æºã€‚
    è´Ÿè´£åˆå§‹åŒ–HTTPå®¢æˆ·ç«¯ã€æ‰§è¡Œå¥åº·æ£€æŸ¥å’Œæ–‡æœ¬ç¼–ç ã€‚
    """
    api_url: str

    _client: httpx.AsyncClient = PrivateAttr()
    _logger: Optional[dg.DagsterLogManager] = PrivateAttr(default=None)
    _dimension: Optional[int] = PrivateAttr(default=None)
    _batch_size: int = PrivateAttr(default=128)

    def setup_for_execution(self, context: Optional[dg.InitResourceContext] = None) -> None:
        """
        åˆå§‹åŒ–èµ„æºã€‚ç°åœ¨å¯ä»¥æ¥å—ä¸€ä¸ªå¯é€‰çš„Dagsterä¸Šä¸‹æ–‡ã€‚
        å¦‚æœcontextä¸ºNoneï¼ˆåœ¨FastAPIç­‰éDagsterç¯å¢ƒä¸­ä½¿ç”¨ï¼‰ï¼Œåˆ™ä½¿ç”¨é»˜è®¤é…ç½®ã€‚

        Args:
            context: Dagsterçš„åˆå§‹åŒ–èµ„æºä¸Šä¸‹æ–‡ï¼Œå¯é€‰ã€‚
        """
        self._logger = context.log if context else logging.getLogger("GGUFEmbeddingResource")
        self._client = httpx.AsyncClient(base_url=self.api_url, timeout=600.0)

        # åŠ¨æ€è®¡ç®—æ‰¹å¤„ç†å¤§å°
        try:
            if context and hasattr(context, 'resources_by_key') and "system_resource" in context.resources_by_key:
                system_resource = context.resources_by_key["system_resource"]
                physical_cores = system_resource._hw_info.cpu_physical_cores if system_resource._hw_info else 4
                self._batch_size = max(128, physical_cores * 64)
            else:
                self._batch_size = 128
        except Exception as e:
            self._logger.error(f"Failed to dynamically set batch size: {e}. Using default 128.", exc_info=True)
            self._batch_size = 128
        # å¥åº·æ£€æŸ¥
        try:
            response = httpx.get(f"{self.api_url}/health")
            response.raise_for_status()
            health_data = response.json()
            if health_data.get("model_loaded"):
                self._dimension = health_data.get("dimension")
            else:
                raise RuntimeError(f"Embedding service at {self.api_url} is not healthy.")
        except Exception as e:
            self._logger.error(f"Failed to connect to embedding service: {e}")
            raise RuntimeError("Could not initialize GGUFEmbeddingResource.") from e

    def teardown_for_execution(self, context: dg.InitResourceContext) -> None:
        """
        åœ¨èµ„æºæ‰§è¡Œç»“æŸåå…³é—­HTTPå®¢æˆ·ç«¯ã€‚

        Args:
            context: Dagsterçš„åˆå§‹åŒ–èµ„æºä¸Šä¸‹æ–‡ã€‚
        """
        if hasattr(self, '_client') and not self._client.is_closed:
            async def _close():
                await self._client.aclose()
            try:
                loop = asyncio.get_running_loop()
                loop.create_task(_close())
            except RuntimeError:
                asyncio.run(_close())

    def get_embedding_dimension(self) -> int:
        """
        è·å–åµŒå…¥å‘é‡çš„ç»´åº¦ã€‚

        Returns:
            int: åµŒå…¥å‘é‡çš„ç»´åº¦ã€‚

        Raises:
            ValueError: å¦‚æœåµŒå…¥ç»´åº¦ä¸å¯ç”¨ã€‚
        """
        if self._dimension is None:
            raise ValueError("Embedding dimension not available.")
        return self._dimension

    def encode(self, texts: List[str], **kwargs: Any) -> List[List[float]]:
        """
        å°†æ–‡æœ¬åˆ—è¡¨ç¼–ç ä¸ºåµŒå…¥å‘é‡ã€‚

        Args:
            texts: å¾…ç¼–ç çš„æ–‡æœ¬åˆ—è¡¨ã€‚
            **kwargs: é¢å¤–çš„å…³é”®å­—å‚æ•°ã€‚

        Returns:
            List[List[float]]: æ–‡æœ¬å¯¹åº”çš„åµŒå…¥å‘é‡åˆ—è¡¨ã€‚
        """
        if not texts:
            return []

        all_embeddings: List[List[float]] = []
        
        # æ‰¹å¤„ç†å¾ªç¯
        for i in range(0, len(texts), self._batch_size):
            batch_texts = texts[i:i + self._batch_size]
            
            async def _async_encode_batch():
                try:
                    response = await self._client.post("/embed", json={"texts": batch_texts})
                    response.raise_for_status()
                    data = response.json()
                    return data.get("embeddings", [])
                except httpx.RequestError as e:
                    self._logger.error(f"Request to embedding service failed for a batch: {e}")
                    return [[] for _ in batch_texts]
                except Exception as e:
                    self._logger.error(f"An unexpected error occurred during embedding a batch: {e}")
                    return [[] for _ in batch_texts]

            # åœ¨å¾ªç¯å†…éƒ¨æ‰§è¡Œå¼‚æ­¥è°ƒç”¨
            try:
                loop = asyncio.get_running_loop()
                if loop.is_running():
                    future = asyncio.run_coroutine_threadsafe(_async_encode_batch(), loop)
                    batch_embeddings = future.result(timeout=600)
                else:
                    batch_embeddings = asyncio.run(_async_encode_batch())
            except RuntimeError:
                batch_embeddings = asyncio.run(_async_encode_batch())

            all_embeddings.extend(batch_embeddings)
            time.sleep(0.1) # åœ¨æ‰¹æ¬¡ä¹‹é—´åŠ å…¥ä¸€ä¸ªå¾®å°çš„å»¶è¿Ÿï¼Œé¿å…ç¬é—´æ‰“çˆ†API

        return all_embeddings

class ChromaDBResourceConfig(dg.Config):
    """
    ChromaDBResource çš„é…ç½®ç±»ã€‚
    """
    collection_name: str = PydanticField(
        default="zhz_rag_collection",
        description="Name of the ChromaDB collection."
    )
    persist_directory: str = PydanticField(
        default=os.path.join(os.getenv("ZHZ_AGENT_PROJECT_ROOT", "/home/zhz/zhz_agent"), "zhz_rag", "stored_data", "chromadb_index"),
        description="Directory to persist ChromaDB data."
    )

class ChromaDBResource(dg.ConfigurableResource):
    """
    ç”¨äºç®¡ç†ChromaDBå‘é‡æ•°æ®åº“çš„Dagsterèµ„æºã€‚
    æ”¯æŒåˆå§‹åŒ–å®¢æˆ·ç«¯ã€æ·»åŠ åµŒå…¥å’ŒæŸ¥è¯¢åµŒå…¥ã€‚
    """
    collection_name: str
    persist_directory: str

    _client: Optional[chromadb.PersistentClient] = PrivateAttr(default=None)
    _collection: Optional[Any] = PrivateAttr(default=None)
    _logger: Optional[dg.DagsterLogManager] = PrivateAttr(default=None)
    _batch_size: int = PrivateAttr(default=4096)

    def setup_for_execution(self, context: dg.InitResourceContext) -> None:
        """
        åˆå§‹åŒ–ChromaDBå®¢æˆ·ç«¯å’Œé›†åˆã€‚

        Args:
            context: Dagsterçš„åˆå§‹åŒ–èµ„æºä¸Šä¸‹æ–‡ã€‚

        Raises:
            RuntimeError: å¦‚æœChromaDBå®¢æˆ·ç«¯æˆ–é›†åˆåˆå§‹åŒ–å¤±è´¥ã€‚
        """
        self._logger = context.log
        os.makedirs(self.persist_directory, exist_ok=True)
        
        try:
            self._client = chromadb.PersistentClient(path=self.persist_directory)
            self._collection = self._client.get_or_create_collection(name=self.collection_name)
        except Exception as e:
            self._logger.error(f"Failed to initialize ChromaDB client or collection: {e}", exc_info=True)
            raise RuntimeError(f"Could not initialize ChromaDBResource due to: {e}") from e

    def teardown_for_execution(self, context: dg.InitResourceContext) -> None:
        """
        æ¸…ç†ChromaDBèµ„æºã€‚

        Args:
            context: Dagsterçš„åˆå§‹åŒ–èµ„æºä¸Šä¸‹æ–‡ã€‚
        """
        if self._client:
            pass # PersistentClient doesn't need explicit close
        self._client = None
        self._collection = None

    def add_embeddings(
        self, 
        ids: List[str], 
        embeddings: List[List[float]], 
        documents: Optional[List[str]] = None, 
        metadatas: Optional[List[Dict[str, Any]]] = None
    ) -> None:
        """
        å‘ChromaDBé›†åˆæ·»åŠ åµŒå…¥å‘é‡ã€‚

        Args:
            ids: æ–‡æ¡£IDåˆ—è¡¨ã€‚
            embeddings: åµŒå…¥å‘é‡åˆ—è¡¨ã€‚
            documents: åŸå§‹æ–‡æ¡£æ–‡æœ¬åˆ—è¡¨ï¼Œå¯é€‰ã€‚
            metadatas: æ–‡æ¡£å…ƒæ•°æ®åˆ—è¡¨ï¼Œå¯é€‰ã€‚

        Raises:
            RuntimeError: å¦‚æœChromaDBé›†åˆæœªåˆå§‹åŒ–ã€‚
            Exception: å¦‚æœæ·»åŠ æ‰¹æ¬¡åˆ°ChromaDBå¤±è´¥ã€‚
        """
        if self._collection is None:
            msg = "ChromaDB collection is not initialized. Cannot add embeddings."
            self._logger.error(msg)
            raise RuntimeError(msg)
        
        if not ids:
            self._logger.warning("add_embeddings called with empty IDs list. Nothing to add.")
            return

        total_items = len(ids)
        for i in range(0, total_items, self._batch_size):
            batch_end = min(i + self._batch_size, total_items)
            batch_ids = ids[i:batch_end]
            batch_embeddings = embeddings[i:batch_end]
            batch_documents = documents[i:batch_end] if documents else None
            batch_metadatas = metadatas[i:batch_end] if metadatas else None

            try:
                self._collection.add(
                    ids=batch_ids,
                    embeddings=batch_embeddings,
                    documents=batch_documents,
                    metadatas=batch_metadatas
                )
            except Exception as e:
                self._logger.error(
                    f"Failed to add batch starting at index {i} to ChromaDB: {e}",
                    exc_info=True
                )
                raise
        
    def query_embeddings(
        self,
        query_embeddings: List[List[float]],
        n_results: int = 5,
        where_filter: Optional[Dict[str, Any]] = None,
        include: Optional[List[str]] = None
    ) -> Optional[Dict[str, Any]]:
        """
        ä»ChromaDBé›†åˆæŸ¥è¯¢åµŒå…¥å‘é‡ã€‚

        Args:
            query_embeddings: æŸ¥è¯¢åµŒå…¥å‘é‡åˆ—è¡¨ã€‚
            n_results: è¿”å›ç»“æœçš„æ•°é‡ï¼Œé»˜è®¤ä¸º5ã€‚
            where_filter: ç”¨äºè¿‡æ»¤ç»“æœçš„æ¡ä»¶å­—å…¸ï¼Œå¯é€‰ã€‚
            include: åŒ…å«åœ¨ç»“æœä¸­çš„å­—æ®µï¼Œé»˜è®¤ä¸º["metadatas", "documents", "distances"]ã€‚

        Returns:
            Optional[Dict[str, Any]]: æŸ¥è¯¢ç»“æœå­—å…¸ï¼Œå¦‚æœæŸ¥è¯¢å¤±è´¥åˆ™ä¸ºNoneã€‚

        Raises:
            RuntimeError: å¦‚æœChromaDBé›†åˆæœªåˆå§‹åŒ–ã€‚
            Exception: å¦‚æœæŸ¥è¯¢åµŒå…¥å¤±è´¥ã€‚
        """
        if self._collection is None:
            msg = "ChromaDB collection is not initialized. Cannot query embeddings."
            self._logger.error(msg)
            raise RuntimeError(msg)

        if include is None:
            include = ["metadatas", "documents", "distances"]

        try:
            results = self._collection.query(
                query_embeddings=query_embeddings,
                n_results=n_results,
                where=where_filter,
                include=include
            )
            return results
        except Exception as e:
            self._logger.error(f"Failed to query embeddings from ChromaDB: {e}", exc_info=True)
            raise
        

class LocalLLMAPIResourceConfig(dg.Config):
    """
    LocalLLMAPIResource çš„é…ç½®ç±»ã€‚
    """
    api_url: str = "http://127.0.0.1:8088/v1/chat/completions"
    default_temperature: float = 0.1
    default_max_new_tokens: int = 2048

class LocalLLMAPIResource(dg.ConfigurableResource):
    """
    ç”¨äºä¸æœ¬åœ°LLM APIæœåŠ¡äº¤äº’çš„Dagsterèµ„æºã€‚
    """
    api_url: str
    default_temperature: float
    default_max_new_tokens: int
    _logger: Optional[dg.DagsterLogManager] = PrivateAttr(default=None)

    def setup_for_execution(self, context: dg.InitResourceContext) -> None:
        """
        åˆå§‹åŒ–æœ¬åœ°LLM APIèµ„æºã€‚

        Args:
            context: Dagsterçš„åˆå§‹åŒ–èµ„æºä¸Šä¸‹æ–‡ã€‚
        """
        self._logger = context.log

    async def generate_structured_output(self, prompt: str, json_schema: Dict[str, Any], temperature: Optional[float] = None, max_new_tokens: Optional[int] = None) -> Dict[str, Any]:
        """
        ä»æœ¬åœ°LLMç”Ÿæˆç»“æ„åŒ–è¾“å‡ºã€‚

        Args:
            prompt: å‘é€ç»™LLMçš„æç¤ºã€‚
            json_schema: æœŸæœ›çš„JSONè¾“å‡ºç»“æ„ã€‚
            temperature: ç”Ÿæˆæ¸©åº¦ï¼Œå¯é€‰ã€‚
            max_new_tokens: ç”Ÿæˆçš„æœ€å¤§æ–°tokenæ•°é‡ï¼Œå¯é€‰ã€‚

        Returns:
            Dict[str, Any]: LLMç”Ÿæˆçš„ç»“æ„åŒ–JSONè¾“å‡ºã€‚

        Raises:
            ValueError: å¦‚æœLLMå“åº”æ ¼å¼ä¸æ­£ç¡®ã€‚
            Exception: å¦‚æœè°ƒç”¨æœ¬åœ°LLMå¤±è´¥ã€‚
        """
        logger_instance = self._logger if self._logger else dg.get_dagster_logger()
        temp_to_use = temperature if temperature is not None else self.default_temperature
        tokens_to_use = max_new_tokens if max_new_tokens is not None else self.default_max_new_tokens
        messages = [{"role": "user", "content": prompt}]
        payload = {"model": "local_kg_extraction_model", "messages": messages, "temperature": temp_to_use, "max_tokens": tokens_to_use, "response_format": {"type": "json_object", "schema": json_schema}}
        try:
            async with httpx.AsyncClient(timeout=httpx.Timeout(300.0)) as client:
                response = await client.post(self.api_url, json=payload)
                response.raise_for_status()
                response_json = response.json()
                if response_json.get("choices") and response_json["choices"][0].get("message"):
                    generated_text = response_json["choices"][0]["message"].get("content", "")
                    return json.loads(generated_text)
                raise ValueError(f"Local LLM response format is incorrect: {response_json}")
        except Exception as e:
            logger_instance.error(f"Error during Local LLM call: {e}", exc_info=True)
            raise

class GeminiAPIResourceConfig(dg.Config):
    """
    GeminiAPIResource çš„é…ç½®ç±»ã€‚
    """
    model_name: str = PydanticField(default="gemini/gemini-1.5-flash-latest", description="Name of the Gemini model.")
    proxy_url: Optional[str] = PydanticField(default_factory=lambda: os.getenv("LITELLM_PROXY_URL"), description="Optional proxy URL for LiteLLM.")
    default_temperature: float = 0.1
    default_max_tokens: int = 2048
    
class GeminiAPIResource(dg.ConfigurableResource):
    """
    ç”¨äºé€šè¿‡LiteLLMè°ƒç”¨Gemini APIçš„Dagsterèµ„æºã€‚
    """
    model_name: str
    proxy_url: Optional[str]
    default_temperature: float
    default_max_tokens: int
    _api_key: Optional[str] = PrivateAttr(default=None)
    _logger: Optional[dg.DagsterLogManager] = PrivateAttr(default=None)

    def setup_for_execution(self, context: dg.InitResourceContext) -> None:
        """
        åˆå§‹åŒ–Gemini APIèµ„æºï¼ŒåŠ è½½APIå¯†é’¥ã€‚

        Args:
            context: Dagsterçš„åˆå§‹åŒ–èµ„æºä¸Šä¸‹æ–‡ã€‚
        """
        self._logger = context.log
        self._api_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
        if not self._api_key: self._logger.warning("Gemini API key not found.")

    async def call_completion(self, messages: List[Dict[str, str]], temperature: Optional[float] = None, max_tokens: Optional[int] = None) -> Optional[str]:
        """
        è°ƒç”¨Gemini APIç”Ÿæˆæ–‡æœ¬è¡¥å…¨ã€‚

        Args:
            messages: å¯¹è¯æ¶ˆæ¯åˆ—è¡¨ã€‚
            temperature: ç”Ÿæˆæ¸©åº¦ï¼Œå¯é€‰ã€‚
            max_tokens: ç”Ÿæˆçš„æœ€å¤§tokenæ•°é‡ï¼Œå¯é€‰ã€‚

        Returns:
            Optional[str]: ç”Ÿæˆçš„æ–‡æœ¬å†…å®¹ï¼Œå¦‚æœè°ƒç”¨å¤±è´¥åˆ™ä¸ºNoneã€‚
        """
        import litellm
        logger_instance = self._logger if self._logger else dg.get_dagster_logger()
        if not self._api_key: return None
        litellm_params = {"model": self.model_name, "messages": messages, "api_key": self._api_key, "temperature": temperature or self.default_temperature, "max_tokens": max_tokens or self.default_max_tokens}
        if self.proxy_url: litellm_params["proxy"] = {"http": self.proxy_url, "https": self.proxy_url}
        try:
            response = await litellm.acompletion(**litellm_params)
            return response.choices[0].message.content if response and response.choices else None
        except Exception as e:
            logger_instance.error(f"Error calling Gemini via LiteLLM: {e}", exc_info=True)
            return None
        
class DuckDBResource(dg.ConfigurableResource):
    """
    ç”¨äºç®¡ç†DuckDBæ•°æ®åº“è¿æ¥çš„Dagsterèµ„æºã€‚
    æ”¯æŒè¿æ¥æ•°æ®åº“ã€åŠ è½½VSSæ‰©å±•å’Œæ‰§è¡Œæ£€æŸ¥ç‚¹ã€‚
    """
    db_file_path: str = PydanticField(
        default=os.path.join(os.getenv("ZHZ_AGENT_PROJECT_ROOT", "/home/zhz/zhz_agent"), "zhz_rag", "stored_data", "duckdb_knowledge_graph.db"),
        description="Path to the DuckDB database file."
    )
    _conn: Optional[duckdb.DuckDBPyConnection] = PrivateAttr(default=None)
    _logger: Optional[dg.DagsterLogManager] = PrivateAttr(default=None)

    def setup_for_execution(self, context: dg.InitResourceContext) -> None:
        """
        åˆå§‹åŒ–DuckDBè¿æ¥å¹¶åŠ è½½VSSæ‰©å±•ã€‚

        Args:
            context: Dagsterçš„åˆå§‹åŒ–èµ„æºä¸Šä¸‹æ–‡ã€‚

        Raises:
            RuntimeError: å¦‚æœDuckDBè¿æ¥æˆ–VSSè®¾ç½®å¤±è´¥ã€‚
        """
        self._logger = context.log

        os.makedirs(os.path.dirname(self.db_file_path), exist_ok=True)
        
        try:
            self._conn = duckdb.connect(database=self.db_file_path, read_only=False)

            self._conn.execute("INSTALL vss;")
            self._conn.execute("LOAD vss;")
            self._conn.execute("SET hnsw_enable_experimental_persistence=true;")

        except Exception as e:
            self._logger.error(f"Error during DuckDB connection or VSS setup: {e}", exc_info=True)
            error_str = str(e).lower()
            if "already installed" in error_str or "already loaded" in error_str:
                self._logger.warning(f"VSS extension seems to be already installed/loaded, continuing...")
            else:
                raise RuntimeError(f"DuckDB connection/VSS setup failed: {e}") from e
        
    def teardown_for_execution(self, context: dg.InitResourceContext) -> None:
        """
        åœ¨èµ„æºæ‰§è¡Œç»“æŸåå…³é—­DuckDBè¿æ¥å¹¶æ‰§è¡Œæ£€æŸ¥ç‚¹ã€‚

        Args:
            context: Dagsterçš„åˆå§‹åŒ–èµ„æºä¸Šä¸‹æ–‡ã€‚
        """
        if self._conn:
            try:
                self._conn.execute("CHECKPOINT;")
            except Exception as e_checkpoint:
                self._logger.error(f"Error executing CHECKPOINT for DuckDB: {e_checkpoint}", exc_info=True)
            finally:
                self._conn.close()
                self._conn = None 
        else:
            self._logger.info("No active DuckDB connection to teardown.")

    @contextmanager
    def get_connection(self) -> Iterator[duckdb.DuckDBPyConnection]:
        """
        è·å–DuckDBæ•°æ®åº“è¿æ¥çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨ã€‚

        Yields:
            duckdb.DuckDBPyConnection: DuckDBæ•°æ®åº“è¿æ¥å¯¹è±¡ã€‚

        Raises:
            ConnectionError: å¦‚æœDuckDBè¿æ¥æœªå»ºç«‹ã€‚
        """
        if not self._conn:
            raise ConnectionError("DuckDB connection not established. Ensure setup_for_execution was successful.")
        yield self._conn

class SystemResource(dg.ConfigurableResource):
    """
    ç”¨äºè·å–ç³»ç»Ÿç¡¬ä»¶ä¿¡æ¯å’Œæ¨èå¹¶å‘ä»»åŠ¡æ•°çš„Dagsterèµ„æºã€‚
    """
    _hw_manager: Optional[Any] = PrivateAttr(default=None)
    _hw_info: Optional[Any] = PrivateAttr(default=None)
    _logger: Optional[dg.DagsterLogManager] = PrivateAttr(default=None)

    def setup_for_execution(self, context: dg.InitResourceContext) -> None:
        """
        åˆå§‹åŒ–ç³»ç»Ÿèµ„æºï¼Œæ£€æµ‹ç¡¬ä»¶ä¿¡æ¯ã€‚

        Args:
            context: Dagsterçš„åˆå§‹åŒ–èµ„æºä¸Šä¸‹æ–‡ã€‚
        """
        self._logger = context.log
        if HardwareManager:
            self._hw_manager = HardwareManager()
            self._hw_info = self._hw_manager.get_hardware_info()
        else:
            self._logger.warning("HardwareManager not available.")

    def get_recommended_concurrent_tasks(self, task_type: str = "cpu_bound_llm") -> int:
        """
        è·å–æ¨èçš„å¹¶å‘ä»»åŠ¡æ•°ã€‚

        Args:
            task_type: ä»»åŠ¡ç±»å‹ï¼Œé»˜è®¤ä¸º"cpu_bound_llm"ã€‚

        Returns:
            int: æ¨èçš„å¹¶å‘ä»»åŠ¡æ•°ã€‚
        """
        if self._hw_manager: return self._hw_manager.recommend_concurrent_tasks(task_type=task_type)
        return 1


--------------------------------------------------------------------------------
æ–‡ä»¶: parsers/__init__.py
--------------------------------------------------------------------------------
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/zhz_rag_pipeline/parsers/__init__.py
import logging # æ·»åŠ  logging å¯¼å…¥
from typing import Callable, Dict, Any, Optional, Union

# å°è¯•å¯¼å…¥ Pydantic æ¨¡å‹ï¼Œå¦‚æœå¤±è´¥ï¼Œåˆ™ç±»å‹åˆ«åä½¿ç”¨ Any
try:
    from ..pydantic_models_dagster import ParsedDocumentOutput
    _ParserOutputType = Optional[ParsedDocumentOutput]
except ImportError:
    _ParserOutputType = Optional[Any] # Fallback

# å®šä¹‰ä¸€ä¸ªç±»å‹åˆ«åï¼Œè¡¨ç¤ºè§£æå‡½æ•°çš„ç­¾å
# è¾“å…¥å¯ä»¥æ˜¯è·¯å¾„(str)æˆ–å†…å®¹(str/bytes)ï¼Œå…ƒæ•°æ®å­—å…¸ï¼Œè¿”å›Pydanticæ¨¡å‹æˆ–å­—å…¸
ParserFunction = Callable[[Union[str, bytes], Dict[str, Any]], _ParserOutputType]

# ä»å„ä¸ªè§£æå™¨æ¨¡å—å¯¼å…¥ä¸»è§£æå‡½æ•°
from .md_parser import parse_markdown_to_structured_output
from .docx_parser import parse_docx_to_structured_output
from .pdf_parser import parse_pdf_to_structured_output
from .xlsx_parser import parse_xlsx_to_structured_output
from .html_parser import parse_html_to_structured_output
from .txt_parser import parse_txt_to_structured_output

logger = logging.getLogger(__name__) # æ·»åŠ  logger å®ä¾‹

# åˆ›å»ºä¸€ä¸ªè§£æå™¨æ³¨å†Œè¡¨ (åˆå¹¶è‡ª parser_dispatcher.py)
PARSER_REGISTRY: Dict[str, ParserFunction] = {
    ".md": parse_markdown_to_structured_output,
    ".docx": parse_docx_to_structured_output,
    ".pdf": parse_pdf_to_structured_output,
    ".xlsx": parse_xlsx_to_structured_output,
    ".html": parse_html_to_structured_output,
    ".htm": parse_html_to_structured_output,  # Alias for html
    ".txt": parse_txt_to_structured_output,
}

def dispatch_parsing( # åˆå¹¶è‡ª parser_dispatcher.py
    file_extension: str,
    content_or_path: Union[str, bytes], # ç¡®ä¿è¿™é‡Œæ˜¯ Union[str, bytes]
    original_metadata: Dict[str, Any]
) -> Optional[Any]: # è¿”å› Optional[Any] ä»¥åŒ¹é…ä¸‹æ¸¸æœŸæœ›
    parser_func = PARSER_REGISTRY.get(file_extension.lower())
    if parser_func:
        try:
            # è°ƒç”¨ç›¸åº”çš„è§£æå‡½æ•°
            # txt_parser å’Œ md_parser, html_parser æœŸæœ› content_str
            # docx_parser, pdf_parser, xlsx_parser æœŸæœ› file_path
            # content_or_path å˜é‡åœ¨ ingestion_assets.py ä¸­å·²ç»æ ¹æ® file_ext åšäº†åŒºåˆ†
            return parser_func(content_or_path, original_metadata)
        except Exception as e:
            logger.error(f"Error calling parser for '{file_extension}' on '{original_metadata.get('source_file_path', 'N/A')}': {e}", exc_info=True)
            return None # è§£æå¤±è´¥è¿”å› None
    else:
        logger.warning(f"No specific parser registered for file type '{file_extension}'.")
        # å°è¯•ä¸€ä¸ªé€šç”¨çš„çº¯æ–‡æœ¬æå–ä½œä¸ºæœ€ç»ˆå›é€€ï¼ˆå¦‚æœé€‚ç”¨ä¸”æœ‰å®ç°ï¼‰
        # æˆ–è€…ç›´æ¥è¿”å›None
        return None

def get_parser(file_extension: str) -> Optional[ParserFunction]: # ä¿ç•™æ­¤å‡½æ•°ä»¥é˜²å…¶ä»–åœ°æ–¹ç”¨åˆ°
    return PARSER_REGISTRY.get(file_extension.lower())

__all__ = [
    "parse_markdown_to_structured_output",
    "parse_docx_to_structured_output",
    "parse_pdf_to_structured_output",
    "parse_xlsx_to_structured_output",
    "parse_html_to_structured_output",
    "parse_txt_to_structured_output",
    "get_parser", # ä¿ç•™
    "dispatch_parsing", # æ–°å¢å¯¼å‡º
    "PARSER_REGISTRY",
    "ParserFunction"
]


--------------------------------------------------------------------------------
æ–‡ä»¶: parsers/docx_parser.py
--------------------------------------------------------------------------------
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/zhz_rag_pipeline/parsers/docx_parser.py

import os
from typing import List, Dict, Any, Optional, Union
import re

# --- æ·»åŠ ï¼šä¸ºå½“å‰æ¨¡å—çš„ logger è¿›è¡ŒåŸºæœ¬é…ç½® ---
import logging
logger = logging.getLogger(__name__)
if not logger.handlers: # é¿å…é‡å¤æ·»åŠ  handler (å¦‚æœæ¨¡å—è¢«å¤šæ¬¡å¯¼å…¥)
    handler = logging.StreamHandler() # è¾“å‡ºåˆ° stderrï¼Œé€šå¸¸ä¼šè¢« Dagster æ•è·
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    logger.setLevel(logging.INFO) # è®¾ç½®å¸Œæœ›çœ‹åˆ°çš„æœ€ä½æ—¥å¿—çº§åˆ« (INFO, DEBUGç­‰)
    # logger.propagate = False # å¯ä»¥è€ƒè™‘è®¾ç½®ï¼Œé˜²æ­¢æ—¥å¿—å‘ä¸Šä¼ æ’­åˆ°æ ¹loggerå¯¼è‡´é‡å¤æ‰“å°ï¼Œä½†é€šå¸¸ Dagster ä¼šå¤„ç†å¥½
logger.info(f"Logger for {__name__} configured in docx_parser.py.") # ç¡®è®¤é…ç½®ç”Ÿæ•ˆ
# --- ç»“æŸæ·»åŠ  ---

# --- ä¾èµ–å¯¼å…¥ä¸å¯ç”¨æ€§æ£€æŸ¥ ---
try:
    from unstructured.partition.docx import partition_docx
    from unstructured.documents.elements import (
        Element as UnstructuredElement,
        Text, 
        NarrativeText,
        Title,
        ListItem,
        Table,
        Image as UnstructuredImage, 
        Header as UnstructuredHeader, 
        Footer as UnstructuredFooter, 
        Address,
        EmailAddress,
        FigureCaption,
        PageBreak as UnstructuredPageBreak, 
        CodeSnippet
    )
    _UNSTRUCTURED_AVAILABLE_DOCX = True
    logging.info("Successfully imported Unstructured for DOCX parsing.")
except ImportError as e_unstructured:
    logging.error(f"Failed to import Unstructured for DOCX: {e_unstructured}. DOCX parsing will have limited functionality.")
    _UNSTRUCTURED_AVAILABLE_DOCX = False
    # åˆ›å»ºå ä½ç¬¦ç±»ä»¥é¿å…åç»­ NameError
    class UnstructuredElement: pass
    class Text: pass                  # type: ignore
    class NarrativeText: pass          # type: ignore
    class Title: pass                  # type: ignore
    class ListItem: pass               # type: ignore
    class Table: pass                  # type: ignore
    class UnstructuredImage: pass      # type: ignore
    class UnstructuredHeader: pass     # type: ignore
    class UnstructuredFooter: pass     # type: ignore
    class Address: pass                # type: ignore
    class EmailAddress: pass           # type: ignore
    class FigureCaption: pass          # type: ignore
    class UnstructuredPageBreak: pass  # type: ignore
    class CodeSnippet: pass            # type: ignore

try:
    from markdownify import markdownify as md # type: ignore
    _MARKDOWNIFY_AVAILABLE = True
except ImportError:
    logging.warning("markdownify library not found. HTML table to Markdown conversion will be skipped.")
    _MARKDOWNIFY_AVAILABLE = False
    def md(html_content: str) -> str: # Fallback
        return f"[Markdownify not available. HTML content: {html_content[:100]}...]"

_PYDANTIC_MODELS_AVAILABLE_DOCX = False
try:
    from ..pydantic_models_dagster import (
        ParsedDocumentOutput, DocumentElementType, TitleElement, NarrativeTextElement,
        ListItemElement, TableElement, CodeBlockElement, PageBreakElement, ImageElement,
        HeaderElement, FooterElement, DocumentElementMetadata
    )
    _PYDANTIC_MODELS_AVAILABLE_DOCX = True
except ImportError:
    class BaseModel: pass
    class DocumentElementMetadata(BaseModel): page_number: Optional[int] = None
    
    class ParsedDocumentOutput(BaseModel): parsed_text: str; elements: list; original_metadata: dict; summary: Optional[str] = None
    class TitleElement(BaseModel): element_type:str="title"; text:str; level:int; metadata: Optional[DocumentElementMetadata] = None
    class NarrativeTextElement(BaseModel): element_type:str="narrative_text"; text:str; metadata: Optional[DocumentElementMetadata] = None
    class ListItemElement(BaseModel): element_type:str="list_item"; text:str; level:int=0; ordered:bool=False; item_number:Optional[Union[int, str]]=None; metadata: Optional[DocumentElementMetadata] = None
    class TableElement(BaseModel): # <--- ä¿®æ”¹æ­¤è¡Œ
        element_type:str="table"; 
        markdown_representation:Optional[str]=None; 
        html_representation:Optional[str]=None; 
        text_representation:Optional[str]=None; # <--- æ·»åŠ æ­¤å­—æ®µ
        caption:Optional[str]=None; 
        metadata: Optional[DocumentElementMetadata] = None
    class CodeBlockElement(BaseModel): element_type:str="code_block"; code:str; language:Optional[str]=None; metadata: Optional[DocumentElementMetadata] = None
    class PageBreakElement(BaseModel): element_type:str="page_break"; metadata: Optional[DocumentElementMetadata] = None
    class ImageElement(BaseModel): element_type:str="image"; alt_text:Optional[str]=None; caption:Optional[str]=None; metadata: Optional[DocumentElementMetadata] = None
    class HeaderElement(BaseModel): element_type:str="header"; text:str; metadata: Optional[DocumentElementMetadata] = None
    class FooterElement(BaseModel): element_type:str="footer"; text:str; metadata: Optional[DocumentElementMetadata] = None
    DocumentElementType = Any

logger = logging.getLogger(__name__)

# --- è¾…åŠ©å‡½æ•° ---
def _create_doc_element_metadata(unstructured_element: UnstructuredElement) -> Optional[Union[DocumentElementMetadata, Dict[str, Any]]]:
    if not hasattr(unstructured_element, 'metadata'):
        return None
        
    meta_data_dict: Dict[str, Any] = {}
    if hasattr(unstructured_element.metadata, 'page_number') and unstructured_element.metadata.page_number is not None:
        meta_data_dict['page_number'] = unstructured_element.metadata.page_number
    
    if hasattr(unstructured_element.metadata, 'filename'):
        meta_data_dict['source_filename'] = unstructured_element.metadata.filename
    if hasattr(unstructured_element.metadata, 'filetype'):
        meta_data_dict['source_filetype'] = unstructured_element.metadata.filetype

    if not meta_data_dict:
        return None

    if _PYDANTIC_MODELS_AVAILABLE_DOCX:
        return DocumentElementMetadata(**meta_data_dict)
    else:
        return meta_data_dict

def _convert_unstructured_elements_to_custom(
    unstructured_elements: List[UnstructuredElement], 
    doc_path_for_log: str # æ·»åŠ ä¸€ä¸ªå‚æ•°ç”¨äºæ—¥å¿—è®°å½•
) -> List[Any]:
    custom_elements: List[Any] = []
    
    file_basename_for_log = os.path.basename(doc_path_for_log)

    # --- ä½¿ç”¨ print è¿›è¡Œå¼ºåˆ¶è°ƒè¯• ---
    logger.info(f"DOCX Parser ({file_basename_for_log}): _convert_unstructured_elements_to_custom received {len(unstructured_elements)} elements from unstructured.")
    if not unstructured_elements:
        logger.warning(f"DOCX Parser ({file_basename_for_log}): Unstructured returned an empty list of elements. No custom elements will be generated by this function initially.")
    
    if not _UNSTRUCTURED_AVAILABLE_DOCX:
        logger.warning(f"DOCX Parser ({file_basename_for_log}): Unstructured library is not available (should have been caught earlier).")
        # ä½œä¸ºå›é€€ï¼Œæˆ‘ä»¬å¯ä»¥å°è¯•å°†æ¯ä¸ªå…ƒç´ çš„æ–‡æœ¬æå–ä¸º NarrativeTextElement (å¦‚æœ unstructured_elements éç©ºä½† _UNSTRUCTURED_AVAILABLE_DOCX æ„å¤–ä¸º False)
        for el_idx, el_fallback in enumerate(unstructured_elements):
            fallback_text = getattr(el_fallback, 'text', f"[Unstructured not fully available - Element {el_idx+1} in {file_basename_for_log}]").strip()
            if fallback_text:
                if _PYDANTIC_MODELS_AVAILABLE_DOCX:
                    custom_elements.append(NarrativeTextElement(text=fallback_text))
                else:
                    custom_elements.append({"element_type": "narrative_text", "text": fallback_text})
        return custom_elements

    for el_idx, el in enumerate(unstructured_elements):
        el_type_name = type(el).__name__
        el_id_str = getattr(el, 'id', 'N/A')
        el_text_preview = getattr(el, 'text', '')[:50].strip().replace('\n', ' ') if getattr(el, 'text', '') else "[NO TEXT]"
        # --- ä¿®æ”¹æ—¥å¿—çº§åˆ« ---
        logger.debug( # <--- ä» info ä¿®æ”¹ä¸º debug
            f"DOCX Parser ({file_basename_for_log}): Processing unstructured element index {el_idx}, "
            f"Type: {el_type_name}, ID: {el_id_str}, Text Preview: '{el_text_preview}'"
        )
        
        # æ‰“å° el.metadata.text_as_html çš„é¢„è§ˆï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        html_preview_from_meta = getattr(el.metadata, 'text_as_html', None) if hasattr(el, 'metadata') else None
        if html_preview_from_meta:
            logger.debug( # <--- ä» info ä¿®æ”¹ä¸º debug
                f"  â””â”€ ({file_basename_for_log}) Unstructured Element (idx {el_idx}, type {el_type_name}) has text_as_html (len: {len(html_preview_from_meta)}). Preview: {html_preview_from_meta[:70]}"
            )
        # --- ç»“æŸä¿®æ”¹ ---
        
        element_metadata = _create_doc_element_metadata(el)
        el_text = el.text.strip() if hasattr(el, 'text') and el.text else ""
        custom_el: Optional[Any] = None

        if isinstance(el, Title):
            level = el.metadata.category_depth if hasattr(el.metadata, 'category_depth') and el.metadata.category_depth is not None else 1
            if el_text:
                logger.debug(f"DOCX Parser ({file_basename_for_log}): Element index {el_idx} is Title (level {level}).")
                if _PYDANTIC_MODELS_AVAILABLE_DOCX: custom_el = TitleElement(text=el_text, level=level, metadata=element_metadata)
                else: custom_el = {"element_type": "title", "text": el_text, "level": level, "metadata": element_metadata}
        
        elif isinstance(el, ListItem):
            level = el.metadata.category_depth if hasattr(el.metadata, 'category_depth') and el.metadata.category_depth is not None else 0
            # å°è¯•ä»å…ƒæ•°æ®è·å–æ›´ç²¾ç¡®çš„åˆ—è¡¨ä¿¡æ¯ (unstructured å¯èƒ½æä¾›)
            # item_number å’Œ ordered çš„é€»è¾‘å¯ä»¥æ ¹æ® unstructured çš„å®é™…è¾“å‡ºæ¥å®Œå–„
            if el_text:
                logger.debug(f"DOCX Parser ({file_basename_for_log}): Element index {el_idx} is ListItem (level {level}).")
                if _PYDANTIC_MODELS_AVAILABLE_DOCX: custom_el = ListItemElement(text=el_text, level=level, metadata=element_metadata)
                else: custom_el = {"element_type": "list_item", "text": el_text, "level": level, "metadata": element_metadata}

        elif isinstance(el, Table):
            logger.info(f"DOCX Parser ({file_basename_for_log}): Element index {el_idx} IS an unstructured.documents.elements.Table object.")
            html_table = el.metadata.text_as_html if hasattr(el.metadata, 'text_as_html') else None
            
            if html_table:
                logger.info(f"DOCX Parser ({file_basename_for_log}): Found HTML table content for Table element (idx {el_idx}). Length: {len(html_table)}. Preview: {html_table[:150]}")
            else:
                logger.warning(f"DOCX Parser ({file_basename_for_log}): No HTML table content (el.metadata.text_as_html) found for Table element (idx {el_idx}) from unstructured. Element ID: {el.id if hasattr(el, 'id') else 'N/A'}")

            md_table = None
            if html_table and _MARKDOWNIFY_AVAILABLE:
                try: 
                    md_table = md(html_table)
                    logger.info(f"DOCX Parser ({file_basename_for_log}): Successfully converted HTML table (idx {el_idx}) to Markdown. MD Length: {len(md_table) if md_table else 0}")
                except Exception as e_md: 
                    logger.warning(f"DOCX Parser ({file_basename_for_log}): Failed to convert HTML table (idx {el_idx}) to Markdown: {e_md}. HTML: {html_table[:100]}")
            
            raw_table_text_fallback = el.text.strip() if hasattr(el, 'text') and el.text else None
            caption_text = None
            if hasattr(el.metadata, 'table_captions') and el.metadata.table_captions:
                    caption_obj = el.metadata.table_captions[0]
                    if hasattr(caption_obj, 'text'):
                            caption_text = caption_obj.text
            
            if not caption_text and hasattr(el.metadata, 'filename'): # Redundant if filename is always doc_path_for_log
                    caption_text = f"Table from {file_basename_for_log}" # Use basename
            final_caption = caption_text if caption_text else "Table"

            final_md_table = md_table
            final_html_table = html_table
            final_text_representation = None

            if not final_md_table and not final_html_table and raw_table_text_fallback:
                logger.info(f"DOCX Parser ({file_basename_for_log}): Table (idx {el_idx}) has no HTML/MD rep, but has raw text from unstructured: '{raw_table_text_fallback[:100]}...' Using it as text_representation.")
                final_text_representation = raw_table_text_fallback
            elif not final_md_table and not final_html_table and not raw_table_text_fallback:
                    logger.warning(f"DOCX Parser ({file_basename_for_log}): Table (idx {el_idx}) has no HTML, Markdown, or raw text representation from unstructured.")


            if _PYDANTIC_MODELS_AVAILABLE_DOCX: 
                custom_el = TableElement(
                    markdown_representation=final_md_table, 
                    html_representation=final_html_table, 
                    text_representation=final_text_representation,
                    caption=final_caption, 
                    metadata=element_metadata
                )
            else: 
                custom_el = {
                    "element_type": "table", 
                    "markdown_representation": final_md_table, 
                    "html_representation": final_html_table, 
                    "text_representation": final_text_representation,
                    "caption": final_caption, 
                    "metadata": element_metadata
                }
        
        elif isinstance(el, (NarrativeText, Text, Address, EmailAddress, FigureCaption)):
            if el_text:
                logger.debug(f"DOCX Parser ({file_basename_for_log}): Element index {el_idx} is NarrativeText/Text like.")
                if _PYDANTIC_MODELS_AVAILABLE_DOCX: custom_el = NarrativeTextElement(text=el_text, metadata=element_metadata)
                else: custom_el = {"element_type": "narrative_text", "text": el_text, "metadata": element_metadata}
        
        elif isinstance(el, UnstructuredHeader):
            if el_text:
                logger.debug(f"DOCX Parser ({file_basename_for_log}): Element index {el_idx} is Header.")
                if _PYDANTIC_MODELS_AVAILABLE_DOCX: custom_el = HeaderElement(text=el_text, metadata=element_metadata)
                else: custom_el = {"element_type": "header", "text": el_text, "metadata": element_metadata}
        
        elif isinstance(el, UnstructuredFooter):
            if el_text:
                logger.debug(f"DOCX Parser ({file_basename_for_log}): Element index {el_idx} is Footer.")
                if _PYDANTIC_MODELS_AVAILABLE_DOCX: custom_el = FooterElement(text=el_text, metadata=element_metadata)
                else: custom_el = {"element_type": "footer", "text": el_text, "metadata": element_metadata}

        elif isinstance(el, UnstructuredImage):
            alt_text = el_text if el_text else (el.metadata.filename if hasattr(el.metadata, 'filename') else "Image")
            logger.debug(f"DOCX Parser ({file_basename_for_log}): Element index {el_idx} is Image. Alt text: {alt_text}")
            if _PYDANTIC_MODELS_AVAILABLE_DOCX: custom_el = ImageElement(alt_text=alt_text, metadata=element_metadata)
            else: custom_el = {"element_type": "image", "alt_text": alt_text, "metadata": element_metadata}

        elif isinstance(el, UnstructuredPageBreak):
            logger.debug(f"DOCX Parser ({file_basename_for_log}): Element index {el_idx} is PageBreak.")
            if _PYDANTIC_MODELS_AVAILABLE_DOCX: custom_el = PageBreakElement(metadata=element_metadata)
            else: custom_el = {"element_type": "page_break", "metadata": element_metadata}
        
        elif isinstance(el, CodeSnippet):
            if el_text:
                logger.debug(f"DOCX Parser ({file_basename_for_log}): Element index {el_idx} is CodeSnippet.")
                if _PYDANTIC_MODELS_AVAILABLE_DOCX: custom_el = CodeBlockElement(code=el_text, metadata=element_metadata) # language can be inferred later if needed
                else: custom_el = {"element_type": "code_block", "code": el_text, "metadata": element_metadata}

        else: 
            # This is the catch-all for any other Unstructured element type
            # or if an element doesn't have text but we still want to represent it (though usually skipped if no text)
            if el_text: # Only create an element if there's text
                logger.warning(f"DOCX Parser ({file_basename_for_log}): Unhandled Unstructured element type: {el_type_name} at index {el_idx}. Treating as NarrativeText. Text: {el_text[:50]}")
                if _PYDANTIC_MODELS_AVAILABLE_DOCX: custom_el = NarrativeTextElement(text=el_text, metadata=element_metadata)
                else: custom_el = {"element_type": "narrative_text", "text": el_text, "_unstructured_type": el_type_name, "metadata": element_metadata}
            elif el_type_name != "CompositeElement": # CompositeElement often has no direct text but contains other elements
                    logger.debug(f"DOCX Parser ({file_basename_for_log}): Skipping Unstructured element type: {el_type_name} at index {el_idx} due to no text content.")

        if custom_el:
            custom_elements.append(custom_el)
            
    return custom_elements


def _generate_linear_text_from_custom_elements(elements: List[Any]) -> str:
    text_parts = []
    for el_data_any in elements:
        el_data: Dict[str, Any] = {}
        if _PYDANTIC_MODELS_AVAILABLE_DOCX and hasattr(el_data_any, 'model_dump'):
            el_data = el_data_any.model_dump(exclude_none=True)
        elif isinstance(el_data_any, dict):
            el_data = el_data_any
        else:
            continue

        el_type = el_data.get("element_type")
        text_content = el_data.get("text", "")
        
        current_element_text = ""
        if el_type == "title":
            current_element_text = f"\n{'#' * el_data.get('level',1)} {text_content}\n"
        elif el_type == "narrative_text":
            current_element_text = text_content + "\n"
        elif el_type == "list_item":
            prefix = f"{el_data.get('item_number', '')}. " if el_data.get('ordered') and el_data.get('item_number') else "- "
            indent = "  " * el_data.get('level', 0)
            current_element_text = f"{indent}{prefix}{text_content}\n"
        elif el_type == "table":
            caption = el_data.get('caption', 'Unnamed Table')
            md_repr = el_data.get('markdown_representation')
            if md_repr: current_element_text = f"\n[Table: {caption}]\n{md_repr}\n"
            elif el_data.get('html_representation'): current_element_text = f"\n[Table (HTML): {caption}]\n{el_data.get('html_representation')[:200]}...\n"
        elif el_type == "code_block":
            lang = el_data.get('language', "") or ""
            code_content = el_data.get('code', "")
            current_element_text = f"\n```{lang}\n{code_content}\n```\n"
        elif el_type == "page_break":
            current_element_text = "\n---\n"
        elif el_type == "header" or el_type == "footer":
            current_element_text = f"\n[{el_type.capitalize()}]: {text_content}\n"
        elif el_type == "image":
            alt_text = el_data.get('alt_text', 'Image')
            current_element_text = f"\n[Image: {alt_text}]\n"
        
        if current_element_text:
            text_parts.append(current_element_text)

    full_text = "".join(text_parts)
    full_text = re.sub(r'\n{3,}', '\n\n', full_text).strip() # Clean up excessive newlines
    return full_text


# --- ä¸»è§£æå‡½æ•° ---
def parse_docx_to_structured_output(
    file_path: str, 
    original_metadata: Dict[str, Any]
) -> Optional[Union[ParsedDocumentOutput, Dict[str, Any]]]:
    file_basename_for_log = os.path.basename(file_path)
    
    # --- ä½¿ç”¨ print è¿›è¡Œå¼ºåˆ¶è°ƒè¯• ---
    logger.info(f"DOCX Parser: Attempting to parse DOCX file: {file_basename_for_log} using Unstructured")
    
    if not _UNSTRUCTURED_AVAILABLE_DOCX:
        logger.error(f"DOCX Parser ({file_basename_for_log}): Unstructured library is not available. DOCX parsing cannot proceed.")
        return None
    try:
        unstructured_elements = partition_docx(
            filename=file_path, 
            strategy="fast", 
            infer_table_structure=True,
        )
        logger.info(f"DOCX Parser ({file_basename_for_log}): Unstructured partitioned DOCX. Found {len(unstructured_elements)} raw elements from partition_docx.")

        custom_elements = _convert_unstructured_elements_to_custom(unstructured_elements, file_path)
        logger.info(f"DOCX Parser ({file_basename_for_log}): _convert_unstructured_elements_to_custom created {len(custom_elements)} custom elements.")
        
        linear_text = _generate_linear_text_from_custom_elements(custom_elements)
        logger.info(f"DOCX Parser ({file_basename_for_log}): Generated linear text (len: {len(linear_text)}). Preview: {linear_text[:100].replace(chr(10), ' ')}")

        if not custom_elements and not linear_text.strip() and unstructured_elements:
            logger.warning(f"DOCX Parser ({file_basename_for_log}): partition_docx returned {len(unstructured_elements)} elements, "
                            "but no custom elements or linear text were generated. This might indicate all elements were skipped "
                            "or had no text content suitable for conversion.")
        elif not custom_elements and not linear_text.strip() and not unstructured_elements:
                logger.warning(f"DOCX Parser ({file_basename_for_log}): partition_docx returned 0 elements, "
                                "and no custom elements or linear text were generated.")

        if _PYDANTIC_MODELS_AVAILABLE_DOCX:
            return ParsedDocumentOutput(
                parsed_text=linear_text,
                elements=custom_elements, 
                original_metadata=original_metadata
            )
        else:
            return {
                "parsed_text": linear_text,
                "elements": custom_elements,
                "original_metadata": original_metadata
            }
            
    except FileNotFoundError:
        logger.error(f"DOCX Parser ({file_basename_for_log}): File not found: {file_path}")
        return None
    except ImportError as ie:
        logger.error(f"DOCX Parser ({file_basename_for_log}): ImportError during DOCX parsing with Unstructured: {ie}.")
        return None
    except Exception as e:
        logger.error(f"DOCX Parser ({file_basename_for_log}): Critical error parsing DOCX file: {e}", exc_info=True)
        error_message = f"[ERROR PARSING DOCX: {file_basename_for_log} - {type(e).__name__}: {str(e)}]"
        if _PYDANTIC_MODELS_AVAILABLE_DOCX:
            return ParsedDocumentOutput(
                parsed_text=error_message,
                elements=[],
                original_metadata=original_metadata
            )
        else:
            return {
                "parsed_text": error_message,
                "elements": [],
                "original_metadata": original_metadata
            }


--------------------------------------------------------------------------------
æ–‡ä»¶: parsers/html_parser.py
--------------------------------------------------------------------------------
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/zhz_rag_pipeline/parsers/html_parser.py
import os
import logging
from typing import List, Dict, Any, Optional, Union, Set
import re

try:
    from bs4 import BeautifulSoup, Tag, NavigableString
    _BS4_AVAILABLE = True
    logging.info("Successfully imported BeautifulSoup4 for HTML parsing.")
except ImportError:
    logging.error("BeautifulSoup4 (bs4) not found. HTML parsing will not be available.")
    _BS4_AVAILABLE = False
    class BeautifulSoup: pass # Placeholder
    class Tag: pass
    class NavigableString: pass


_PYDANTIC_MODELS_AVAILABLE_HTML = False
try:
    from ..pydantic_models_dagster import (
        ParsedDocumentOutput, DocumentElementType, TitleElement, NarrativeTextElement,
        ListItemElement, TableElement, CodeBlockElement, PageBreakElement,
        DocumentElementMetadata
    )
    _PYDANTIC_MODELS_AVAILABLE_HTML = True
except ImportError:
    class BaseModel: pass
    class DocumentElementMetadata(BaseModel): page_number: Optional[int] = None
    class ParsedDocumentOutput(BaseModel): parsed_text: str; elements: list; original_metadata: dict; summary: Optional[str] = None
    class TitleElement(BaseModel): element_type:str="title"; text:str; level:int; metadata: Optional[DocumentElementMetadata] = None
    class NarrativeTextElement(BaseModel): element_type:str="narrative_text"; text:str; metadata: Optional[DocumentElementMetadata] = None
    class ListItemElement(BaseModel): element_type:str="list_item"; text:str; level:int=0; ordered:bool=False; item_number:Optional[Union[int, str]]=None; metadata: Optional[DocumentElementMetadata] = None
    class TableElement(BaseModel): element_type:str="table"; markdown_representation:Optional[str]=None; html_representation:Optional[str]=None; caption:Optional[str]=None; metadata: Optional[DocumentElementMetadata] = None
    class CodeBlockElement(BaseModel): element_type:str="code_block"; code:str; language:Optional[str]=None; metadata: Optional[DocumentElementMetadata] = None
    class PageBreakElement(BaseModel): element_type:str="page_break"; metadata: Optional[DocumentElementMetadata] = None
    DocumentElementType = Any

logger = logging.getLogger(__name__)

# Tags to typically ignore for main content extraction
IGNORE_TAGS_HTML = ['script', 'style', 'nav', 'footer', 'header', 'aside', 'form', 'meta', 'link', 'button', 'input', 'noscript', 'iframe', 'canvas', 'svg', 'path']
# Tags that define a semantic block but we want to process their children
CONTAINER_TAGS_HTML = ['div', 'section', 'article', 'main', 'body', 'figure', 'figcaption', 'details', 'summary']


def _table_to_markdown(table_tag: Tag) -> str:
    """Converts a BeautifulSoup table Tag to a Markdown string."""
    md_rows = []
    header_processed = False
    
    # Process header (thead)
    thead = table_tag.find('thead')
    if thead:
        header_rows_tags = thead.find_all('tr')
        for hr_tag in header_rows_tags:
            header_cells = hr_tag.find_all(['th', 'td'])
            if header_cells:
                header_texts = [cell.get_text(separator=' ', strip=True) for cell in header_cells]
                md_rows.append("| " + " | ".join(header_texts) + " |")
                if not header_processed: # Add separator only after the first header row group
                    md_rows.append("| " + " | ".join(["---"] * len(header_texts)) + " |")
                    header_processed = True
    
    # Process body (tbody or direct tr in table)
    tbody = table_tag.find('tbody')
    if not tbody: # If no tbody, look for tr directly under table
        rows_to_process = table_tag.find_all('tr', recursive=False)
    else:
        rows_to_process = tbody.find_all('tr')
        
    for row_tag in rows_to_process:
        # Skip if this row was already processed as part of thead (if thead was missing)
        if not header_processed and row_tag.find('th'):
            header_cells = row_tag.find_all(['th', 'td'])
            header_texts = [cell.get_text(separator=' ', strip=True) for cell in header_cells]
            md_rows.append("| " + " | ".join(header_texts) + " |")
            md_rows.append("| " + " | ".join(["---"] * len(header_texts)) + " |")
            header_processed = True
            continue
        
        cell_texts = [cell.get_text(separator=' ', strip=True) for cell in row_tag.find_all('td')]
        if cell_texts: # Only add row if it has content
            md_rows.append("| " + " | ".join(cell_texts) + " |")
            
    return "\n".join(md_rows)

def _convert_html_tag_to_elements_recursive(tag: Tag, elements_list: List[Any], processed_tags: Set[Tag], current_list_level: int = 0):
    """
    Recursively processes a BeautifulSoup Tag and its children to extract structured elements.
    Modifies elements_list in place.
    """
    if tag in processed_tags or not isinstance(tag, Tag) or tag.name in IGNORE_TAGS_HTML:
        return

    tag_name = tag.name.lower()
    element_metadata = None # Placeholder for now, can be enhanced to include source line numbers etc.
    
    created_element = False

    if tag_name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
        level = int(tag_name[1:])
        text = tag.get_text(strip=True)
        if text:
            if _PYDANTIC_MODELS_AVAILABLE_HTML: elements_list.append(TitleElement(text=text, level=level, metadata=element_metadata))
            else: elements_list.append({"element_type": "title", "text": text, "level": level, "metadata": element_metadata})
            created_element = True
    
    elif tag_name == 'p':
        text = tag.get_text(strip=True)
        if text:
            if _PYDANTIC_MODELS_AVAILABLE_HTML: elements_list.append(NarrativeTextElement(text=text, metadata=element_metadata))
            else: elements_list.append({"element_type": "narrative_text", "text": text, "metadata": element_metadata})
            created_element = True

    elif tag_name in ['ul', 'ol']:
        ordered = tag_name == 'ol'
        start_num = int(tag.get('start', '1')) if ordered else 1
        
        # Iterate over direct children that are <li>
        direct_li_children = [child for child in tag.children if isinstance(child, Tag) and child.name == 'li']
        for i, li_tag in enumerate(direct_li_children):
            if li_tag in processed_tags: continue
            
            # Extract text directly under <li>, excluding text from nested lists
            li_text_parts = []
            for content_child in li_tag.contents:
                if isinstance(content_child, NavigableString):
                    stripped_text = content_child.strip()
                    if stripped_text: li_text_parts.append(stripped_text)
                elif isinstance(content_child, Tag) and content_child.name not in ['ul', 'ol']: # Get text from non-list children
                    li_text_parts.append(content_child.get_text(strip=True))
            
            final_li_text = " ".join(li_text_parts).strip()

            if final_li_text:
                item_num_str = str(start_num + i) if ordered else None
                if _PYDANTIC_MODELS_AVAILABLE_HTML: elements_list.append(ListItemElement(text=final_li_text, level=current_list_level, ordered=ordered, item_number=item_num_str, metadata=element_metadata))
                else: elements_list.append({"element_type": "list_item", "text": final_li_text, "level": current_list_level, "ordered": ordered, "item_number": item_num_str, "metadata": element_metadata})
            
            processed_tags.add(li_tag) # Mark <li> as processed for its direct text
            # Recursively process children of this <li> for nested lists or other elements
            for child_of_li in li_tag.children:
                if isinstance(child_of_li, Tag):
                     _convert_html_tag_to_elements_recursive(child_of_li, elements_list, processed_tags, current_list_level + 1)
        created_element = True # The list itself is an element boundary

    elif tag_name == 'table':
        md_table = _table_to_markdown(tag)
        caption_tag = tag.find('caption')
        caption_text = caption_tag.get_text(strip=True) if caption_tag else None
        if md_table or caption_text : # Only add if table has content or caption
            if _PYDANTIC_MODELS_AVAILABLE_HTML: elements_list.append(TableElement(markdown_representation=md_table, html_representation=str(tag), caption=caption_text, metadata=element_metadata))
            else: elements_list.append({"element_type": "table", "markdown_representation": md_table, "html_representation": str(tag), "caption": caption_text, "metadata": element_metadata})
        created_element = True

    elif tag_name == 'pre':
        code_tag = tag.find('code')
        code_text, lang = "", None
        if code_tag:
            code_text = code_tag.get_text() # Keep original spacing and newlines
            lang_class = code_tag.get('class', [])
            if lang_class: lang = next((cls.split('language-')[-1] for cls in lang_class if cls.startswith('language-')), None)
        else:
            code_text = tag.get_text()
        
        if code_text.strip(): # Check if there's actual code
            if _PYDANTIC_MODELS_AVAILABLE_HTML: elements_list.append(CodeBlockElement(code=code_text.strip('\n'), language=lang, metadata=element_metadata))
            else: elements_list.append({"element_type": "code_block", "code": code_text.strip('\n'), "language": lang, "metadata": element_metadata})
        created_element = True

    elif tag_name == 'blockquote':
        text = tag.get_text(strip=True)
        if text:
            if _PYDANTIC_MODELS_AVAILABLE_HTML: elements_list.append(NarrativeTextElement(text=text, metadata=element_metadata))
            else: elements_list.append({"element_type": "narrative_text", "text": text, "_is_blockquote": True, "metadata": element_metadata})
        created_element = True

    elif tag_name == 'hr':
        if _PYDANTIC_MODELS_AVAILABLE_HTML: elements_list.append(PageBreakElement(metadata=element_metadata))
        else: elements_list.append({"element_type": "page_break", "metadata": element_metadata})
        created_element = True
    
    processed_tags.add(tag)
    # If the tag itself wasn't a specific block element we handled, or it's a known container,
    # process its children.
    if not created_element or tag_name in CONTAINER_TAGS_HTML:
        for child in tag.children:
            if isinstance(child, Tag):
                _convert_html_tag_to_elements_recursive(child, elements_list, processed_tags, current_list_level)
            elif isinstance(child, NavigableString): # Handle loose text not in <p> etc.
                loose_text = child.strip()
                if loose_text and tag_name not in ['ul', 'ol']: # Avoid adding list item text twice
                    if _PYDANTIC_MODELS_AVAILABLE_HTML: elements_list.append(NarrativeTextElement(text=loose_text, metadata=element_metadata))
                    else: elements_list.append({"element_type": "narrative_text", "text": loose_text, "_is_loose_text": True, "metadata": element_metadata})

def _generate_linear_text_from_html_elements(elements: List[Any]) -> str:
    # This function is identical to the one in docx_parser.py, can be refactored to common_utils later.
    text_parts = []
    for el_data_any in elements:
        el_data: Dict[str, Any] = {}
        if _PYDANTIC_MODELS_AVAILABLE_HTML and hasattr(el_data_any, 'model_dump'):
            el_data = el_data_any.model_dump(exclude_none=True)
        elif isinstance(el_data_any, dict):
            el_data = el_data_any
        else: continue

        el_type = el_data.get("element_type")
        text_content = el_data.get("text", "")
        current_element_text = ""
        if el_type == "title": current_element_text = f"\n{'#' * el_data.get('level',1)} {text_content}\n"
        elif el_type == "narrative_text": current_element_text = text_content + "\n"
        elif el_type == "list_item":
            prefix = f"{el_data.get('item_number', '')}. " if el_data.get('ordered') and el_data.get('item_number') else "- "
            indent = "  " * el_data.get('level', 0)
            current_element_text = f"{indent}{prefix}{text_content}\n"
        elif el_type == "table":
            caption = el_data.get('caption','Unnamed Table')
            md_repr = el_data.get('markdown_representation')
            if md_repr: current_element_text = f"\n[Table: {caption}]\n{md_repr}\n"
        elif el_type == "code_block":
            lang = el_data.get('language', "") or ""
            code_content = el_data.get('code', "")
            current_element_text = f"\n```{lang}\n{code_content}\n```\n"
        elif el_type == "page_break": current_element_text = "\n---\n"
        if current_element_text: text_parts.append(current_element_text)
    full_text = "".join(text_parts)
    return re.sub(r'\n{3,}', '\n\n', full_text).strip()

def parse_html_to_structured_output(
    html_content_str: str, 
    original_metadata: Dict[str, Any]
) -> Optional[Union[ParsedDocumentOutput, Dict[str, Any]]]:
    logger.info(f"Attempting to parse HTML content (length: {len(html_content_str)} chars) using BeautifulSoup4")
    if not _BS4_AVAILABLE:
        logger.error("BeautifulSoup4 (bs4) is not available. HTML parsing cannot proceed.")
        return None

    elements: List[Any] = []
    try:
        # Try lxml first, then html.parser
        try:
            soup = BeautifulSoup(html_content_str, "lxml")
        except Exception: # Fallback if lxml is not installed or fails
            logger.warning("lxml parser not available or failed, falling back to html.parser for HTML.")
            soup = BeautifulSoup(html_content_str, "html.parser")

        # Attempt to find the main content area
        main_content_area = soup.find('article') or soup.find('main') or soup.body
        if not main_content_area:
            logger.warning("Could not find <article>, <main>, or <body> tag. Parsing entire document if possible.")
            main_content_area = soup # Fallback to entire soup object

        # Remove ignored tags before processing
        for ignore_tag_name in IGNORE_TAGS_HTML:
            for tag_to_remove in main_content_area.find_all(ignore_tag_name):
                tag_to_remove.decompose()
        
        processed_tags_set: Set[Tag] = set()
        _convert_html_tag_to_elements_recursive(main_content_area, elements, processed_tags_set)
        
        logger.info(f"Converted HTML to {len(elements)} custom elements.")
        
        linear_text = _generate_linear_text_from_html_elements(elements)
        logger.info(f"Generated linear text from HTML elements (length: {len(linear_text)}). Preview: {linear_text[:200]}")

        if _PYDANTIC_MODELS_AVAILABLE_HTML:
            return ParsedDocumentOutput(
                parsed_text=linear_text,
                elements=elements, # type: ignore
                original_metadata=original_metadata
            )
        else:
            return {
                "parsed_text": linear_text,
                "elements": elements,
                "original_metadata": original_metadata
            }
    except Exception as e:
        logger.error(f"Error parsing HTML content with BeautifulSoup4: {e}", exc_info=True)
        return None


--------------------------------------------------------------------------------
æ–‡ä»¶: parsers/md_parser.py
--------------------------------------------------------------------------------
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/zhz_rag_pipeline/parsers/md_parser.py
import os
from markdown_it import MarkdownIt
from markdown_it.tree import SyntaxTreeNode
import logging
from typing import List, Dict, Any, Optional, Union
import re

# --- Pydantic æ¨¡å‹å¯¼å…¥å’Œå ä½ç¬¦å®šä¹‰ ---
_PARSER_PYDANTIC_AVAILABLE = False
try:
    from ..pydantic_models_dagster import (
        ParsedDocumentOutput, DocumentElementType, TitleElement, NarrativeTextElement,
        ListItemElement, TableElement, CodeBlockElement, PageBreakElement,
        DocumentElementMetadata
    )
    _PARSER_PYDANTIC_AVAILABLE = True
except ImportError:
    class BaseModel: pass
    class DocumentElementMetadata(BaseModel): page_number: Optional[int] = None
    class ParsedDocumentOutput(BaseModel): parsed_text: str; elements: list; original_metadata: dict; summary: Optional[str] = None
    class TitleElement(BaseModel): element_type:str="title"; text:str; level:int; metadata: Optional[DocumentElementMetadata] = None
    class NarrativeTextElement(BaseModel): element_type:str="narrative_text"; text:str; metadata: Optional[DocumentElementMetadata] = None
    class ListItemElement(BaseModel): element_type:str="list_item"; text:str; level:int=0; ordered:bool=False; item_number:Optional[Union[int, str]]=None; metadata: Optional[DocumentElementMetadata] = None
    class TableElement(BaseModel): element_type:str="table"; markdown_representation:Optional[str]=None; html_representation:Optional[str]=None; caption:Optional[str]=None; metadata: Optional[DocumentElementMetadata] = None
    class CodeBlockElement(BaseModel): element_type:str="code_block"; code:str; language:Optional[str]=None; metadata: Optional[DocumentElementMetadata] = None
    class PageBreakElement(BaseModel): element_type:str="page_break"; metadata: Optional[DocumentElementMetadata] = None
    DocumentElementType = Any

logger = logging.getLogger(__name__)

# --- è¾…åŠ©å‡½æ•° ---
def _get_node_text_content(node: SyntaxTreeNode, exclude_lists_and_tables: bool = False) -> str:
    if node.type == "text":
        return node.content
    if node.type == "softbreak":
        return " "
    if node.type == "hardbreak":
        return "\n"
    if node.type == "code_inline":
        return f"`{node.content}`"
    
    if exclude_lists_and_tables and node.type in ["bullet_list", "ordered_list", "table"]:
        return ""

    content = ""
    if node.children:
        for child in node.children:
            content += _get_node_text_content(child, exclude_lists_and_tables)
    return content

def _convert_table_node_to_markdown(table_node: SyntaxTreeNode) -> str:
    md_rows = []
    
    thead_node = next((child for child in table_node.children if child.type == 'thead'), None)
    tbody_node = next((child for child in table_node.children if child.type == 'tbody'), None)

    header_texts = []
    if thead_node:
        tr_node_header = next((child for child in thead_node.children if child.type == 'tr'), None)
        if tr_node_header:
            header_texts = [_get_node_text_content(cell).strip() for cell in tr_node_header.children if cell.type == 'th']
    elif tbody_node: 
        first_row_in_tbody = next((child for child in tbody_node.children if child.type == 'tr'), None)
        if first_row_in_tbody and all(cell.type == 'th' for cell in first_row_in_tbody.children):
             header_texts = [_get_node_text_content(cell).strip() for cell in first_row_in_tbody.children]

    if header_texts:
        md_rows.append("| " + " | ".join(header_texts) + " |")
        md_rows.append("| " + " | ".join(["---"] * len(header_texts)) + " |")

    rows_container = tbody_node if tbody_node else table_node 
    
    first_row_in_container_is_header = False
    if not header_texts and rows_container: # åªæœ‰åœ¨æ²¡æœ‰theadä¸”å®¹å™¨å­˜åœ¨æ—¶ï¼Œæ‰æ£€æŸ¥ç¬¬ä¸€è¡Œæ˜¯å¦æ˜¯è¡¨å¤´
        first_tr = next((child for child in rows_container.children if child.type == 'tr'), None)
        if first_tr and all(cell.type == 'th' for cell in first_tr.children):
            # å¦‚æœç¬¬ä¸€è¡Œå…¨æ˜¯thï¼Œä½œä¸ºè¡¨å¤´å¤„ç†
            header_texts_from_body = [_get_node_text_content(cell).strip() for cell in first_tr.children]
            if header_texts_from_body:
                md_rows.append("| " + " | ".join(header_texts_from_body) + " |")
                md_rows.append("| " + " | ".join(["---"] * len(header_texts_from_body)) + " |")
                first_row_in_container_is_header = True

    if rows_container: # ç¡®ä¿ rows_container å­˜åœ¨
        for row_idx, tr_node in enumerate(child for child in rows_container.children if child.type == 'tr'):
            # å¦‚æœç¬¬ä¸€è¡Œå·²ç»è¢«ä½œä¸ºè¡¨å¤´å¤„ç†äº†ï¼Œåˆ™è·³è¿‡å®ƒ
            if first_row_in_container_is_header and row_idx == 0:
                continue
            
            # å¦‚æœå·²ç»é€šè¿‡ thead å¤„ç†äº†è¡¨å¤´ï¼Œé‚£ä¹ˆ tbody/table ä¸‹çš„æ‰€æœ‰ tr éƒ½åº”è§†ä¸ºæ•°æ®è¡Œ
            # å¦‚æœæ²¡æœ‰é€šè¿‡ thead å¤„ç†è¡¨å¤´ï¼Œå¹¶ä¸”å½“å‰è¡Œä¹Ÿä¸æ˜¯è¢«æ¨æ–­ä¸ºè¡¨å¤´çš„ tbody ç¬¬ä¸€è¡Œï¼Œé‚£ä¹ˆå®ƒä¹Ÿæ˜¯æ•°æ®è¡Œ
            cell_texts = [_get_node_text_content(cell).strip() for cell in tr_node.children if cell.type == 'td']
            if cell_texts or len(tr_node.children) > 0 : 
                md_rows.append("| " + " | ".join(cell_texts) + " |")
            
    return "\n".join(md_rows)

# --- ä¸»è½¬æ¢å‡½æ•° ---
def _convert_md_tree_to_elements(root_node: SyntaxTreeNode) -> List[Any]: 
    elements: List[Any] = []
    
    def _process_node_recursive(node: SyntaxTreeNode, current_semantic_level: int = 0, list_ctx: Optional[Dict] = None):
        nonlocal elements
        current_metadata = None 

        node_type = node.type
        
        if node_type == "heading":
            level = int(node.tag[1:])
            text = _get_node_text_content(node).strip()
            if text or node.children: 
                if _PARSER_PYDANTIC_AVAILABLE: elements.append(TitleElement(text=text, level=level, metadata=current_metadata))
                else: elements.append({"element_type": "title", "text": text, "level": level, "metadata": current_metadata})
        
        elif node_type == "paragraph":
            text = _get_node_text_content(node).strip()
            if text:
                if _PARSER_PYDANTIC_AVAILABLE: elements.append(NarrativeTextElement(text=text, metadata=current_metadata))
                else: elements.append({"element_type": "narrative_text", "text": text, "metadata": current_metadata})

        elif node_type == "bullet_list" or node_type == "ordered_list":
            is_ordered_list = (node_type == "ordered_list")
            child_list_ctx = {
                "ordered": is_ordered_list,
                "start_num": int(node.attrs.get("start", 1)) if node.attrs and is_ordered_list else 1,
                "item_idx_in_list": 0 
            }
            for child_node in node.children:
                if child_node.type == "list_item":
                    _process_node_recursive(child_node, current_semantic_level + 1, child_list_ctx)
        
        elif node_type == "list_item":
            item_text = _get_node_text_content(node, exclude_lists_and_tables=True).strip()
            
            if item_text and list_ctx: 
                display_level = current_semantic_level - 1 
                item_number_str = None
                if list_ctx["ordered"]:
                    item_number_str = str(list_ctx["start_num"] + list_ctx["item_idx_in_list"])
                    list_ctx["item_idx_in_list"] += 1
                else: 
                    item_number_str = node.markup if node.markup else "-" 

                if _PARSER_PYDANTIC_AVAILABLE:
                    elements.append(ListItemElement(
                        text=item_text, level=display_level, 
                        ordered=list_ctx["ordered"], 
                        item_number=item_number_str, metadata=current_metadata
                    ))
                else:
                    elements.append({
                        "element_type": "list_item", "text": item_text, 
                        "level": display_level, "ordered": list_ctx["ordered"], 
                        "item_number": item_number_str, "metadata": current_metadata
                    })
            
            for child_node in node.children:
                if child_node.type in ["bullet_list", "ordered_list"]:
                    _process_node_recursive(child_node, current_semantic_level, None) # Pass current_semantic_level for nested list

        elif node_type == "table":
            md_table_representation = _convert_table_node_to_markdown(node)
            if md_table_representation:
                if _PARSER_PYDANTIC_AVAILABLE: elements.append(TableElement(markdown_representation=md_table_representation, metadata=current_metadata))
                else: elements.append({"element_type": "table", "markdown_representation": md_table_representation, "metadata": current_metadata})

        elif node_type == "fence" or node_type == "code_block":
            code_content = node.content.strip('\n') 
            lang = node.info.strip() if node.info else None
            if _PARSER_PYDANTIC_AVAILABLE: elements.append(CodeBlockElement(code=code_content, language=lang, metadata=current_metadata))
            else: elements.append({"element_type": "code_block", "code": code_content, "language": lang, "metadata": current_metadata})

        elif node_type == "hr":
            if _PARSER_PYDANTIC_AVAILABLE: elements.append(PageBreakElement(metadata=current_metadata))
            else: elements.append({"element_type": "page_break", "metadata": current_metadata})

        elif node_type == "blockquote":
            text = _get_node_text_content(node).strip()
            if text:
                if _PARSER_PYDANTIC_AVAILABLE: elements.append(NarrativeTextElement(text=text, metadata=current_metadata))
                else: elements.append({"element_type": "narrative_text", "text": text, "_is_blockquote": True, "metadata": current_metadata})
        
        elif node.children and node_type not in ["list_item", "heading", "paragraph", "table", "fence", "code_block", "blockquote", "hr", "bullet_list", "ordered_list"]: # Avoid re-processing children of already handled types
             for child in node.children:
                _process_node_recursive(child, current_semantic_level, list_ctx) # Pass context along

    _process_node_recursive(root_node) 
    return elements

def _generate_parsed_text_from_elements_internal(elements: List[Any]) -> str:
    text_parts = []
    for el_data_any in elements:
        el_data = {}
        if _PARSER_PYDANTIC_AVAILABLE and hasattr(el_data_any, 'model_dump'): el_data = el_data_any.model_dump()
        elif isinstance(el_data_any, dict): el_data = el_data_any
        else: continue
        el_type = el_data.get("element_type")
        if el_type == "title": text_parts.append(f"\n{'#' * el_data.get('level',1)} {el_data.get('text','')}\n")
        elif el_type == "narrative_text": text_parts.append(el_data.get('text','') + "\n")
        elif el_type == "list_item":
            item_num_display = str(el_data.get('item_number','-')) 
            prefix = f"{item_num_display}. " if el_data.get('ordered') else f"{item_num_display} "
            indent = "  " * el_data.get('level',0)
            text_parts.append(f"{indent}{prefix}{el_data.get('text','')}\n")
        elif el_type == "table":
            caption_text = str(el_data.get('caption')) if el_data.get('caption') is not None else 'Unnamed Table'
            if el_data.get('markdown_representation'): 
                text_parts.append(f"\n[Table: {caption_text}]\n{el_data['markdown_representation']}\n")
            elif el_data.get('text_representation'): 
                text_parts.append(f"\n[Table: {caption_text}]\n{el_data['text_representation']}\n")
        elif el_type == "code_block":
            lang = el_data.get('language', "") or ""
            text_parts.append(f"\n```{lang}\n{el_data.get('code','')}\n```\n")
        elif el_type == "page_break": text_parts.append("\n---\n")
        if el_type not in ['list_item'] and text_parts and (not text_parts[-1].endswith("\n\n") and not text_parts[-1].endswith("\n---\n\n") ) : 
             text_parts.append("\n") 
             
    raw_text = "".join(text_parts)
    cleaned_text = raw_text.strip()
    cleaned_text = cleaned_text.replace('\r\n', '\n') 
    cleaned_text = re.sub(r'\n{3,}', '\n\n', cleaned_text) 
    return cleaned_text

def parse_markdown_to_structured_output(md_content_str: str, original_metadata: Dict[str, Any]) -> Optional[Union[ParsedDocumentOutput, Dict[str, Any]]]:
    logger.info(f"Parsing Markdown content (length: {len(md_content_str)} chars) using markdown-it-py with SyntaxTreeNode...")
    try:
        md_parser = MarkdownIt("commonmark", {'linkify': True}).enable("table")
        tokens = md_parser.parse(md_content_str)
        
        root_syntax_node = SyntaxTreeNode(tokens)
        structured_elements = _convert_md_tree_to_elements(root_syntax_node) 

        linear_text = _generate_parsed_text_from_elements_internal(structured_elements)

        if _PARSER_PYDANTIC_AVAILABLE:
            return ParsedDocumentOutput(
                parsed_text=linear_text,
                elements=structured_elements, 
                original_metadata=original_metadata
            )
        else:
            return {
                "parsed_text": linear_text,
                "elements": structured_elements,
                "original_metadata": original_metadata
            }
    except Exception as e:
        logger.error(f"Error in parse_markdown_to_structured_output: {e}", exc_info=True)
        return None

# --- Placeholder for other parsers (ä¿æŒä¸å˜) ---
def parse_docx_to_structured_output(file_path: str, original_metadata: Dict[str, Any]) -> Optional[Union[ParsedDocumentOutput, Dict[str, Any]]]:
    logger.info(f"DOCX parser placeholder for: {file_path}") 
    text = f"[DOCX content of {os.path.basename(file_path)}]"
    if _PARSER_PYDANTIC_AVAILABLE: return ParsedDocumentOutput(parsed_text=text, elements=[NarrativeTextElement(text=text)], original_metadata=original_metadata) 
    return {"parsed_text": text, "elements": [{"element_type":"narrative_text", "text":text}], "original_metadata": original_metadata}

def parse_pdf_to_structured_output(file_path: str, original_metadata: Dict[str, Any]) -> Optional[Union[ParsedDocumentOutput, Dict[str, Any]]]:
    logger.info(f"PDF parser placeholder for: {file_path}") 
    text = f"[PDF content of {os.path.basename(file_path)}]"
    if _PARSER_PYDANTIC_AVAILABLE: return ParsedDocumentOutput(parsed_text=text, elements=[NarrativeTextElement(text=text)], original_metadata=original_metadata) 
    return {"parsed_text": text, "elements": [{"element_type":"narrative_text", "text":text}], "original_metadata": original_metadata}

def parse_xlsx_to_structured_output(file_path: str, original_metadata: Dict[str, Any]) -> Optional[Union[ParsedDocumentOutput, Dict[str, Any]]]:
    logger.info(f"XLSX parser placeholder for: {file_path}") 
    text = f"[XLSX content of {os.path.basename(file_path)}]"
    if _PARSER_PYDANTIC_AVAILABLE: return ParsedDocumentOutput(parsed_text=text, elements=[NarrativeTextElement(text=text)], original_metadata=original_metadata) 
    return {"parsed_text": text, "elements": [{"element_type":"narrative_text", "text":text}], "original_metadata": original_metadata}
        
def parse_html_to_structured_output(html_content_str: str, original_metadata: Dict[str, Any]) -> Optional[Union[ParsedDocumentOutput, Dict[str, Any]]]:
    logger.info(f"HTML parser placeholder for content length: {len(html_content_str)}") 
    text = f"[HTML content snippet: {html_content_str[:100]}]"
    if _PARSER_PYDANTIC_AVAILABLE: return ParsedDocumentOutput(parsed_text=text, elements=[NarrativeTextElement(text=text)], original_metadata=original_metadata) 
    return {"parsed_text": text, "elements": [{"element_type":"narrative_text", "text":text}], "original_metadata": original_metadata}

def parse_txt_to_structured_output(txt_content_str: str, original_metadata: Dict[str, Any]) -> Optional[Union[ParsedDocumentOutput, Dict[str, Any]]]:
    logger.info(f"TXT parser for content length: {len(txt_content_str)}") 
    if _PARSER_PYDANTIC_AVAILABLE:
        return ParsedDocumentOutput(
            parsed_text=txt_content_str, 
            elements=[NarrativeTextElement(text=txt_content_str)], 
            original_metadata=original_metadata
        ) 
    return {
        "parsed_text": txt_content_str, 
        "elements": [{"element_type":"narrative_text", "text":txt_content_str}], 
        "original_metadata": original_metadata
    }


--------------------------------------------------------------------------------
æ–‡ä»¶: parsers/pdf_parser.py
--------------------------------------------------------------------------------
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/zhz_rag_pipeline/parsers/pdf_parser.py
import os
import logging
from typing import List, Dict, Any, Optional, Union
import re

try:
    import fitz  # PyMuPDF
    _PYMUPDF_AVAILABLE = True
    logging.info("Successfully imported PyMuPDF (fitz) for PDF parsing.")
except ImportError:
    logging.error("PyMuPDF (fitz) not found. PDF parsing will not be available.")
    _PYMUPDF_AVAILABLE = False
    # å ä½ç¬¦ï¼Œä»¥é˜² fitz æœªå®‰è£…æ—¶ä»£ç å°è¯•å¼•ç”¨å®ƒ
    class fitz: 
        class Document: pass
        class Page: pass
        class Rect: pass 

_PYDANTIC_MODELS_AVAILABLE_PDF = False
try:
    from ..pydantic_models_dagster import (
        ParsedDocumentOutput, DocumentElementType, NarrativeTextElement,
        DocumentElementMetadata, PageBreakElement
    )
    _PYDANTIC_MODELS_AVAILABLE_PDF = True
except ImportError:
    class BaseModel: pass
    class DocumentElementMetadata(BaseModel): page_number: Optional[int] = None
    class ParsedDocumentOutput(BaseModel): parsed_text: str; elements: list; original_metadata: dict; summary: Optional[str] = None
    class NarrativeTextElement(BaseModel): element_type:str="narrative_text"; text:str; metadata: Optional[DocumentElementMetadata] = None
    class PageBreakElement(BaseModel): element_type:str="page_break"; metadata: Optional[DocumentElementMetadata] = None
    DocumentElementType = Any

logger = logging.getLogger(__name__)

def _create_pdf_element_metadata(page_number: Optional[int] = None) -> Optional[Union[DocumentElementMetadata, Dict[str, Any]]]:
    meta_data_dict: Dict[str, Any] = {}
    if page_number is not None:
        meta_data_dict['page_number'] = page_number
    
    if not meta_data_dict:
        return None

    if _PYDANTIC_MODELS_AVAILABLE_PDF:
        return DocumentElementMetadata(**meta_data_dict)
    else:
        return meta_data_dict

def parse_pdf_to_structured_output(
    file_path: str, 
    original_metadata: Dict[str, Any]
) -> Optional[Union[ParsedDocumentOutput, Dict[str, Any]]]:
    logger.info(f"Attempting to parse PDF file: {file_path} using PyMuPDF (fitz)")
    if not _PYMUPDF_AVAILABLE:
        logger.error("PyMuPDF (fitz) is not available. PDF parsing cannot proceed.")
        return None

    elements: List[Any] = []
    full_text_parts: List[str] = []

    try:
        doc = fitz.open(file_path)
        logger.info(f"PyMuPDF opened PDF. Pages: {doc.page_count}")

        for page_num in range(doc.page_count):
            page = doc.load_page(page_num)
            
            # ä½¿ç”¨ "dict" æ¨¡å¼æå–æ–‡æœ¬å—ï¼Œå¹¶æŒ‰é˜…è¯»é¡ºåºæ’åº
            page_content_blocks = page.get_text("dict", sort=True).get("blocks", [])
            
            page_text_collected = []
            
            if page_content_blocks:
                for block in page_content_blocks:
                    if block['type'] == 0: # 0 è¡¨ç¤ºæ–‡æœ¬å—
                        block_text_lines = []
                        for line in block.get("lines", []):
                            line_content = "".join([span.get("text", "") for span in line.get("spans", [])])
                            block_text_lines.append(line_content)
                        block_text_content = "\n".join(block_text_lines).strip()
                        if block_text_content:
                            page_text_collected.append(block_text_content)
            
            if page_text_collected:
                # å°†æ•´é¡µçš„æ–‡æœ¬ä½œä¸ºä¸€ä¸ª NarrativeTextElement
                page_full_text = "\n\n".join(page_text_collected) # ç”¨åŒæ¢è¡Œç¬¦åˆ†éš”æ¥è‡ªä¸åŒå—çš„æ–‡æœ¬
                element_metadata = _create_pdf_element_metadata(page_number=page_num + 1)
                if _PYDANTIC_MODELS_AVAILABLE_PDF:
                    elements.append(NarrativeTextElement(text=page_full_text, metadata=element_metadata)) # type: ignore
                else:
                    elements.append({"element_type": "narrative_text", "text": page_full_text, "metadata": element_metadata})
                full_text_parts.append(page_full_text)
            
            # åœ¨æ¯é¡µï¼ˆé™¤äº†æœ€åä¸€é¡µï¼‰ä¹‹åæ·»åŠ ä¸€ä¸ª PageBreakElement
            if page_num < doc.page_count - 1:
                if _PYDANTIC_MODELS_AVAILABLE_PDF:
                    elements.append(PageBreakElement(metadata=_create_pdf_element_metadata(page_number=page_num + 1))) # type: ignore
                else:
                    elements.append({"element_type": "page_break", "metadata": _create_pdf_element_metadata(page_number=page_num + 1)})


        doc.close()
        
        linear_text = "\n\n--- Page Break ---\n\n".join(full_text_parts) # ç”¨ç‰¹æ®Šæ ‡è®°åˆ†éš”é¡µé¢æ–‡æœ¬
        linear_text = re.sub(r'\n{3,}', '\n\n', linear_text).strip()

        if _PYDANTIC_MODELS_AVAILABLE_PDF:
            return ParsedDocumentOutput(
                parsed_text=linear_text,
                elements=elements, # type: ignore
                original_metadata=original_metadata
            )
        else:
            return {
                "parsed_text": linear_text,
                "elements": elements,
                "original_metadata": original_metadata
            }

    except FileNotFoundError:
        logger.error(f"PDF file not found: {file_path}")
        return None
    except Exception as e:
        logger.error(f"Error parsing PDF file {file_path} with PyMuPDF: {e}", exc_info=True)
        return None


--------------------------------------------------------------------------------
æ–‡ä»¶: parsers/txt_parser.py
--------------------------------------------------------------------------------
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/zhz_rag_pipeline/parsers/txt_parser.py
import os
import logging
from typing import Dict, Any, Optional, Union

_PYDANTIC_MODELS_AVAILABLE_TXT = False
try:
    from ..pydantic_models_dagster import (
        ParsedDocumentOutput, NarrativeTextElement,
        DocumentElementMetadata
    )
    _PYDANTIC_MODELS_AVAILABLE_TXT = True
except ImportError:
    class BaseModel: pass
    class DocumentElementMetadata(BaseModel): page_number: Optional[int] = None # Not really applicable for txt
    class ParsedDocumentOutput(BaseModel): parsed_text: str; elements: list; original_metadata: dict; summary: Optional[str] = None
    class NarrativeTextElement(BaseModel): element_type:str="narrative_text"; text:str; metadata: Optional[DocumentElementMetadata] = None
    # DocumentElementType is not strictly needed here as we only create NarrativeTextElement

logger = logging.getLogger(__name__)

def parse_txt_to_structured_output(
    txt_content_str: str, # For .txt, we expect the content string directly
    original_metadata: Dict[str, Any]
) -> Optional[Union[ParsedDocumentOutput, Dict[str, Any]]]:
    logger.info(f"Attempting to parse TXT content (length: {len(txt_content_str)} chars)")

    # For .txt files, the entire content is treated as a single narrative text block.
    # No complex structure is assumed or extracted.
    
    elements = []
    element_metadata = None # No specific sub-element metadata for a single block txt file

    if _PYDANTIC_MODELS_AVAILABLE_TXT:
        elements.append(NarrativeTextElement(text=txt_content_str, metadata=element_metadata)) # type: ignore
        doc_output = ParsedDocumentOutput(
            parsed_text=txt_content_str,
            elements=elements, # type: ignore
            original_metadata=original_metadata
        )
    else:
        elements.append({"element_type": "narrative_text", "text": txt_content_str, "metadata": element_metadata})
        doc_output = {
            "parsed_text": txt_content_str,
            "elements": elements,
            "original_metadata": original_metadata
        }
    
    logger.info(f"Successfully processed TXT content into a single element.")
    return doc_output


--------------------------------------------------------------------------------
æ–‡ä»¶: parsers/xlsx_parser.py
--------------------------------------------------------------------------------
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/zhz_rag_pipeline/parsers/xlsx_parser.py
import os
import logging
from typing import List, Dict, Any, Optional, Union
import pandas as pd

# ç¡®ä¿å®‰è£…äº† 'pandas', 'openpyxl', 'tabulate'
# pip install pandas openpyxl tabulate

# --- Pydantic æ¨¡å‹å¯¼å…¥å’Œå ä½ç¬¦å®šä¹‰ (ä¿æŒä¸å˜) ---
_PYDANTIC_MODELS_AVAILABLE_XLSX = False
try:
    from ..pydantic_models_dagster import (
        ParsedDocumentOutput, TableElement, DocumentElementMetadata
    )
    _PYDANTIC_MODELS_AVAILABLE_XLSX = True
except ImportError:
    class BaseModel: pass
    class DocumentElementMetadata(BaseModel): page_number: Optional[int] = None
    class ParsedDocumentOutput(BaseModel): parsed_text: str; elements: list; original_metadata: dict; summary: Optional[str] = None
    class TableElement(BaseModel): element_type: str = "table"; markdown_representation: Optional[str] = None; html_representation: Optional[str] = None; caption: Optional[str] = None; metadata: Optional[DocumentElementMetadata] = None

logger = logging.getLogger(__name__)

# --- è¾…åŠ©å‡½æ•° (ä¿æŒä¸å˜) ---
def _create_xlsx_element_metadata(sheet_index: int, table_index_in_sheet: int) -> Optional[Union[DocumentElementMetadata, Dict[str, Any]]]:
    meta_data_dict: Dict[str, Any] = {
        'page_number': sheet_index, # ä½¿ç”¨ page_number è¡¨ç¤ºå·¥ä½œè¡¨ç´¢å¼•
        'custom_properties': {'table_index_in_sheet': table_index_in_sheet}
    }
    if _PYDANTIC_MODELS_AVAILABLE_XLSX:
        return DocumentElementMetadata(**meta_data_dict)
    else:
        return meta_data_dict

# --- ä¸»è§£æå‡½æ•° (å…¨æ–°ç‰ˆæœ¬) ---
def parse_xlsx_to_structured_output(
    file_path: str,
    original_metadata: Dict[str, Any]
) -> Optional[Union[ParsedDocumentOutput, Dict[str, Any]]]:
    """
    V2: è§£æXLSXæ–‡ä»¶ï¼Œæ™ºèƒ½è¯†åˆ«å¹¶æå–æ¯ä¸ªå·¥ä½œè¡¨å†…çš„å¤šä¸ªç‹¬ç«‹è¡¨æ ¼ã€‚
    """
    logger.info(f"Attempting to parse XLSX file with multi-table support: {file_path}")
    
    try:
        xls = pd.ExcelFile(file_path)
    except Exception as e:
        logger.error(f"Failed to open Excel file {file_path}: {e}", exc_info=True)
        return None

    all_elements: List[Any] = []
    
    for sheet_idx, sheet_name in enumerate(xls.sheet_names):
        try:
            # è¯»å–æ•´ä¸ªå·¥ä½œè¡¨ï¼Œä¸æŒ‡å®šè¡¨å¤´ï¼Œä»¥ä¾¿æˆ‘ä»¬æ‰‹åŠ¨æŸ¥æ‰¾
            df_full = pd.read_excel(xls, sheet_name=sheet_name, header=None)
            
            if df_full.empty:
                logger.info(f"Sheet '{sheet_name}' is empty. Skipping.")
                continue

            # é€šè¿‡æŸ¥æ‰¾ç©ºè¡Œæ¥è¯†åˆ«è¡¨æ ¼å—
            is_row_empty = df_full.isnull().all(axis=1)
            # è·å–ç©ºè¡Œçš„ç´¢å¼•
            empty_row_indices = is_row_empty[is_row_empty].index.tolist()
            
            table_blocks: List[pd.DataFrame] = []
            last_split_index = -1
            
            for empty_idx in empty_row_indices:
                block_df = df_full.iloc[last_split_index + 1 : empty_idx].dropna(how='all')
                if not block_df.empty:
                    table_blocks.append(block_df)
                last_split_index = empty_idx
            
            # æ·»åŠ æœ€åä¸€ä¸ªå—ï¼ˆä»æœ€åä¸€ä¸ªç©ºè¡Œåˆ°æœ«å°¾ï¼‰
            final_block_df = df_full.iloc[last_split_index + 1 :].dropna(how='all')
            if not final_block_df.empty:
                table_blocks.append(final_block_df)

            logger.info(f"Sheet '{sheet_name}' was split into {len(table_blocks)} potential table blocks.")

            for table_idx, block_df in enumerate(table_blocks):
                # å°†ç¬¬ä¸€è¡Œä½œä¸ºè¡¨å¤´
                header = block_df.iloc[0]
                table_data = block_df[1:]
                table_data.columns = header
                
                md_representation = table_data.to_markdown(index=False)
                table_caption = f"å†…å®¹æ¥è‡ªæ–‡ä»¶ '{os.path.basename(file_path)}' çš„å·¥ä½œè¡¨ '{sheet_name}' (è¡¨æ ¼ {table_idx + 1})"
                
                element_metadata = _create_xlsx_element_metadata(
                    sheet_index=sheet_idx, 
                    table_index_in_sheet=table_idx
                )

                if _PYDANTIC_MODELS_AVAILABLE_XLSX:
                    table_el = TableElement(
                        markdown_representation=md_representation,
                        caption=table_caption,
                        metadata=element_metadata
                    )
                    all_elements.append(table_el)
                else:
                    all_elements.append({
                        "element_type": "table",
                        "markdown_representation": md_representation,
                        "caption": table_caption,
                        "metadata": element_metadata
                    })
                logger.info(f"  Successfully created TableElement for table {table_idx+1} in sheet '{sheet_name}'.")
                logger.debug(f"    - Table {table_idx+1} content preview: {md_representation[:200].replace(chr(10), ' ')}...")


        except Exception as e_sheet:
            logger.error(f"Failed to process sheet '{sheet_name}' in {file_path}: {e_sheet}", exc_info=True)
            continue
    
    if _PYDANTIC_MODELS_AVAILABLE_XLSX:
        return ParsedDocumentOutput(
            parsed_text="",
            elements=all_elements,
            original_metadata=original_metadata,
            summary=f"ä»æ–‡ä»¶ '{os.path.basename(file_path)}' ä¸­è§£æäº† {len(all_elements)} ä¸ªç‹¬ç«‹çš„è¡¨æ ¼ã€‚"
        )
    else:
        return {
            "parsed_text": "",
            "elements": all_elements,
            "original_metadata": original_metadata,
            "summary": f"ä»æ–‡ä»¶ '{os.path.basename(file_path)}' ä¸­è§£æäº† {len(all_elements)} ä¸ªç‹¬ç«‹çš„è¡¨æ ¼ã€‚"
        }


