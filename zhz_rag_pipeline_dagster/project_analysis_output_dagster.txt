项目分析输出
================================================================================

--- 目录结构树 ---
📂 zhz_rag_pipeline_dagster/
├── 📂 parsers/
│   ├── 📄 __init__.py
│   ├── 📄 docx_parser.py
│   ├── 📄 html_parser.py
│   ├── 📄 md_parser.py
│   ├── 📄 pdf_parser.py
│   ├── 📄 txt_parser.py
│   └── 📄 xlsx_parser.py
└── 📂 zhz_rag_pipeline/
    ├── 📄 __init__.py
    ├── 📄 custom_io_managers.py
    ├── 📄 definitions.py
    ├── 📄 document_parsers.py
    ├── 📄 evaluation_assets.py
    ├── 📄 ingestion_assets.py
    ├── 📄 processing_assets.py
    ├── 📄 pydantic_models_dagster.py
    └── 📄 resources.py


--- 文件内容 ---
--------------------------------------------------------------------------------
文件: zhz_rag_pipeline/__init__.py
--------------------------------------------------------------------------------
# zhz_rag_pipeline/__init__.py
# This file makes Python treat the directory as a package.
# You can also define a __version__ or import key modules here if needed.


--------------------------------------------------------------------------------
文件: zhz_rag_pipeline/custom_io_managers.py
--------------------------------------------------------------------------------
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/zhz_rag_pipeline/custom_io_managers.py
import json
import os
from typing import List, Type, Union, get_args, get_origin, Any, Optional 
from dagster import UPathIOManager, InputContext, OutputContext, DagsterInvariantViolationError
from pydantic import BaseModel as PydanticBaseModel
from upath import UPath

class PydanticListJsonIOManager(UPathIOManager):
    extension: str = ".jsonl"

    def __init__(self, base_dir: Optional[str] = None): # Changed base_path to base_dir for clarity
        resolved_base_dir: UPath
        if base_dir:
            resolved_base_dir = UPath(base_dir).resolve() # Resolve to absolute path
        else:
            # Default to <DAGSTER_HOME>/storage/pydantic_jsonl_io
            # DAGSTER_HOME defaults to ~/.dagster, but can be overridden by env var
            dagster_home_str = os.getenv("DAGSTER_HOME", os.path.join(os.path.expanduser("~"), ".dagster"))
            resolved_base_dir = UPath(dagster_home_str) / "storage" / "pydantic_jsonl_io"
        
        # Ensure the directory exists
        try:
            resolved_base_dir.mkdir(parents=True, exist_ok=True)
        except Exception as e:
            # Log this error appropriately, perhaps using a direct print if logger isn't set up yet
            # or re-raise as a Dagster-specific error.
            print(f"[PydanticListJsonIOManager __init__] ERROR: Could not create bafef __init__(self, base_dir: Optse directory {resolved_base_dir}: {e}")
            # Depending on Dagster's init sequence, context.log might not be available here.
            # It's safer to let UPathIOManager handle its own base_path or ensure dir exists before.
            # For now, we proceed, UPathIOManager might handle it or fail later.

        super().__init__(base_path=resolved_base_dir)
        # Log the final base path used by the UPathIOManager instance
        # self.log available after super().__init__() in ConfigurableIOManager context
        # For direct instantiation, we might need to pass a logger or use a global one.
        # print(f"[PydanticListJsonIOManager __init__] Initialized with resolved base_path: {self.base_path}")


    def dump_to_path(self, context: OutputContext, obj: List[PydanticBaseModel], path: UPath):
        context.log.info(f"[PydanticListJsonIOManager dump_to_path] Attempting to dump to resolved path: {path.resolve()}")
        
        if not isinstance(obj, list):
            msg = f"Expected a list of Pydantic models, got {type(obj)}"
            context.log.error(msg)
            raise TypeError(msg)
        
        # Optional: More robust type checking for list items if needed, using context.dagster_type
        # For now, assume obj is List[PydanticBaseModel] based on upstream asset's type hint.

        try:
            with path.open("w", encoding="utf-8") as f:
                for model_instance in obj:
                    if not isinstance(model_instance, PydanticBaseModel):
                        context.log.warning(f"Item in list is not a Pydantic model: {type(model_instance)}. Skipping.")
                        continue
                    json_str = model_instance.json() # Pydantic V1
                    f.write(json_str + "\n")
            context.log.info(f"[PydanticListJsonIOManager dump_to_path] Successfully dumped {len(obj)} items to {path.resolve()}")
        except Exception as e:
            context.log.error(f"[PydanticListJsonIOManager dump_to_path] Failed to dump object to {path.resolve()}: {e}", exc_info=True)
            raise

    def load_from_path(self, context: InputContext, path: UPath) -> List[PydanticBaseModel]:
        context.log.info(f"[PydanticListJsonIOManager load_from_path] Attempting to load from resolved path: {path.resolve()}")
        
        list_typing_type = context.dagster_type.typing_type
        origin = get_origin(list_typing_type)
        args = get_args(list_typing_type)

        if not (origin is list and args and issubclass(args[0], PydanticBaseModel)):
            msg = (
                f"PydanticListJsonIOManager can only handle inputs of type List[PydanticModel], "
                f"but got {list_typing_type} for input '{context.name}'."
            )
            context.log.error(msg)
            raise DagsterInvariantViolationError(msg) # Use Dagster specific error
        
        model_type: Type[PydanticBaseModel] = args[0]
        context.log.info(f"[PydanticListJsonIOManager load_from_path] Target model type for list items: {model_type.__name__}")

        loaded_models: List[PydanticBaseModel] = []
        if not path.exists():
            context.log.warning(f"[PydanticListJsonIOManager load_from_path] File not found at {path.resolve()}, returning empty list for input '{context.name}'.")
            return loaded_models

        try:
            with path.open("r", encoding="utf-8") as f:
                for line_number, line in enumerate(f, 1):
                    line_content = line.strip()
                    if not line_content:
                        continue
                    try:
                        model_instance = model_type.parse_raw(line_content) # Pydantic V1
                        loaded_models.append(model_instance)
                    except Exception as e_parse:
                        context.log.error(
                            f"[PydanticListJsonIOManager load_from_path] Failed to parse JSON line {line_number} "
                            f"into {model_type.__name__} from {path.resolve()}: {e_parse}. "
                            f"Line content (first 100 chars): '{line_content[:100]}...'",
                            exc_info=True
                        )
                        # Optionally re-raise or decide to skip problematic lines
                        # For now, we'll skip
            context.log.info(f"[PydanticListJsonIOManager load_from_path] Successfully loaded {len(loaded_models)} instances of {model_type.__name__} from {path.resolve()}")
        except Exception as e_read:
            context.log.error(f"[PydanticListJsonIOManager load_from_path] Failed to read or process file {path.resolve()}: {e_read}", exc_info=True)
            raise # Re-raise if file reading itself fails catastrophically
            
        return loaded_models


--------------------------------------------------------------------------------
文件: zhz_rag_pipeline/definitions.py
--------------------------------------------------------------------------------
import dagster as dg
import os
from dagster import Definitions

from zhz_rag_pipeline_dagster.zhz_rag_pipeline.ingestion_assets import all_ingestion_assets
from zhz_rag_pipeline_dagster.zhz_rag_pipeline.processing_assets import all_processing_assets

from zhz_rag_pipeline_dagster.zhz_rag_pipeline.resources import (
    GGUFEmbeddingResource,
    ChromaDBResource,
    LocalLLMAPIResource,
    DuckDBResource,
    GeminiAPIResource,
    SystemResource
)
from zhz_rag_pipeline_dagster.zhz_rag_pipeline.custom_io_managers import PydanticListJsonIOManager

# --- 根据官方文档，将所有资产组合在一起 ---
all_assets = all_ingestion_assets + all_processing_assets

# --- 根据官方文档，定义一个包含所有资源的字典 ---
# Dagster 会自动为每个资产提供它所需要的资源
all_resources = {
    # IO 管理器，键名必须是 "io_manager" 才能被默认使用
    "io_manager": PydanticListJsonIOManager(),
    
    # 其他应用级资源
    "embedder": GGUFEmbeddingResource(
        api_url=os.getenv("EMBEDDING_API_URL", "http://127.0.0.1:8089")
    ),
    "chroma_db": ChromaDBResource(
        collection_name=os.getenv("CHROMA_COLLECTION_NAME", "zhz_rag_collection"),
        persist_directory=os.path.join(os.getenv("ZHZ_AGENT_PROJECT_ROOT", "/home/zhz/zhz_agent"), "zhz_rag", "stored_data", "chromadb_index")
    ),
    "LocalLLM_api": LocalLLMAPIResource(
        api_url="http://127.0.0.1:8088/v1/chat/completions",
        default_temperature=0.1,
        default_max_new_tokens=2048
    ),
    "duckdb_kg": DuckDBResource(
        db_file_path=os.path.join(os.getenv("ZHZ_AGENT_PROJECT_ROOT", "/home/zhz/zhz_agent"), "zhz_rag", "stored_data", "duckdb_knowledge_graph.db")
    ),
    "gemini_api": GeminiAPIResource(
        model_name="gemini/gemini-1.5-flash-latest",
        proxy_url=os.getenv("LITELLM_PROXY_URL"),
        default_temperature=0.1,
        default_max_tokens=2048
    ),
    "system_resource": SystemResource()
}

# --- 创建最终的、简洁的 Definitions 对象 ---
defs = Definitions(
    assets=all_assets,
    resources=all_resources
)


--------------------------------------------------------------------------------
文件: zhz_rag_pipeline/document_parsers.py
--------------------------------------------------------------------------------
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/zhz_rag_pipeline/document_parsers.py
import os
from markdown_it import MarkdownIt
import logging
from typing import List, Dict, Any, Optional, Union, Literal 
from bs4 import BeautifulSoup
import pandas as pd

# --- 添加 Unstructured 的导入 ---
try:
    from unstructured.partition.docx import partition_docx
    from unstructured.documents.elements import Element as UnstructuredElement, Title, NarrativeText, ListItem, Table, Image, Header, Footer
    _UNSTRUCTURED_AVAILABLE = True
    print("INFO (document_parsers.py): Successfully imported Unstructured for DOCX.")
except ImportError:
    print("WARNING (document_parsers.py): Unstructured library not found. DOCX parsing will be a placeholder.")
    _UNSTRUCTURED_AVAILABLE = False
    class UnstructuredElement: pass # Dummy
# --- 结束添加 ---

# --- 导入我们定义的Pydantic模型 ---
# 假设这个文件和 pydantic_models_dagster.py 在同一个包下或能通过PYTHONPATH找到
try:
    from .pydantic_models_dagster import ( # 使用相对导入
        ParsedDocumentOutput,
        DocumentElementType, 
        TitleElement,
        NarrativeTextElement,
        ListItemElement,
        TableElement,
        CodeBlockElement,
        PageBreakElement,
        DocumentElementMetadata 
    )
    _PYDANTIC_MODELS_AVAILABLE_PARSERS = True
except ImportError:
    print("WARNING (document_parsers.py): Could not import Pydantic models. Using fallback Any/dict.")
    _PYDANTIC_MODELS_AVAILABLE_PARSERS = False
    class BaseModel: pass
    class DocumentElementMetadata(BaseModel): page_number: Optional[int] = None
    class ParsedDocumentOutput(BaseModel): parsed_text: str; elements: list; original_metadata: dict; summary: Optional[str] = None
    class TitleElement(BaseModel): element_type:str="title"; text:str; level:int; metadata: Optional[DocumentElementMetadata] = None
    class NarrativeTextElement(BaseModel): element_type:str="narrative_text"; text:str; metadata: Optional[DocumentElementMetadata] = None
    class ListItemElement(BaseModel): element_type:str="list_item"; text:str; level:int=0; ordered:bool=False; item_number:Optional[Union[int, str]]=None; metadata: Optional[DocumentElementMetadata] = None
    class TableElement(BaseModel): element_type:str="table"; markdown_representation:Optional[str]=None; metadata: Optional[DocumentElementMetadata] = None
    class CodeBlockElement(BaseModel): element_type:str="code_block"; code:str; language:Optional[str]=None; metadata: Optional[DocumentElementMetadata] = None
    class PageBreakElement(BaseModel): element_type:str="page_break"; metadata: Optional[DocumentElementMetadata] = None
    DocumentElementType = Any


logger = logging.getLogger(__name__) # 每个模块用自己的logger

# --- Markdown 解析逻辑 (从 poc_md_markdown_it.py 迁移并封装) ---



def _get_text_from_md_inline(inline_tokens: Optional[List[Any]]) -> str:
    # (这里是 get_text_from_inline_tokens 函数的完整代码)
    text_content = ""
    if inline_tokens is None: return ""
    for token in inline_tokens:
        if token.type == 'text':
            text_content += token.content
        elif token.type == 'code_inline':
            text_content += f"`{token.content}`"
        elif token.type == 'softbreak':
            text_content += ' ' 
        elif token.type == 'hardbreak':
            text_content += '\n'
        elif token.children: 
            text_content += _get_text_from_md_inline(token.children)
    return text_content

def _convert_md_tokens_to_elements_internal(tokens: list) -> List[Any]:
    # (这里是 convert_md_tokens_to_elements 函数的完整代码，但将其重命名为内部函数)
    # (并确保它在 _PYDANTIC_MODELS_AVAILABLE_PARSERS 为True时创建Pydantic实例，否则创建字典)
    elements: List[Any] = []
    idx = 0
    list_level_stack = [] 

    while idx < len(tokens):
        token = tokens[idx]

        if token.type == 'heading_open':
            level = int(token.tag[1:])
            idx_content = idx + 1
            text = ""
            if idx_content < len(tokens) and tokens[idx_content].type == 'inline':
                text = _get_text_from_md_inline(tokens[idx_content].children).strip()
            if _PYDANTIC_MODELS_AVAILABLE_PARSERS: elements.append(TitleElement(text=text, level=level))
            else: elements.append({"element_type": "title", "text": text, "level": level})
            idx = idx_content + 2 
            continue

        elif token.type == 'paragraph_open':
            is_list_item_para = False
            if list_level_stack and token.level >= list_level_stack[-1]["level"]:
                pass 
            if not is_list_item_para or not list_level_stack: 
                idx_content = idx + 1
                text = ""
                if idx_content < len(tokens) and tokens[idx_content].type == 'inline':
                    text = _get_text_from_md_inline(tokens[idx_content].children).strip()
                if text:
                    if _PYDANTIC_MODELS_AVAILABLE_PARSERS: elements.append(NarrativeTextElement(text=text))
                    else: elements.append({"element_type": "narrative_text", "text": text})
            idx = idx + 2 
            if idx < len(tokens) and tokens[idx-1].type == 'inline': 
                idx +=1 
            continue
        
        elif token.type == 'bullet_list_open':
            list_level_stack.append({"ordered": False, "level": token.level})
            idx += 1
            continue
        elif token.type == 'ordered_list_open':
            start_num = token.attrs.get('start', 1)
            list_level_stack.append({"ordered": True, "current_num": start_num, "level": token.level})
            idx += 1
            continue
        
        elif token.type == 'list_item_open':
            item_text = ""
            li_level = token.level
            next_token_idx = idx + 1
            if next_token_idx < len(tokens):
                next_token = tokens[next_token_idx]
                if next_token.type == 'paragraph_open' and next_token.level == li_level + 1 :
                    inline_idx = next_token_idx + 1
                    if inline_idx < len(tokens) and tokens[inline_idx].type == 'inline':
                        item_text = _get_text_from_md_inline(tokens[inline_idx].children).strip()
                elif next_token.type == 'inline' and next_token.level == li_level +1 :
                    item_text = _get_text_from_md_inline(next_token.children).strip()
            
            if list_level_stack:
                list_info = list_level_stack[-1]
                item_num_val = None
                if list_info["ordered"]:
                    item_num_val = list_info["current_num"]
                    list_info["current_num"] += 1
                
                if _PYDANTIC_MODELS_AVAILABLE_PARSERS:
                    elements.append(ListItemElement(
                        text=item_text, level=token.level, ordered=list_info["ordered"],
                        item_number=str(item_num_val) if item_num_val is not None else None))
                else:
                    elements.append({"element_type": "list_item", "text": item_text, "level":token.level, 
                                     "ordered":list_info["ordered"], "item_number":str(item_num_val) if item_num_val is not None else None})

            temp_idx = idx + 1; nesting_count = 0
            while temp_idx < len(tokens):
                if tokens[temp_idx].type == 'list_item_open' and tokens[temp_idx].level == li_level:
                    if nesting_count == 0: idx = temp_idx; break
                if tokens[temp_idx].type == 'list_item_open': nesting_count +=1
                if tokens[temp_idx].type == 'list_item_close':
                    if nesting_count == 0 and tokens[temp_idx].level == li_level: idx = temp_idx + 1; break
                    nesting_count -=1
                temp_idx += 1
            else: idx = temp_idx
            continue

        elif token.type in ['bullet_list_close', 'ordered_list_close']:
            if list_level_stack: list_level_stack.pop()
            idx += 1
            continue

        elif token.type == 'table_open':
            header_content = []; body_rows_cells = []; current_row_cells = []; in_thead = False
            temp_idx = idx + 1
            while temp_idx < len(tokens) and tokens[temp_idx].type != 'table_close':
                t_token = tokens[temp_idx]
                if t_token.type == 'thead_open': in_thead = True
                elif t_token.type == 'thead_close': in_thead = False
                elif t_token.type == 'tr_open': current_row_cells = []
                elif t_token.type in ['th_open', 'td_open']:
                    content_idx = temp_idx + 1
                    if content_idx < len(tokens) and tokens[content_idx].type == 'inline':
                        current_row_cells.append(_get_text_from_md_inline(tokens[content_idx].children).strip())
                elif t_token.type == 'tr_close':
                    if current_row_cells:
                        if in_thead or (not header_content and not body_rows_cells): header_content.append(list(current_row_cells))
                        else: body_rows_cells.append(list(current_row_cells))
                temp_idx += 1
            md_table_str = ""
            if header_content:
                md_table_str += "| " + " | ".join(header_content[0]) + " |\n"
                md_table_str += "| " + " | ".join(["---"] * len(header_content[0])) + " |\n"
            for row_data_list in body_rows_cells: md_table_str += "| " + " | ".join(row_data_list) + " |\n"
            if _PYDANTIC_MODELS_AVAILABLE_PARSERS: elements.append(TableElement(markdown_representation=md_table_str.strip()))
            else: elements.append({"element_type": "table", "markdown_representation": md_table_str.strip()})
            idx = temp_idx + 1 
            continue

        elif token.type == 'fence' or token.type == 'code_block':
            code_content = token.content.strip(); lang = token.info.strip() if token.info else None
            if _PYDANTIC_MODELS_AVAILABLE_PARSERS: elements.append(CodeBlockElement(code=code_content, language=lang))
            else: elements.append({"element_type": "code_block", "code": code_content, "language": lang})
            idx += 1
            continue
        
        elif token.type == 'hr':
            if _PYDANTIC_MODELS_AVAILABLE_PARSERS: elements.append(PageBreakElement())
            else: elements.append({"element_type": "page_break"})
            idx += 1
            continue
        
        elif token.type == 'blockquote_open':
            blockquote_text_parts = []; temp_idx = idx + 1; start_level = token.level
            while temp_idx < len(tokens):
                bq_token = tokens[temp_idx]
                if bq_token.type == 'blockquote_close' and bq_token.level == start_level: idx = temp_idx; break
                if bq_token.type == 'paragraph_open':
                    para_content_idx = temp_idx + 1
                    if para_content_idx < len(tokens) and tokens[para_content_idx].type == 'inline':
                        blockquote_text_parts.append(_get_text_from_md_inline(tokens[para_content_idx].children).strip())
                    temp_idx = para_content_idx + 1 
                    if temp_idx < len(tokens) and tokens[temp_idx].type == 'paragraph_close': temp_idx +=1
                    else: temp_idx -=1
                temp_idx +=1
            else: idx = temp_idx
            if blockquote_text_parts:
                full_text = "\n".join(blockquote_text_parts)
                if _PYDANTIC_MODELS_AVAILABLE_PARSERS: elements.append(NarrativeTextElement(text=full_text)) 
                else: elements.append({"element_type": "narrative_text", "text": full_text, "_is_blockquote": True})
            idx +=1
            continue
        idx += 1 
    return elements

def _generate_parsed_text_from_elements_internal(elements: List[Any]) -> str:
    # (这里是 generate_parsed_text_from_elements 函数的完整代码)
    # (确保它在 _PYDANTIC_MODELS_AVAILABLE_PARSERS 为True时能处理Pydantic实例，否则处理字典)
    text_parts = []
    for el_data_any in elements:
        el_data = {}
        if _PYDANTIC_MODELS_AVAILABLE_PARSERS and hasattr(el_data_any, 'model_dump'):
            el_data = el_data_any.model_dump() 
        elif isinstance(el_data_any, dict):
            el_data = el_data_any
        else: continue

        el_type = el_data.get("element_type")
        if el_type == "title": text_parts.append(f"\n{'#' * el_data.get('level',1)} {el_data.get('text','')}\n")
        elif el_type == "narrative_text": text_parts.append(el_data.get('text','') + "\n")
        elif el_type == "list_item":
            prefix = f"{el_data.get('item_number','')}. " if el_data.get('ordered') and el_data.get('item_number') else "- "
            indent = "  " * el_data.get('level',0)
            text_parts.append(f"{indent}{prefix}{el_data.get('text','')}\n")
        elif el_type == "table":
            if el_data.get('markdown_representation'): text_parts.append(f"\n[Table: {el_data.get('caption','Unnamed Table')}]\n{el_data['markdown_representation']}\n")
            elif el_data.get('text_representation'): text_parts.append(f"\n[Table: {el_data.get('caption','Unnamed Table')}]\n{el_data['text_representation']}\n")
        elif el_type == "code_block":
            lang = el_data.get('language', "") or ""
            text_parts.append(f"\n```{lang}\n{el_data.get('code','')}\n```\n")
        elif el_type == "page_break": text_parts.append("\n---\n")
        text_parts.append("\n") 
    return "".join(text_parts).strip().replace("\n\n\n", "\n\n").replace("\n\n\n", "\n\n")

def _convert_unstructured_to_pydantic(elements: List[UnstructuredElement]) -> List[DocumentElementType]:  # type: ignore
    """Converts a list of Unstructured elements to our internal Pydantic models."""
    pydantic_elements = []
    for el in elements:
        meta = DocumentElementMetadata(page_number=getattr(el.metadata, 'page_number', None))
        
        if isinstance(el, Title):
            pydantic_elements.append(TitleElement(text=el.text, level=getattr(el.metadata, 'category_depth', 1), metadata=meta))
        elif isinstance(el, NarrativeText):
            pydantic_elements.append(NarrativeTextElement(text=el.text, metadata=meta))
        elif isinstance(el, ListItem):
            pydantic_elements.append(ListItemElement(text=el.text, metadata=meta))
        elif isinstance(el, Table):
            # Unstructured v0.12+ has built-in markdown conversion
            pydantic_elements.append(TableElement(markdown_representation=getattr(el, 'text_as_html', str(el)), metadata=meta))
        elif isinstance(el, (Header, Footer, Image)):
             # We can choose to ignore headers, footers, and images for now
             continue
        else:
            # Fallback for any other element types
            if el.text.strip():
                 pydantic_elements.append(NarrativeTextElement(text=el.text, metadata=meta))
                 
    return pydantic_elements


def parse_markdown_to_structured_output(md_content_str: str, original_metadata: Dict[str, Any]) -> Optional[ParsedDocumentOutput]:
    """
    Top-level function to parse markdown string and return ParsedDocumentOutput.
    """
    logger.info(f"Parsing Markdown content (length: {len(md_content_str)} chars)...")
    try:
        md_parser = MarkdownIt("commonmark").enable("table") # Removed "breaks":True based on last log
        tokens = md_parser.parse(md_content_str)
        
        structured_elements = _convert_md_tokens_to_elements_internal(tokens)
        linear_text = _generate_parsed_text_from_elements_internal(structured_elements)

        if _PYDANTIC_MODELS_AVAILABLE_PARSERS:
            return ParsedDocumentOutput(
                parsed_text=linear_text,
                elements=structured_elements,
                original_metadata=original_metadata
            )
        else: # Fallback if Pydantic models aren't available (e.g. PoC context)
            return {
                 "parsed_text": linear_text,
                 "elements": structured_elements,
                 "original_metadata": original_metadata
            } # type: ignore 
    except Exception as e:
        logger.error(f"Error in parse_markdown_to_structured_output: {e}", exc_info=True)
        return None

# --- Placeholder for other parsers ---
def parse_docx_to_structured_output(file_path: str, original_metadata: Dict[str, Any]) -> Optional[ParsedDocumentOutput]:
    """
    Parses a DOCX file using Unstructured.io, extracts rich metadata,
    and converts elements to our internal Pydantic models.
    """
    if not _UNSTRUCTURED_AVAILABLE:
        logger.error("Unstructured library is not available. Cannot parse DOCX files.")
        return None
        
    logger.info(f"Parsing DOCX with Unstructured: {file_path}")
    
    try:
        # 使用 Unstructured 解析文档，设置策略以获取更干净的数据
        unstructured_elements = partition_docx(
            filename=file_path,
            strategy="hi_res",  # 使用高分辨率策略以更好地处理布局
            infer_table_structure=True, # 开启表格结构推断
        )
        
        # --- 关键：合并 Unstructured 提取的元数据 ---
        # partition_docx 返回的第一个元素通常包含文档级别的元数据
        doc_level_meta = {}
        if unstructured_elements:
             # Unstructured v0.12+ 将元数据附加到每个元素上
             # 我们从第一个元素获取通用元数据
             doc_level_meta = unstructured_elements[0].metadata.to_dict()

        # 将 Unstructured 的元数据与我们传入的原始元数据合并
        # Unstructured 的元数据优先级更高，因为它更具体
        combined_metadata = {**original_metadata, **doc_level_meta}
        # 移除一些不需要的内部键
        combined_metadata.pop('parent_id', None)
        combined_metadata.pop('category_depth', None)

        # 将 Unstructured 元素转换为我们自己的 Pydantic 模型
        pydantic_elements = _convert_unstructured_to_pydantic(unstructured_elements)
        
        if not pydantic_elements:
            logger.warning(f"No content elements were extracted from DOCX file: {file_path}")
            return None

        # 从转换后的元素生成线性文本表示
        linear_text = _generate_parsed_text_from_elements_internal(pydantic_elements)

        if _PYDANTIC_MODELS_AVAILABLE_PARSERS:
            return ParsedDocumentOutput(
                parsed_text=linear_text,
                elements=pydantic_elements,
                original_metadata=combined_metadata # <--- 使用合并后的元数据
            )
        else:
            # Fallback for non-pydantic environment (should not happen in production)
            return {
                "parsed_text": linear_text,
                "elements": pydantic_elements,
                "original_metadata": combined_metadata
            } # type: ignore
            
    except Exception as e:
        logger.error(f"Error parsing DOCX file '{file_path}' with Unstructured: {e}", exc_info=True)
        return None
    
def parse_pdf_to_structured_output(file_path: str, original_metadata: Dict[str, Any]) -> Optional[ParsedDocumentOutput]:
    logger.info(f"Parsing PDF: {file_path} (Not yet fully implemented in document_parsers.py)")
    # Here you would integrate the PyMuPDF logic from your PoC
    text_content = f"[Placeholder: PDF content for {os.path.basename(file_path)}]"
    elements = []
    if _PYDANTIC_MODELS_AVAILABLE_PARSERS:
        elements.append(NarrativeTextElement(text=text_content))
        return ParsedDocumentOutput(parsed_text=text_content, elements=elements, original_metadata=original_metadata)
    else:
        elements.append({"element_type":"narrative_text", "text":text_content})
        return {"parsed_text":text_content, "elements":elements, "original_metadata":original_metadata} # type: ignore

def parse_xlsx_to_structured_output(file_path: str, original_metadata: Dict[str, Any]) -> Optional[ParsedDocumentOutput]:
    """
    Parses an XLSX file using pandas, converts each sheet to a Markdown table,
    and enriches metadata with filename and sheet name.
    """
    logger.info(f"Parsing XLSX with pandas: {file_path}")
    
    try:
        xls = pd.ExcelFile(file_path)
        all_elements: List[DocumentElementType] = [] # type: ignore
        full_text_representation = ""

        # --- 核心修正：确保filename在元数据中 ---
        # 无论上游是否提供，我们在这里都以文件路径为准，强制添加/覆盖
        base_metadata = original_metadata.copy()
        base_metadata['filename'] = os.path.basename(file_path)

        for sheet_name in xls.sheet_names:
            try:
                df = pd.read_excel(xls, sheet_name=sheet_name)

                # --- 新增：在处理前，清洗所有字符串类型单元格的前后空格 ---
                for col in df.columns:
                    if df[col].dtype == 'object':
                        df[col] = df[col].str.strip()
                # --- 新增结束 ---

                # 过滤掉完全为空的行和列，避免无效的Markdown输出
                df.dropna(how='all', axis=0, inplace=True)
                df.dropna(how='all', axis=1, inplace=True)

                if df.empty:
                    logger.info(f"  Skipping empty sheet: {sheet_name}")
                    continue
                
                # 将DataFrame转换为Markdown格式的字符串
                markdown_table = df.to_markdown(index=False)
                
                # 为每个工作表创建一个标题和一个表格元素
                sheet_title_text = f"Sheet: {sheet_name}"
                full_text_representation += f"## {sheet_title_text}\n\n{markdown_table}\n\n"
                
                # 为工作表标题创建元素
                all_elements.append(TitleElement(
                    text=sheet_title_text,
                    level=2,
                    metadata=DocumentElementMetadata(page_number=xls.sheet_names.index(sheet_name) + 1)
                ))
                
                # 为表格本身创建元素
                sheet_meta = base_metadata.copy()
                sheet_meta['sheet_name'] = sheet_name
                
                all_elements.append(TableElement(
                    markdown_representation=markdown_table,
                    metadata=DocumentElementMetadata(**sheet_meta) # 传递特定于工作表的元数据
                ))
                logger.info(f"  Successfully parsed sheet: {sheet_name}")

            except Exception as e_sheet:
                logger.error(f"  Failed to parse sheet '{sheet_name}' in file '{file_path}': {e_sheet}", exc_info=True)
                continue

        if not all_elements:
            logger.warning(f"No data parsed from any sheet in XLSX file: {file_path}")
            return None
        
        # 返回包含所有工作表内容的单个 ParsedDocumentOutput
        return ParsedDocumentOutput(
            parsed_text=full_text_representation,
            elements=all_elements,
            original_metadata=base_metadata # 返回包含正确filename的文档级元数据
        )

    except Exception as e:
        logger.error(f"Error parsing XLSX file '{file_path}': {e}", exc_info=True)
        return None

def parse_html_to_structured_output(html_content_str: str, original_metadata: Dict[str, Any]) -> Optional[ParsedDocumentOutput]:
    """
    Parses an HTML string using BeautifulSoup, extracts meaningful elements,
    and converts them to our internal Pydantic models.
    """
    logger.info(f"Parsing HTML content with BeautifulSoup (length: {len(html_content_str)} chars)...")
    if not html_content_str.strip():
        return None

    try:
        soup = BeautifulSoup(html_content_str, 'lxml')
        
        for script_or_style in soup(["script", "style"]):
            script_or_style.decompose()

        elements: List[DocumentElementType] = [] # type: ignore
        
        for tag in soup.find_all(['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'p', 'div', 'li', 'pre']):
            text = tag.get_text(separator=' ', strip=True)
            if text:
                if tag.name.startswith('h'):
                    level = int(tag.name[1:])
                    elements.append(TitleElement(text=text, level=level))
                elif tag.name == 'li':
                    elements.append(ListItemElement(text=text))
                elif tag.name == 'pre':
                    elements.append(CodeBlockElement(code=text))
                else:
                    elements.append(NarrativeTextElement(text=text))
        
        for table in soup.find_all('table'):
            header = [th.get_text(strip=True) for th in table.find_all('th')]
            rows = []
            for tr in table.find_all('tr'):
                cells = [td.get_text(strip=True) for td in tr.find_all('td')]
                if cells:
                    rows.append(cells)
            
            if header or rows:
                md_table_parts = []
                if header:
                    md_table_parts.append(f"| {' | '.join(header)} |")
                    md_table_parts.append(f"|{'---|' * len(header)}")
                for row in rows:
                    md_table_parts.append(f"| {' | '.join(row)} |")
                
                elements.append(TableElement(markdown_representation='\n'.join(md_table_parts)))

        if not elements:
            logger.warning("No structured elements found in HTML content after parsing.")
            return None

        linear_text = _generate_parsed_text_from_elements_internal(elements)

        return ParsedDocumentOutput(
            parsed_text=linear_text,
            elements=elements,
            original_metadata=original_metadata
        )
    except Exception as e:
        logger.error(f"Error parsing HTML content with BeautifulSoup: {e}", exc_info=True)
        return None


--------------------------------------------------------------------------------
文件: zhz_rag_pipeline/evaluation_assets.py
--------------------------------------------------------------------------------
import dagster as dg
import os
from typing import Dict, List, Any # Optional 可能之后会用到

# 从项目中导入我们重构的批量评估函数和相关工具/常量
from zhz_rag.evaluation.batch_eval_cypher import run_cypher_batch_evaluation
from zhz_rag.evaluation.batch_eval_answer import run_answer_batch_evaluation
from zhz_rag.evaluation.analyze_cypher import perform_cypher_evaluation_analysis
from zhz_rag.evaluation.analyze_answer import perform_answer_evaluation_analysis
from zhz_rag.utils.common_utils import (
    find_latest_rag_interaction_log,
    RAG_INTERACTION_LOGS_DIR,
    EVALUATION_RESULTS_LOGS_DIR,
    get_evaluation_result_log_filepath
)
# 导入 GeminiAPIResource 以声明资源依赖
from zhz_rag_pipeline_dagster.zhz_rag_pipeline.resources import GeminiAPIResource

# --- 资产定义 ---

@dg.asset(
    name="latest_rag_interaction_log_for_evaluation",
    description="Provides the filepath of the latest RAG interaction log to be used for evaluation.",
    group_name="evaluation_pipeline",
    compute_kind="python" # 可选，指明计算类型
)
def latest_rag_interaction_log_for_evaluation_asset(context: dg.AssetExecutionContext) -> str:
    """
    Finds and returns the path to the latest RAG interaction log file.
    """
    log_filepath = find_latest_rag_interaction_log(RAG_INTERACTION_LOGS_DIR)
    if not log_filepath or not os.path.exists(log_filepath):
        error_msg = f"No RAG interaction log file found in directory: {RAG_INTERACTION_LOGS_DIR}"
        context.log.error(error_msg)
        raise dg.Failure(description=error_msg)
    
    context.log.info(f"Using RAG interaction log for evaluation: {log_filepath}")
    context.add_output_metadata({"log_filepath": log_filepath, "filename": os.path.basename(log_filepath)})
    return log_filepath

@dg.asset(
    name="batch_cypher_evaluations_log", # 资产名称最好能反映它产出的是日志文件
    description="Runs batch evaluation of Cypher queries and produces an evaluation log file.",
    group_name="evaluation_pipeline",
    compute_kind="python",
    # deps=[latest_rag_interaction_log_for_evaluation_asset] # 通过函数参数自动推断依赖
)
async def batch_cypher_evaluation_log_asset(
    context: dg.AssetExecutionContext,
    gemini_api: GeminiAPIResource,
    latest_rag_interaction_log_for_evaluation: str # <--- 修改参数名
) -> dg.Output[str]:
    context.log.info(f"Starting batch Cypher evaluation using log file: {latest_rag_interaction_log_for_evaluation}") # <--- 使用新参数名
    
    # 从 Dagster 配置中获取参数，或使用默认/环境变量
    # 这里我们先用之前脚本中的方式，未来可以转为 Dagster run_config
    app_version = os.getenv("APP_VERSION_TAG", "dagster_cypher_eval_0.2")
    # 对于 use_simulated_api，在 Dagster 中通常会通过资源配置或 op_config 来控制，
    # 而不是直接依赖环境变量，这样更灵活。但为了保持与脚本一致，暂时保留。
    use_simulated = os.getenv("USE_SIMULATED_GEMINI_CYPHER_EVAL", "false").lower() == "true"
    api_delay = float(os.getenv("GEMINI_API_CALL_DELAY_SECONDS", "4.1"))

    if use_simulated:
        context.log.warning("Cypher evaluation asset is using SIMULATED Gemini API calls.")

    # 调用我们重构的、现在接受 gemini_resource 的批量评估函数
    eval_stats = await run_cypher_batch_evaluation(
        gemini_resource_for_evaluator=gemini_api, # 传递注入的 Dagster 资源
        rag_interaction_log_filepath=latest_rag_interaction_log_for_evaluation,
        app_version=app_version,
        use_simulated_api=use_simulated, # 这个参数现在由 run_cypher_batch_evaluation 内部处理
        api_call_delay=api_delay
    )
    context.log.info(f"Batch Cypher evaluation completed. Statistics: {eval_stats}")

    # 确定输出的评估结果日志文件名 (与 evaluator.py 中一致)
    output_log_filepath = get_evaluation_result_log_filepath(evaluation_name="cypher_gemini_flash")
    
    # 确保目录存在 (get_evaluation_result_log_filepath 内部的 log_interaction_data 会处理)
    # 但这里我们也可以提前确保，或者依赖 log_interaction_data
    os.makedirs(os.path.dirname(output_log_filepath), exist_ok=True)
            
    metadata = {"evaluation_stats": eval_stats, "output_filepath": output_log_filepath}
    if eval_stats.get("cypher_queries_evaluated", 0) == 0:
        metadata["warning"] = "No Cypher queries were evaluated. Output log might be empty."
        context.log.warning(metadata["warning"])

    return dg.Output(output_log_filepath, metadata=metadata)


@dg.asset(
    name="batch_answer_evaluations_log", # 资产名称
    description="Runs batch evaluation of generated answers from RAG logs using Gemini.",
    group_name="evaluation_pipeline",
    compute_kind="python",
    # deps=[latest_rag_interaction_log_for_evaluation_asset] # 通过函数参数自动推断依赖
)
async def batch_answer_evaluation_log_asset(
    context: dg.AssetExecutionContext,
    gemini_api: GeminiAPIResource,
    latest_rag_interaction_log_for_evaluation: str # <--- 修改参数名
) -> dg.Output[str]:
    context.log.info(f"Starting batch Answer evaluation using log file: {latest_rag_interaction_log_for_evaluation}") # <--- 使用新参数名
    app_version = os.getenv("APP_VERSION_TAG", "dagster_answer_eval_0.2")
    use_simulated = os.getenv("USE_SIMULATED_GEMINI_ANSWER_EVAL", "false").lower() == "true"
    api_delay = float(os.getenv("GEMINI_API_CALL_DELAY_SECONDS", "4.1"))

    if use_simulated:
        context.log.warning("Answer evaluation asset is using SIMULATED Gemini API calls.")

    eval_stats = await run_answer_batch_evaluation(
        gemini_resource_for_evaluator=gemini_api, # 传递注入的 Dagster 资源
        rag_interaction_log_filepath=latest_rag_interaction_log_for_evaluation,
        app_version=app_version,
        use_simulated_api=use_simulated, # 这个参数现在由 run_answer_batch_evaluation 内部处理
        api_call_delay=api_delay
    )
    context.log.info(f"Batch Answer evaluation completed. Statistics: {eval_stats}")

    output_log_filepath = get_evaluation_result_log_filepath(evaluation_name="answer_gemini_flash")
    os.makedirs(os.path.dirname(output_log_filepath), exist_ok=True)

    metadata = {"evaluation_stats": eval_stats, "output_filepath": output_log_filepath}
    if eval_stats.get("answers_evaluated", 0) == 0:
        metadata["warning"] = "No answers were evaluated. Output log might be empty."
        context.log.warning(metadata["warning"])
        
    return dg.Output(output_log_filepath, metadata=metadata)

@dg.asset(
    name="cypher_evaluation_analysis_report", # 资产名称
    description="Generates a CSV analysis report from Cypher evaluation results.",
    group_name="evaluation_pipeline",
    compute_kind="python",
    # deps=[batch_cypher_evaluation_log_asset] # 通过函数参数自动推断依赖
)
def cypher_analysis_report_asset(
    context: dg.AssetExecutionContext,
    batch_cypher_evaluations_log: str # 上游资产的输出 (即 cypher 评估日志文件的路径)
) -> dg.Output[str]: # 输出 CSV 报告文件的路径
    """
    Analyzes Cypher evaluation logs and produces a CSV report.
    """
    context.log.info(f"Starting Cypher evaluation analysis using log file: {batch_cypher_evaluations_log}")

    if not os.path.exists(batch_cypher_evaluations_log):
        error_msg = f"Input Cypher evaluation log file not found: {batch_cypher_evaluations_log}"
        context.log.error(error_msg)
        raise dg.Failure(description=error_msg)

    # 构建输出CSV文件的路径
    # 我们希望CSV文件也存储在 EVALUATION_RESULTS_LOGS_DIR 目录下
    # 文件名可以基于输入日志名或固定一个模式
    base_input_log_name = os.path.basename(batch_cypher_evaluations_log)
    # 从 "eval_results_cypher_gemini_flash_YYYYMMDD.jsonl" 生成 "analysis_cypher_gemini_flash_YYYYMMDD.csv"
    if base_input_log_name.startswith("eval_results_") and base_input_log_name.endswith(".jsonl"):
        analysis_file_name = "analysis_" + base_input_log_name[len("eval_results_"):-len(".jsonl")] + ".csv"
    else: # Fallback naming
        analysis_file_name = f"analysis_cypher_report_{context.run_id[:8]}.csv"
    
    output_csv_filepath = os.path.join(EVALUATION_RESULTS_LOGS_DIR, analysis_file_name)
    
    success = perform_cypher_evaluation_analysis(
        evaluation_log_filepath=batch_cypher_evaluations_log,
        output_csv_filepath=output_csv_filepath
    )

    if success:
        context.log.info(f"Cypher evaluation analysis report generated: {output_csv_filepath}")
        return dg.Output(output_csv_filepath, metadata={"output_csv_filepath": output_csv_filepath, "source_log": base_input_log_name})
    else:
        error_msg = f"Cypher evaluation analysis failed for log file: {batch_cypher_evaluations_log}"
        context.log.error(error_msg)
        raise dg.Failure(description=error_msg)


@dg.asset(
    name="answer_evaluation_analysis_report", # 资产名称
    description="Generates a CSV analysis report from Answer evaluation results.",
    group_name="evaluation_pipeline",
    compute_kind="python",
    # deps=[batch_answer_evaluations_log_asset] # 通过函数参数自动推断依赖
)
def answer_analysis_report_asset(
    context: dg.AssetExecutionContext,
    batch_answer_evaluations_log: str # 上游资产的输出 (即 answer 评估日志文件的路径)
) -> dg.Output[str]: # 输出 CSV 报告文件的路径
    """
    Analyzes Answer evaluation logs and produces a CSV report.
    """
    context.log.info(f"Starting Answer evaluation analysis using log file: {batch_answer_evaluations_log}")

    if not os.path.exists(batch_answer_evaluations_log):
        error_msg = f"Input Answer evaluation log file not found: {batch_answer_evaluations_log}"
        context.log.error(error_msg)
        raise dg.Failure(description=error_msg)

    base_input_log_name = os.path.basename(batch_answer_evaluations_log)
    if base_input_log_name.startswith("eval_results_") and base_input_log_name.endswith(".jsonl"):
        analysis_file_name = "analysis_" + base_input_log_name[len("eval_results_"):-len(".jsonl")] + ".csv"
    else: # Fallback naming
        analysis_file_name = f"analysis_answer_report_{context.run_id[:8]}.csv"
        
    output_csv_filepath = os.path.join(EVALUATION_RESULTS_LOGS_DIR, analysis_file_name)

    success = perform_answer_evaluation_analysis(
        evaluation_log_filepath=batch_answer_evaluations_log,
        output_csv_filepath=output_csv_filepath
    )

    if success:
        context.log.info(f"Answer evaluation analysis report generated: {output_csv_filepath}")
        return dg.Output(output_csv_filepath, metadata={"output_csv_filepath": output_csv_filepath, "source_log": base_input_log_name})
    else:
        error_msg = f"Answer evaluation analysis failed for log file: {batch_answer_evaluations_log}"
        context.log.error(error_msg)
        raise dg.Failure(description=error_msg)

# 将所有评估相关的资产收集到一个列表中，方便在 definitions.py 中引用
all_evaluation_assets = [
    latest_rag_interaction_log_for_evaluation_asset,
    batch_cypher_evaluation_log_asset,
    batch_answer_evaluation_log_asset,
    cypher_analysis_report_asset, # <--- 新增
    answer_analysis_report_asset, # <--- 新增
]


--------------------------------------------------------------------------------
文件: zhz_rag_pipeline/ingestion_assets.py
--------------------------------------------------------------------------------
# zhz_rag_pipeline/ingestion_assets.py
import dagster as dg
import os
from typing import List, Dict, Any, Union, Optional
from datetime import datetime, timezone

# --- 修改：导入分发器并设置Pydantic可用性标志 ---
# 尝试导入Pydantic模型，并设置一个标志，以便在模型不可用时代码可以优雅地降级。
try:
    from .pydantic_models_dagster import LoadedDocumentOutput, ParsedDocumentOutput, NarrativeTextElement
    _PYDANTIC_AVAILABLE = True
except ImportError:
    LoadedDocumentOutput = dict  # type: ignore
    ParsedDocumentOutput = dict  # type: ignore
    NarrativeTextElement = dict  # type: ignore
    _PYDANTIC_AVAILABLE = False

from .parsers import dispatch_parsing # <--- 修改导入路径
# --- 修改结束 ---

class LoadDocumentsConfig(dg.Config):
    documents_directory: str = "/home/zhz/zhz_agent/data/raw_documents/" # 更新后的原始文档目录
    allowed_extensions: List[str] = [".txt", ".md", ".docx", ".pdf", ".xlsx", ".html", ".htm"] # 扩大允许范围以测试所有解析器

@dg.asset(
    name="raw_documents",
    description="Loads raw documents from a specified directory.",
    group_name="ingestion"
)
def load_documents_asset(
    context: dg.AssetExecutionContext, 
    config: LoadDocumentsConfig
) -> List[LoadedDocumentOutput]:  # type: ignore
    
    loaded_docs: List[LoadedDocumentOutput] = []  # type: ignore
    target_directory = config.documents_directory
    allowed_exts = tuple(config.allowed_extensions) 

    context.log.info(f"Scanning directory: {target_directory} for files with extensions: {allowed_exts}")

    if not os.path.isdir(target_directory):
        context.log.error(f"Directory not found: {target_directory}")
        return loaded_docs

    for filename in os.listdir(target_directory):
        file_path = os.path.join(target_directory, filename)
        if os.path.isfile(file_path):
            file_name_lower = filename.lower()
            file_extension = os.path.splitext(file_name_lower)[1]

            if file_extension in allowed_exts:
                context.log.info(f"Found matching file: {file_path}")
                try:
                    file_stat = os.stat(file_path)
                    
                    # --- VITAL FIX: Do not pass raw_content ---
                    # The downstream parser will handle reading the file from the path.
                    doc_output_data = {
                        "document_path": file_path,
                        "file_type": file_extension,
                        # raw_content is intentionally omitted
                        "metadata": {
                            "filename": filename,
                            "source_directory": target_directory,
                            "size_bytes": file_stat.st_size,
                            "creation_time_utc": datetime.fromtimestamp(file_stat.st_ctime, tz=timezone.utc).isoformat(),
                            "modified_time_utc": datetime.fromtimestamp(file_stat.st_mtime, tz=timezone.utc).isoformat()
                        }
                    }

                    if _PYDANTIC_AVAILABLE:
                        loaded_docs.append(LoadedDocumentOutput(**doc_output_data))
                    else:
                        loaded_docs.append(doc_output_data)

                    context.log.info(f"Successfully created LoadedDocumentOutput for: {file_path}")
                except Exception as e:
                    context.log.error(f"Failed to process file {file_path}: {e}")
            else:
                context.log.debug(f"Skipping file with non-allowed extension: {file_path}")
        else:
            context.log.debug(f"Skipping non-file item: {file_path}")
            
    if not loaded_docs:
        context.log.warning(f"No matching documents found in {target_directory}")

    if loaded_docs:
        first_doc_path = loaded_docs[0].document_path if _PYDANTIC_AVAILABLE and loaded_docs else "N/A"
        context.add_output_metadata(
            metadata={
                "num_documents_loaded": len(loaded_docs),
                "first_document_path": first_doc_path
            }
        )
    return loaded_docs



@dg.asset(
    name="parsed_documents",
    description="Parses loaded documents into text and extracts basic structure using a dispatcher.",
    group_name="ingestion"
)
def parse_document_asset(
    context: dg.AssetExecutionContext,
    raw_documents: List[LoadedDocumentOutput] # type: ignore
) -> List[ParsedDocumentOutput]: # type: ignore
    
    parsed_docs_output_list: List[ParsedDocumentOutput] = [] # type: ignore
    context.log.info(f"Received {len(raw_documents)} documents to parse.")

    for doc_input in raw_documents:
        doc_path = doc_input.document_path
        file_ext = doc_input.file_type.lower()
        original_metadata = doc_input.metadata.copy()
        original_metadata["source_file_path"] = doc_path

        context.log.info(f"Attempting to parse document: {doc_path} (Type: {file_ext})")

        try:
            # --- VITAL REFACTOR ---
            # 直接将文件路径传递给解析器，不再处理字节内容
            # 对于文本文件，解析器内部自己会用 'rt' 模式读取
            # 对于二进制文件(pdf, docx, xlsx)，解析器会用 'rb' 模式或相应库读取
            parsed_output = dispatch_parsing(file_ext, doc_path, original_metadata)

            if not parsed_output:
                context.log.warning(f"Parser for '{file_ext}' returned no output for {doc_path}. Creating a fallback.")
                fallback_text = f"[Content Not Parsed by Specific Parser: {doc_path}]"
                elements = [NarrativeTextElement(text=fallback_text)]
                parsed_output = ParsedDocumentOutput(
                    parsed_text=fallback_text,
                    elements=elements,
                    original_metadata=original_metadata
                )

            # 确保输出总是 Pydantic 模型
            if isinstance(parsed_output, dict):
                parsed_output = ParsedDocumentOutput(**parsed_output)
            
            parsed_docs_output_list.append(parsed_output)
            context.log.info(f"Successfully processed: {doc_path}")

        except Exception as e:
            context.log.error(f"Critical error during parsing asset for {doc_path}: {e}", exc_info=True)
            error_text = f"[Critical Parsing Exception for {doc_path}: {str(e)}]"
            elements = [NarrativeTextElement(text=error_text)]
            error_output = ParsedDocumentOutput(
                parsed_text=error_text,
                elements=elements,
                original_metadata=original_metadata
            )
            parsed_docs_output_list.append(error_output)

    if parsed_docs_output_list:
        context.add_output_metadata(
            metadata={
                "num_documents_processed_for_parsing": len(raw_documents),
                "num_parsed_document_outputs_generated": len(parsed_docs_output_list),
            }
        )
    return parsed_docs_output_list



all_ingestion_assets = [load_documents_asset, parse_document_asset]


--------------------------------------------------------------------------------
文件: zhz_rag_pipeline/processing_assets.py
--------------------------------------------------------------------------------
#  文件: zhz_rag_pipeline_dagster/zhz_rag_pipeline/processing_assets.py

import json
import asyncio
import re
import dagster as dg
from typing import List, Dict, Any, Optional, Union
import uuid
from langchain_text_splitters import RecursiveCharacterTextSplitter
import hashlib
import pandas as pd
from zhz_rag.utils.common_utils import normalize_text_for_id
from zhz_rag_pipeline_dagster.zhz_rag_pipeline.pydantic_models_dagster import (
    ChunkOutput,
    ParsedDocumentOutput,
    EmbeddingOutput,
    KGTripleSetOutput,
    ExtractedEntity,
    ExtractedRelation,
    # --- 添加导入我们需要的元素类型 ---
    TitleElement,
    NarrativeTextElement,
    ListItemElement,
    TableElement,
    CodeBlockElement,
    ImageElement,
    PageBreakElement,
    HeaderElement,
    FooterElement,
    DocumentElementMetadata
    # --- 结束添加 ---
)
from zhz_rag_pipeline_dagster.zhz_rag_pipeline.resources import (
    GGUFEmbeddingResource,
    ChromaDBResource,
    DuckDBResource,
    LocalLLMAPIResource,
    SystemResource  # <--- 添加这一行以导入 SystemResource
)
import jieba
import bm25s
import pickle
import numpy as np
import os
from zhz_rag.utils.common_utils import normalize_text_for_id

_PYDANTIC_AVAILABLE = False
try:
    from .pydantic_models_dagster import ( # 使用相对导入
        ChunkOutput,
        ParsedDocumentOutput,
        EmbeddingOutput,
        KGTripleSetOutput,
        ExtractedEntity,
        ExtractedRelation,
        TitleElement,
        NarrativeTextElement,
        ListItemElement,
        TableElement,
        CodeBlockElement,
        ImageElement,
        PageBreakElement,
        HeaderElement,
        FooterElement,
        DocumentElementMetadata # <--- 确保这里导入了 DocumentElementMetadata
    )
    _PYDANTIC_AVAILABLE = True
    # 如果 Pydantic 可用，我们也可以直接从模型中获取 DocumentElementType
    # from .pydantic_models_dagster import DocumentElementType # 如果需要更精确的类型提示
except ImportError:
    # 定义占位符
    class BaseModel: pass
    class ChunkOutput(BaseModel): pass
    class ParsedDocumentOutput(BaseModel): pass
    class EmbeddingOutput(BaseModel): pass
    class KGTripleSetOutput(BaseModel): pass
    class ExtractedEntity(BaseModel): pass
    class ExtractedRelation(BaseModel): pass
    class TitleElement(BaseModel): pass
    class NarrativeTextElement(BaseModel): pass
    class ListItemElement(BaseModel): pass
    class TableElement(BaseModel): pass
    class CodeBlockElement(BaseModel): pass
    class ImageElement(BaseModel): pass
    class PageBreakElement(BaseModel): pass
    class HeaderElement(BaseModel): pass
    class FooterElement(BaseModel): pass
    class DocumentElementMetadata(BaseModel): pass # <--- 定义占位符
    DocumentElementType = Any # type: ignore
# --- 结束 Pydantic 模型导入 ---

import logging
logger = logging.getLogger(__name__)
if not logger.handlers:
    handler = logging.StreamHandler()
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    logger.setLevel(logging.DEBUG) # <--- 确保是 DEBUG
    logger.info(f"Logger for {__name__} (processing_assets) configured with DEBUG level.")


class TextChunkerConfig(dg.Config):
    chunk_size: int = 1000 
    chunk_overlap: int = 100
    max_element_text_length_before_split: int = 1200 # 一个1200字符的段落如果语义连贯，可以考虑不切分。
    target_sentence_split_chunk_size: int = 600    # 略微增大子块的目标大小，使其包含更多上下文。
    sentence_split_chunk_overlap_sentences: int = 2  # 增加到2句重叠，以期在子块之间提供更好的语义连接。
    # --- 合并策略参数 ---
    min_chunk_length_to_avoid_merge: int = 250
    max_merged_chunk_size: int = 750


def split_text_into_sentences(text: str) -> List[str]:
    """
    Splits text into sentences using a regex-based approach.
    Handles common sentence terminators and aims to preserve meaningful units.
    """
    if not text:
        return []
    # 改进的句子分割正则表达式，考虑了中英文句号、问号、感叹号
    # 并尝试处理省略号和一些特殊情况。
    # (?<=[。？！\.!\?]) 会匹配这些标点符号后面的位置 (lookbehind)
    # \s* 匹配标点后的任意空格
    # (?!$) 确保不是字符串末尾 (避免在末尾标点后产生空句子)
    # 对于中文，句号、问号、感叹号通常直接结束句子。
    # 对于英文，. ! ? 后面通常有空格或换行。
    
    # 一个更简单的版本，直接按标点分割，然后清理
    sentences = re.split(r'([。？！\.!\?])', text)
    result = []
    current_sentence = ""
    for i in range(0, len(sentences), 2):
        part = sentences[i]
        terminator = sentences[i+1] if i+1 < len(sentences) else ""
        current_sentence = part + terminator
        if current_sentence.strip():
            result.append(current_sentence.strip())
    
    # 如果上面的分割不理想，可以尝试更复杂的，但这个简单版本通常够用
    # 例如：
    # sentences = re.split(r'(?<=[。？！\.!\?])\s*', text)
    # sentences = [s.strip() for s in sentences if s.strip()]
    return result if result else [text] # 如果无法分割，返回原文本作为一个句子


# --- START: 覆盖这个函数 ---
def split_markdown_table_by_rows(
    markdown_table_text: str,
    target_chunk_size: int,
    max_chunk_size: int,
    context: Optional[dg.AssetExecutionContext] = None
) -> List[Dict[str, Any]]:
    """
    Splits a Markdown table string by its data rows.
    It now tries to create smaller chunks, even for short tables, by grouping a few rows together.
    """
    sub_chunks_data: List[Dict[str, Any]] = []
    lines = markdown_table_text.strip().split('\n')
    
    if len(lines) < 2:
        if context: context.log.warning(f"Markdown table has less than 2 lines. Cannot process for row splitting.")
        return [{"text": markdown_table_text, "start_row_index": -1, "end_row_index": -1}]

    header_row = lines[0]
    separator_row = lines[1]
    data_rows = lines[2:]

    if not data_rows:
        if context: context.log.warning("Markdown table has no data rows. Returning header and separator as single chunk.")
        return [{"text": f"{header_row}\n{separator_row}", "start_row_index": -1, "end_row_index": -1}]

    # --- 新的、更激进的分割逻辑 ---
    current_sub_chunk_lines = []
    current_sub_chunk_start_row_idx = 0
    
    # 定义每个块的目标行数，例如 2-3 行，可以根据需要调整
    ROWS_PER_CHUNK = 2

    for i in range(0, len(data_rows), ROWS_PER_CHUNK):
        chunk_of_rows = data_rows[i:i + ROWS_PER_CHUNK]
        
        # 每个块都包含表头和分隔符，以保证上下文完整
        sub_chunk_text = "\n".join([header_row, separator_row] + chunk_of_rows)
        
        start_row_index = i
        end_row_index = i + len(chunk_of_rows) - 1

        sub_chunks_data.append({
            "text": sub_chunk_text,
            "start_row_index": start_row_index,
            "end_row_index": end_row_index
        })
        if context:
            context.log.debug(f"  Table sub-chunk created: data rows index {start_row_index}-{end_row_index}")
    
    return sub_chunks_data
# --- END: 覆盖结束 ---


def split_code_block_by_blank_lines(
    code_text: str,
    target_chunk_size: int, # 复用配置，但对于代码块，这个更像是一个上限指导
    max_chunk_size: int,    # 作为硬上限
    context: Optional[dg.AssetExecutionContext] = None
) -> List[str]:
    """
    Splits a code block string by blank lines (one or more empty lines).
    Tries to keep resulting chunks from exceeding max_chunk_size.
    """
    if not code_text.strip():
        return []

    # 使用正则表达式匹配一个或多个连续的空行作为分隔符
    # \n\s*\n 匹配一个换行符，后跟零或多个空白字符，再跟一个换行符
    potential_splits = re.split(r'(\n\s*\n)', code_text) # 保留分隔符以便后续处理
    
    sub_chunks = []
    current_chunk_lines = []
    current_chunk_char_count = 0

    # 第一个块总是从头开始
    if potential_splits:
        first_part = potential_splits.pop(0).strip()
        if first_part:
            current_chunk_lines.append(first_part)
            current_chunk_char_count += len(first_part)

    while potential_splits:
        delimiter = potential_splits.pop(0) # 这是分隔符 \n\s*\n
        if not potential_splits: # 没有更多内容了
            if delimiter.strip(): # 如果分隔符本身不是纯空白，也算内容
                 current_chunk_lines.append(delimiter.rstrip()) # 保留末尾的换行
                 current_chunk_char_count += len(delimiter.rstrip())
            break 
        
        next_part = potential_splits.pop(0).strip()
        if not next_part: # 如果下一个部分是空的，只处理分隔符
            if delimiter.strip():
                current_chunk_lines.append(delimiter.rstrip())
                current_chunk_char_count += len(delimiter.rstrip())
            continue

        # 检查加入 delimiter 和 next_part 是否会超长
        # 对于代码，我们通常希望在逻辑断点（空行）处分割，即使块较小
        # 但如果单个由空行分隔的块本身就超过 max_chunk_size，则需要进一步处理（目前简单截断或接受）
        
        # 简化逻辑：如果当前块非空，并且加入下一个部分（包括分隔的空行）会超过目标大小，
        # 或者严格超过最大大小，则结束当前块。
        # 这里的分隔符（空行）本身也应该被视为块的一部分，或者作为块的自然结束。

        # 更简单的策略：每个由 re.split 分割出来的非空部分（即代码段）自成一块
        # 如果代码段本身过长，则接受它，或者未来再细分
        if current_chunk_lines: # 如果当前块有内容
            # 检查如果加上 next_part 是否会超长（这里可以简化，因为空行分割通常意味着逻辑单元）
            # 我们先假设每个由空行分割的块都是一个独立的单元
            sub_chunks.append("\n".join(current_chunk_lines))
            if context: context.log.debug(f"  Code sub-chunk created (blank line split), len: {current_chunk_char_count}")
            current_chunk_lines = []
            current_chunk_char_count = 0
        
        if next_part: # 开始新的块
            current_chunk_lines.append(next_part)
            current_chunk_char_count += len(next_part)

    # 添加最后一个正在构建的子块
    if current_chunk_lines:
        sub_chunks.append("\n".join(current_chunk_lines))
        if context: context.log.debug(f"  Code sub-chunk created (blank line split, last), len: {current_chunk_char_count}")

    if not sub_chunks and code_text: # 如果完全没分割出任何东西（例如代码没有空行）
        if context: context.log.warning("Code block splitting by blank lines resulted in no sub-chunks. Returning original code block.")
        # 对于这种情况，我们可能需要一个字符分割器作为最终回退
        # 但为了简单起见，我们先返回原始代码块
        # 如果原始代码块 > max_chunk_size，它仍然会是一个大块
        if len(code_text) > max_chunk_size:
            if context: context.log.warning(f"  Original code block (len: {len(code_text)}) exceeds max_chunk_size ({max_chunk_size}) and was not split by blank lines. Consider character splitting as fallback.")
            # 这里可以插入 RecursiveCharacterTextSplitter 逻辑
            # from langchain_text_splitters import RecursiveCharacterTextSplitter
            # char_splitter = RecursiveCharacterTextSplitter(chunk_size=max_chunk_size, chunk_overlap=0, separators=["\n", " ", ""])
            # sub_chunks = char_splitter.split_text(code_text)
            # if context: context.log.info(f"    Fallback: Code block character split into {len(sub_chunks)} parts.")
            # return sub_chunks
        return [code_text] # 暂时返回原块

    # 过滤掉完全是空字符串的块 (re.split 可能产生)
    final_sub_chunks = [chunk for chunk in sub_chunks if chunk.strip()]
    return final_sub_chunks if final_sub_chunks else [code_text]


def _get_element_text(element: Any, context: dg.AssetExecutionContext) -> Optional[str]:
    """
    Extracts the text content from a DocumentElement, handling various types.
    """
    # 检查 element 是否有 text 属性
    if hasattr(element, 'text') and isinstance(element.text, str) and element.text.strip():
        return element.text.strip()
    
    # 对TableElement，它的markdown表示更有用
    if isinstance(element, TableElement) and hasattr(element, 'markdown_representation'):
        return element.markdown_representation
        
    # 对CodeBlockElement，它的code属性是内容
    if isinstance(element, CodeBlockElement) and hasattr(element, 'code'):
        return element.code

    # 最后的防线：尝试将元素转为字符串，但这通常表示有未处理的类型
    # context.log.warning(f"Element of type {type(element).__name__} has no direct text attribute. Falling back to str().")
    # return str(element)
    return None # 如果没有明确的文本内容，则返回None，避免注入描述性文字



@dg.asset(
    name="text_chunks",
    description="Cleans/chunks documents. Splits long elements, merges short ones, enriches with contextual metadata.",
    group_name="processing",
    deps=["parsed_documents"]
)
def clean_chunk_text_asset(
    context: dg.AssetExecutionContext,
    config: TextChunkerConfig,
    parsed_documents: List[ParsedDocumentOutput]
) -> List[ChunkOutput]:
    all_chunks: List[ChunkOutput] = []

    for doc_idx, parsed_doc in enumerate(parsed_documents):
        # --- START: 核心修正 - 统一提取并传递文档级元数据 (逻辑保持不变) ---
        doc_meta = parsed_doc.original_metadata
        doc_filename = doc_meta.get("filename") or doc_meta.get("file_name") or f"doc_{doc_idx}"
        doc_creation_date = doc_meta.get("creation_date") or doc_meta.get("creation_datetime")
        doc_last_modified = doc_meta.get("last_modified") or doc_meta.get("last_modified_datetime")
        doc_author = doc_meta.get("author") or doc_meta.get("authors")

        document_level_metadata = {
            "filename": doc_filename,
            "creation_date": doc_creation_date,
            "last_modified": doc_last_modified,
            "author": doc_author,
        }
        document_level_metadata = {k: v for k, v in document_level_metadata.items() if v is not None}
        
        context.log.info(f"Processing document for chunking: {doc_filename}")
        # --- END: 核心修正 ---

        # --- 【【【新增的核心修改】】】构建元数据前缀 ---
        # 将关键元数据（如文件名）格式化为一个字符串，将附加到每个块的文本内容之前。
        # 这使得BM25等关键词检索器也能“看到”这些元数据。
        metadata_prefix_for_text = f"[Source Document: {doc_filename}] "
        # 还可以添加其他你认为对检索重要的元数据
        if doc_author:
            metadata_prefix_for_text += f"[Author: {doc_author}] "
        # --- 【【【新增结束】】】 ---

        current_title_hierarchy: Dict[int, str] = {}
        doc_internal_chunk_counter = 0

        for element_idx, element in enumerate(parsed_doc.elements):
            parent_id = str(uuid.uuid4())
            
            base_chunk_meta = document_level_metadata.copy()
            
            element_type_str = getattr(element, 'element_type', type(element).__name__)
            base_chunk_meta.update({
                "parent_id": parent_id,
                "paragraph_type": element_type_str,
                "source_element_index": element_idx,
            })

            if isinstance(element, TitleElement):
                 title_level = getattr(element, 'level', 1)
                 keys_to_remove = [lvl for lvl in current_title_hierarchy if lvl >= title_level]
                 for key in keys_to_remove:
                     del current_title_hierarchy[key]
                 current_title_hierarchy[title_level] = getattr(element, 'text', '').strip()
            
            for level, title in current_title_hierarchy.items():
                base_chunk_meta[f"title_hierarchy_{level}"] = title
            
            if hasattr(element, 'metadata') and element.metadata:
                page_num = getattr(element.metadata, 'page_number', None)
                if page_num is not None:
                    base_chunk_meta['page_number'] = page_num + 1

            sub_chunks: List[Dict[str, Any]] = []
            
            text_content = _get_element_text(element, context)
            
            if not text_content:
                context.log.debug(f"Skipping element {element_idx} in {doc_filename} due to empty content.")
                continue

            # --- 【【【修正的判断逻辑】】】 ---
            if element_type_str == "TableElement":
                 base_chunk_meta["paragraph_type"] = "table"
                 sub_chunks = split_markdown_table_by_rows(text_content, config.target_sentence_split_chunk_size, config.max_merged_chunk_size, context)
            else:
                if len(text_content) > config.max_element_text_length_before_split:
                    sentences = split_text_into_sentences(text_content)
                    for sent in sentences:
                        if sent.strip():
                            sub_chunks.append({"text": sent.strip()})
                else:
                    sub_chunks.append({"text": text_content})

            for sub_chunk_data in sub_chunks:
                doc_internal_chunk_counter += 1
                chunk_meta_final = base_chunk_meta.copy()
                chunk_meta_final["chunk_number_in_doc"] = doc_internal_chunk_counter
                
                if "start_row_index" in sub_chunk_data:
                    chunk_meta_final["table_original_start_row"] = sub_chunk_data["start_row_index"]
                    chunk_meta_final["table_original_end_row"] = sub_chunk_data["end_row_index"]

                # --- 【【【新增的核心修改】】】将元数据前缀和块文本内容结合 ---
                final_chunk_text = metadata_prefix_for_text + sub_chunk_data["text"]

                all_chunks.append(ChunkOutput(
                    chunk_text=final_chunk_text, # <--- 使用结合后的文本
                    source_document_id=doc_filename,
                    chunk_metadata=chunk_meta_final
                ))

    context.log.info(f"Chunking process finished. Total chunks generated: {len(all_chunks)}")
    if all_chunks:
        # 强制打印最后一个块的元数据和文本，看filename是否存在
        context.log.info(f"Sample final chunk TEXT: {all_chunks[-1].chunk_text[:300]}...")
        context.log.info(f"Sample final chunk METADATA: {all_chunks[-1].chunk_metadata}")
    
    context.add_output_metadata(metadata={"total_chunks_generated": len(all_chunks)})
    return all_chunks

@dg.asset(
    name="text_embeddings",
    description="Generates vector embeddings for text chunks.",
    group_name="processing",
    deps=["text_chunks"]
)
def generate_embeddings_asset( # <--- 保持同步，因为 GGUFEmbeddingResource.encode 是同步包装
    context: dg.AssetExecutionContext,
    text_chunks: List[ChunkOutput],
    embedder: GGUFEmbeddingResource
) -> List[EmbeddingOutput]:
    # +++ 新增打印语句 +++
    context.log.info(f"generate_embeddings_asset: Received {len(text_chunks)} text_chunks.")
    if text_chunks:
        context.log.info(f"generate_embeddings_asset: First chunk text (first 100 chars): '{text_chunks[0].chunk_text[:100]}'")
        context.log.info(f"generate_embeddings_asset: First chunk metadata: {text_chunks[0].chunk_metadata}")
    # +++ 结束新增打印语句 +++

    all_embeddings: List[EmbeddingOutput] = []
    if not text_chunks:
        context.log.warning("generate_embeddings_asset: No text chunks received, returning empty list.") # 添加一个明确的警告
        return all_embeddings
    
    # --- 确保 chunk_texts_to_encode 不为空才调用 embedder.encode ---
    chunk_texts_to_encode = [chunk.chunk_text for chunk in text_chunks if chunk.chunk_text and chunk.chunk_text.strip()]
    
    if not chunk_texts_to_encode:
        context.log.warning("generate_embeddings_asset: All received text chunks are empty or whitespace after filtering. Returning empty list.")
        # 即使原始 text_chunks 非空，但如果所有 chunk_text 都无效，也应该返回空 embedding 列表
        # 并且要确保下游知道期望的 EmbeddingOutput 数量可能是0
        return all_embeddings # 返回空列表是正确的

    vectors = embedder.encode(chunk_texts_to_encode)

    # --- 确保正确地将嵌入结果映射回原始的 text_chunks 列表（如果数量可能不一致）---
    # 当前的逻辑是假设 vectors 和 chunk_texts_to_encode 一一对应，并且 text_chunks 的顺序与 chunk_texts_to_encode 过滤前的顺序相关
    # 如果 chunk_texts_to_encode 进行了过滤，这里的循环需要更小心
    
    # 一个更安全的映射方式是，只为那些实际被编码的文本块创建 EmbeddingOutput
    # 但这要求下游能处理 EmbeddingOutput 列表长度可能小于 ChunkOutput 列表长度的情况，
    # 或者，我们应该为那些被过滤掉的 chunk 也创建一个带有零向量的 EmbeddingOutput。
    # 我们之前的 LocalModelHandler 修改是为了处理单个空文本，现在这里是资产层面的。

    # 保持与 LocalModelHandler 类似的健壮性：为所有传入的 text_chunks 生成 EmbeddingOutput，
    # 如果其文本为空或嵌入失败，则使用零向量。

    embedding_map = {text: vec for text, vec in zip(chunk_texts_to_encode, vectors)}

    for i, chunk_input in enumerate(text_chunks):
        model_name_for_log = os.getenv("EMBEDDING_MODEL_PATH", "API_Based_Embedder")
        embedding_vector_for_chunk = [0.0] * embedder.get_embedding_dimension() # 默认为零向量

        if chunk_input.chunk_text and chunk_input.chunk_text.strip() and chunk_input.chunk_text in embedding_map:
            embedding_vector_for_chunk = embedding_map[chunk_input.chunk_text]
        elif chunk_input.chunk_text and chunk_input.chunk_text.strip(): 
            # 文本有效但没有在 embedding_map 中找到 (可能因为 embedder.encode 内部的某些问题)
            context.log.warning(f"generate_embeddings_asset: Valid chunk text for chunk_id {chunk_input.chunk_id} was not found in embedding_map. Using zero vector.")
        else: # 文本本身就是空的
            context.log.info(f"generate_embeddings_asset: Chunk_id {chunk_input.chunk_id} has empty text. Using zero vector.")


        all_embeddings.append(EmbeddingOutput(
            chunk_id=chunk_input.chunk_id,
            chunk_text=chunk_input.chunk_text, # 存储原始文本，即使它是空的
            embedding_vector=embedding_vector_for_chunk,
            embedding_model_name=model_name_for_log,
            original_chunk_metadata=chunk_input.chunk_metadata
        ))
    
    context.add_output_metadata(metadata={"total_embeddings_generated": len(all_embeddings)})
    return all_embeddings


@dg.asset(
    name="vector_store_embeddings",
    description="Stores text embeddings into a ChromaDB vector store.",
    group_name="indexing",
    deps=["text_embeddings"]
)
def vector_storage_asset(
    context: dg.AssetExecutionContext,
    text_embeddings: List[EmbeddingOutput],
    chroma_db: ChromaDBResource
) -> None:
    if not text_embeddings:
        context.log.warning("vector_storage_asset: No embeddings received, nothing to store in ChromaDB.")
        context.add_output_metadata(metadata={"num_embeddings_stored": 0})
        return

    # --- START: 核心修复 ---
    # 筛选出那些拥有有效（非空）嵌入向量的条目
    valid_embeddings_to_store: List[EmbeddingOutput] = []
    for emb in text_embeddings:
        if emb.embedding_vector and len(emb.embedding_vector) > 0:
            valid_embeddings_to_store.append(emb)
        else:
            context.log.warning(f"Skipping storage for chunk_id {emb.chunk_id} due to empty embedding vector.")
    
    if not valid_embeddings_to_store:
        context.log.warning("vector_storage_asset: No valid embeddings found after filtering. Nothing to store.")
        context.add_output_metadata(metadata={"num_embeddings_stored": 0, "num_invalid_embeddings_skipped": len(text_embeddings)})
        return
        
    ids_to_store = [emb.chunk_id for emb in valid_embeddings_to_store]
    embeddings_to_store = [emb.embedding_vector for emb in valid_embeddings_to_store]
    documents_to_store = [emb.chunk_text for emb in valid_embeddings_to_store]
    cleaned_metadatas: List[Dict[str, Any]] = []

    for i, emb_output in enumerate(valid_embeddings_to_store):
    # --- END: 核心修复 ---
        original_meta = emb_output.original_chunk_metadata if isinstance(emb_output.original_chunk_metadata, dict) else {}
        meta = original_meta.copy()
        
        meta["chunk_text_in_meta"] = str(emb_output.chunk_text) if emb_output.chunk_text is not None else "[TEXT IS NULL]"

        cleaned_meta_item: Dict[str, Any] = {}
        for key, value in meta.items():
            if isinstance(value, dict):
                if key == "title_hierarchy" and not value: 
                    cleaned_meta_item[key] = "None"
                    context.log.debug(f"Metadata for chunk {emb_output.chunk_id}: Replaced empty title_hierarchy dict with 'None' string.")
                else:
                    try:
                        cleaned_meta_item[key] = json.dumps(value, ensure_ascii=False)
                    except TypeError:
                        cleaned_meta_item[key] = str(value)
                        context.log.warning(f"Metadata for chunk {emb_output.chunk_id}: Could not JSON serialize dict for key '{key}', used str(). Value: {str(value)[:100]}...")
            elif isinstance(value, list):
                try:
                    cleaned_meta_item[key] = json.dumps(value, ensure_ascii=False)
                except TypeError:
                    cleaned_meta_item[key] = str(value)
                    context.log.warning(f"Metadata for chunk {emb_output.chunk_id}: Could not JSON serialize list for key '{key}', used str(). Value: {str(value)[:100]}...")
            elif value is None:
                cleaned_meta_item[key] = "" 
            else: 
                cleaned_meta_item[key] = value
        cleaned_metadatas.append(cleaned_meta_item)

    # +++ 新增日志 +++
    if embeddings_to_store:
        context.log.info(f"vector_storage_asset: Sample embedding vector to be stored (first item, first 10 elements): {str(embeddings_to_store[0][:10]) if embeddings_to_store[0] else 'None'}")
        context.log.info(f"vector_storage_asset: Length of first embedding vector to be stored: {len(embeddings_to_store[0]) if embeddings_to_store[0] else 'N/A'}")
        is_first_all_zeros = all(v == 0.0 for v in embeddings_to_store[0]) if embeddings_to_store[0] else "N/A"
        context.log.info(f"vector_storage_asset: Is first sample embedding all zeros: {is_first_all_zeros}")
    # +++ 结束新增日志 +++

    context.log.info(f"vector_storage_asset: Preparing to add/update {len(ids_to_store)} items to ChromaDB collection '{chroma_db.collection_name}'.")
    if ids_to_store:
        context.log.info(f"vector_storage_asset: Sample ID to store: {ids_to_store[0]}")
        # 确保 documents_to_store 也有对应内容，并且不是 None
        sample_doc_text = "[EMPTY DOCUMENT]"
        if documents_to_store and documents_to_store[0] is not None:
            sample_doc_text = str(documents_to_store[0])[:100] # 显示前100字符
        elif documents_to_store and documents_to_store[0] is None:
            sample_doc_text = "[DOCUMENT IS NULL]"
        context.log.info(f"vector_storage_asset: Sample document to store (from documents_to_store, first 100 chars): '{sample_doc_text}'")
        
        sample_meta_text = "[NO METADATA]"
        if cleaned_metadatas:
            sample_meta_text = str(cleaned_metadatas[0])[:200] # 显示元数据摘要
        context.log.info(f"vector_storage_asset: Sample cleaned metadata for first item: {sample_meta_text}")

    try:
        chroma_db.add_embeddings(
            ids=ids_to_store, 
            embeddings=embeddings_to_store, 
            documents=documents_to_store, # 传递真实的文本内容给ChromaDB的documents字段
            metadatas=cleaned_metadatas
        )
        # 尝试获取并记录操作后的集合计数
        # 注意: chroma_db._collection 可能是私有属性，直接访问不推荐，但为了调试可以尝试
        # 更好的方式是 ChromaDBResource 提供一个 get_collection_count() 方法
        collection_count_after_add = -1 # 默认值
        try:
            if chroma_db._collection: # 确保 _collection 不是 None
                 collection_count_after_add = chroma_db._collection.count()
        except Exception as e_count:
            context.log.warning(f"vector_storage_asset: Could not get collection count after add: {e_count}")

        context.add_output_metadata(metadata={"num_embeddings_stored": len(ids_to_store), "collection_count_after_add": collection_count_after_add})
        context.log.info(f"vector_storage_asset: Successfully called add_embeddings. Stored {len(ids_to_store)} items. Collection count now: {collection_count_after_add}")
    except Exception as e_chroma_add:
        context.log.error(f"vector_storage_asset: Failed to add embeddings to ChromaDB: {e_chroma_add}", exc_info=True)
        raise

class BM25IndexConfig(dg.Config):
    index_file_path: str = "/home/zhz/zhz_agent/zhz_rag/stored_data/bm25_index/"


@dg.asset(
    name="keyword_index",
    description="Builds and persists a BM25 keyword index from text chunks.",
    group_name="indexing",
    deps=["text_chunks"]
)
def keyword_index_asset(
    context: dg.AssetExecutionContext,
    config: BM25IndexConfig, # 确保 BM25IndexConfig 在文件某处已定义
    text_chunks: List[ChunkOutput]
) -> None:
    if not text_chunks:
        context.log.warning("keyword_index_asset: No text chunks received, skipping BM25 index building.")
        context.add_output_metadata(metadata={"num_documents_indexed": 0, "index_directory_path": config.index_file_path})
        return

    # --- 新增：检查并记录空文本块 ---
    valid_chunks_for_indexing: List[ChunkOutput] = []
    for idx, chunk in enumerate(text_chunks):
        if chunk.chunk_text and chunk.chunk_text.strip():
            valid_chunks_for_indexing.append(chunk)
        else:
            context.log.warning(f"keyword_index_asset: Chunk {idx} (ID: {chunk.chunk_id}) has empty or whitespace-only text. Skipping for BM25 indexing.")
    
    if not valid_chunks_for_indexing:
        context.log.warning("keyword_index_asset: All received text chunks have empty or whitespace-only text after filtering. Skipping BM25 index building.")
        context.add_output_metadata(metadata={"num_documents_indexed": 0, "index_directory_path": config.index_file_path})
        return
    # --- 结束新增 ---

    # 使用过滤后的有效块
    corpus_texts = [chunk.chunk_text for chunk in valid_chunks_for_indexing]
    document_ids = [chunk.chunk_id for chunk in valid_chunks_for_indexing] # 确保ID与有效文本对应

    context.log.info(f"keyword_index_asset: Preparing to index {len(corpus_texts)} valid text chunks for BM25.")
    if corpus_texts: # 仅在有数据时打印样本
        context.log.info(f"keyword_index_asset: Sample document ID for BM25: {document_ids[0]}")
        context.log.info(f"keyword_index_asset: Sample document text for BM25 (first 50 chars): '{str(corpus_texts[0])[:50]}'")

    try:
        corpus_tokenized_jieba = [list(jieba.cut_for_search(text)) for text in corpus_texts]
        context.log.info(f"keyword_index_asset: Tokenized {len(corpus_tokenized_jieba)} texts for BM25.")
        
        bm25_model = bm25s.BM25() # 使用默认参数初始化
        context.log.info("keyword_index_asset: BM25 model initialized.")
        
        bm25_model.index(corpus_tokenized_jieba)
        indexed_doc_count = len(bm25_model.doc_freqs) if hasattr(bm25_model, 'doc_freqs') and bm25_model.doc_freqs is not None else len(corpus_tokenized_jieba)
        context.log.info(f"keyword_index_asset: BM25 model indexing complete for {indexed_doc_count} documents.")
        
        index_directory = config.index_file_path
        context.log.info(f"keyword_index_asset: BM25 index will be saved to directory: {index_directory}")
        os.makedirs(index_directory, exist_ok=True)
        
        bm25_model.save(index_directory) 
        context.log.info(f"keyword_index_asset: bm25_model.save('{index_directory}') called.")
        
        doc_ids_path = os.path.join(index_directory, "doc_ids.pkl")
        with open(doc_ids_path, 'wb') as f_out:
            pickle.dump(document_ids, f_out)
        context.log.info(f"keyword_index_asset: doc_ids.pkl saved to {doc_ids_path} with {len(document_ids)} IDs.")
        
        # 验证文件是否真的创建了
        expected_params_file = os.path.join(index_directory, "params.index.json") # bm25s 保存时会创建这个
        if os.path.exists(expected_params_file) and os.path.exists(doc_ids_path):
            context.log.info(f"keyword_index_asset: Verified that BM25 index files (e.g., params.index.json, doc_ids.pkl) exist in {index_directory}.")
        else:
            context.log.error(f"keyword_index_asset: BM25 index files (e.g., params.index.json or doc_ids.pkl) NOT FOUND in {index_directory} after save operations!")
            context.log.error(f"keyword_index_asset: Check - params.index.json exists: {os.path.exists(expected_params_file)}")
            context.log.error(f"keyword_index_asset: Check - doc_ids.pkl exists: {os.path.exists(doc_ids_path)}")
            # 如果文件未找到，可能需要抛出异常以使资产失败
            # raise FileNotFoundError(f"BM25 index files not found in {index_directory} after save.")

        context.add_output_metadata(
            metadata={
                "num_documents_indexed": len(corpus_texts), 
                "index_directory_path": index_directory,
                "bm25_corpus_size_actual": indexed_doc_count
            }
        )
        context.log.info("keyword_index_asset: BM25 indexing and saving completed successfully.")
    except Exception as e_bm25:
        context.log.error(f"keyword_index_asset: Error during BM25 indexing or saving: {e_bm25}", exc_info=True)
        raise

# --- KG Extraction 相关的配置和资产 ---


# class KGExtractionConfig(dg.Config):
#     extraction_prompt_template: str = KG_EXTRACTION_SINGLE_CHUNK_PROMPT_TEMPLATE_V1
#     local_llm_model_name: str = "Qwen3-1.7B-GGUF_via_llama.cpp"

# DEFAULT_KG_EXTRACTION_SCHEMA = {
#     "type": "object",
#     "properties": {
#         "entities": {
#             "type": "array",
#             "items": {
#                 "type": "object",
#                 "properties": {
#                     "text": {"type": "string", "description": "提取到的实体原文"},
#                     "label": {"type": "string", "description": "实体类型 (例如: PERSON, ORGANIZATION, TASK)"}
#                 },
#                 "required": ["text", "label"]
#             },
#             "description": "从文本中提取出的实体列表。"
#         },
#         "relations": {
#             "type": "array",
#             "items": {
#                 "type": "object",
#                 "properties": {
#                     "head_entity_text": {"type": "string", "description": "头实体的文本"},
#                     "head_entity_label": {"type": "string", "description": "头实体的类型 (例如: PERSON, TASK)"},
#                     "relation_type": {"type": "string", "description": "关系类型 (例如: WORKS_AT, ASSIGNED_TO)"},
#                     "tail_entity_text": {"type": "string", "description": "尾实体的文本"},
#                     "tail_entity_label": {"type": "string", "description": "尾实体的类型 (例如: ORGANIZATION, PERSON)"}
#                 },
#                 "required": ["head_entity_text", "head_entity_label", "relation_type", "tail_entity_text", "tail_entity_label"]
#             },
#             "description": "从文本中提取出的关系三元组列表。"
#         }
#     },
#     "required": ["entities", "relations"]
# }


# @dg.asset(
#     name="kg_extractions",
#     description="Extracts entities and relations from text chunks for knowledge graph construction.",
#     group_name="kg_building",
#     io_manager_key="pydantic_json_io_manager",
#     deps=["text_chunks"]
# )
# async def kg_extraction_asset(
#     context: dg.AssetExecutionContext, # Pylance 提示 dg.AssetExecutionContext 未定义 "SystemResource"
#     text_chunks: List[ChunkOutput],
#     config: KGExtractionConfig,
#     LocalLLM_api: LocalLLMAPIResource,
#     system_info: SystemResource  # <--- 我们添加了 system_info
# ) -> List[KGTripleSetOutput]:
#     all_kg_outputs: List[KGTripleSetOutput] = []
#     if not text_chunks:
#         context.log.info("No text chunks received for KG extraction, skipping.")
#         return all_kg_outputs

#     total_input_chunks = len(text_chunks)
#     total_entities_extracted_overall = 0
#     total_relations_extracted_overall = 0
#     successfully_processed_chunks_count = 0
    
#     # 并发控制参数
#     recommended_concurrency = system_info.get_recommended_concurrent_tasks(task_type="kg_extraction_llm")
#     CONCURRENT_REQUESTS_LIMIT = max(1, recommended_concurrency) # 直接使用HAL推荐，但至少为1
#     context.log.info(f"HAL recommended concurrency for 'kg_extraction_llm': {recommended_concurrency}. Effective limit set to: {CONCURRENT_REQUESTS_LIMIT}")
#     semaphore = asyncio.Semaphore(CONCURRENT_REQUESTS_LIMIT)


#     async def extract_kg_for_chunk(chunk: ChunkOutput) -> Optional[KGTripleSetOutput]:
#         async with semaphore:
#             # 使用单个chunk的prompt模板
#             prompt = config.extraction_prompt_template.format(text_to_extract=chunk.chunk_text)
#             try:
#                 context.log.debug(f"Starting KG extraction for chunk_id: {chunk.chunk_id}, Text (start): {chunk.chunk_text[:100]}...")
#                 structured_response = await LocalLLM_api.generate_structured_output(
#                     prompt=prompt, 
#                     json_schema=DEFAULT_KG_EXTRACTION_SCHEMA # 使用单个对象的schema
#                 )
                
#                 # 确保 structured_response 是字典类型
#                 if not isinstance(structured_response, dict):
#                     context.log.error(f"Failed KG extraction for chunk {chunk.chunk_id}: LLM response was not a dict. Got: {type(structured_response)}. Response: {str(structured_response)[:200]}")
#                     return None

#                 entities_data = structured_response.get("entities", [])
#                 extracted_entities_list = [
#                     ExtractedEntity(text=normalize_text_for_id(e.get("text","")), label=e.get("label","UNKNOWN").upper())
#                     for e in entities_data if isinstance(e, dict)
#                 ]
                
#                 relations_data = structured_response.get("relations", [])
#                 extracted_relations_list = [
#                     ExtractedRelation(
#                         head_entity_text=r.get('head_entity_text',""), 
#                         head_entity_label=r.get('head_entity_label',"UNKNOWN").upper(), 
#                         relation_type=r.get('relation_type',"UNKNOWN").upper(), 
#                         tail_entity_text=r.get('tail_entity_text',""), 
#                         tail_entity_label=r.get('tail_entity_label',"UNKNOWN").upper()
#                     ) 
#                     for r in relations_data if isinstance(r, dict) and 
#                                                r.get('head_entity_text') and r.get('head_entity_label') and
#                                                r.get('relation_type') and r.get('tail_entity_text') and
#                                                r.get('tail_entity_label')
#                 ]
                
#                 context.log.debug(f"Finished KG extraction for chunk_id: {chunk.chunk_id}. Entities: {len(extracted_entities_list)}, Relations: {len(extracted_relations_list)}")
#                 return KGTripleSetOutput(
#                     chunk_id=chunk.chunk_id,
#                     extracted_entities=extracted_entities_list,
#                     extracted_relations=extracted_relations_list,
#                     extraction_model_name=config.local_llm_model_name,
#                     original_chunk_metadata=chunk.chunk_metadata
#                 )
#             except Exception as e:
#                 context.log.error(f"Failed KG extraction for chunk {chunk.chunk_id}: {e}", exc_info=True)
#                 return None 

#     context.log.info(f"Starting KG extraction for {total_input_chunks} chunks with concurrency limit: {CONCURRENT_REQUESTS_LIMIT}.")
    
#     tasks = [extract_kg_for_chunk(chunk) for chunk in text_chunks]
    
#     results = await asyncio.gather(*tasks)
    
#     context.log.info(f"Finished all KG extraction tasks. Received {len(results)} results (including potential None for failures).")

#     for result_item in results:
#         if result_item and isinstance(result_item, KGTripleSetOutput):
#             all_kg_outputs.append(result_item)
#             total_entities_extracted_overall += len(result_item.extracted_entities)
#             total_relations_extracted_overall += len(result_item.extracted_relations)
#             successfully_processed_chunks_count +=1
#         elif result_item is None:
#             context.log.warning("A KG extraction task failed and returned None.")
            
#     context.log.info(f"KG extraction complete. Successfully processed {successfully_processed_chunks_count} out of {total_input_chunks} chunks.")
#     context.add_output_metadata(
#         metadata={
#             "total_chunks_input_to_kg": total_input_chunks, # 恢复为 total_input_chunks
#             "chunks_successfully_extracted_kg": successfully_processed_chunks_count,
#             "total_entities_extracted": total_entities_extracted_overall, 
#             "total_relations_extracted": total_relations_extracted_overall
#             # 移除了批处理相关的元数据 "total_batches_processed", "batch_size_configured"
#         }
#     )
#     return all_kg_outputs


# # --- KuzuDB 构建资产链 ---

# @dg.asset(
#     name="duckdb_schema", # <--- 修改资产名称
#     description="Creates the base schema (node and relation tables) in DuckDB.",
#     group_name="kg_building",
#     # deps=[kg_extraction_asset] # 保持依赖，确保在提取之后创建schema (逻辑上)
#                                  # 虽然schema创建本身不直接使用提取结果，但流水线顺序上合理
# )
# def duckdb_schema_asset(context: dg.AssetExecutionContext, duckdb_kg: DuckDBResource, embedder: GGUFEmbeddingResource): # <--- 修改函数名和资源参数
#     context.log.info("--- Starting DuckDB Schema Creation Asset ---")
    
#     # 获取嵌入维度，与KuzuDB时类似
#     EMBEDDING_DIM = embedder.get_embedding_dimension()
#     if not EMBEDDING_DIM:
#         raise ValueError("Could not determine embedding dimension from GGUFEmbeddingResource.")

#     node_table_ddl = f"""
#     CREATE TABLE IF NOT EXISTS ExtractedEntity (
#         id_prop VARCHAR PRIMARY KEY,
#         text VARCHAR,
#         label VARCHAR,
#         embedding FLOAT[{EMBEDDING_DIM}]
#     );
#     """

#     relation_table_ddl = f"""
#     CREATE TABLE IF NOT EXISTS KGExtractionRelation (
#         relation_id VARCHAR PRIMARY KEY,
#         source_node_id_prop VARCHAR,
#         target_node_id_prop VARCHAR,
#         relation_type VARCHAR
#         -- Optional: FOREIGN KEY (source_node_id_prop) REFERENCES ExtractedEntity(id_prop),
#         -- Optional: FOREIGN KEY (target_node_id_prop) REFERENCES ExtractedEntity(id_prop)
#     );
#     """
#     # 也可以为关系表的 (source, target, type) 创建复合唯一索引或普通索引以加速查询
#     relation_index_ddl = """
#     CREATE INDEX IF NOT EXISTS idx_relation_source_target_type 
#     ON KGExtractionRelation (source_node_id_prop, target_node_id_prop, relation_type);
#     """
    
#     ddl_commands = [node_table_ddl, relation_table_ddl, relation_index_ddl]

#     try:
#         with duckdb_kg.get_connection() as conn:
#             context.log.info("Executing DuckDB DDL commands...")
#             for command_idx, command in enumerate(ddl_commands):
#                 context.log.debug(f"Executing DDL {command_idx+1}:\n{command.strip()}")
#                 conn.execute(command)
#             context.log.info("DuckDB Schema DDL commands executed successfully.")
#     except Exception as e_ddl:
#         context.log.error(f"Error during DuckDB schema creation: {e_ddl}", exc_info=True)
#         raise
#     context.log.info("--- DuckDB Schema Creation Asset Finished ---")


# @dg.asset(
#     name="duckdb_nodes", # <--- 修改资产名称
#     description="Loads all unique extracted entities as nodes into DuckDB.",
#     group_name="kg_building",
#     deps=[duckdb_schema_asset, kg_extraction_asset] # <--- 修改依赖
# )
# def duckdb_nodes_asset(
#     context: dg.AssetExecutionContext,
#     kg_extractions: List[KGTripleSetOutput], # 来自 kg_extraction_asset 的输出
#     duckdb_kg: DuckDBResource,               # <--- 修改资源参数
#     embedder: GGUFEmbeddingResource          # 保持对 embedder 的依赖，用于生成嵌入
# ):
#         # --- START: 移动并强化初始日志 ---
#     print("<<<<< duckdb_nodes_asset FUNCTION ENTERED - PRINTING TO STDOUT >>>>>", flush=True) 
#     # 尝试使用 context.log，如果它此时可用
#     try:
#         context.log.info("<<<<< duckdb_nodes_asset FUNCTION CALLED - VIA CONTEXT.LOG - VERY BEGINNING >>>>>")
#     except Exception as e_log_init:
#         print(f"Context.log not available at the very beginning of duckdb_nodes_asset: {e_log_init}", flush=True)
#     # --- END: 移动并强化初始日志 ---

#     context.log.info("--- Starting DuckDB Node Loading Asset (Using INSERT ON CONFLICT) ---")
#     if not kg_extractions:
#         context.log.warning("No KG extractions received. Skipping node loading.")
#         return

#     # +++ 新增调试日志：检查表是否存在 +++
#     try:
#         with duckdb_kg.get_connection() as conn_debug:
#             context.log.info("Attempting to list tables in DuckDB from duckdb_nodes_asset:")
#             tables = conn_debug.execute("SHOW TABLES;").fetchall()
#             context.log.info(f"Tables found: {tables}")
#             if any('"ExtractedEntity"' in str(table_row).upper() for table_row in tables) or \
#                any('ExtractedEntity' in str(table_row) for table_row in tables) : # 检查大小写不敏感的匹配
#                 context.log.info("Table 'ExtractedEntity' (or similar) IS visible at the start of duckdb_nodes_asset.")
#             else:
#                 context.log.warning("Table 'ExtractedEntity' IS NOT visible at the start of duckdb_nodes_asset. Schema asset might not have run correctly or changes are not reflected.")
#     except Exception as e_debug_show:
#         context.log.error(f"Error trying to list tables in duckdb_nodes_asset: {e_debug_show}")
#     # +++ 结束新增调试日志 +++
    
#     unique_nodes_data_for_insert: List[Dict[str, Any]] = []
#     unique_nodes_keys = set() # 用于在Python层面去重，避免多次尝试插入相同实体

#     for kg_set in kg_extractions:
#         for entity in kg_set.extracted_entities:
#             # 规范化文本和标签，用于生成唯一键和存储
#             normalized_text = normalize_text_for_id(entity.text)
#             normalized_label = entity.label.upper() # 确保标签大写
            
#             # 为实体生成唯一ID (基于规范化文本和标签的哈希值)
#             # 注意：如果同一个实体（相同文本和标签）在不同chunk中被提取，它们的id_prop会一样
#             node_id_prop = hashlib.md5(f"{normalized_text}_{normalized_label}".encode('utf-8')).hexdigest()
            
#             node_unique_key_for_py_dedup = (node_id_prop) # 使用id_prop进行Python层面的去重

#             if node_unique_key_for_py_dedup not in unique_nodes_keys:
#                 unique_nodes_keys.add(node_unique_key_for_py_dedup)
                
#                 # 生成嵌入向量 (与KuzuDB时逻辑相同)
#                 embedding_vector_list = embedder.encode([normalized_text]) # embedder.encode期望一个列表
#                 final_embedding_for_db: List[float]

#                 if embedding_vector_list and embedding_vector_list[0] and \
#                    isinstance(embedding_vector_list[0], list) and \
#                    len(embedding_vector_list[0]) == embedder.get_embedding_dimension():
#                     final_embedding_for_db = embedding_vector_list[0]
#                 else:
#                     context.log.warning(f"Failed to generate valid embedding for node: {normalized_text} ({normalized_label}). Using zero vector. Embedding result: {embedding_vector_list}")
#                     final_embedding_for_db = [0.0] * embedder.get_embedding_dimension()
                    
#                 unique_nodes_data_for_insert.append({
#                     "id_prop": node_id_prop,
#                     "text": normalized_text,
#                     "label": normalized_label,
#                     "embedding": final_embedding_for_db # DuckDB的FLOAT[]可以直接接受Python的List[float]
#                 })

#     if not unique_nodes_data_for_insert:
#         context.log.warning("No unique nodes found in extractions to load into DuckDB.")
#         return

#     nodes_processed_count = 0
#     nodes_inserted_count = 0
#     nodes_updated_count = 0

#     upsert_sql = f"""
#     INSERT INTO "ExtractedEntity" (id_prop, text, label, embedding)
#     VALUES (?, ?, ?, ?)
#     ON CONFLICT (id_prop) DO UPDATE SET
#         text = excluded.text,
#         label = excluded.label,
#         embedding = excluded.embedding;
#     """
#     # excluded.column_name 用于引用试图插入但导致冲突的值

#     try:
#         with duckdb_kg.get_connection() as conn:
#             context.log.info(f"Attempting to UPSERT {len(unique_nodes_data_for_insert)} unique nodes into DuckDB ExtractedEntity table...")
            
#             # DuckDB 支持 executemany 用于批量操作，但对于 ON CONFLICT，逐条执行或构造大型 VALUES 列表可能更直接
#             # 或者使用 pandas DataFrame + duckdb.register + CREATE TABLE AS / INSERT INTO SELECT
#             # 这里为了清晰，我们先用循环执行，对于几千到几万个节点，性能尚可接受
#             # 如果节点数量非常大 (几十万以上)，应考虑更优化的批量upsert策略

#             for node_data_dict in unique_nodes_data_for_insert:
#                 params = (
#                     node_data_dict["id_prop"],
#                     node_data_dict["text"],
#                     node_data_dict["label"],
#                     node_data_dict["embedding"]
#                 )
#                 try:
#                     # conn.execute() 对于 DML (如 INSERT, UPDATE) 不直接返回受影响的行数
#                     # 但我们可以假设它成功了，除非抛出异常
#                     conn.execute(upsert_sql, params)
#                     # 无法直接判断是insert还是update，除非查询前后对比，这里简化处理
#                     nodes_processed_count += 1 
#                 except Exception as e_upsert_item:
#                     context.log.error(f"Error UPSERTING node with id_prop {node_data_dict.get('id_prop')} into DuckDB: {e_upsert_item}", exc_info=True)
            
#             # 我们可以查一下表中的总行数来间接了解情况
#             total_rows_after = conn.execute('SELECT COUNT(*) FROM "ExtractedEntity"').fetchone()[0]
#             context.log.info(f"Successfully processed {nodes_processed_count} node upsert operations into DuckDB.")
#             context.log.info(f"Total rows in ExtractedEntity table after upsert: {total_rows_after}")

#     except Exception as e_db_nodes:
#         context.log.error(f"Error during DuckDB node loading: {e_db_nodes}", exc_info=True)
#         raise
    
#     context.add_output_metadata({
#         "nodes_prepared_for_upsert": len(unique_nodes_data_for_insert),
#         "nodes_processed_by_upsert_statement": nodes_processed_count,
#     })
#     context.log.info("--- DuckDB Node Loading Asset Finished ---")


# @dg.asset(
#     name="duckdb_relations", # <--- 修改资产名称
#     description="Loads all extracted relationships into DuckDB.",
#     group_name="kg_building",
#     deps=[duckdb_nodes_asset] # <--- 修改依赖
# )
# def duckdb_relations_asset(
#     context: dg.AssetExecutionContext, 
#     kg_extractions: List[KGTripleSetOutput], # 来自 kg_extraction_asset
#     duckdb_kg: DuckDBResource                # <--- 修改资源参数
# ):
#     context.log.info("--- Starting DuckDB Relation Loading Asset ---")
#     if not kg_extractions:
#         context.log.warning("No KG extractions received. Skipping relation loading.")
#         return

#     relations_to_insert: List[Dict[str, str]] = []
#     unique_relation_keys = set() # 用于在Python层面去重

#     for kg_set in kg_extractions:
#         for rel in kg_set.extracted_relations:
#             # 从实体文本和标签生成源节点和目标节点的ID (与 duckdb_nodes_asset 中一致)
#             source_node_text_norm = normalize_text_for_id(rel.head_entity_text)
#             source_node_label_norm = rel.head_entity_label.upper()
#             source_node_id = hashlib.md5(f"{source_node_text_norm}_{source_node_label_norm}".encode('utf-8')).hexdigest()

#             target_node_text_norm = normalize_text_for_id(rel.tail_entity_text)
#             target_node_label_norm = rel.tail_entity_label.upper()
#             target_node_id = hashlib.md5(f"{target_node_text_norm}_{target_node_label_norm}".encode('utf-8')).hexdigest()
            
#             relation_type_norm = rel.relation_type.upper()

#             # 为关系本身生成一个唯一ID
#             relation_unique_str = f"{source_node_id}_{relation_type_norm}_{target_node_id}"
#             relation_id = hashlib.md5(relation_unique_str.encode('utf-8')).hexdigest()

#             if relation_id not in unique_relation_keys:
#                 unique_relation_keys.add(relation_id)
#                 relations_to_insert.append({
#                     "relation_id": relation_id,
#                     "source_node_id_prop": source_node_id,
#                     "target_node_id_prop": target_node_id,
#                     "relation_type": relation_type_norm
#                 })
    
#     if not relations_to_insert:
#         context.log.warning("No unique relations found in extractions to load into DuckDB.")
#         return

#     relations_processed_count = 0
    
#     # 使用 INSERT INTO ... ON CONFLICT DO NOTHING 来避免插入重复的关系 (基于 relation_id)
#     insert_sql = """
#     INSERT INTO KGExtractionRelation (relation_id, source_node_id_prop, target_node_id_prop, relation_type)
#     VALUES (?, ?, ?, ?)
#     ON CONFLICT (relation_id) DO NOTHING;
#     """

#     try:
#         with duckdb_kg.get_connection() as conn:
#             context.log.info(f"Attempting to INSERT {len(relations_to_insert)} unique relations into DuckDB KGExtractionRelation table...")
            
#             for rel_data_dict in relations_to_insert:
#                 params = (
#                     rel_data_dict["relation_id"],
#                     rel_data_dict["source_node_id_prop"],
#                     rel_data_dict["target_node_id_prop"],
#                     rel_data_dict["relation_type"]
#                 )
#                 try:
#                     conn.execute(insert_sql, params)
#                     # DuckDB的execute对于INSERT ON CONFLICT DO NOTHING不直接返回是否插入
#                     # 但我们可以假设它成功处理了（要么插入，要么忽略）
#                     relations_processed_count += 1
#                 except Exception as e_insert_item:
#                     context.log.error(f"Error INSERTING relation with id {rel_data_dict.get('relation_id')} into DuckDB: {e_insert_item}", exc_info=True)
            
#             total_rels_after = conn.execute("SELECT COUNT(*) FROM KGExtractionRelation").fetchone()[0]
#             context.log.info(f"Successfully processed {relations_processed_count} relation insert (ON CONFLICT DO NOTHING) operations.")
#             context.log.info(f"Total rows in KGExtractionRelation table after inserts: {total_rels_after}")

#     except Exception as e_db_rels:
#         context.log.error(f"Error during DuckDB relation loading: {e_db_rels}", exc_info=True)
#         raise
        
#     context.add_output_metadata({
#         "relations_prepared_for_insert": len(relations_to_insert),
#         "relations_processed_by_insert_statement": relations_processed_count,
#     })
#     context.log.info("--- DuckDB Relation Loading Asset Finished ---")



# @dg.asset(
#     name="duckdb_vector_index", # <--- 修改资产名称
#     description="Creates the HNSW vector index on the embedding column in DuckDB.",
#     group_name="kg_building",
#     deps=[duckdb_relations_asset]  # <--- 修改依赖
# )
# def duckdb_vector_index_asset(
#     context: dg.AssetExecutionContext, 
#     duckdb_kg: DuckDBResource # <--- 修改资源参数
# ):
#     context.log.info("--- Starting DuckDB Vector Index Creation Asset ---")
    
#     table_to_index = "ExtractedEntity"
#     column_to_index = "embedding"
#     # 索引名可以自定义，通常包含表名、列名和类型
#     index_name = f"{table_to_index}_{column_to_index}_hnsw_idx"
#     metric_type = "l2sq" # 欧氏距离的平方，与我们测试时一致

#     # DuckDB 的 CREATE INDEX ... USING HNSW 语句
#     # IF NOT EXISTS 确保了幂等性
#     index_creation_sql = f"""
#     CREATE INDEX IF NOT EXISTS {index_name} 
#     ON {table_to_index} USING HNSW ({column_to_index}) 
#     WITH (metric='{metric_type}');
#     """

#     try:
#         with duckdb_kg.get_connection() as conn:
#             # 在创建索引前，确保vss扩展已加载且持久化已开启 (虽然DuckDBResource的setup已做)
#             try:
#                 conn.execute("LOAD vss;")
#                 conn.execute("SET hnsw_enable_experimental_persistence=true;")
#                 context.log.info("DuckDB: VSS extension loaded and HNSW persistence re-confirmed for index creation asset.")
#             except Exception as e_vss_setup_idx:
#                 context.log.warning(f"DuckDB: Failed to re-confirm VSS setup for index asset: {e_vss_setup_idx}. "
#                                      "Proceeding, assuming it was set by DuckDBResource.")

#             context.log.info(f"Executing DuckDB vector index creation command:\n{index_creation_sql.strip()}")
#             conn.execute(index_creation_sql)
#             context.log.info(f"DuckDB vector index '{index_name}' creation command executed successfully (or index already existed).")

#     except Exception as e_index_asset:
#         context.log.error(f"Error during DuckDB vector index creation: {e_index_asset}", exc_info=True)
#         raise
    
#     context.log.info("--- DuckDB Vector Index Creation Asset Finished ---")


# --- 更新 all_processing_assets 列表 ---
all_processing_assets = [
    clean_chunk_text_asset,
    generate_embeddings_asset,
    vector_storage_asset,
    keyword_index_asset,
    # kg_extraction_asset,
    # duckdb_schema_asset,
    # duckdb_nodes_asset,
    # duckdb_relations_asset,
    # duckdb_vector_index_asset,
]


--------------------------------------------------------------------------------
文件: zhz_rag_pipeline/pydantic_models_dagster.py
--------------------------------------------------------------------------------
# zhz_rag_pipeline/pydantic_models_dagster.py
from typing import List, Dict, Any, Union, Optional, Literal
# --- 修改：从 pydantic 导入 BaseModel 和 Field ---
from pydantic import BaseModel, Field
# --- 修改结束 ---
import uuid
# from typing import List # 这行是多余的，因为上面已经从 typing 导入了 List

class LoadedDocumentOutput(BaseModel):
    document_path: str
    file_type: str
    # --- VITAL FIX: Make raw_content optional and always a string ---
    raw_content: Optional[str] = None # raw_content is now optional and will only hold decoded text
    metadata: Dict[str, Any]

# --- 修改：在 ParsedDocumentOutput 定义之前定义其依赖的 Element 类型 ---
class DocumentElementMetadata(BaseModel):
    """通用元数据，可附加到任何文档元素上"""
    page_number: Optional[int] = None
    source_coordinates: Optional[Dict[str, float]] = None # 例如，PDF中的bbox
    custom_properties: Optional[Dict[str, Any]] = None # 其他特定于元素的属性

class TitleElement(BaseModel):
    element_type: Literal["title"] = "title"
    text: str
    level: int # 例如 1 代表 H1, 2 代表 H2
    metadata: Optional[DocumentElementMetadata] = None

class NarrativeTextElement(BaseModel): # 普通段落文本
    element_type: Literal["narrative_text"] = "narrative_text"
    text: str
    metadata: Optional[DocumentElementMetadata] = None

class ListItemElement(BaseModel):
    element_type: Literal["list_item"] = "list_item"
    text: str
    level: int = 0 # 列表嵌套层级，0代表顶层列表项
    ordered: bool = False # True代表有序列表项, False代表无序
    item_number: Optional[Union[int, str]] = None # 例如 "1", "a", "*"
    metadata: Optional[DocumentElementMetadata] = None

class TableElement(BaseModel):
    element_type: Literal["table"] = "table"
    text_representation: Optional[str] = None 
    markdown_representation: Optional[str] = None
    html_representation: Optional[str] = None
    caption: Optional[str] = None
    metadata: Optional[DocumentElementMetadata] = None

class CodeBlockElement(BaseModel):
    element_type: Literal["code_block"] = "code_block"
    code: str
    language: Optional[str] = None
    metadata: Optional[DocumentElementMetadata] = None

class ImageElement(BaseModel): 
    element_type: Literal["image"] = "image"
    alt_text: Optional[str] = None
    caption: Optional[str] = None
    metadata: Optional[DocumentElementMetadata] = None

class PageBreakElement(BaseModel):
    element_type: Literal["page_break"] = "page_break"
    metadata: Optional[DocumentElementMetadata] = None
    
class HeaderElement(BaseModel):
    element_type: Literal["header"] = "header"
    text: str
    metadata: Optional[DocumentElementMetadata] = None

class FooterElement(BaseModel):
    element_type: Literal["footer"] = "footer"
    text: str
    metadata: Optional[DocumentElementMetadata] = None

DocumentElementType = Union[
    TitleElement, 
    NarrativeTextElement, 
    ListItemElement, 
    TableElement, 
    CodeBlockElement,
    ImageElement,
    PageBreakElement,
    HeaderElement,
    FooterElement
]

class ParsedDocumentOutput(BaseModel):
    parsed_text: str = Field(description="文档内容的线性化纯文本表示，尽可能保留语义。") 
    elements: List[DocumentElementType] = Field(default_factory=list, description="从文档中解析出的结构化元素列表。")
    original_metadata: Dict[str, Any] = Field(description="关于原始文档的元数据，如文件名、路径、大小等。")
    summary: Optional[str] = None
# --- 已有模型 ---
class ChunkOutput(BaseModel):
    chunk_id: str = Field(default_factory=lambda: str(uuid.uuid4())) # 确保 Field 被导入
    chunk_text: str
    source_document_id: str 
    chunk_metadata: Dict[str, Any]

class EmbeddingOutput(BaseModel):
    chunk_id: str 
    chunk_text: str 
    embedding_vector: List[float]
    embedding_model_name: str 
    original_chunk_metadata: Dict[str, Any]

class ExtractedEntity(BaseModel):
    text: str 
    label: str 

class ExtractedRelation(BaseModel):
    head_entity_text: str
    head_entity_label: str
    relation_type: str
    tail_entity_text: str
    tail_entity_label: str

class KGTripleSetOutput(BaseModel):
    chunk_id: str 
    extracted_entities: List[ExtractedEntity] = Field(default_factory=list)
    extracted_relations: List[ExtractedRelation] = Field(default_factory=list) 
    extraction_model_name: str 
    original_chunk_metadata: Dict[str, Any]


--------------------------------------------------------------------------------
文件: zhz_rag_pipeline/resources.py
--------------------------------------------------------------------------------
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/zhz_rag_pipeline/resources.py

import logging
import dagster as dg
import chromadb
from typing import List, Dict, Any, Optional, Iterator
import httpx
import json
import os
from contextlib import asynccontextmanager, contextmanager
from pydantic import Field as PydanticField, PrivateAttr
import asyncio
import time 
import duckdb
import sys
from queue import Empty
from pathlib import Path


# --- 日志和硬件管理器导入 ---
try:
    from zhz_rag.utils.interaction_logger import get_logger
except ImportError:
    import logging
    def get_logger(name: str) -> logging.Logger:
        logger = logging.getLogger(name)
        if not logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
            handler.setFormatter(formatter)
            logger.addHandler(handler)
        return logger

try:
    from zhz_rag.utils.hardware_manager import HardwareManager, HardwareInfo
except ImportError as e_hal_import:
    print(f"ERROR: Failed to import HardwareManager/HardwareInfo: {e_hal_import}. HAL features will be disabled.")
    HardwareManager = None
    HardwareInfo = None


# --- GGUFEmbeddingResource: API客户端版本 ---

class GGUFEmbeddingResourceConfig(dg.Config):
    """
    GGUFEmbeddingResource 的配置类。
    """
    api_url: str = PydanticField(
        default="http://127.0.0.1:8089",
        description="URL of the standalone embedding API service."
    )

class GGUFEmbeddingResource(dg.ConfigurableResource):
    """
    用于与GGUF嵌入API服务交互的Dagster资源。
    负责初始化HTTP客户端、执行健康检查和文本编码。
    """
    api_url: str

    _client: httpx.AsyncClient = PrivateAttr()
    _logger: Optional[dg.DagsterLogManager] = PrivateAttr(default=None)
    _dimension: Optional[int] = PrivateAttr(default=None)
    _batch_size: int = PrivateAttr(default=128)

    def setup_for_execution(self, context: Optional[dg.InitResourceContext] = None) -> None:
        """
        初始化资源。现在可以接受一个可选的Dagster上下文。
        如果context为None（在FastAPI等非Dagster环境中使用），则使用默认配置。

        Args:
            context: Dagster的初始化资源上下文，可选。
        """
        self._logger = context.log if context else logging.getLogger("GGUFEmbeddingResource")
        self._client = httpx.AsyncClient(base_url=self.api_url, timeout=600.0)

        # 动态计算批处理大小
        try:
            if context and hasattr(context, 'resources_by_key') and "system_resource" in context.resources_by_key:
                system_resource = context.resources_by_key["system_resource"]
                physical_cores = system_resource._hw_info.cpu_physical_cores if system_resource._hw_info else 4
                self._batch_size = max(128, physical_cores * 64)
            else:
                self._batch_size = 128
        except Exception as e:
            self._logger.error(f"Failed to dynamically set batch size: {e}. Using default 128.", exc_info=True)
            self._batch_size = 128
        # 健康检查
        try:
            response = httpx.get(f"{self.api_url}/health")
            response.raise_for_status()
            health_data = response.json()
            if health_data.get("model_loaded"):
                self._dimension = health_data.get("dimension")
            else:
                raise RuntimeError(f"Embedding service at {self.api_url} is not healthy.")
        except Exception as e:
            self._logger.error(f"Failed to connect to embedding service: {e}")
            raise RuntimeError("Could not initialize GGUFEmbeddingResource.") from e

    def teardown_for_execution(self, context: dg.InitResourceContext) -> None:
        """
        在资源执行结束后关闭HTTP客户端。

        Args:
            context: Dagster的初始化资源上下文。
        """
        if hasattr(self, '_client') and not self._client.is_closed:
            async def _close():
                await self._client.aclose()
            try:
                loop = asyncio.get_running_loop()
                loop.create_task(_close())
            except RuntimeError:
                asyncio.run(_close())

    def get_embedding_dimension(self) -> int:
        """
        获取嵌入向量的维度。

        Returns:
            int: 嵌入向量的维度。

        Raises:
            ValueError: 如果嵌入维度不可用。
        """
        if self._dimension is None:
            raise ValueError("Embedding dimension not available.")
        return self._dimension

    def encode(self, texts: List[str], **kwargs: Any) -> List[List[float]]:
        """
        将文本列表编码为嵌入向量。

        Args:
            texts: 待编码的文本列表。
            **kwargs: 额外的关键字参数。

        Returns:
            List[List[float]]: 文本对应的嵌入向量列表。
        """
        if not texts:
            return []

        all_embeddings: List[List[float]] = []
        
        # 批处理循环
        for i in range(0, len(texts), self._batch_size):
            batch_texts = texts[i:i + self._batch_size]
            
            async def _async_encode_batch():
                try:
                    response = await self._client.post("/embed", json={"texts": batch_texts})
                    response.raise_for_status()
                    data = response.json()
                    return data.get("embeddings", [])
                except httpx.RequestError as e:
                    self._logger.error(f"Request to embedding service failed for a batch: {e}")
                    return [[] for _ in batch_texts]
                except Exception as e:
                    self._logger.error(f"An unexpected error occurred during embedding a batch: {e}")
                    return [[] for _ in batch_texts]

            # 在循环内部执行异步调用
            try:
                loop = asyncio.get_running_loop()
                if loop.is_running():
                    future = asyncio.run_coroutine_threadsafe(_async_encode_batch(), loop)
                    batch_embeddings = future.result(timeout=600)
                else:
                    batch_embeddings = asyncio.run(_async_encode_batch())
            except RuntimeError:
                batch_embeddings = asyncio.run(_async_encode_batch())

            all_embeddings.extend(batch_embeddings)
            time.sleep(0.1) # 在批次之间加入一个微小的延迟，避免瞬间打爆API

        return all_embeddings

class ChromaDBResourceConfig(dg.Config):
    """
    ChromaDBResource 的配置类。
    """
    collection_name: str = PydanticField(
        default="zhz_rag_collection",
        description="Name of the ChromaDB collection."
    )
    persist_directory: str = PydanticField(
        default=os.path.join(os.getenv("ZHZ_AGENT_PROJECT_ROOT", "/home/zhz/zhz_agent"), "zhz_rag", "stored_data", "chromadb_index"),
        description="Directory to persist ChromaDB data."
    )

class ChromaDBResource(dg.ConfigurableResource):
    """
    用于管理ChromaDB向量数据库的Dagster资源。
    支持初始化客户端、添加嵌入和查询嵌入。
    """
    collection_name: str
    persist_directory: str

    _client: Optional[chromadb.PersistentClient] = PrivateAttr(default=None)
    _collection: Optional[Any] = PrivateAttr(default=None)
    _logger: Optional[dg.DagsterLogManager] = PrivateAttr(default=None)
    _batch_size: int = PrivateAttr(default=4096)

    def setup_for_execution(self, context: dg.InitResourceContext) -> None:
        """
        初始化ChromaDB客户端和集合。

        Args:
            context: Dagster的初始化资源上下文。

        Raises:
            RuntimeError: 如果ChromaDB客户端或集合初始化失败。
        """
        self._logger = context.log
        os.makedirs(self.persist_directory, exist_ok=True)
        
        try:
            self._client = chromadb.PersistentClient(path=self.persist_directory)
            self._collection = self._client.get_or_create_collection(name=self.collection_name)
        except Exception as e:
            self._logger.error(f"Failed to initialize ChromaDB client or collection: {e}", exc_info=True)
            raise RuntimeError(f"Could not initialize ChromaDBResource due to: {e}") from e

    def teardown_for_execution(self, context: dg.InitResourceContext) -> None:
        """
        清理ChromaDB资源。

        Args:
            context: Dagster的初始化资源上下文。
        """
        if self._client:
            pass # PersistentClient doesn't need explicit close
        self._client = None
        self._collection = None

    def add_embeddings(
        self, 
        ids: List[str], 
        embeddings: List[List[float]], 
        documents: Optional[List[str]] = None, 
        metadatas: Optional[List[Dict[str, Any]]] = None
    ) -> None:
        """
        向ChromaDB集合添加嵌入向量。

        Args:
            ids: 文档ID列表。
            embeddings: 嵌入向量列表。
            documents: 原始文档文本列表，可选。
            metadatas: 文档元数据列表，可选。

        Raises:
            RuntimeError: 如果ChromaDB集合未初始化。
            Exception: 如果添加批次到ChromaDB失败。
        """
        if self._collection is None:
            msg = "ChromaDB collection is not initialized. Cannot add embeddings."
            self._logger.error(msg)
            raise RuntimeError(msg)
        
        if not ids:
            self._logger.warning("add_embeddings called with empty IDs list. Nothing to add.")
            return

        total_items = len(ids)
        for i in range(0, total_items, self._batch_size):
            batch_end = min(i + self._batch_size, total_items)
            batch_ids = ids[i:batch_end]
            batch_embeddings = embeddings[i:batch_end]
            batch_documents = documents[i:batch_end] if documents else None
            batch_metadatas = metadatas[i:batch_end] if metadatas else None

            try:
                self._collection.add(
                    ids=batch_ids,
                    embeddings=batch_embeddings,
                    documents=batch_documents,
                    metadatas=batch_metadatas
                )
            except Exception as e:
                self._logger.error(
                    f"Failed to add batch starting at index {i} to ChromaDB: {e}",
                    exc_info=True
                )
                raise
        
    def query_embeddings(
        self,
        query_embeddings: List[List[float]],
        n_results: int = 5,
        where_filter: Optional[Dict[str, Any]] = None,
        include: Optional[List[str]] = None
    ) -> Optional[Dict[str, Any]]:
        """
        从ChromaDB集合查询嵌入向量。

        Args:
            query_embeddings: 查询嵌入向量列表。
            n_results: 返回结果的数量，默认为5。
            where_filter: 用于过滤结果的条件字典，可选。
            include: 包含在结果中的字段，默认为["metadatas", "documents", "distances"]。

        Returns:
            Optional[Dict[str, Any]]: 查询结果字典，如果查询失败则为None。

        Raises:
            RuntimeError: 如果ChromaDB集合未初始化。
            Exception: 如果查询嵌入失败。
        """
        if self._collection is None:
            msg = "ChromaDB collection is not initialized. Cannot query embeddings."
            self._logger.error(msg)
            raise RuntimeError(msg)

        if include is None:
            include = ["metadatas", "documents", "distances"]

        try:
            results = self._collection.query(
                query_embeddings=query_embeddings,
                n_results=n_results,
                where=where_filter,
                include=include
            )
            return results
        except Exception as e:
            self._logger.error(f"Failed to query embeddings from ChromaDB: {e}", exc_info=True)
            raise
        

class LocalLLMAPIResourceConfig(dg.Config):
    """
    LocalLLMAPIResource 的配置类。
    """
    api_url: str = "http://127.0.0.1:8088/v1/chat/completions"
    default_temperature: float = 0.1
    default_max_new_tokens: int = 2048

class LocalLLMAPIResource(dg.ConfigurableResource):
    """
    用于与本地LLM API服务交互的Dagster资源。
    """
    api_url: str
    default_temperature: float
    default_max_new_tokens: int
    _logger: Optional[dg.DagsterLogManager] = PrivateAttr(default=None)

    def setup_for_execution(self, context: dg.InitResourceContext) -> None:
        """
        初始化本地LLM API资源。

        Args:
            context: Dagster的初始化资源上下文。
        """
        self._logger = context.log

    async def generate_structured_output(self, prompt: str, json_schema: Dict[str, Any], temperature: Optional[float] = None, max_new_tokens: Optional[int] = None) -> Dict[str, Any]:
        """
        从本地LLM生成结构化输出。

        Args:
            prompt: 发送给LLM的提示。
            json_schema: 期望的JSON输出结构。
            temperature: 生成温度，可选。
            max_new_tokens: 生成的最大新token数量，可选。

        Returns:
            Dict[str, Any]: LLM生成的结构化JSON输出。

        Raises:
            ValueError: 如果LLM响应格式不正确。
            Exception: 如果调用本地LLM失败。
        """
        logger_instance = self._logger if self._logger else dg.get_dagster_logger()
        temp_to_use = temperature if temperature is not None else self.default_temperature
        tokens_to_use = max_new_tokens if max_new_tokens is not None else self.default_max_new_tokens
        messages = [{"role": "user", "content": prompt}]
        payload = {"model": "local_kg_extraction_model", "messages": messages, "temperature": temp_to_use, "max_tokens": tokens_to_use, "response_format": {"type": "json_object", "schema": json_schema}}
        try:
            async with httpx.AsyncClient(timeout=httpx.Timeout(300.0)) as client:
                response = await client.post(self.api_url, json=payload)
                response.raise_for_status()
                response_json = response.json()
                if response_json.get("choices") and response_json["choices"][0].get("message"):
                    generated_text = response_json["choices"][0]["message"].get("content", "")
                    return json.loads(generated_text)
                raise ValueError(f"Local LLM response format is incorrect: {response_json}")
        except Exception as e:
            logger_instance.error(f"Error during Local LLM call: {e}", exc_info=True)
            raise

class GeminiAPIResourceConfig(dg.Config):
    """
    GeminiAPIResource 的配置类。
    """
    model_name: str = PydanticField(default="gemini/gemini-1.5-flash-latest", description="Name of the Gemini model.")
    proxy_url: Optional[str] = PydanticField(default_factory=lambda: os.getenv("LITELLM_PROXY_URL"), description="Optional proxy URL for LiteLLM.")
    default_temperature: float = 0.1
    default_max_tokens: int = 2048
    
class GeminiAPIResource(dg.ConfigurableResource):
    """
    用于通过LiteLLM调用Gemini API的Dagster资源。
    """
    model_name: str
    proxy_url: Optional[str]
    default_temperature: float
    default_max_tokens: int
    _api_key: Optional[str] = PrivateAttr(default=None)
    _logger: Optional[dg.DagsterLogManager] = PrivateAttr(default=None)

    def setup_for_execution(self, context: dg.InitResourceContext) -> None:
        """
        初始化Gemini API资源，加载API密钥。

        Args:
            context: Dagster的初始化资源上下文。
        """
        self._logger = context.log
        self._api_key = os.getenv("GEMINI_API_KEY") or os.getenv("GOOGLE_API_KEY")
        if not self._api_key: self._logger.warning("Gemini API key not found.")

    async def call_completion(self, messages: List[Dict[str, str]], temperature: Optional[float] = None, max_tokens: Optional[int] = None) -> Optional[str]:
        """
        调用Gemini API生成文本补全。

        Args:
            messages: 对话消息列表。
            temperature: 生成温度，可选。
            max_tokens: 生成的最大token数量，可选。

        Returns:
            Optional[str]: 生成的文本内容，如果调用失败则为None。
        """
        import litellm
        logger_instance = self._logger if self._logger else dg.get_dagster_logger()
        if not self._api_key: return None
        litellm_params = {"model": self.model_name, "messages": messages, "api_key": self._api_key, "temperature": temperature or self.default_temperature, "max_tokens": max_tokens or self.default_max_tokens}
        if self.proxy_url: litellm_params["proxy"] = {"http": self.proxy_url, "https": self.proxy_url}
        try:
            response = await litellm.acompletion(**litellm_params)
            return response.choices[0].message.content if response and response.choices else None
        except Exception as e:
            logger_instance.error(f"Error calling Gemini via LiteLLM: {e}", exc_info=True)
            return None
        
class DuckDBResource(dg.ConfigurableResource):
    """
    用于管理DuckDB数据库连接的Dagster资源。
    支持连接数据库、加载VSS扩展和执行检查点。
    """
    db_file_path: str = PydanticField(
        default=os.path.join(os.getenv("ZHZ_AGENT_PROJECT_ROOT", "/home/zhz/zhz_agent"), "zhz_rag", "stored_data", "duckdb_knowledge_graph.db"),
        description="Path to the DuckDB database file."
    )
    _conn: Optional[duckdb.DuckDBPyConnection] = PrivateAttr(default=None)
    _logger: Optional[dg.DagsterLogManager] = PrivateAttr(default=None)

    def setup_for_execution(self, context: dg.InitResourceContext) -> None:
        """
        初始化DuckDB连接并加载VSS扩展。

        Args:
            context: Dagster的初始化资源上下文。

        Raises:
            RuntimeError: 如果DuckDB连接或VSS设置失败。
        """
        self._logger = context.log

        os.makedirs(os.path.dirname(self.db_file_path), exist_ok=True)
        
        try:
            self._conn = duckdb.connect(database=self.db_file_path, read_only=False)

            self._conn.execute("INSTALL vss;")
            self._conn.execute("LOAD vss;")
            self._conn.execute("SET hnsw_enable_experimental_persistence=true;")

        except Exception as e:
            self._logger.error(f"Error during DuckDB connection or VSS setup: {e}", exc_info=True)
            error_str = str(e).lower()
            if "already installed" in error_str or "already loaded" in error_str:
                self._logger.warning(f"VSS extension seems to be already installed/loaded, continuing...")
            else:
                raise RuntimeError(f"DuckDB connection/VSS setup failed: {e}") from e
        
    def teardown_for_execution(self, context: dg.InitResourceContext) -> None:
        """
        在资源执行结束后关闭DuckDB连接并执行检查点。

        Args:
            context: Dagster的初始化资源上下文。
        """
        if self._conn:
            try:
                self._conn.execute("CHECKPOINT;")
            except Exception as e_checkpoint:
                self._logger.error(f"Error executing CHECKPOINT for DuckDB: {e_checkpoint}", exc_info=True)
            finally:
                self._conn.close()
                self._conn = None 
        else:
            self._logger.info("No active DuckDB connection to teardown.")

    @contextmanager
    def get_connection(self) -> Iterator[duckdb.DuckDBPyConnection]:
        """
        获取DuckDB数据库连接的上下文管理器。

        Yields:
            duckdb.DuckDBPyConnection: DuckDB数据库连接对象。

        Raises:
            ConnectionError: 如果DuckDB连接未建立。
        """
        if not self._conn:
            raise ConnectionError("DuckDB connection not established. Ensure setup_for_execution was successful.")
        yield self._conn

class SystemResource(dg.ConfigurableResource):
    """
    用于获取系统硬件信息和推荐并发任务数的Dagster资源。
    """
    _hw_manager: Optional[Any] = PrivateAttr(default=None)
    _hw_info: Optional[Any] = PrivateAttr(default=None)
    _logger: Optional[dg.DagsterLogManager] = PrivateAttr(default=None)

    def setup_for_execution(self, context: dg.InitResourceContext) -> None:
        """
        初始化系统资源，检测硬件信息。

        Args:
            context: Dagster的初始化资源上下文。
        """
        self._logger = context.log
        if HardwareManager:
            self._hw_manager = HardwareManager()
            self._hw_info = self._hw_manager.get_hardware_info()
        else:
            self._logger.warning("HardwareManager not available.")

    def get_recommended_concurrent_tasks(self, task_type: str = "cpu_bound_llm") -> int:
        """
        获取推荐的并发任务数。

        Args:
            task_type: 任务类型，默认为"cpu_bound_llm"。

        Returns:
            int: 推荐的并发任务数。
        """
        if self._hw_manager: return self._hw_manager.recommend_concurrent_tasks(task_type=task_type)
        return 1


--------------------------------------------------------------------------------
文件: parsers/__init__.py
--------------------------------------------------------------------------------
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/zhz_rag_pipeline/parsers/__init__.py
import logging # 添加 logging 导入
from typing import Callable, Dict, Any, Optional, Union

# 尝试导入 Pydantic 模型，如果失败，则类型别名使用 Any
try:
    from ..pydantic_models_dagster import ParsedDocumentOutput
    _ParserOutputType = Optional[ParsedDocumentOutput]
except ImportError:
    _ParserOutputType = Optional[Any] # Fallback

# 定义一个类型别名，表示解析函数的签名
# 输入可以是路径(str)或内容(str/bytes)，元数据字典，返回Pydantic模型或字典
ParserFunction = Callable[[Union[str, bytes], Dict[str, Any]], _ParserOutputType]

# 从各个解析器模块导入主解析函数
from .md_parser import parse_markdown_to_structured_output
from .docx_parser import parse_docx_to_structured_output
from .pdf_parser import parse_pdf_to_structured_output
from .xlsx_parser import parse_xlsx_to_structured_output
from .html_parser import parse_html_to_structured_output
from .txt_parser import parse_txt_to_structured_output

logger = logging.getLogger(__name__) # 添加 logger 实例

# 创建一个解析器注册表 (合并自 parser_dispatcher.py)
PARSER_REGISTRY: Dict[str, ParserFunction] = {
    ".md": parse_markdown_to_structured_output,
    ".docx": parse_docx_to_structured_output,
    ".pdf": parse_pdf_to_structured_output,
    ".xlsx": parse_xlsx_to_structured_output,
    ".html": parse_html_to_structured_output,
    ".htm": parse_html_to_structured_output,  # Alias for html
    ".txt": parse_txt_to_structured_output,
}

def dispatch_parsing( # 合并自 parser_dispatcher.py
    file_extension: str,
    content_or_path: Union[str, bytes], # 确保这里是 Union[str, bytes]
    original_metadata: Dict[str, Any]
) -> Optional[Any]: # 返回 Optional[Any] 以匹配下游期望
    parser_func = PARSER_REGISTRY.get(file_extension.lower())
    if parser_func:
        try:
            # 调用相应的解析函数
            # txt_parser 和 md_parser, html_parser 期望 content_str
            # docx_parser, pdf_parser, xlsx_parser 期望 file_path
            # content_or_path 变量在 ingestion_assets.py 中已经根据 file_ext 做了区分
            return parser_func(content_or_path, original_metadata)
        except Exception as e:
            logger.error(f"Error calling parser for '{file_extension}' on '{original_metadata.get('source_file_path', 'N/A')}': {e}", exc_info=True)
            return None # 解析失败返回 None
    else:
        logger.warning(f"No specific parser registered for file type '{file_extension}'.")
        # 尝试一个通用的纯文本提取作为最终回退（如果适用且有实现）
        # 或者直接返回None
        return None

def get_parser(file_extension: str) -> Optional[ParserFunction]: # 保留此函数以防其他地方用到
    return PARSER_REGISTRY.get(file_extension.lower())

__all__ = [
    "parse_markdown_to_structured_output",
    "parse_docx_to_structured_output",
    "parse_pdf_to_structured_output",
    "parse_xlsx_to_structured_output",
    "parse_html_to_structured_output",
    "parse_txt_to_structured_output",
    "get_parser", # 保留
    "dispatch_parsing", # 新增导出
    "PARSER_REGISTRY",
    "ParserFunction"
]


--------------------------------------------------------------------------------
文件: parsers/docx_parser.py
--------------------------------------------------------------------------------
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/zhz_rag_pipeline/parsers/docx_parser.py

import os
from typing import List, Dict, Any, Optional, Union
import re

# --- 添加：为当前模块的 logger 进行基本配置 ---
import logging
logger = logging.getLogger(__name__)
if not logger.handlers: # 避免重复添加 handler (如果模块被多次导入)
    handler = logging.StreamHandler() # 输出到 stderr，通常会被 Dagster 捕获
    formatter = logging.Formatter('%(asctime)s - %(name)s - %(levelname)s - %(message)s')
    handler.setFormatter(formatter)
    logger.addHandler(handler)
    logger.setLevel(logging.INFO) # 设置希望看到的最低日志级别 (INFO, DEBUG等)
    # logger.propagate = False # 可以考虑设置，防止日志向上传播到根logger导致重复打印，但通常 Dagster 会处理好
logger.info(f"Logger for {__name__} configured in docx_parser.py.") # 确认配置生效
# --- 结束添加 ---

# --- 依赖导入与可用性检查 ---
try:
    from unstructured.partition.docx import partition_docx
    from unstructured.documents.elements import (
        Element as UnstructuredElement,
        Text, 
        NarrativeText,
        Title,
        ListItem,
        Table,
        Image as UnstructuredImage, 
        Header as UnstructuredHeader, 
        Footer as UnstructuredFooter, 
        Address,
        EmailAddress,
        FigureCaption,
        PageBreak as UnstructuredPageBreak, 
        CodeSnippet
    )
    _UNSTRUCTURED_AVAILABLE_DOCX = True
    logging.info("Successfully imported Unstructured for DOCX parsing.")
except ImportError as e_unstructured:
    logging.error(f"Failed to import Unstructured for DOCX: {e_unstructured}. DOCX parsing will have limited functionality.")
    _UNSTRUCTURED_AVAILABLE_DOCX = False
    # 创建占位符类以避免后续 NameError
    class UnstructuredElement: pass
    class Text: pass                  # type: ignore
    class NarrativeText: pass          # type: ignore
    class Title: pass                  # type: ignore
    class ListItem: pass               # type: ignore
    class Table: pass                  # type: ignore
    class UnstructuredImage: pass      # type: ignore
    class UnstructuredHeader: pass     # type: ignore
    class UnstructuredFooter: pass     # type: ignore
    class Address: pass                # type: ignore
    class EmailAddress: pass           # type: ignore
    class FigureCaption: pass          # type: ignore
    class UnstructuredPageBreak: pass  # type: ignore
    class CodeSnippet: pass            # type: ignore

try:
    from markdownify import markdownify as md # type: ignore
    _MARKDOWNIFY_AVAILABLE = True
except ImportError:
    logging.warning("markdownify library not found. HTML table to Markdown conversion will be skipped.")
    _MARKDOWNIFY_AVAILABLE = False
    def md(html_content: str) -> str: # Fallback
        return f"[Markdownify not available. HTML content: {html_content[:100]}...]"

_PYDANTIC_MODELS_AVAILABLE_DOCX = False
try:
    from ..pydantic_models_dagster import (
        ParsedDocumentOutput, DocumentElementType, TitleElement, NarrativeTextElement,
        ListItemElement, TableElement, CodeBlockElement, PageBreakElement, ImageElement,
        HeaderElement, FooterElement, DocumentElementMetadata
    )
    _PYDANTIC_MODELS_AVAILABLE_DOCX = True
except ImportError:
    class BaseModel: pass
    class DocumentElementMetadata(BaseModel): page_number: Optional[int] = None
    
    class ParsedDocumentOutput(BaseModel): parsed_text: str; elements: list; original_metadata: dict; summary: Optional[str] = None
    class TitleElement(BaseModel): element_type:str="title"; text:str; level:int; metadata: Optional[DocumentElementMetadata] = None
    class NarrativeTextElement(BaseModel): element_type:str="narrative_text"; text:str; metadata: Optional[DocumentElementMetadata] = None
    class ListItemElement(BaseModel): element_type:str="list_item"; text:str; level:int=0; ordered:bool=False; item_number:Optional[Union[int, str]]=None; metadata: Optional[DocumentElementMetadata] = None
    class TableElement(BaseModel): # <--- 修改此行
        element_type:str="table"; 
        markdown_representation:Optional[str]=None; 
        html_representation:Optional[str]=None; 
        text_representation:Optional[str]=None; # <--- 添加此字段
        caption:Optional[str]=None; 
        metadata: Optional[DocumentElementMetadata] = None
    class CodeBlockElement(BaseModel): element_type:str="code_block"; code:str; language:Optional[str]=None; metadata: Optional[DocumentElementMetadata] = None
    class PageBreakElement(BaseModel): element_type:str="page_break"; metadata: Optional[DocumentElementMetadata] = None
    class ImageElement(BaseModel): element_type:str="image"; alt_text:Optional[str]=None; caption:Optional[str]=None; metadata: Optional[DocumentElementMetadata] = None
    class HeaderElement(BaseModel): element_type:str="header"; text:str; metadata: Optional[DocumentElementMetadata] = None
    class FooterElement(BaseModel): element_type:str="footer"; text:str; metadata: Optional[DocumentElementMetadata] = None
    DocumentElementType = Any

logger = logging.getLogger(__name__)

# --- 辅助函数 ---
def _create_doc_element_metadata(unstructured_element: UnstructuredElement) -> Optional[Union[DocumentElementMetadata, Dict[str, Any]]]:
    if not hasattr(unstructured_element, 'metadata'):
        return None
        
    meta_data_dict: Dict[str, Any] = {}
    if hasattr(unstructured_element.metadata, 'page_number') and unstructured_element.metadata.page_number is not None:
        meta_data_dict['page_number'] = unstructured_element.metadata.page_number
    
    if hasattr(unstructured_element.metadata, 'filename'):
        meta_data_dict['source_filename'] = unstructured_element.metadata.filename
    if hasattr(unstructured_element.metadata, 'filetype'):
        meta_data_dict['source_filetype'] = unstructured_element.metadata.filetype

    if not meta_data_dict:
        return None

    if _PYDANTIC_MODELS_AVAILABLE_DOCX:
        return DocumentElementMetadata(**meta_data_dict)
    else:
        return meta_data_dict

def _convert_unstructured_elements_to_custom(
    unstructured_elements: List[UnstructuredElement], 
    doc_path_for_log: str # 添加一个参数用于日志记录
) -> List[Any]:
    custom_elements: List[Any] = []
    
    file_basename_for_log = os.path.basename(doc_path_for_log)

    # --- 使用 print 进行强制调试 ---
    logger.info(f"DOCX Parser ({file_basename_for_log}): _convert_unstructured_elements_to_custom received {len(unstructured_elements)} elements from unstructured.")
    if not unstructured_elements:
        logger.warning(f"DOCX Parser ({file_basename_for_log}): Unstructured returned an empty list of elements. No custom elements will be generated by this function initially.")
    
    if not _UNSTRUCTURED_AVAILABLE_DOCX:
        logger.warning(f"DOCX Parser ({file_basename_for_log}): Unstructured library is not available (should have been caught earlier).")
        # 作为回退，我们可以尝试将每个元素的文本提取为 NarrativeTextElement (如果 unstructured_elements 非空但 _UNSTRUCTURED_AVAILABLE_DOCX 意外为 False)
        for el_idx, el_fallback in enumerate(unstructured_elements):
            fallback_text = getattr(el_fallback, 'text', f"[Unstructured not fully available - Element {el_idx+1} in {file_basename_for_log}]").strip()
            if fallback_text:
                if _PYDANTIC_MODELS_AVAILABLE_DOCX:
                    custom_elements.append(NarrativeTextElement(text=fallback_text))
                else:
                    custom_elements.append({"element_type": "narrative_text", "text": fallback_text})
        return custom_elements

    for el_idx, el in enumerate(unstructured_elements):
        el_type_name = type(el).__name__
        el_id_str = getattr(el, 'id', 'N/A')
        el_text_preview = getattr(el, 'text', '')[:50].strip().replace('\n', ' ') if getattr(el, 'text', '') else "[NO TEXT]"
        # --- 修改日志级别 ---
        logger.debug( # <--- 从 info 修改为 debug
            f"DOCX Parser ({file_basename_for_log}): Processing unstructured element index {el_idx}, "
            f"Type: {el_type_name}, ID: {el_id_str}, Text Preview: '{el_text_preview}'"
        )
        
        # 打印 el.metadata.text_as_html 的预览（如果存在）
        html_preview_from_meta = getattr(el.metadata, 'text_as_html', None) if hasattr(el, 'metadata') else None
        if html_preview_from_meta:
            logger.debug( # <--- 从 info 修改为 debug
                f"  └─ ({file_basename_for_log}) Unstructured Element (idx {el_idx}, type {el_type_name}) has text_as_html (len: {len(html_preview_from_meta)}). Preview: {html_preview_from_meta[:70]}"
            )
        # --- 结束修改 ---
        
        element_metadata = _create_doc_element_metadata(el)
        el_text = el.text.strip() if hasattr(el, 'text') and el.text else ""
        custom_el: Optional[Any] = None

        if isinstance(el, Title):
            level = el.metadata.category_depth if hasattr(el.metadata, 'category_depth') and el.metadata.category_depth is not None else 1
            if el_text:
                logger.debug(f"DOCX Parser ({file_basename_for_log}): Element index {el_idx} is Title (level {level}).")
                if _PYDANTIC_MODELS_AVAILABLE_DOCX: custom_el = TitleElement(text=el_text, level=level, metadata=element_metadata)
                else: custom_el = {"element_type": "title", "text": el_text, "level": level, "metadata": element_metadata}
        
        elif isinstance(el, ListItem):
            level = el.metadata.category_depth if hasattr(el.metadata, 'category_depth') and el.metadata.category_depth is not None else 0
            # 尝试从元数据获取更精确的列表信息 (unstructured 可能提供)
            # item_number 和 ordered 的逻辑可以根据 unstructured 的实际输出来完善
            if el_text:
                logger.debug(f"DOCX Parser ({file_basename_for_log}): Element index {el_idx} is ListItem (level {level}).")
                if _PYDANTIC_MODELS_AVAILABLE_DOCX: custom_el = ListItemElement(text=el_text, level=level, metadata=element_metadata)
                else: custom_el = {"element_type": "list_item", "text": el_text, "level": level, "metadata": element_metadata}

        elif isinstance(el, Table):
            logger.info(f"DOCX Parser ({file_basename_for_log}): Element index {el_idx} IS an unstructured.documents.elements.Table object.")
            html_table = el.metadata.text_as_html if hasattr(el.metadata, 'text_as_html') else None
            
            if html_table:
                logger.info(f"DOCX Parser ({file_basename_for_log}): Found HTML table content for Table element (idx {el_idx}). Length: {len(html_table)}. Preview: {html_table[:150]}")
            else:
                logger.warning(f"DOCX Parser ({file_basename_for_log}): No HTML table content (el.metadata.text_as_html) found for Table element (idx {el_idx}) from unstructured. Element ID: {el.id if hasattr(el, 'id') else 'N/A'}")

            md_table = None
            if html_table and _MARKDOWNIFY_AVAILABLE:
                try: 
                    md_table = md(html_table)
                    logger.info(f"DOCX Parser ({file_basename_for_log}): Successfully converted HTML table (idx {el_idx}) to Markdown. MD Length: {len(md_table) if md_table else 0}")
                except Exception as e_md: 
                    logger.warning(f"DOCX Parser ({file_basename_for_log}): Failed to convert HTML table (idx {el_idx}) to Markdown: {e_md}. HTML: {html_table[:100]}")
            
            raw_table_text_fallback = el.text.strip() if hasattr(el, 'text') and el.text else None
            caption_text = None
            if hasattr(el.metadata, 'table_captions') and el.metadata.table_captions:
                    caption_obj = el.metadata.table_captions[0]
                    if hasattr(caption_obj, 'text'):
                            caption_text = caption_obj.text
            
            if not caption_text and hasattr(el.metadata, 'filename'): # Redundant if filename is always doc_path_for_log
                    caption_text = f"Table from {file_basename_for_log}" # Use basename
            final_caption = caption_text if caption_text else "Table"

            final_md_table = md_table
            final_html_table = html_table
            final_text_representation = None

            if not final_md_table and not final_html_table and raw_table_text_fallback:
                logger.info(f"DOCX Parser ({file_basename_for_log}): Table (idx {el_idx}) has no HTML/MD rep, but has raw text from unstructured: '{raw_table_text_fallback[:100]}...' Using it as text_representation.")
                final_text_representation = raw_table_text_fallback
            elif not final_md_table and not final_html_table and not raw_table_text_fallback:
                    logger.warning(f"DOCX Parser ({file_basename_for_log}): Table (idx {el_idx}) has no HTML, Markdown, or raw text representation from unstructured.")


            if _PYDANTIC_MODELS_AVAILABLE_DOCX: 
                custom_el = TableElement(
                    markdown_representation=final_md_table, 
                    html_representation=final_html_table, 
                    text_representation=final_text_representation,
                    caption=final_caption, 
                    metadata=element_metadata
                )
            else: 
                custom_el = {
                    "element_type": "table", 
                    "markdown_representation": final_md_table, 
                    "html_representation": final_html_table, 
                    "text_representation": final_text_representation,
                    "caption": final_caption, 
                    "metadata": element_metadata
                }
        
        elif isinstance(el, (NarrativeText, Text, Address, EmailAddress, FigureCaption)):
            if el_text:
                logger.debug(f"DOCX Parser ({file_basename_for_log}): Element index {el_idx} is NarrativeText/Text like.")
                if _PYDANTIC_MODELS_AVAILABLE_DOCX: custom_el = NarrativeTextElement(text=el_text, metadata=element_metadata)
                else: custom_el = {"element_type": "narrative_text", "text": el_text, "metadata": element_metadata}
        
        elif isinstance(el, UnstructuredHeader):
            if el_text:
                logger.debug(f"DOCX Parser ({file_basename_for_log}): Element index {el_idx} is Header.")
                if _PYDANTIC_MODELS_AVAILABLE_DOCX: custom_el = HeaderElement(text=el_text, metadata=element_metadata)
                else: custom_el = {"element_type": "header", "text": el_text, "metadata": element_metadata}
        
        elif isinstance(el, UnstructuredFooter):
            if el_text:
                logger.debug(f"DOCX Parser ({file_basename_for_log}): Element index {el_idx} is Footer.")
                if _PYDANTIC_MODELS_AVAILABLE_DOCX: custom_el = FooterElement(text=el_text, metadata=element_metadata)
                else: custom_el = {"element_type": "footer", "text": el_text, "metadata": element_metadata}

        elif isinstance(el, UnstructuredImage):
            alt_text = el_text if el_text else (el.metadata.filename if hasattr(el.metadata, 'filename') else "Image")
            logger.debug(f"DOCX Parser ({file_basename_for_log}): Element index {el_idx} is Image. Alt text: {alt_text}")
            if _PYDANTIC_MODELS_AVAILABLE_DOCX: custom_el = ImageElement(alt_text=alt_text, metadata=element_metadata)
            else: custom_el = {"element_type": "image", "alt_text": alt_text, "metadata": element_metadata}

        elif isinstance(el, UnstructuredPageBreak):
            logger.debug(f"DOCX Parser ({file_basename_for_log}): Element index {el_idx} is PageBreak.")
            if _PYDANTIC_MODELS_AVAILABLE_DOCX: custom_el = PageBreakElement(metadata=element_metadata)
            else: custom_el = {"element_type": "page_break", "metadata": element_metadata}
        
        elif isinstance(el, CodeSnippet):
            if el_text:
                logger.debug(f"DOCX Parser ({file_basename_for_log}): Element index {el_idx} is CodeSnippet.")
                if _PYDANTIC_MODELS_AVAILABLE_DOCX: custom_el = CodeBlockElement(code=el_text, metadata=element_metadata) # language can be inferred later if needed
                else: custom_el = {"element_type": "code_block", "code": el_text, "metadata": element_metadata}

        else: 
            # This is the catch-all for any other Unstructured element type
            # or if an element doesn't have text but we still want to represent it (though usually skipped if no text)
            if el_text: # Only create an element if there's text
                logger.warning(f"DOCX Parser ({file_basename_for_log}): Unhandled Unstructured element type: {el_type_name} at index {el_idx}. Treating as NarrativeText. Text: {el_text[:50]}")
                if _PYDANTIC_MODELS_AVAILABLE_DOCX: custom_el = NarrativeTextElement(text=el_text, metadata=element_metadata)
                else: custom_el = {"element_type": "narrative_text", "text": el_text, "_unstructured_type": el_type_name, "metadata": element_metadata}
            elif el_type_name != "CompositeElement": # CompositeElement often has no direct text but contains other elements
                    logger.debug(f"DOCX Parser ({file_basename_for_log}): Skipping Unstructured element type: {el_type_name} at index {el_idx} due to no text content.")

        if custom_el:
            custom_elements.append(custom_el)
            
    return custom_elements


def _generate_linear_text_from_custom_elements(elements: List[Any]) -> str:
    text_parts = []
    for el_data_any in elements:
        el_data: Dict[str, Any] = {}
        if _PYDANTIC_MODELS_AVAILABLE_DOCX and hasattr(el_data_any, 'model_dump'):
            el_data = el_data_any.model_dump(exclude_none=True)
        elif isinstance(el_data_any, dict):
            el_data = el_data_any
        else:
            continue

        el_type = el_data.get("element_type")
        text_content = el_data.get("text", "")
        
        current_element_text = ""
        if el_type == "title":
            current_element_text = f"\n{'#' * el_data.get('level',1)} {text_content}\n"
        elif el_type == "narrative_text":
            current_element_text = text_content + "\n"
        elif el_type == "list_item":
            prefix = f"{el_data.get('item_number', '')}. " if el_data.get('ordered') and el_data.get('item_number') else "- "
            indent = "  " * el_data.get('level', 0)
            current_element_text = f"{indent}{prefix}{text_content}\n"
        elif el_type == "table":
            caption = el_data.get('caption', 'Unnamed Table')
            md_repr = el_data.get('markdown_representation')
            if md_repr: current_element_text = f"\n[Table: {caption}]\n{md_repr}\n"
            elif el_data.get('html_representation'): current_element_text = f"\n[Table (HTML): {caption}]\n{el_data.get('html_representation')[:200]}...\n"
        elif el_type == "code_block":
            lang = el_data.get('language', "") or ""
            code_content = el_data.get('code', "")
            current_element_text = f"\n```{lang}\n{code_content}\n```\n"
        elif el_type == "page_break":
            current_element_text = "\n---\n"
        elif el_type == "header" or el_type == "footer":
            current_element_text = f"\n[{el_type.capitalize()}]: {text_content}\n"
        elif el_type == "image":
            alt_text = el_data.get('alt_text', 'Image')
            current_element_text = f"\n[Image: {alt_text}]\n"
        
        if current_element_text:
            text_parts.append(current_element_text)

    full_text = "".join(text_parts)
    full_text = re.sub(r'\n{3,}', '\n\n', full_text).strip() # Clean up excessive newlines
    return full_text


# --- 主解析函数 ---
def parse_docx_to_structured_output(
    file_path: str, 
    original_metadata: Dict[str, Any]
) -> Optional[Union[ParsedDocumentOutput, Dict[str, Any]]]:
    file_basename_for_log = os.path.basename(file_path)
    
    # --- 使用 print 进行强制调试 ---
    logger.info(f"DOCX Parser: Attempting to parse DOCX file: {file_basename_for_log} using Unstructured")
    
    if not _UNSTRUCTURED_AVAILABLE_DOCX:
        logger.error(f"DOCX Parser ({file_basename_for_log}): Unstructured library is not available. DOCX parsing cannot proceed.")
        return None
    try:
        unstructured_elements = partition_docx(
            filename=file_path, 
            strategy="fast", 
            infer_table_structure=True,
        )
        logger.info(f"DOCX Parser ({file_basename_for_log}): Unstructured partitioned DOCX. Found {len(unstructured_elements)} raw elements from partition_docx.")

        custom_elements = _convert_unstructured_elements_to_custom(unstructured_elements, file_path)
        logger.info(f"DOCX Parser ({file_basename_for_log}): _convert_unstructured_elements_to_custom created {len(custom_elements)} custom elements.")
        
        linear_text = _generate_linear_text_from_custom_elements(custom_elements)
        logger.info(f"DOCX Parser ({file_basename_for_log}): Generated linear text (len: {len(linear_text)}). Preview: {linear_text[:100].replace(chr(10), ' ')}")

        if not custom_elements and not linear_text.strip() and unstructured_elements:
            logger.warning(f"DOCX Parser ({file_basename_for_log}): partition_docx returned {len(unstructured_elements)} elements, "
                            "but no custom elements or linear text were generated. This might indicate all elements were skipped "
                            "or had no text content suitable for conversion.")
        elif not custom_elements and not linear_text.strip() and not unstructured_elements:
                logger.warning(f"DOCX Parser ({file_basename_for_log}): partition_docx returned 0 elements, "
                                "and no custom elements or linear text were generated.")

        if _PYDANTIC_MODELS_AVAILABLE_DOCX:
            return ParsedDocumentOutput(
                parsed_text=linear_text,
                elements=custom_elements, 
                original_metadata=original_metadata
            )
        else:
            return {
                "parsed_text": linear_text,
                "elements": custom_elements,
                "original_metadata": original_metadata
            }
            
    except FileNotFoundError:
        logger.error(f"DOCX Parser ({file_basename_for_log}): File not found: {file_path}")
        return None
    except ImportError as ie:
        logger.error(f"DOCX Parser ({file_basename_for_log}): ImportError during DOCX parsing with Unstructured: {ie}.")
        return None
    except Exception as e:
        logger.error(f"DOCX Parser ({file_basename_for_log}): Critical error parsing DOCX file: {e}", exc_info=True)
        error_message = f"[ERROR PARSING DOCX: {file_basename_for_log} - {type(e).__name__}: {str(e)}]"
        if _PYDANTIC_MODELS_AVAILABLE_DOCX:
            return ParsedDocumentOutput(
                parsed_text=error_message,
                elements=[],
                original_metadata=original_metadata
            )
        else:
            return {
                "parsed_text": error_message,
                "elements": [],
                "original_metadata": original_metadata
            }


--------------------------------------------------------------------------------
文件: parsers/html_parser.py
--------------------------------------------------------------------------------
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/zhz_rag_pipeline/parsers/html_parser.py
import os
import logging
from typing import List, Dict, Any, Optional, Union, Set
import re

try:
    from bs4 import BeautifulSoup, Tag, NavigableString
    _BS4_AVAILABLE = True
    logging.info("Successfully imported BeautifulSoup4 for HTML parsing.")
except ImportError:
    logging.error("BeautifulSoup4 (bs4) not found. HTML parsing will not be available.")
    _BS4_AVAILABLE = False
    class BeautifulSoup: pass # Placeholder
    class Tag: pass
    class NavigableString: pass


_PYDANTIC_MODELS_AVAILABLE_HTML = False
try:
    from ..pydantic_models_dagster import (
        ParsedDocumentOutput, DocumentElementType, TitleElement, NarrativeTextElement,
        ListItemElement, TableElement, CodeBlockElement, PageBreakElement,
        DocumentElementMetadata
    )
    _PYDANTIC_MODELS_AVAILABLE_HTML = True
except ImportError:
    class BaseModel: pass
    class DocumentElementMetadata(BaseModel): page_number: Optional[int] = None
    class ParsedDocumentOutput(BaseModel): parsed_text: str; elements: list; original_metadata: dict; summary: Optional[str] = None
    class TitleElement(BaseModel): element_type:str="title"; text:str; level:int; metadata: Optional[DocumentElementMetadata] = None
    class NarrativeTextElement(BaseModel): element_type:str="narrative_text"; text:str; metadata: Optional[DocumentElementMetadata] = None
    class ListItemElement(BaseModel): element_type:str="list_item"; text:str; level:int=0; ordered:bool=False; item_number:Optional[Union[int, str]]=None; metadata: Optional[DocumentElementMetadata] = None
    class TableElement(BaseModel): element_type:str="table"; markdown_representation:Optional[str]=None; html_representation:Optional[str]=None; caption:Optional[str]=None; metadata: Optional[DocumentElementMetadata] = None
    class CodeBlockElement(BaseModel): element_type:str="code_block"; code:str; language:Optional[str]=None; metadata: Optional[DocumentElementMetadata] = None
    class PageBreakElement(BaseModel): element_type:str="page_break"; metadata: Optional[DocumentElementMetadata] = None
    DocumentElementType = Any

logger = logging.getLogger(__name__)

# Tags to typically ignore for main content extraction
IGNORE_TAGS_HTML = ['script', 'style', 'nav', 'footer', 'header', 'aside', 'form', 'meta', 'link', 'button', 'input', 'noscript', 'iframe', 'canvas', 'svg', 'path']
# Tags that define a semantic block but we want to process their children
CONTAINER_TAGS_HTML = ['div', 'section', 'article', 'main', 'body', 'figure', 'figcaption', 'details', 'summary']


def _table_to_markdown(table_tag: Tag) -> str:
    """Converts a BeautifulSoup table Tag to a Markdown string."""
    md_rows = []
    header_processed = False
    
    # Process header (thead)
    thead = table_tag.find('thead')
    if thead:
        header_rows_tags = thead.find_all('tr')
        for hr_tag in header_rows_tags:
            header_cells = hr_tag.find_all(['th', 'td'])
            if header_cells:
                header_texts = [cell.get_text(separator=' ', strip=True) for cell in header_cells]
                md_rows.append("| " + " | ".join(header_texts) + " |")
                if not header_processed: # Add separator only after the first header row group
                    md_rows.append("| " + " | ".join(["---"] * len(header_texts)) + " |")
                    header_processed = True
    
    # Process body (tbody or direct tr in table)
    tbody = table_tag.find('tbody')
    if not tbody: # If no tbody, look for tr directly under table
        rows_to_process = table_tag.find_all('tr', recursive=False)
    else:
        rows_to_process = tbody.find_all('tr')
        
    for row_tag in rows_to_process:
        # Skip if this row was already processed as part of thead (if thead was missing)
        if not header_processed and row_tag.find('th'):
            header_cells = row_tag.find_all(['th', 'td'])
            header_texts = [cell.get_text(separator=' ', strip=True) for cell in header_cells]
            md_rows.append("| " + " | ".join(header_texts) + " |")
            md_rows.append("| " + " | ".join(["---"] * len(header_texts)) + " |")
            header_processed = True
            continue
        
        cell_texts = [cell.get_text(separator=' ', strip=True) for cell in row_tag.find_all('td')]
        if cell_texts: # Only add row if it has content
            md_rows.append("| " + " | ".join(cell_texts) + " |")
            
    return "\n".join(md_rows)

def _convert_html_tag_to_elements_recursive(tag: Tag, elements_list: List[Any], processed_tags: Set[Tag], current_list_level: int = 0):
    """
    Recursively processes a BeautifulSoup Tag and its children to extract structured elements.
    Modifies elements_list in place.
    """
    if tag in processed_tags or not isinstance(tag, Tag) or tag.name in IGNORE_TAGS_HTML:
        return

    tag_name = tag.name.lower()
    element_metadata = None # Placeholder for now, can be enhanced to include source line numbers etc.
    
    created_element = False

    if tag_name in ['h1', 'h2', 'h3', 'h4', 'h5', 'h6']:
        level = int(tag_name[1:])
        text = tag.get_text(strip=True)
        if text:
            if _PYDANTIC_MODELS_AVAILABLE_HTML: elements_list.append(TitleElement(text=text, level=level, metadata=element_metadata))
            else: elements_list.append({"element_type": "title", "text": text, "level": level, "metadata": element_metadata})
            created_element = True
    
    elif tag_name == 'p':
        text = tag.get_text(strip=True)
        if text:
            if _PYDANTIC_MODELS_AVAILABLE_HTML: elements_list.append(NarrativeTextElement(text=text, metadata=element_metadata))
            else: elements_list.append({"element_type": "narrative_text", "text": text, "metadata": element_metadata})
            created_element = True

    elif tag_name in ['ul', 'ol']:
        ordered = tag_name == 'ol'
        start_num = int(tag.get('start', '1')) if ordered else 1
        
        # Iterate over direct children that are <li>
        direct_li_children = [child for child in tag.children if isinstance(child, Tag) and child.name == 'li']
        for i, li_tag in enumerate(direct_li_children):
            if li_tag in processed_tags: continue
            
            # Extract text directly under <li>, excluding text from nested lists
            li_text_parts = []
            for content_child in li_tag.contents:
                if isinstance(content_child, NavigableString):
                    stripped_text = content_child.strip()
                    if stripped_text: li_text_parts.append(stripped_text)
                elif isinstance(content_child, Tag) and content_child.name not in ['ul', 'ol']: # Get text from non-list children
                    li_text_parts.append(content_child.get_text(strip=True))
            
            final_li_text = " ".join(li_text_parts).strip()

            if final_li_text:
                item_num_str = str(start_num + i) if ordered else None
                if _PYDANTIC_MODELS_AVAILABLE_HTML: elements_list.append(ListItemElement(text=final_li_text, level=current_list_level, ordered=ordered, item_number=item_num_str, metadata=element_metadata))
                else: elements_list.append({"element_type": "list_item", "text": final_li_text, "level": current_list_level, "ordered": ordered, "item_number": item_num_str, "metadata": element_metadata})
            
            processed_tags.add(li_tag) # Mark <li> as processed for its direct text
            # Recursively process children of this <li> for nested lists or other elements
            for child_of_li in li_tag.children:
                if isinstance(child_of_li, Tag):
                     _convert_html_tag_to_elements_recursive(child_of_li, elements_list, processed_tags, current_list_level + 1)
        created_element = True # The list itself is an element boundary

    elif tag_name == 'table':
        md_table = _table_to_markdown(tag)
        caption_tag = tag.find('caption')
        caption_text = caption_tag.get_text(strip=True) if caption_tag else None
        if md_table or caption_text : # Only add if table has content or caption
            if _PYDANTIC_MODELS_AVAILABLE_HTML: elements_list.append(TableElement(markdown_representation=md_table, html_representation=str(tag), caption=caption_text, metadata=element_metadata))
            else: elements_list.append({"element_type": "table", "markdown_representation": md_table, "html_representation": str(tag), "caption": caption_text, "metadata": element_metadata})
        created_element = True

    elif tag_name == 'pre':
        code_tag = tag.find('code')
        code_text, lang = "", None
        if code_tag:
            code_text = code_tag.get_text() # Keep original spacing and newlines
            lang_class = code_tag.get('class', [])
            if lang_class: lang = next((cls.split('language-')[-1] for cls in lang_class if cls.startswith('language-')), None)
        else:
            code_text = tag.get_text()
        
        if code_text.strip(): # Check if there's actual code
            if _PYDANTIC_MODELS_AVAILABLE_HTML: elements_list.append(CodeBlockElement(code=code_text.strip('\n'), language=lang, metadata=element_metadata))
            else: elements_list.append({"element_type": "code_block", "code": code_text.strip('\n'), "language": lang, "metadata": element_metadata})
        created_element = True

    elif tag_name == 'blockquote':
        text = tag.get_text(strip=True)
        if text:
            if _PYDANTIC_MODELS_AVAILABLE_HTML: elements_list.append(NarrativeTextElement(text=text, metadata=element_metadata))
            else: elements_list.append({"element_type": "narrative_text", "text": text, "_is_blockquote": True, "metadata": element_metadata})
        created_element = True

    elif tag_name == 'hr':
        if _PYDANTIC_MODELS_AVAILABLE_HTML: elements_list.append(PageBreakElement(metadata=element_metadata))
        else: elements_list.append({"element_type": "page_break", "metadata": element_metadata})
        created_element = True
    
    processed_tags.add(tag)
    # If the tag itself wasn't a specific block element we handled, or it's a known container,
    # process its children.
    if not created_element or tag_name in CONTAINER_TAGS_HTML:
        for child in tag.children:
            if isinstance(child, Tag):
                _convert_html_tag_to_elements_recursive(child, elements_list, processed_tags, current_list_level)
            elif isinstance(child, NavigableString): # Handle loose text not in <p> etc.
                loose_text = child.strip()
                if loose_text and tag_name not in ['ul', 'ol']: # Avoid adding list item text twice
                    if _PYDANTIC_MODELS_AVAILABLE_HTML: elements_list.append(NarrativeTextElement(text=loose_text, metadata=element_metadata))
                    else: elements_list.append({"element_type": "narrative_text", "text": loose_text, "_is_loose_text": True, "metadata": element_metadata})

def _generate_linear_text_from_html_elements(elements: List[Any]) -> str:
    # This function is identical to the one in docx_parser.py, can be refactored to common_utils later.
    text_parts = []
    for el_data_any in elements:
        el_data: Dict[str, Any] = {}
        if _PYDANTIC_MODELS_AVAILABLE_HTML and hasattr(el_data_any, 'model_dump'):
            el_data = el_data_any.model_dump(exclude_none=True)
        elif isinstance(el_data_any, dict):
            el_data = el_data_any
        else: continue

        el_type = el_data.get("element_type")
        text_content = el_data.get("text", "")
        current_element_text = ""
        if el_type == "title": current_element_text = f"\n{'#' * el_data.get('level',1)} {text_content}\n"
        elif el_type == "narrative_text": current_element_text = text_content + "\n"
        elif el_type == "list_item":
            prefix = f"{el_data.get('item_number', '')}. " if el_data.get('ordered') and el_data.get('item_number') else "- "
            indent = "  " * el_data.get('level', 0)
            current_element_text = f"{indent}{prefix}{text_content}\n"
        elif el_type == "table":
            caption = el_data.get('caption','Unnamed Table')
            md_repr = el_data.get('markdown_representation')
            if md_repr: current_element_text = f"\n[Table: {caption}]\n{md_repr}\n"
        elif el_type == "code_block":
            lang = el_data.get('language', "") or ""
            code_content = el_data.get('code', "")
            current_element_text = f"\n```{lang}\n{code_content}\n```\n"
        elif el_type == "page_break": current_element_text = "\n---\n"
        if current_element_text: text_parts.append(current_element_text)
    full_text = "".join(text_parts)
    return re.sub(r'\n{3,}', '\n\n', full_text).strip()

def parse_html_to_structured_output(
    html_content_str: str, 
    original_metadata: Dict[str, Any]
) -> Optional[Union[ParsedDocumentOutput, Dict[str, Any]]]:
    logger.info(f"Attempting to parse HTML content (length: {len(html_content_str)} chars) using BeautifulSoup4")
    if not _BS4_AVAILABLE:
        logger.error("BeautifulSoup4 (bs4) is not available. HTML parsing cannot proceed.")
        return None

    elements: List[Any] = []
    try:
        # Try lxml first, then html.parser
        try:
            soup = BeautifulSoup(html_content_str, "lxml")
        except Exception: # Fallback if lxml is not installed or fails
            logger.warning("lxml parser not available or failed, falling back to html.parser for HTML.")
            soup = BeautifulSoup(html_content_str, "html.parser")

        # Attempt to find the main content area
        main_content_area = soup.find('article') or soup.find('main') or soup.body
        if not main_content_area:
            logger.warning("Could not find <article>, <main>, or <body> tag. Parsing entire document if possible.")
            main_content_area = soup # Fallback to entire soup object

        # Remove ignored tags before processing
        for ignore_tag_name in IGNORE_TAGS_HTML:
            for tag_to_remove in main_content_area.find_all(ignore_tag_name):
                tag_to_remove.decompose()
        
        processed_tags_set: Set[Tag] = set()
        _convert_html_tag_to_elements_recursive(main_content_area, elements, processed_tags_set)
        
        logger.info(f"Converted HTML to {len(elements)} custom elements.")
        
        linear_text = _generate_linear_text_from_html_elements(elements)
        logger.info(f"Generated linear text from HTML elements (length: {len(linear_text)}). Preview: {linear_text[:200]}")

        if _PYDANTIC_MODELS_AVAILABLE_HTML:
            return ParsedDocumentOutput(
                parsed_text=linear_text,
                elements=elements, # type: ignore
                original_metadata=original_metadata
            )
        else:
            return {
                "parsed_text": linear_text,
                "elements": elements,
                "original_metadata": original_metadata
            }
    except Exception as e:
        logger.error(f"Error parsing HTML content with BeautifulSoup4: {e}", exc_info=True)
        return None


--------------------------------------------------------------------------------
文件: parsers/md_parser.py
--------------------------------------------------------------------------------
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/zhz_rag_pipeline/parsers/md_parser.py
import os
from markdown_it import MarkdownIt
from markdown_it.tree import SyntaxTreeNode
import logging
from typing import List, Dict, Any, Optional, Union
import re

# --- Pydantic 模型导入和占位符定义 ---
_PARSER_PYDANTIC_AVAILABLE = False
try:
    from ..pydantic_models_dagster import (
        ParsedDocumentOutput, DocumentElementType, TitleElement, NarrativeTextElement,
        ListItemElement, TableElement, CodeBlockElement, PageBreakElement,
        DocumentElementMetadata
    )
    _PARSER_PYDANTIC_AVAILABLE = True
except ImportError:
    class BaseModel: pass
    class DocumentElementMetadata(BaseModel): page_number: Optional[int] = None
    class ParsedDocumentOutput(BaseModel): parsed_text: str; elements: list; original_metadata: dict; summary: Optional[str] = None
    class TitleElement(BaseModel): element_type:str="title"; text:str; level:int; metadata: Optional[DocumentElementMetadata] = None
    class NarrativeTextElement(BaseModel): element_type:str="narrative_text"; text:str; metadata: Optional[DocumentElementMetadata] = None
    class ListItemElement(BaseModel): element_type:str="list_item"; text:str; level:int=0; ordered:bool=False; item_number:Optional[Union[int, str]]=None; metadata: Optional[DocumentElementMetadata] = None
    class TableElement(BaseModel): element_type:str="table"; markdown_representation:Optional[str]=None; html_representation:Optional[str]=None; caption:Optional[str]=None; metadata: Optional[DocumentElementMetadata] = None
    class CodeBlockElement(BaseModel): element_type:str="code_block"; code:str; language:Optional[str]=None; metadata: Optional[DocumentElementMetadata] = None
    class PageBreakElement(BaseModel): element_type:str="page_break"; metadata: Optional[DocumentElementMetadata] = None
    DocumentElementType = Any

logger = logging.getLogger(__name__)

# --- 辅助函数 ---
def _get_node_text_content(node: SyntaxTreeNode, exclude_lists_and_tables: bool = False) -> str:
    if node.type == "text":
        return node.content
    if node.type == "softbreak":
        return " "
    if node.type == "hardbreak":
        return "\n"
    if node.type == "code_inline":
        return f"`{node.content}`"
    
    if exclude_lists_and_tables and node.type in ["bullet_list", "ordered_list", "table"]:
        return ""

    content = ""
    if node.children:
        for child in node.children:
            content += _get_node_text_content(child, exclude_lists_and_tables)
    return content

def _convert_table_node_to_markdown(table_node: SyntaxTreeNode) -> str:
    md_rows = []
    
    thead_node = next((child for child in table_node.children if child.type == 'thead'), None)
    tbody_node = next((child for child in table_node.children if child.type == 'tbody'), None)

    header_texts = []
    if thead_node:
        tr_node_header = next((child for child in thead_node.children if child.type == 'tr'), None)
        if tr_node_header:
            header_texts = [_get_node_text_content(cell).strip() for cell in tr_node_header.children if cell.type == 'th']
    elif tbody_node: 
        first_row_in_tbody = next((child for child in tbody_node.children if child.type == 'tr'), None)
        if first_row_in_tbody and all(cell.type == 'th' for cell in first_row_in_tbody.children):
             header_texts = [_get_node_text_content(cell).strip() for cell in first_row_in_tbody.children]

    if header_texts:
        md_rows.append("| " + " | ".join(header_texts) + " |")
        md_rows.append("| " + " | ".join(["---"] * len(header_texts)) + " |")

    rows_container = tbody_node if tbody_node else table_node 
    
    first_row_in_container_is_header = False
    if not header_texts and rows_container: # 只有在没有thead且容器存在时，才检查第一行是否是表头
        first_tr = next((child for child in rows_container.children if child.type == 'tr'), None)
        if first_tr and all(cell.type == 'th' for cell in first_tr.children):
            # 如果第一行全是th，作为表头处理
            header_texts_from_body = [_get_node_text_content(cell).strip() for cell in first_tr.children]
            if header_texts_from_body:
                md_rows.append("| " + " | ".join(header_texts_from_body) + " |")
                md_rows.append("| " + " | ".join(["---"] * len(header_texts_from_body)) + " |")
                first_row_in_container_is_header = True

    if rows_container: # 确保 rows_container 存在
        for row_idx, tr_node in enumerate(child for child in rows_container.children if child.type == 'tr'):
            # 如果第一行已经被作为表头处理了，则跳过它
            if first_row_in_container_is_header and row_idx == 0:
                continue
            
            # 如果已经通过 thead 处理了表头，那么 tbody/table 下的所有 tr 都应视为数据行
            # 如果没有通过 thead 处理表头，并且当前行也不是被推断为表头的 tbody 第一行，那么它也是数据行
            cell_texts = [_get_node_text_content(cell).strip() for cell in tr_node.children if cell.type == 'td']
            if cell_texts or len(tr_node.children) > 0 : 
                md_rows.append("| " + " | ".join(cell_texts) + " |")
            
    return "\n".join(md_rows)

# --- 主转换函数 ---
def _convert_md_tree_to_elements(root_node: SyntaxTreeNode) -> List[Any]: 
    elements: List[Any] = []
    
    def _process_node_recursive(node: SyntaxTreeNode, current_semantic_level: int = 0, list_ctx: Optional[Dict] = None):
        nonlocal elements
        current_metadata = None 

        node_type = node.type
        
        if node_type == "heading":
            level = int(node.tag[1:])
            text = _get_node_text_content(node).strip()
            if text or node.children: 
                if _PARSER_PYDANTIC_AVAILABLE: elements.append(TitleElement(text=text, level=level, metadata=current_metadata))
                else: elements.append({"element_type": "title", "text": text, "level": level, "metadata": current_metadata})
        
        elif node_type == "paragraph":
            text = _get_node_text_content(node).strip()
            if text:
                if _PARSER_PYDANTIC_AVAILABLE: elements.append(NarrativeTextElement(text=text, metadata=current_metadata))
                else: elements.append({"element_type": "narrative_text", "text": text, "metadata": current_metadata})

        elif node_type == "bullet_list" or node_type == "ordered_list":
            is_ordered_list = (node_type == "ordered_list")
            child_list_ctx = {
                "ordered": is_ordered_list,
                "start_num": int(node.attrs.get("start", 1)) if node.attrs and is_ordered_list else 1,
                "item_idx_in_list": 0 
            }
            for child_node in node.children:
                if child_node.type == "list_item":
                    _process_node_recursive(child_node, current_semantic_level + 1, child_list_ctx)
        
        elif node_type == "list_item":
            item_text = _get_node_text_content(node, exclude_lists_and_tables=True).strip()
            
            if item_text and list_ctx: 
                display_level = current_semantic_level - 1 
                item_number_str = None
                if list_ctx["ordered"]:
                    item_number_str = str(list_ctx["start_num"] + list_ctx["item_idx_in_list"])
                    list_ctx["item_idx_in_list"] += 1
                else: 
                    item_number_str = node.markup if node.markup else "-" 

                if _PARSER_PYDANTIC_AVAILABLE:
                    elements.append(ListItemElement(
                        text=item_text, level=display_level, 
                        ordered=list_ctx["ordered"], 
                        item_number=item_number_str, metadata=current_metadata
                    ))
                else:
                    elements.append({
                        "element_type": "list_item", "text": item_text, 
                        "level": display_level, "ordered": list_ctx["ordered"], 
                        "item_number": item_number_str, "metadata": current_metadata
                    })
            
            for child_node in node.children:
                if child_node.type in ["bullet_list", "ordered_list"]:
                    _process_node_recursive(child_node, current_semantic_level, None) # Pass current_semantic_level for nested list

        elif node_type == "table":
            md_table_representation = _convert_table_node_to_markdown(node)
            if md_table_representation:
                if _PARSER_PYDANTIC_AVAILABLE: elements.append(TableElement(markdown_representation=md_table_representation, metadata=current_metadata))
                else: elements.append({"element_type": "table", "markdown_representation": md_table_representation, "metadata": current_metadata})

        elif node_type == "fence" or node_type == "code_block":
            code_content = node.content.strip('\n') 
            lang = node.info.strip() if node.info else None
            if _PARSER_PYDANTIC_AVAILABLE: elements.append(CodeBlockElement(code=code_content, language=lang, metadata=current_metadata))
            else: elements.append({"element_type": "code_block", "code": code_content, "language": lang, "metadata": current_metadata})

        elif node_type == "hr":
            if _PARSER_PYDANTIC_AVAILABLE: elements.append(PageBreakElement(metadata=current_metadata))
            else: elements.append({"element_type": "page_break", "metadata": current_metadata})

        elif node_type == "blockquote":
            text = _get_node_text_content(node).strip()
            if text:
                if _PARSER_PYDANTIC_AVAILABLE: elements.append(NarrativeTextElement(text=text, metadata=current_metadata))
                else: elements.append({"element_type": "narrative_text", "text": text, "_is_blockquote": True, "metadata": current_metadata})
        
        elif node.children and node_type not in ["list_item", "heading", "paragraph", "table", "fence", "code_block", "blockquote", "hr", "bullet_list", "ordered_list"]: # Avoid re-processing children of already handled types
             for child in node.children:
                _process_node_recursive(child, current_semantic_level, list_ctx) # Pass context along

    _process_node_recursive(root_node) 
    return elements

def _generate_parsed_text_from_elements_internal(elements: List[Any]) -> str:
    text_parts = []
    for el_data_any in elements:
        el_data = {}
        if _PARSER_PYDANTIC_AVAILABLE and hasattr(el_data_any, 'model_dump'): el_data = el_data_any.model_dump()
        elif isinstance(el_data_any, dict): el_data = el_data_any
        else: continue
        el_type = el_data.get("element_type")
        if el_type == "title": text_parts.append(f"\n{'#' * el_data.get('level',1)} {el_data.get('text','')}\n")
        elif el_type == "narrative_text": text_parts.append(el_data.get('text','') + "\n")
        elif el_type == "list_item":
            item_num_display = str(el_data.get('item_number','-')) 
            prefix = f"{item_num_display}. " if el_data.get('ordered') else f"{item_num_display} "
            indent = "  " * el_data.get('level',0)
            text_parts.append(f"{indent}{prefix}{el_data.get('text','')}\n")
        elif el_type == "table":
            caption_text = str(el_data.get('caption')) if el_data.get('caption') is not None else 'Unnamed Table'
            if el_data.get('markdown_representation'): 
                text_parts.append(f"\n[Table: {caption_text}]\n{el_data['markdown_representation']}\n")
            elif el_data.get('text_representation'): 
                text_parts.append(f"\n[Table: {caption_text}]\n{el_data['text_representation']}\n")
        elif el_type == "code_block":
            lang = el_data.get('language', "") or ""
            text_parts.append(f"\n```{lang}\n{el_data.get('code','')}\n```\n")
        elif el_type == "page_break": text_parts.append("\n---\n")
        if el_type not in ['list_item'] and text_parts and (not text_parts[-1].endswith("\n\n") and not text_parts[-1].endswith("\n---\n\n") ) : 
             text_parts.append("\n") 
             
    raw_text = "".join(text_parts)
    cleaned_text = raw_text.strip()
    cleaned_text = cleaned_text.replace('\r\n', '\n') 
    cleaned_text = re.sub(r'\n{3,}', '\n\n', cleaned_text) 
    return cleaned_text

def parse_markdown_to_structured_output(md_content_str: str, original_metadata: Dict[str, Any]) -> Optional[Union[ParsedDocumentOutput, Dict[str, Any]]]:
    logger.info(f"Parsing Markdown content (length: {len(md_content_str)} chars) using markdown-it-py with SyntaxTreeNode...")
    try:
        md_parser = MarkdownIt("commonmark", {'linkify': True}).enable("table")
        tokens = md_parser.parse(md_content_str)
        
        root_syntax_node = SyntaxTreeNode(tokens)
        structured_elements = _convert_md_tree_to_elements(root_syntax_node) 

        linear_text = _generate_parsed_text_from_elements_internal(structured_elements)

        if _PARSER_PYDANTIC_AVAILABLE:
            return ParsedDocumentOutput(
                parsed_text=linear_text,
                elements=structured_elements, 
                original_metadata=original_metadata
            )
        else:
            return {
                "parsed_text": linear_text,
                "elements": structured_elements,
                "original_metadata": original_metadata
            }
    except Exception as e:
        logger.error(f"Error in parse_markdown_to_structured_output: {e}", exc_info=True)
        return None

# --- Placeholder for other parsers (保持不变) ---
def parse_docx_to_structured_output(file_path: str, original_metadata: Dict[str, Any]) -> Optional[Union[ParsedDocumentOutput, Dict[str, Any]]]:
    logger.info(f"DOCX parser placeholder for: {file_path}") 
    text = f"[DOCX content of {os.path.basename(file_path)}]"
    if _PARSER_PYDANTIC_AVAILABLE: return ParsedDocumentOutput(parsed_text=text, elements=[NarrativeTextElement(text=text)], original_metadata=original_metadata) 
    return {"parsed_text": text, "elements": [{"element_type":"narrative_text", "text":text}], "original_metadata": original_metadata}

def parse_pdf_to_structured_output(file_path: str, original_metadata: Dict[str, Any]) -> Optional[Union[ParsedDocumentOutput, Dict[str, Any]]]:
    logger.info(f"PDF parser placeholder for: {file_path}") 
    text = f"[PDF content of {os.path.basename(file_path)}]"
    if _PARSER_PYDANTIC_AVAILABLE: return ParsedDocumentOutput(parsed_text=text, elements=[NarrativeTextElement(text=text)], original_metadata=original_metadata) 
    return {"parsed_text": text, "elements": [{"element_type":"narrative_text", "text":text}], "original_metadata": original_metadata}

def parse_xlsx_to_structured_output(file_path: str, original_metadata: Dict[str, Any]) -> Optional[Union[ParsedDocumentOutput, Dict[str, Any]]]:
    logger.info(f"XLSX parser placeholder for: {file_path}") 
    text = f"[XLSX content of {os.path.basename(file_path)}]"
    if _PARSER_PYDANTIC_AVAILABLE: return ParsedDocumentOutput(parsed_text=text, elements=[NarrativeTextElement(text=text)], original_metadata=original_metadata) 
    return {"parsed_text": text, "elements": [{"element_type":"narrative_text", "text":text}], "original_metadata": original_metadata}
        
def parse_html_to_structured_output(html_content_str: str, original_metadata: Dict[str, Any]) -> Optional[Union[ParsedDocumentOutput, Dict[str, Any]]]:
    logger.info(f"HTML parser placeholder for content length: {len(html_content_str)}") 
    text = f"[HTML content snippet: {html_content_str[:100]}]"
    if _PARSER_PYDANTIC_AVAILABLE: return ParsedDocumentOutput(parsed_text=text, elements=[NarrativeTextElement(text=text)], original_metadata=original_metadata) 
    return {"parsed_text": text, "elements": [{"element_type":"narrative_text", "text":text}], "original_metadata": original_metadata}

def parse_txt_to_structured_output(txt_content_str: str, original_metadata: Dict[str, Any]) -> Optional[Union[ParsedDocumentOutput, Dict[str, Any]]]:
    logger.info(f"TXT parser for content length: {len(txt_content_str)}") 
    if _PARSER_PYDANTIC_AVAILABLE:
        return ParsedDocumentOutput(
            parsed_text=txt_content_str, 
            elements=[NarrativeTextElement(text=txt_content_str)], 
            original_metadata=original_metadata
        ) 
    return {
        "parsed_text": txt_content_str, 
        "elements": [{"element_type":"narrative_text", "text":txt_content_str}], 
        "original_metadata": original_metadata
    }


--------------------------------------------------------------------------------
文件: parsers/pdf_parser.py
--------------------------------------------------------------------------------
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/zhz_rag_pipeline/parsers/pdf_parser.py
import os
import logging
from typing import List, Dict, Any, Optional, Union
import re

try:
    import fitz  # PyMuPDF
    _PYMUPDF_AVAILABLE = True
    logging.info("Successfully imported PyMuPDF (fitz) for PDF parsing.")
except ImportError:
    logging.error("PyMuPDF (fitz) not found. PDF parsing will not be available.")
    _PYMUPDF_AVAILABLE = False
    # 占位符，以防 fitz 未安装时代码尝试引用它
    class fitz: 
        class Document: pass
        class Page: pass
        class Rect: pass 

_PYDANTIC_MODELS_AVAILABLE_PDF = False
try:
    from ..pydantic_models_dagster import (
        ParsedDocumentOutput, DocumentElementType, NarrativeTextElement,
        DocumentElementMetadata, PageBreakElement
    )
    _PYDANTIC_MODELS_AVAILABLE_PDF = True
except ImportError:
    class BaseModel: pass
    class DocumentElementMetadata(BaseModel): page_number: Optional[int] = None
    class ParsedDocumentOutput(BaseModel): parsed_text: str; elements: list; original_metadata: dict; summary: Optional[str] = None
    class NarrativeTextElement(BaseModel): element_type:str="narrative_text"; text:str; metadata: Optional[DocumentElementMetadata] = None
    class PageBreakElement(BaseModel): element_type:str="page_break"; metadata: Optional[DocumentElementMetadata] = None
    DocumentElementType = Any

logger = logging.getLogger(__name__)

def _create_pdf_element_metadata(page_number: Optional[int] = None) -> Optional[Union[DocumentElementMetadata, Dict[str, Any]]]:
    meta_data_dict: Dict[str, Any] = {}
    if page_number is not None:
        meta_data_dict['page_number'] = page_number
    
    if not meta_data_dict:
        return None

    if _PYDANTIC_MODELS_AVAILABLE_PDF:
        return DocumentElementMetadata(**meta_data_dict)
    else:
        return meta_data_dict

def parse_pdf_to_structured_output(
    file_path: str, 
    original_metadata: Dict[str, Any]
) -> Optional[Union[ParsedDocumentOutput, Dict[str, Any]]]:
    logger.info(f"Attempting to parse PDF file: {file_path} using PyMuPDF (fitz)")
    if not _PYMUPDF_AVAILABLE:
        logger.error("PyMuPDF (fitz) is not available. PDF parsing cannot proceed.")
        return None

    elements: List[Any] = []
    full_text_parts: List[str] = []

    try:
        doc = fitz.open(file_path)
        logger.info(f"PyMuPDF opened PDF. Pages: {doc.page_count}")

        for page_num in range(doc.page_count):
            page = doc.load_page(page_num)
            
            # 使用 "dict" 模式提取文本块，并按阅读顺序排序
            page_content_blocks = page.get_text("dict", sort=True).get("blocks", [])
            
            page_text_collected = []
            
            if page_content_blocks:
                for block in page_content_blocks:
                    if block['type'] == 0: # 0 表示文本块
                        block_text_lines = []
                        for line in block.get("lines", []):
                            line_content = "".join([span.get("text", "") for span in line.get("spans", [])])
                            block_text_lines.append(line_content)
                        block_text_content = "\n".join(block_text_lines).strip()
                        if block_text_content:
                            page_text_collected.append(block_text_content)
            
            if page_text_collected:
                # 将整页的文本作为一个 NarrativeTextElement
                page_full_text = "\n\n".join(page_text_collected) # 用双换行符分隔来自不同块的文本
                element_metadata = _create_pdf_element_metadata(page_number=page_num + 1)
                if _PYDANTIC_MODELS_AVAILABLE_PDF:
                    elements.append(NarrativeTextElement(text=page_full_text, metadata=element_metadata)) # type: ignore
                else:
                    elements.append({"element_type": "narrative_text", "text": page_full_text, "metadata": element_metadata})
                full_text_parts.append(page_full_text)
            
            # 在每页（除了最后一页）之后添加一个 PageBreakElement
            if page_num < doc.page_count - 1:
                if _PYDANTIC_MODELS_AVAILABLE_PDF:
                    elements.append(PageBreakElement(metadata=_create_pdf_element_metadata(page_number=page_num + 1))) # type: ignore
                else:
                    elements.append({"element_type": "page_break", "metadata": _create_pdf_element_metadata(page_number=page_num + 1)})


        doc.close()
        
        linear_text = "\n\n--- Page Break ---\n\n".join(full_text_parts) # 用特殊标记分隔页面文本
        linear_text = re.sub(r'\n{3,}', '\n\n', linear_text).strip()

        if _PYDANTIC_MODELS_AVAILABLE_PDF:
            return ParsedDocumentOutput(
                parsed_text=linear_text,
                elements=elements, # type: ignore
                original_metadata=original_metadata
            )
        else:
            return {
                "parsed_text": linear_text,
                "elements": elements,
                "original_metadata": original_metadata
            }

    except FileNotFoundError:
        logger.error(f"PDF file not found: {file_path}")
        return None
    except Exception as e:
        logger.error(f"Error parsing PDF file {file_path} with PyMuPDF: {e}", exc_info=True)
        return None


--------------------------------------------------------------------------------
文件: parsers/txt_parser.py
--------------------------------------------------------------------------------
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/zhz_rag_pipeline/parsers/txt_parser.py
import os
import logging
from typing import Dict, Any, Optional, Union

_PYDANTIC_MODELS_AVAILABLE_TXT = False
try:
    from ..pydantic_models_dagster import (
        ParsedDocumentOutput, NarrativeTextElement,
        DocumentElementMetadata
    )
    _PYDANTIC_MODELS_AVAILABLE_TXT = True
except ImportError:
    class BaseModel: pass
    class DocumentElementMetadata(BaseModel): page_number: Optional[int] = None # Not really applicable for txt
    class ParsedDocumentOutput(BaseModel): parsed_text: str; elements: list; original_metadata: dict; summary: Optional[str] = None
    class NarrativeTextElement(BaseModel): element_type:str="narrative_text"; text:str; metadata: Optional[DocumentElementMetadata] = None
    # DocumentElementType is not strictly needed here as we only create NarrativeTextElement

logger = logging.getLogger(__name__)

def parse_txt_to_structured_output(
    txt_content_str: str, # For .txt, we expect the content string directly
    original_metadata: Dict[str, Any]
) -> Optional[Union[ParsedDocumentOutput, Dict[str, Any]]]:
    logger.info(f"Attempting to parse TXT content (length: {len(txt_content_str)} chars)")

    # For .txt files, the entire content is treated as a single narrative text block.
    # No complex structure is assumed or extracted.
    
    elements = []
    element_metadata = None # No specific sub-element metadata for a single block txt file

    if _PYDANTIC_MODELS_AVAILABLE_TXT:
        elements.append(NarrativeTextElement(text=txt_content_str, metadata=element_metadata)) # type: ignore
        doc_output = ParsedDocumentOutput(
            parsed_text=txt_content_str,
            elements=elements, # type: ignore
            original_metadata=original_metadata
        )
    else:
        elements.append({"element_type": "narrative_text", "text": txt_content_str, "metadata": element_metadata})
        doc_output = {
            "parsed_text": txt_content_str,
            "elements": elements,
            "original_metadata": original_metadata
        }
    
    logger.info(f"Successfully processed TXT content into a single element.")
    return doc_output


--------------------------------------------------------------------------------
文件: parsers/xlsx_parser.py
--------------------------------------------------------------------------------
# /home/zhz/zhz_agent/zhz_rag_pipeline_dagster/zhz_rag_pipeline/parsers/xlsx_parser.py
import os
import logging
from typing import List, Dict, Any, Optional, Union
import pandas as pd

# 确保安装了 'pandas', 'openpyxl', 'tabulate'
# pip install pandas openpyxl tabulate

# --- Pydantic 模型导入和占位符定义 (保持不变) ---
_PYDANTIC_MODELS_AVAILABLE_XLSX = False
try:
    from ..pydantic_models_dagster import (
        ParsedDocumentOutput, TableElement, DocumentElementMetadata
    )
    _PYDANTIC_MODELS_AVAILABLE_XLSX = True
except ImportError:
    class BaseModel: pass
    class DocumentElementMetadata(BaseModel): page_number: Optional[int] = None
    class ParsedDocumentOutput(BaseModel): parsed_text: str; elements: list; original_metadata: dict; summary: Optional[str] = None
    class TableElement(BaseModel): element_type: str = "table"; markdown_representation: Optional[str] = None; html_representation: Optional[str] = None; caption: Optional[str] = None; metadata: Optional[DocumentElementMetadata] = None

logger = logging.getLogger(__name__)

# --- 辅助函数 (保持不变) ---
def _create_xlsx_element_metadata(sheet_index: int, table_index_in_sheet: int) -> Optional[Union[DocumentElementMetadata, Dict[str, Any]]]:
    meta_data_dict: Dict[str, Any] = {
        'page_number': sheet_index, # 使用 page_number 表示工作表索引
        'custom_properties': {'table_index_in_sheet': table_index_in_sheet}
    }
    if _PYDANTIC_MODELS_AVAILABLE_XLSX:
        return DocumentElementMetadata(**meta_data_dict)
    else:
        return meta_data_dict

# --- 主解析函数 (全新版本) ---
def parse_xlsx_to_structured_output(
    file_path: str,
    original_metadata: Dict[str, Any]
) -> Optional[Union[ParsedDocumentOutput, Dict[str, Any]]]:
    """
    V2: 解析XLSX文件，智能识别并提取每个工作表内的多个独立表格。
    """
    logger.info(f"Attempting to parse XLSX file with multi-table support: {file_path}")
    
    try:
        xls = pd.ExcelFile(file_path)
    except Exception as e:
        logger.error(f"Failed to open Excel file {file_path}: {e}", exc_info=True)
        return None

    all_elements: List[Any] = []
    
    for sheet_idx, sheet_name in enumerate(xls.sheet_names):
        try:
            # 读取整个工作表，不指定表头，以便我们手动查找
            df_full = pd.read_excel(xls, sheet_name=sheet_name, header=None)
            
            if df_full.empty:
                logger.info(f"Sheet '{sheet_name}' is empty. Skipping.")
                continue

            # 通过查找空行来识别表格块
            is_row_empty = df_full.isnull().all(axis=1)
            # 获取空行的索引
            empty_row_indices = is_row_empty[is_row_empty].index.tolist()
            
            table_blocks: List[pd.DataFrame] = []
            last_split_index = -1
            
            for empty_idx in empty_row_indices:
                block_df = df_full.iloc[last_split_index + 1 : empty_idx].dropna(how='all')
                if not block_df.empty:
                    table_blocks.append(block_df)
                last_split_index = empty_idx
            
            # 添加最后一个块（从最后一个空行到末尾）
            final_block_df = df_full.iloc[last_split_index + 1 :].dropna(how='all')
            if not final_block_df.empty:
                table_blocks.append(final_block_df)

            logger.info(f"Sheet '{sheet_name}' was split into {len(table_blocks)} potential table blocks.")

            for table_idx, block_df in enumerate(table_blocks):
                # 将第一行作为表头
                header = block_df.iloc[0]
                table_data = block_df[1:]
                table_data.columns = header
                
                md_representation = table_data.to_markdown(index=False)
                table_caption = f"内容来自文件 '{os.path.basename(file_path)}' 的工作表 '{sheet_name}' (表格 {table_idx + 1})"
                
                element_metadata = _create_xlsx_element_metadata(
                    sheet_index=sheet_idx, 
                    table_index_in_sheet=table_idx
                )

                if _PYDANTIC_MODELS_AVAILABLE_XLSX:
                    table_el = TableElement(
                        markdown_representation=md_representation,
                        caption=table_caption,
                        metadata=element_metadata
                    )
                    all_elements.append(table_el)
                else:
                    all_elements.append({
                        "element_type": "table",
                        "markdown_representation": md_representation,
                        "caption": table_caption,
                        "metadata": element_metadata
                    })
                logger.info(f"  Successfully created TableElement for table {table_idx+1} in sheet '{sheet_name}'.")
                logger.debug(f"    - Table {table_idx+1} content preview: {md_representation[:200].replace(chr(10), ' ')}...")


        except Exception as e_sheet:
            logger.error(f"Failed to process sheet '{sheet_name}' in {file_path}: {e_sheet}", exc_info=True)
            continue
    
    if _PYDANTIC_MODELS_AVAILABLE_XLSX:
        return ParsedDocumentOutput(
            parsed_text="",
            elements=all_elements,
            original_metadata=original_metadata,
            summary=f"从文件 '{os.path.basename(file_path)}' 中解析了 {len(all_elements)} 个独立的表格。"
        )
    else:
        return {
            "parsed_text": "",
            "elements": all_elements,
            "original_metadata": original_metadata,
            "summary": f"从文件 '{os.path.basename(file_path)}' 中解析了 {len(all_elements)} 个独立的表格。"
        }


